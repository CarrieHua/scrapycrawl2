id=26153	REOPENED	None	Apache httpd-2	Core (	2.0.48	PC All	P1 critical	Apache HTTPD Bugs Mailing List	2004-01-15 04:12 UTC by	Jeremy Bae	2005-03-20 17:06 UTC (	0 users	on cygwin environment, any files can be retrieved by malicious users.Apache 1.3.29 and 2.0.48 (source compile version) vulnerabilitycf.	Can anyone on cgywin verify this issue? This should likely goto security@ if it is a real issue!(Apache 1.3.29) has been fixed by Stipe Tolj.is not yet fixed.According to the ChangeLog, CAN-2002-0661 this was fixed in the 2.0.40 release.It is similar to CAN-2002-0661, but new bug of Apache (2.0.48 and below on Cygwin).look at the difference between CAN-2002-0661 and this (%2e).<CAN-2002-0661 attack signature><this bug attack signature>I've tested this on Apache 2.0.48 (cygwin), and it did work.I guess CAN-2002-0661 patch didn't applied to Cygwin portion.	4.0	id=32328	15	False	True	trapni	1
id=32328	REOPENED	None	Apache httpd-2	mod_rewrite (	2.5-HEAD	All All	P1 enhancement	Apache HTTPD Bugs Mailing List	2004-11-19 22:44 UTC by	Christian Parpart	2011-06-13 10:30 UTC (	3 users	This patch will add ampersand escaping to apache2 as recently posted to the dev@httpd list. example use (from URL above): RewriteMap ampescape int:ampescape RewriteRule ^/(.*)$ /index.php?title=${ampescape:$1} [L,QSA] regards, Christian Parpart.	Createdadds the ampescape functionand here the patchCreatedadds the ampescape function* adapted patch to ASF's coding style* the old patch was against 2.0.52, this patch is against HEADLooks good here, and this has been added as a default patch in Gentoo.too special, as discussed on dev (long time ago).So I'm still -1 on it.André Malo, maybe you have the time for writing the proposed more-generic extension? Because I (in my case) actually don't have it :(I have to agree with Andrés -1 - we could be adding a bunch of these to handleall sorts of special cases and that hardly makes sense.If anything, we need a way to do something like the unix tr, not a special case"hack".Could someone tell me, what the problem (?) described on that url has to do withthe patch? The "obvious" rewriterule there is just plain wrong:RewriteRule ^/(.*)\?(.*)$ /index.php?title=$1&$2 [L]RewriteRules don't match the querystring. Period. There's no known issue aboutit. The obvious rule would be:RewriteRule ^/(.*) /index.php?title=$1 [L,QSA]What am I missing?The problem with rewriting /(.*) to /index.php?title=$1 is that $1 containing &would not escaped correctly, even if the user's URL had escaped & to %26.For example, /AT%26T would be rewritten to /index.php?title=AT&T instead of/index.php?title=AT%26T - causing title to only contain 'AT' instead of theexpected 'AT&T'. I think this patch is important even though it is too special because & is aimportant character in query strings - just as / is a very important characterin path strings - it is quite possible that this case would more often withother web applications if people made more use of mod_rewrite.From the latest patch:unsigned char *copy = (char *)apr_palloc(r->pool, 3 * strlen(key) + 3);shouldn't that be char *copy = (char *)apr_palloc(r->pool, 3 * strlen(key) + 3);since your doing a cast to (char *) instead to (unsigned char *) _and_ since thefunction returns char * instead of unsigned char * as per its definition?yeah, makes sense in any way, however, there are more "unsigned" that might be eliminated then. Some (longer) time ago, httpd-dev mailinglist members recommented in writing a MORE GENERIC variant of this patch, I can't remember exactly, however, it should be done anyway in order to get something like this functionality in. (I'm still not that familar with this kinda apache API anyway :(The same problem occurs with # (%23) and is even more destructive there:RewriteRule ^/(.*) /index.php?title=$1&something=else/Foo%#23Barwill get rewritten to:/index.php?title=Foo#Bar&something=elseThe 'Bar&something=else' is interpreted as a fragment identifier (i.e. page anchor) and ignored on the server side. The proposed patch is pretty short-sighted because it only treats one symptom, not the cause.Why does mod rewrite need to unescape these characters in the first place? Special characters like & and # do not mean the same as %26 and %23 within in the context of an URL. By unescaping, this information is being lost...At the very least, this unescaping should be optional.I think you can fix most issues by just using the 'escape' RewriteMap on the substitute, but this is far from practical as it needs to be set globally for the entire server. This rules it out for hosted environments where usually the most you get is .htaccess. Is there any reason why the built-in map functions (toupper, tolower, escape, unescape) still need a very redundant RewriteMap directive?So I guess the optimal solution would either:- Allow you to turn off this automatic unescaping with a rewriterule flag (or similar) in htaccess- or Allow you to use the built-in map functions directly without requiring those redundant RewriteMap directives(In reply to)Special characters like &unescaping, thisAt the early beginning, when the internal request processing starts, apacheunescapes the URL-path once. This is not done by mod_rewrite, this happensbefore mod_rewrite is involved and I think this is also a part of the securityconcept. If you are using your rewrite rules in directory context, you have a filename (aphysical path, e.g. /var/www/abc) while the per-dir prefix is stripped (soyou're matching only against the local path 'abc' if your rules are stored in/var/www/). How would you map some unescaped URL-path to the file system?There's no way to make the unescaping process optional for a physical path indirectory context.URL-path and QueryString have different rules for encoding. The QueryString isleft untouched (by browser [except spaces] and server) while reserved andspecial chars in the URL-path must be requested hex-encoded by the client.Apache unescapes URL-path in order to process the request.A way to soften this problem would be a map function which encodes allnon-[a-zA-Z0-9/,._-] characters into their %FF hex representation as discussedabove.If you need the unescaped uri with all its consequences, use the ENVTHE_REQUEST, which contains the full untouched request string likeGET /foo%20bar?foo=bar HTTP/1.1BTW: You can also analyze $_SERVER['REQUEST_URI'] within your php script and setthe variable 'title' there. That would be another workaround for scripts (typo3is using this method).***has been marked as a duplicate of this bug. ****** This bug has been marked as a duplicate of***This PR is an enhancement request to implement a new internal map function whichstill needs to be written more-generic.	15.0	id=57795	9	True	True	bob	1
id=42558	REOPENED	None	Lenya	Miscellaneous (	2.0	All All	P1 enhancement	Lenya Developers	2007-05-31 08:43 UTC by	Markus Angst	2010-07-20 09:04 UTC (	0 users	Make it possible to use lenya.properties.xml per publication, in addition to theexisting places (Lenya, modules). The precedence order is:1. Publication2. Lenya local3. Modules4. Lenya	CreatedPatch for enhancement described belowRenaming Lenya 1.4 to 2.0CreatedPatch for enhancement described below, second versionJust tried another one. I don't know if in loadPublicationPropertiesIfNotDone- the avalon stuff is handled correctly (does the publication have to bereleased?)- there could be synchronization issues or if the ThreadSafe implementation ofPropertiesModule takes already care of thisIn initialize() the properties of all modules are collected during the Cocooncomponents initialization phase regardless of the publications referencing them.These references are probably unknown at this point, but I think this should bedocumented because it can lead to property name conflicts between modules. Thesame holds true for the publication properties. The way I implemented them apublication should not use properties of another publication, is not evenguaranteed to see them and name conflicts could occur as well (not to mentionpublication templating). I see two solutions:- modules and publications have to use name prefixes- properties have to be saved (and may only be read) together with an "owner id"(In reply to)i think we should make prefixes mandatory: for modules, it should be<module-name>.your.property. for publications, it could be<pub-id>.your.property, but i guess we should leave the choice to the admin.users may also want to overload global properties...plus we need to find out and document really well in which order properties areloaded and how to override them.(In reply to)iiuc, you're not dealing with avalon components directly (PublicationUtil doesit for you). so you should be fine.not sure about this. someone else should look at it. anyone?anyway, i think this is a very useful addition, and i'm +1 for adding it.(In reply to)No, you need to release all components that you looked up withmanager.lookup(...) AFAIR. So it is good as gold.Why do you think of synchronization issues? >(In reply to)I am as well for prefixing, additional a check/record of double named properties.The case of double properties is very common because we implement a fallbackmechanism aka ant properties files. Ant properties work the following order:1. cli (e.g. -Dmodule=a)2. build files - normally more then one (local.)build.properties. First in wins.For us it is (before the patch)a) SystemPropertiesb) local.lenya.properties.xmlc) modulesd) lenya.properties.xmlI thought I wrote some documentation about it but cannot find it.One thing that I do not like too much is the loadPublicationPropertiesIfNotDonenot been done in initialize().ATM I am unsure how to do it myself maybe Andreas has an idea. If not I will apply the patch within the next 72 hours.(In reply to)Because on should always think about synchronization issues :)But at a first glance the module doesn't change its state (or the state ofanother object) after the initialization, so this should be fine.(In reply to)That should read "one", sorry.(In reply to)(see)Now I understand the synchronization issues. The module changes state with thegetAtribute!thorsten, markus: what's the state of this issue? i think it would be nice tohave it in 2.0, since it seems to address a real user need.markus, if you've got some time: can you describe what you're using this for?then we can add some documentation once it's in.Just realized that I made a really stupid mistake: I replied to some of theabove posts on the dev mailing list instead of bugzilla. Really silly. Iapologize for this.The state of this issue is as follows:1.0The publication properties cannot be read in the initialization phase. At leastI don't know how. The approach withRequest request = contextUtility.getRequest();DocumentFactory factory = DocumentUtil.getDocumentFactory(serviceManager, request);Publication[] pubs = publicationManager.getPublications(factory);cannot be used in the initialization phase. You get "Unable to get the requestobject from the context" when you try it. Being able to callpublicationManager.getPublications() without the factory parameter could maybehelp, but this doesn't seem to be possible unless some of the recent changes ofAndreas have changed this.1.1This leads to the workaround with loadPublicationPropertiesIfNotDone, calledfrom all get* methods which circumvents the problem by lazy loading thepublication properties.1.2This in turn led to the question if this could cause synchronization problems. Idon't think calling loadPublicationPropertiesIfNotDone twice in parallel wouldbe very dangerous but I am not fit enough to judge this.2.0Jörn suggested to (re-)use aggregating fallback for properties of templatedpublications.2.1I found AggregatingFallbackSourceFactory which provides the protocol called"aggregate-fallback://". I think to be able to reuse it some of the code ofgetSource would have to be refactored. I am not able to do this myself withouthelp. If anybody with a better understanding of the big picture could have alook at it and tell me if this is the right way to go i would give it a try.(There are some similarities in code with FallbackSourceFactory.getSource, somaybe this would have to be taken into account as well.)Committed.I as well added a sample to the default pub in.There are still some issues to solve regarding the documentation (we do not haveany).(In reply to)Actually in loadXMLPropertiesFromURI we use a Source to resolve the uri meaning you can use loadXMLPropertiesFromURI(filteringProperties,"aggregate-fallback://"+PROPERTY_NAME, true);I made an attempt to do that. It works for me so far, but there is the functionPropertiesModule.getAttributeValues which i don't fully understand. I put somecode there but commented it out. Maybe somebody can review it and enlightenme... Thanks!Will draft a short doc and put it in the wiki.CreatedPatch for aggregate-fallback, requires older patch (which is already applied).I put a documentation draft on the wiki:. It is open fordiscussion. Comments and edits welcome.Any news here ? Still required ?	20.0	id=49827	12	True	False	marc-andre.marion	1
id=37770	REOPENED	None	Apache httpd-2	mod_proxy (	2.2.3	Sun Solaris	P1 blocker	Apache HTTPD Bugs Mailing List	2005-12-03 13:40 UTC by	Christophe Yayon	2016-10-09 10:57 UTC (	26 users	Hi all,I have configured httpd-2.2.0 (release) with mod_proxy and virtual hosting andwhen i stress it, even low (10 simultaneous users with Mercury Load Runner) iget some errors messages and status code 502 on the client browser.The problem occurs only when i do some load tests, when i request from singleclient i have no problem...My error.log :[Sat Dec 03 13:16:25 2005] [error] [client 10.133.2.63] proxy: error readingstatus line from remote server (null), referer:[Sat Dec 03 13:16:25 2005] [error] [client 10.133.2.63] proxy: Error readingfrom remote server returned by /xxxx.gif, referer:[Sat Dec 03 13:16:25 2005] [error] [client 10.133.2.63] proxy: error readingstatus line from remote server (null), referer:[Sat Dec 03 13:16:25 2005] [error] [client 10.133.2.63] proxy: Error readingfrom remote server returned by /polen/styles/polen.css, referer:[Sat Dec 03 13:16:26 2005] [error] [client 10.133.2.63] proxy: error readingstatus line from remote server (null), referer:[Sat Dec 03 13:16:26 2005] [error] [client 10.133.2.63] proxy: Error readingfrom remote server returned by /polen/images/droite2.gif, referer:[Sat Dec 03 13:16:28 2005] [error] [client 10.133.2.63] (9)Bad file descriptor:proxy: error reading responseMy config :<VirtualHost *>ServerName portail.polen.xxxxxxx.frCustomLog /var/log/apache21/access-portail.log combined<Location />Order deny,allowAllow from all</Location>RewriteEngine OnRewriteRule "(.*)""}" [P,L]</VirtualHost>I have tried with mod_proxy_balancer, mod_rewrite [P] (which call mod_proxy)and of course mod_proxy...The problem seems coming from mod_proxy.	Merge from Christophe's mail to the dev list():Hi all,i have just tried with the last 2.1.10 and i have the same problem :1. in error logs :[Sat Nov 26 19:29:55 2005] [error] [client 10.133.2.63] proxy: error readingstatus line from remote server (null), referer:[Sat Nov 26 19:29:55 2005] [error] [client 10.133.2.63] proxy: Error readingfrom remote server returned by /polen/images/passe.gif, referer:[Sat Nov 26 19:29:56 2005] [notice] child pid 25976 exit signal Segmentationfault (11), possible coredump in /tmp2. i used gdb to get a trace :gdb /opt/apache21/bin/httpd /tmp/core.25976GNU gdb 6.3-debianCopyright 2004 Free Software Foundation, Inc.GDB is free software, covered by the GNU General Public License, and you arewelcome to change it and/or distribute copies of it under certain conditions.Type "show copying" to see the conditions.There is absolutely no warranty for GDB. Type "show warranty" for details.This GDB was configured as "i386-linux"...Using host libthread_db library"/lib/tls/libthread_db.so.1".Core was generated by `/opt/apache21/bin/httpd -k start'.Program terminated with signal 11, Segmentation fault.warning: current_sos: Can't read pathname for load map: Input/output errorReading symbols from /lib/tls/libm.so.6...done.Loaded symbols for /lib/tls/libm.so.6Reading symbols from /opt/apache21/lib/libaprutil-1.so.0...done.Loaded symbols for /opt/apache21/lib/libaprutil-1.so.0Reading symbols from /usr/lib/libdb-4.3.so...done.Loaded symbols for /usr/lib/libdb-4.3.soReading symbols from /usr/lib/libexpat.so.1...done.Loaded symbols for /usr/lib/libexpat.so.1Reading symbols from /opt/apache21/lib/libapr-1.so.0...done.Loaded symbols for /opt/apache21/lib/libapr-1.so.0Reading symbols from /lib/tls/librt.so.1...done.Loaded symbols for /lib/tls/librt.so.1Reading symbols from /lib/tls/libcrypt.so.1...done.Loaded symbols for /lib/tls/libcrypt.so.1Reading symbols from /lib/tls/libpthread.so.0...done.Loaded symbols for /lib/tls/libpthread.so.0Reading symbols from /lib/tls/libdl.so.2...done.Loaded symbols for /lib/tls/libdl.so.2Reading symbols from /lib/tls/libc.so.6...done.Loaded symbols for /lib/tls/libc.so.6Reading symbols from /lib/ld-linux.so.2...done.Loaded symbols for /lib/ld-linux.so.2Reading symbols from /lib/tls/libnss_compat.so.2...done.Loaded symbols for /lib/tls/libnss_compat.so.2Reading symbols from /lib/tls/libnsl.so.1...done.Loaded symbols for /lib/tls/libnsl.so.1Reading symbols from /lib/tls/libnss_nis.so.2...done.Loaded symbols for /lib/tls/libnss_nis.so.2Reading symbols from /lib/tls/libnss_files.so.2...done.Loaded symbols for /lib/tls/libnss_files.so.2Reading symbols from /opt/apache21/modules/mod_authz_host.so...done.Loaded symbols for /opt/apache21/modules/mod_authz_host.soReading symbols from /opt/apache21/modules/mod_dumpio.so...done.Loaded symbols for /opt/apache21/modules/mod_dumpio.soReading symbols from /opt/apache21/modules/mod_include.so...done.Loaded symbols for /opt/apache21/modules/mod_include.soReading symbols from /opt/apache21/modules/mod_log_config.so...done.Loaded symbols for /opt/apache21/modules/mod_log_config.soReading symbols from /opt/apache21/modules/mod_logio.so...done.Loaded symbols for /opt/apache21/modules/mod_logio.soReading symbols from /opt/apache21/modules/mod_env.so...done.Loaded symbols for /opt/apache21/modules/mod_env.soReading symbols from /opt/apache21/modules/mod_expires.so...done.Loaded symbols for /opt/apache21/modules/mod_expires.soReading symbols from /opt/apache21/modules/mod_headers.so...done.Loaded symbols for /opt/apache21/modules/mod_headers.soReading symbols from /opt/apache21/modules/mod_ident.so...done.Loaded symbols for /opt/apache21/modules/mod_ident.soReading symbols from /opt/apache21/modules/mod_usertrack.so...done.Loaded symbols for /opt/apache21/modules/mod_usertrack.soReading symbols from /opt/apache21/modules/mod_unique_id.so...done.Loaded symbols for /opt/apache21/modules/mod_unique_id.soReading symbols from /opt/apache21/modules/mod_setenvif.so...done.Loaded symbols for /opt/apache21/modules/mod_setenvif.soReading symbols from /opt/apache21/modules/mod_version.so...done.Loaded symbols for /opt/apache21/modules/mod_version.soReading symbols from /opt/apache21/modules/mod_proxy.so...done.Loaded symbols for /opt/apache21/modules/mod_proxy.soReading symbols from /opt/apache21/modules/mod_proxy_connect.so...done.Loaded symbols for /opt/apache21/modules/mod_proxy_connect.soReading symbols from /opt/apache21/modules/mod_proxy_http.so...done.Loaded symbols for /opt/apache21/modules/mod_proxy_http.soReading symbols from /opt/apache21/modules/mod_proxy_balancer.so...done.Loaded symbols for /opt/apache21/modules/mod_proxy_balancer.soReading symbols from /opt/apache21/modules/mod_mime.so...done.Loaded symbols for /opt/apache21/modules/mod_mime.soReading symbols from /opt/apache21/modules/mod_status.so...done.Loaded symbols for /opt/apache21/modules/mod_status.soReading symbols from /opt/apache21/modules/mod_autoindex.so...done.Loaded symbols for /opt/apache21/modules/mod_autoindex.soReading symbols from /opt/apache21/modules/mod_asis.so...done.Loaded symbols for /opt/apache21/modules/mod_asis.soReading symbols from /opt/apache21/modules/mod_info.so...done.Loaded symbols for /opt/apache21/modules/mod_info.soReading symbols from /opt/apache21/modules/mod_vhost_alias.so...done.Loaded symbols for /opt/apache21/modules/mod_vhost_alias.soReading symbols from /opt/apache21/modules/mod_dir.so...done.Loaded symbols for /opt/apache21/modules/mod_dir.soReading symbols from /opt/apache21/modules/mod_alias.so...done.Loaded symbols for /opt/apache21/modules/mod_alias.soReading symbols from /opt/apache21/modules/mod_rewrite.so...done.Loaded symbols for /opt/apache21/modules/mod_rewrite.soReading symbols from /opt/apache21/modules/mod_cache.so...done.Loaded symbols for /opt/apache21/modules/mod_cache.soReading symbols from /opt/apache21/modules/mod_mem_cache.so...done.Loaded symbols for /opt/apache21/modules/mod_mem_cache.soReading symbols from /opt/apache21/modules/mod_ssl.so...done.Loaded symbols for /opt/apache21/modules/mod_ssl.soReading symbols from /usr/lib/i686/cmov/libssl.so.0.9.8...done.Loaded symbols for /usr/lib/i686/cmov/libssl.so.0.9.8Reading symbols from /usr/lib/i686/cmov/libcrypto.so.0.9.8...done.Loaded symbols for /usr/lib/i686/cmov/libcrypto.so.0.9.8Reading symbols from /usr/lib/libz.so.1...done.Loaded symbols for /usr/lib/libz.so.1Reading symbols from /lib/libgcc_s.so.1...done.Loaded symbols for /lib/libgcc_s.so.1Reading symbols from /lib/tls/libnss_dns.so.2...done.Loaded symbols for /lib/tls/libnss_dns.so.2Reading symbols from /lib/tls/libresolv.so.2...done.Loaded symbols for /lib/tls/libresolv.so.2#0 apr_brigade_cleanup (data=0x8398980) at buckets/apr_brigade.c:4444 apr_bucket_delete(e);(gdb) bt full#0 apr_brigade_cleanup (data=0x8398980) at buckets/apr_brigade.c:44No locals.#1 0xb7f24e7d in apr_brigade_destroy (b=0x8398980) at buckets/apr_brigade.c:53No locals.#2 0x080726f4 in ap_core_output_filter (f=0xaa083538, b=0x8398980) atcore_filters.c:876 hdtr = {headers = 0xb41c3ff8, numheaders = 134740504, trailers =0x8392910, numtrailers = 0} bytes_sent = 0 flags = 0 nbytes = 1429 nvec_trailers = 0 vec = {{iov_base = 0x838d208, iov_len = 206}, {iov_base = 0x8394a09,iov_len = 1223}, {iov_base = 0x0, iov_len = 0}, {iov_base = 0xaa083aa0, iov_len= 2852666000}, {iov_base = 0xb41c3f48, iov_len = 3083217623}, {iov_base = 0x0, iov_len = 2852666000}, {iov_base = 0xb41c3f48, iov_len = 3083217643},{iov_base = 0x8392910, iov_len = 0}, {iov_base = 0xb41c3f30, iov_len = 0},{iov_base = 0x838e938, iov_len = 0}, {iov_base = 0xb41c3f68, iov_len =134646597}, {iov_base = 0x0, iov_len = 0}, {iov_base = 0x0, iov_len = 0}, {iov_base = 0x8393960, iov_len= 137967992}, {iov_base = 0xb41c3ff8, iov_len = 134740481}, {iov_base =0x8393960, iov_len = 137967992}} flen = Variable "flen" is not available.(gdb) thread apply all bt fullThread 27 (process 25976):#0 0x00000293 in ?? ()No symbol table info available.#1 0x00000000 in ?? ()No symbol table info available.#2 0x00000000 in ?? ()No symbol table info available.#3 0x00000000 in ?? ()No symbol table info available.#4 0x00000000 in ?? ()No symbol table info available.#5 0x00000000 in ?? ()No symbol table info available.#6 0x00000000 in ?? ()No symbol table info available.#7 0x00000000 in ?? ()No symbol table info available.#8 0x00000000 in ?? ()No symbol table info available.#9 0x00000000 in ?? ()---Type <return> to continue, or q <return> to quit---(In reply to)[..cut..]Strange that the remote server is null.[..cut..]Possibly a corrupted brigade. I am a little bit astonished that the stacktraceisn't longer.So could you please do the following:1. Add the contents of .gdbinit from the root directory of the 2.1.10 tar ballto your .gdbinit (see)2. Issue the following commands:gdb /opt/apache21/bin/httpd /tmp/core.25976btframe 2dump_brigade bdump_filters f[..cut..]Please set your LogLevel also to debug and let us know the contents of the errorlog once the problem happens again.Hum, very very strange, when i have LogLevel debug, problem NEVER occur !, but ihave any other LogLevel (info, notice, warn, crit, ...) the problem occur again(but no more detailed log information than info - which i posted).I have retry 10 times, with much of stress (500 simultaneous users) but no luck... no more error message like my bug report post !SorryHi again,Here is more informations about the problem, sometimes i even get a segmentationfault (11) :I have done some differents stress tests :1. with a debug LogLevel : no problem (see my last post), i have the problemonly with any other level (warn, notice, info, crit, ...)2. with others backends servers (simple .html on an apache 2.0.54 instead of a.jsp on a Tomcat-5.5.12) : no problem. Of course, no error on Tomcat backendserver, and when i bypass reverse proxy, or when i use apache 2.0.55 (instead of2.2.0 with mod_proxy), i don't get any error.3. sometimes i get a segmentation fault and here a trace (i have used .gdbinitof httpd-2.2.0 directory) :------------------gdb /opt/apache22/bin/httpd /tmp/core.7637 GNU gdb 6.3-debianCopyright 2004 Free Software Foundation, Inc.GDB is free software, covered by the GNU General Public License, and you arewelcome to change it and/or distribute copies of it under certain conditions.Type "show copying" to see the conditions.There is absolutely no warranty for GDB. Type "show warranty" for details.This GDB was configured as "i386-linux"...Using host libthread_db library"/lib/tls/libthread_db.so.1".Core was generated by `/opt/apache22/bin/httpd -k start'.Program terminated with signal 11, Segmentation fault.warning: current_sos: Can't read pathname for load map: Input/output errorReading symbols from /lib/tls/libm.so.6...done.Loaded symbols for /lib/tls/libm.so.6Reading symbols from /opt/apache22/lib/libaprutil-1.so.0...done.Loaded symbols for /opt/apache22/lib/libaprutil-1.so.0Reading symbols from /usr/lib/libdb-4.3.so...done.Loaded symbols for /usr/lib/libdb-4.3.soReading symbols from /usr/lib/libexpat.so.1...done.Loaded symbols for /usr/lib/libexpat.so.1Reading symbols from /opt/apache22/lib/libapr-1.so.0...done.Loaded symbols for /opt/apache22/lib/libapr-1.so.0Reading symbols from /lib/tls/librt.so.1...done.Loaded symbols for /lib/tls/librt.so.1Reading symbols from /lib/tls/libcrypt.so.1...done.Loaded symbols for /lib/tls/libcrypt.so.1Reading symbols from /lib/tls/libpthread.so.0...done.Loaded symbols for /lib/tls/libpthread.so.0Reading symbols from /lib/tls/libdl.so.2...done.Loaded symbols for /lib/tls/libdl.so.2Reading symbols from /lib/tls/libc.so.6...done.Loaded symbols for /lib/tls/libc.so.6Reading symbols from /lib/ld-linux.so.2...done.Loaded symbols for /lib/ld-linux.so.2Reading symbols from /lib/tls/libnss_compat.so.2...done.Loaded symbols for /lib/tls/libnss_compat.so.2Reading symbols from /lib/tls/libnsl.so.1...done.Loaded symbols for /lib/tls/libnsl.so.1Reading symbols from /lib/tls/libnss_nis.so.2...done.Loaded symbols for /lib/tls/libnss_nis.so.2Reading symbols from /lib/tls/libnss_files.so.2...done.Loaded symbols for /lib/tls/libnss_files.so.2Reading symbols from /opt/apache22/modules/mod_authz_host.so...done.Loaded symbols for /opt/apache22/modules/mod_authz_host.soReading symbols from /opt/apache22/modules/mod_dumpio.so...done.Loaded symbols for /opt/apache22/modules/mod_dumpio.soReading symbols from /opt/apache22/modules/mod_include.so...done.Loaded symbols for /opt/apache22/modules/mod_include.soReading symbols from /opt/apache22/modules/mod_log_config.so...done.Loaded symbols for /opt/apache22/modules/mod_log_config.soReading symbols from /opt/apache22/modules/mod_logio.so...done.Loaded symbols for /opt/apache22/modules/mod_logio.soReading symbols from /opt/apache22/modules/mod_env.so...done.Loaded symbols for /opt/apache22/modules/mod_env.soReading symbols from /opt/apache22/modules/mod_expires.so...done.Loaded symbols for /opt/apache22/modules/mod_expires.soReading symbols from /opt/apache22/modules/mod_headers.so...done.Loaded symbols for /opt/apache22/modules/mod_headers.soReading symbols from /opt/apache22/modules/mod_ident.so...done.Loaded symbols for /opt/apache22/modules/mod_ident.soReading symbols from /opt/apache22/modules/mod_usertrack.so...done.Loaded symbols for /opt/apache22/modules/mod_usertrack.soReading symbols from /opt/apache22/modules/mod_unique_id.so...done.Loaded symbols for /opt/apache22/modules/mod_unique_id.soReading symbols from /opt/apache22/modules/mod_setenvif.so...done.Loaded symbols for /opt/apache22/modules/mod_setenvif.soReading symbols from /opt/apache22/modules/mod_version.so...done.Loaded symbols for /opt/apache22/modules/mod_version.soReading symbols from /opt/apache22/modules/mod_proxy.so...done.Loaded symbols for /opt/apache22/modules/mod_proxy.soReading symbols from /opt/apache22/modules/mod_proxy_connect.so...done.Loaded symbols for /opt/apache22/modules/mod_proxy_connect.soReading symbols from /opt/apache22/modules/mod_proxy_http.so...done.Loaded symbols for /opt/apache22/modules/mod_proxy_http.soReading symbols from /opt/apache22/modules/mod_proxy_balancer.so...done.Loaded symbols for /opt/apache22/modules/mod_proxy_balancer.soReading symbols from /opt/apache22/modules/mod_mime.so...done.Loaded symbols for /opt/apache22/modules/mod_mime.soReading symbols from /opt/apache22/modules/mod_status.so...done.Loaded symbols for /opt/apache22/modules/mod_status.soReading symbols from /opt/apache22/modules/mod_autoindex.so...done.Loaded symbols for /opt/apache22/modules/mod_autoindex.soReading symbols from /opt/apache22/modules/mod_asis.so...done.Loaded symbols for /opt/apache22/modules/mod_asis.soReading symbols from /opt/apache22/modules/mod_info.so...done.Loaded symbols for /opt/apache22/modules/mod_info.soReading symbols from /opt/apache22/modules/mod_vhost_alias.so...done.Loaded symbols for /opt/apache22/modules/mod_vhost_alias.soReading symbols from /opt/apache22/modules/mod_dir.so...done.Loaded symbols for /opt/apache22/modules/mod_dir.soReading symbols from /opt/apache22/modules/mod_alias.so...done.Loaded symbols for /opt/apache22/modules/mod_alias.soReading symbols from /opt/apache22/modules/mod_rewrite.so...done.Loaded symbols for /opt/apache22/modules/mod_rewrite.soReading symbols from /opt/apache22/modules/mod_cache.so...done.Loaded symbols for /opt/apache22/modules/mod_cache.soReading symbols from /opt/apache22/modules/mod_mem_cache.so...done.Loaded symbols for /opt/apache22/modules/mod_mem_cache.soReading symbols from /opt/apache22/modules/mod_ssl.so...done.Loaded symbols for /opt/apache22/modules/mod_ssl.soReading symbols from /usr/lib/i686/cmov/libssl.so.0.9.8...done.Loaded symbols for /usr/lib/i686/cmov/libssl.so.0.9.8Reading symbols from /usr/lib/i686/cmov/libcrypto.so.0.9.8...done.Loaded symbols for /usr/lib/i686/cmov/libcrypto.so.0.9.8Reading symbols from /usr/lib/libz.so.1...done.Loaded symbols for /usr/lib/libz.so.1Reading symbols from /lib/libgcc_s.so.1...done.Loaded symbols for /lib/libgcc_s.so.1Reading symbols from /lib/tls/libnss_dns.so.2...done.Loaded symbols for /lib/tls/libnss_dns.so.2Reading symbols from /lib/tls/libresolv.so.2...done.Loaded symbols for /lib/tls/libresolv.so.2#0 0xb7dc43f1 in __read_nocancel () from /lib/tls/libpthread.so.0(gdb) bt#0 0xb7dc43f1 in __read_nocancel () from /lib/tls/libpthread.so.0#1 0x08085ea7 in ap_mpm_pod_check (pod=0xfffffe00) at pod.c:54#2 0x08083f69 in child_main (child_num_arg=Variable "child_num_arg" is notavailable.) at worker.c:1233#3 0x080840d0 in make_child (s=0x80a7d88, slot=0) at worker.c:1316#4 0x0808417b in startup_children (number_to_start=2) at worker.c:1350#5 0x08084ce1 in ap_mpm_run (_pconf=0x80a60a8, plog=0x80d4160, s=0x80a7d88) atworker.c:1700#6 0x08061bef in main (argc=3, argv=0xbfe76ca4) at main.c:712(gdb) frame 2#2 0x08083f69 in child_main (child_num_arg=Variable "child_num_arg" is notavailable.) at worker.c:12331233 rv = ap_mpm_pod_check(pod);(gdb) dump_brigade bNo symbol "b" in current context.(gdb) dump_filters fNo symbol "f" in current context.--------------------------------Please do the following things:1. Apply the steps (dump_brigade and co) I had described to the old core dump(core.25976).2. Apply the command the following commands to your latest core dump(/tmp/core.7637):info threadsthread apply all btI have just tried with prefork mpm method instead of worker, and no more problem...I appears that it is worker mpm with is the origin of the problem...Should i open a new bug report ?1. sorry i have deleted it ...2. (gdb) info threads 27 process 7640 0x00000212 in ?? () 26 process 7641 0x00000212 in ?? () 25 process 7642 0x00000212 in ?? () 24 process 7643 0x00000212 in ?? () 23 process 7644 0x00000212 in ?? () 22 process 7645 0x00000212 in ?? () 21 process 7646 0x00000246 in ?? () 20 process 7647 0x00000246 in ?? () 19 process 7648 0x00000212 in ?? () 18 process 7649 0x00000212 in ?? () 17 process 7650 0x00000212 in ?? () 16 process 7651 0x00000212 in ?? () 15 process 7652 0x00000212 in ?? () 14 process 7653 0x00000212 in ?? () 13 process 7654 0x00000212 in ?? () 12 process 7655 0x00000212 in ?? () 11 process 7656 0x00000212 in ?? () 10 process 7657 0x00000246 in ?? () 9 process 7659 0x00000212 in ?? () 8 process 7660 0x00000246 in ?? () 7 process 7661 0x00010292 in ?? () 6 process 7662 0x00000212 in ?? () 5 process 7663 0x00000212 in ?? () 4 process 7664 0x00000212 in ?? () 3 process 7665 0x00000246 in ?? () 2 process 7666 0x00000293 in ?? ()* 1 process 7637 0xb7dc43f1 in __read_nocancel () from /lib/tls/libpthread.so.0(gdb) thread apply all btThread 27 (process 7640):#0 0x00000212 in ?? ()Cannot access memory at address 0x0#0 0xb7dc43f1 in __read_nocancel () from /lib/tls/libpthread.so.0see my last post, when i use mpm prefork, no more problem, it appears that itcome from worker...should i open a new bug report ?No, you do not need to create a new bug report. We keep this one open.***has been marked as a duplicate of this bug. ***OS -> All since I see this on WinXP (even the crash).Apologies for the bug spam, but I thought I should note that this isn'thappening to me when load testing. This is happening in real-world usage. Thecrashes are coming about every two hours or so. Apache 2.2.0's proxy is unusablein a production environment.Christophe said that he cannot reproduce the error with LogLevel debug. Are youable to reproduce the error with LogLevel debug? If yes, it would be veryhelpful if you could attach the error log here.[Mon Dec 12 13:49:44 2005] [debug] mod_proxy_http.c(54): proxy: HTTP:canonicalising URL //shserver2.ssl.berkeley.edu/[Mon Dec 12 13:49:44 2005] [debug] proxy_util.c(1382): [client 127.0.0.1] proxy:*: found forward proxy worker for[Mon Dec 12 13:49:44 2005] [debug] mod_proxy.c(736): Running scheme http handler(attempt 0)[Mon Dec 12 13:49:44 2005] [debug] mod_proxy_http.c(1661): proxy: HTTP: servingURL[Mon Dec 12 13:49:44 2005] [debug] proxy_util.c(1754): proxy: HTTP: has acquiredconnection for (*)[Mon Dec 12 13:49:44 2005] [debug] proxy_util.c(1811): proxy: connectingto shserver2.ssl.berkeley.edu:80[Mon Dec 12 13:49:44 2005] [debug] proxy_util.c(1911): proxy: connected / toshserver2.ssl.berkeley.edu:80[Mon Dec 12 13:49:44 2005] [debug] proxy_util.c(2005): proxy: HTTP: fam 2 socketcreated to connect to *[Mon Dec 12 13:49:44 2005] [debug] proxy_util.c(2101): proxy: HTTP: connectioncomplete to 66.28.250.122:80 (shserver2.ssl.berkeley.edu)[Mon Dec 12 13:49:44 2005] [debug] proxy_util.c(1769): proxy: (null): hasreleased connection for (*)[Mon Dec 12 13:49:44 2005] [error] [client 127.0.0.1] proxy: error readingstatus line from remote server (null)[Mon Dec 12 13:49:44 2005] [error] [client 127.0.0.1] proxy: Error reading fromremote server returned byhttpd 2.2.2 contains many proxy fixes. Could you please check if the problem isstill there with 2.2.2?We have tried(In reply to)isWe upgraded 6 of our proxy servers to 2.2.2 and are still experiencing problems.Is anyone else able to reproduce the problem in 2.2.2???Thanks.I to have seen this with 2.2.2 and also the latest cvs. Everything works ok withprefork mpm.The problem also only accours for us when we use an IIS backend device, with anapache its ok. From a tcpdump the only diff I can see between the two sessionsis that the IIS closes the connection when the keep-alive time out is reachedwith a TCP RST packet, while the apache does a Fin/Ack handshake. When IIS sendsthis RST packet the next request to the mod_proxy device thats hits the workerwith the RST connection returns the 502 error msg.Our fix (read hack) was to set the timeout to 24hours on all the backend devices.hi all,i have just tried with 2.2.2 and apache/mod_proxy_ajp+balancer as backend(mod_proxy_balancer -> mod_proxy_balancer/ajp -> tomcat ajp) with the same loadtest in my first post, and i have only status line error message when i get atimeout from backend which is normal in my case...i used apache 2.2.2 with worker mpm...we are trying another load test and production switch if it ok thursday night...i return you friday, i hope it will be ok...does anyone encounter segmentation fault with mpm worker ?We are experiencing the same problem using Apache 2.2 on Windows 2000 Server.The backend is a combination of Tomcat / JBoss and IIS. Yet the problem onlyseems to affect IIS. C:\Program Files\Apache Software Foundation\Apache2.2\bin>httpd.exe -lCompiled in modules: core.c mod_win32.c mpm_winnt.c http_core.c mod_so.cC:\Program Files\Apache Software Foundation\Apache2.2\bin>httpd.exe -vServer version: Apache/2.2.2Server built: Apr 29 2006 18:32:31This is an extract from the log file:[Tue May 16 15:55:50 2006] [notice] Parent: Received restart signal --Restarting the server.[Tue May 16 15:55:50 2006] [notice] Apache/2.2.2 (Win32) configured -- resumingnormal operations[Tue May 16 15:55:50 2006] [notice] Server built: Apr 29 2006 18:32:31[Tue May 16 15:55:50 2006] [notice] Child 2624: Exit event signaled. Childprocess is ending.[Tue May 16 15:55:50 2006] [notice] Parent: Created child process 1864[Tue May 16 15:55:50 2006] [notice] Child 1864: Child process is running[Tue May 16 15:55:51 2006] [notice] Child 2624: Released the start mutex[Tue May 16 15:55:51 2006] [notice] Child 1864: Acquired the start mutex.[Tue May 16 15:55:51 2006] [notice] Child 1864: Starting 250 worker threads.[Tue May 16 15:55:51 2006] [notice] Child 1864: Starting thread to listen onport 8080.[Tue May 16 15:55:52 2006] [notice] Child 2624: Waiting for 250 worker threadsto exit.[Tue May 16 15:55:52 2006] [notice] Child 2624: All worker threads have exited.[Tue May 16 15:55:52 2006] [notice] Child 2624: Child process is exiting[Tue May 16 16:54:10 2006] [error] [client 81.187.164.45] proxy: error readingstatus line from remote server 192.168.99.88[Tue May 16 16:54:10 2006] [error] [client 81.187.164.45] proxy: Error readingfrom remote server returned by /support/[Tue May 16 18:59:11 2006] [error] [client 81.29.74.130] proxy: error readingstatus line from remote server 192.168.99.88[Tue May 16 18:59:11 2006] [error] [client 81.29.74.130] proxy: Error readingfrom remote server returned by /support/huns/post.asp[Tue May 16 19:48:42 2006] [error] [client 207.46.98.44] proxy: error readingstatus line from remote server 192.168.99.88[Tue May 16 19:48:42 2006] [error] [client 207.46.98.44] proxy: Error readingfrom remote server returned by /robots.txt[Tue May 16 19:59:10 2006] [error] [client 81.29.74.130] proxy: error readingstatus line from remote server 192.168.99.88[Tue May 16 19:59:10 2006] [error] [client 81.29.74.130] proxy: Error readingfrom remote server returned by /support/huns/post.asp[Tue May 16 20:56:43 2006] [error] [client 80.176.89.29] proxy: error readingstatus line from remote server 192.168.99.88[Tue May 16 20:56:43 2006] [error] [client 80.176.89.29] proxy: Error readingfrom remote server returned by /support/helpdesk/viewrequest.asp[Tue May 16 20:58:22 2006] [error] [client 207.46.98.44] proxy: error readingstatus line from remote server 192.168.99.88[Tue May 16 20:58:22 2006] [error] [client 207.46.98.44] proxy: Error readingfrom remote server returned by /default.asp[Tue May 16 21:59:11 2006] [error] [client 81.29.74.130] proxy: error readingstatus line from remote server 192.168.99.88[Tue May 16 21:59:11 2006] [error] [client 81.29.74.130] proxy: Error readingfrom remote server returned by /support/huns/post.asp[Tue May 16 22:59:10 2006] [error] [client 81.29.74.130] proxy: error readingstatus line from remote server 192.168.99.88[Tue May 16 22:59:10 2006] [error] [client 81.29.74.130] proxy: Error readingfrom remote server returned by /support/huns/post.asp[Tue May 16 23:59:10 2006] [error] [client 81.29.74.130] proxy: error readingstatus line from remote server 192.168.99.88[Tue May 16 23:59:10 2006] [error] [client 81.29.74.130] proxy: Error readingfrom remote server returned by /support/huns/post.asp[Wed May 17 00:59:11 2006] [error] [client 81.29.74.130] proxy: error readingstatus line from remote server 192.168.99.88[Wed May 17 00:59:11 2006] [error] [client 81.29.74.130] proxy: Error readingfrom remote server returned by /support/huns/post.asp[Wed May 17 01:33:59 2006] [error] [client 207.46.98.44] proxy: error readingstatus line from remote server 192.168.99.88[Wed May 17 01:33:59 2006] [error] [client 207.46.98.44] proxy: Error readingfrom remote server returned by /robots.txt[Wed May 17 01:59:11 2006] [error] [client 81.29.74.130] proxy: error readingstatus line from remote server 192.168.99.88[Wed May 17 01:59:11 2006] [error] [client 81.29.74.130] proxy: Error readingfrom remote server returned by /support/huns/post.asp[Wed May 17 02:11:08 2006] [error] [client 68.142.212.239] proxy: error readingstatus line from remote server 192.168.99.88[Wed May 17 02:11:08 2006] [error] [client 68.142.212.239] proxy: Error readingfrom remote server returned by /robots.txt[Wed May 17 02:59:08 2006] [error] [client 68.142.212.239] proxy: error readingstatus line from remote server 192.168.99.88[Wed May 17 02:59:08 2006] [error] [client 68.142.212.239] proxy: Error readingfrom remote server returned by /img/front/fnt_utilities.jpg[Wed May 17 02:59:11 2006] [error] [client 81.29.74.130] proxy: error readingstatus line from remote server 192.168.99.88[Wed May 17 02:59:11 2006] [error] [client 81.29.74.130] proxy: Error readingfrom remote server returned by /support/huns/post.asp[Wed May 17 03:59:11 2006] [error] [client 81.29.74.130] proxy: error readingstatus line from remote server 192.168.99.88[Wed May 17 03:59:11 2006] [error] [client 81.29.74.130] proxy: Error readingfrom remote server returned by /support/huns/post.asp[Wed May 17 04:48:46 2006] [warn] (OS 64)The specified network name is no longeravailable. : winnt_accept: Asynchronous AcceptEx failed.[Wed May 17 04:59:11 2006] [error] [client 81.29.74.130] proxy: error readingstatus line from remote server 192.168.99.88[Wed May 17 04:59:11 2006] [error] [client 81.29.74.130] proxy: Error readingfrom remote server returned by /support/huns/post.asp[Wed May 17 05:59:10 2006] [error] [client 81.29.74.130] proxy: error readingstatus line from remote server 192.168.99.88[Wed May 17 05:59:10 2006] [error] [client 81.29.74.130] proxy: Error readingfrom remote server returned by /support/huns/post.asp[Wed May 17 06:59:10 2006] [error] [client 81.29.74.130] proxy: error readingstatus line from remote server 192.168.99.88[Wed May 17 06:59:10 2006] [error] [client 81.29.74.130] proxy: Error readingfrom remote server returned by /support/huns/post.asp[Wed May 17 07:59:11 2006] [error] [client 81.29.74.130] proxy: error readingstatus line from remote server 192.168.99.88[Wed May 17 07:59:11 2006] [error] [client 81.29.74.130] proxy: Error readingfrom remote server returned by /support/huns/post.asp[Wed May 17 08:29:25 2006] [error] [client 81.187.164.45] proxy: error readingstatus line from remote server 192.168.99.88[Wed May 17 08:29:25 2006] [error] [client 81.187.164.45] proxy: Error readingfrom remote server returned by /support/helpdesk/viewrequest.asp[Wed May 17 09:01:38 2006] [notice] Parent: Received restart signal --Restarting the server.[Wed May 17 09:01:38 2006] [notice] Apache/2.2.2 (Win32) configured -- resumingnormal operationsWe installed 2.2.2 and turned debugging on but it does not appear to give us any further information.Here is a portion of the error log that contains the errors:[Fri May 19 04:02:28 2006] [error] [client 69.60.122.139] proxy: error reading status line from remote server 192.198.54.130[Fri May 19 04:02:28 2006] [error] [client 69.60.122.139] proxy: Error reading from remote server returned by /community/ruralmeded/model.htm[Fri May 19 05:02:27 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/calm.htm[Fri May 19 05:02:27 2006] [debug] proxy_util.c(1378): [client 72.30.133.212] proxy: http: found workerfor[Fri May 19 05:02:27 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 05:02:27 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 05:02:27 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/calm.htm to 192.198.54.130:80[Fri May 19 05:23:12 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/security.htm[Fri May 19 05:23:12 2006] [debug] proxy_util.c(1378): [client 72.30.101.219] proxy: http: found workerfor[Fri May 19 05:23:12 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 05:23:12 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 05:23:12 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/security.htm to 192.198.54.130:80[Fri May 19 05:59:07 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/[Fri May 19 05:59:07 2006] [debug] proxy_util.c(1378): [client 24.96.199.46] proxy: http: found workerfor, referer:[Fri May 19 05:59:07 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 05:59:07 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 05:59:07 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/ to 192.198.54.130:80[Fri May 19 06:06:19 2006] [debug] proxy_util.c(1378): [client 65.214.39.180] proxy: http: found workerfor, referer:[Fri May 19 06:06:19 2006] [debug] proxy_util.c(1378): [client 65.214.39.180] proxy: http: found workerfor, referer:[Fri May 19 06:37:19 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/nphl.htm[Fri May 19 06:37:19 2006] [debug] proxy_util.c(1378): [client 66.249.72.77] proxy: http: found workerfor[Fri May 19 06:37:19 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:37:19 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:37:19 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/nphl.htm to 192.198.54.130:80[Fri May 19 06:47:07 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/rural/mederrors/faq.htm[Fri May 19 06:47:07 2006] [debug] proxy_util.c(1378): [client 68.142.249.93] proxy: http: found workerfor[Fri May 19 06:47:07 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:47:07 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:47:07 2006] [debug] proxy_util.c(1951): proxy: connected /rural/mederrors/faq.htm to 192.198.54.130:80[Fri May 19 06:48:37 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/rural/mederrors/[Fri May 19 06:48:37 2006] [debug] proxy_util.c(1378): [client 66.249.72.77] proxy: http: found workerfor[Fri May 19 06:48:37 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:48:37 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:48:37 2006] [debug] proxy_util.c(1951): proxy: connected /rural/mederrors/ to 192.198.54.130:80[Fri May 19 06:49:29 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/nphl.htm[Fri May 19 06:49:29 2006] [debug] proxy_util.c(1378): [client 24.96.199.46] proxy: http: found workerfor, referer:[Fri May 19 06:49:29 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:49:29 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:49:29 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/nphl.htm to 192.198.54.130:80[Fri May 19 06:49:32 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/sambol.htm[Fri May 19 06:49:32 2006] [debug] proxy_util.c(1378): [client 24.96.199.46] proxy: http: found workerfor, referer:[Fri May 19 06:49:32 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:49:32 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:49:32 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/sambol.htm to 192.198.54.130:80[Fri May 19 06:49:42 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/inthenews.htm[Fri May 19 06:49:42 2006] [debug] proxy_util.c(1378): [client 24.96.199.46] proxy: http: found workerfor, referer:[Fri May 19 06:49:42 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:49:42 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:49:42 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/inthenews.htm to 192.198.54.130:80[Fri May 19 06:52:30 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/sambol.htm[Fri May 19 06:52:30 2006] [debug] proxy_util.c(1378): [client 66.249.72.77] proxy: http: found workerfor[Fri May 19 06:52:30 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:52:30 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:52:30 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/sambol.htm to 192.198.54.130:80[Fri May 19 06:52:33 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/newsarchive.htm[Fri May 19 06:52:33 2006] [debug] proxy_util.c(1378): [client 66.249.72.77] proxy: http: found workerfor[Fri May 19 06:52:33 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:52:33 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:52:33 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/newsarchive.htm to 192.198.54.130:80[Fri May 19 06:59:06 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/rural/mederrors/partners.htm[Fri May 19 06:59:06 2006] [debug] proxy_util.c(1378): [client 66.249.72.77] proxy: http: found workerfor[Fri May 19 06:59:06 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:59:06 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:59:06 2006] [debug] proxy_util.c(1951): proxy: connected /rural/mederrors/partners.htm to 192.198.54.130:80[Fri May 19 07:51:38 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/Community/ruralmeded/admissions/admissions_and_medical_errors.htm[Fri May 19 07:51:38 2006] [debug] proxy_util.c(1378): [client 66.249.72.77] proxy: http: found workerforors.htm[Fri May 19 07:51:38 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URLors.htm[Fri May 19 07:51:38 2006] [debug] proxy_util.c(1858): proxy: connectingors.htm to 192.198.54.130:80[Fri May 19 07:51:38 2006] [debug] proxy_util.c(1951): proxy: connected /Community/ruralmeded/admissions/admissions_and_medical_errors.htm to 192.198.54.130:80[Fri May 19 08:00:19 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/coping.htm[Fri May 19 08:00:19 2006] [debug] proxy_util.c(1378): [client 65.38.102.153] proxy: http: found workerfor[Fri May 19 08:00:19 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:00:19 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:00:19 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/coping.htm to 192.198.54.130:80[Fri May 19 08:00:19 2006] [debug] proxy_util.c(1378): [client 65.38.102.153] proxy: http: found workerfor, referer:[Fri May 19 08:14:21 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/calm.htm[Fri May 19 08:14:21 2006] [debug] proxy_util.c(1378): [client 65.214.44.82] proxy: http: found workerfor[Fri May 19 08:14:21 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:14:21 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:14:21 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/calm.htm to 192.198.54.130:80[Fri May 19 08:20:15 2006] [debug] proxy_util.c(1378): [client 69.86.74.229] proxy: http: found workerfor, referer:[Fri May 19 08:20:15 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/system.htm[Fri May 19 08:20:15 2006] [debug] proxy_util.c(1378): [client 69.86.74.229] proxy: http: found workerfor, referer:[Fri May 19 08:20:15 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:20:15 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:20:15 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/system.htm to 192.198.54.130:80[Fri May 19 08:20:16 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/newsarchive.htm[Fri May 19 08:20:16 2006] [debug] proxy_util.c(1378): [client 69.86.74.229] proxy: http: found workerfor, referer:[Fri May 19 08:20:16 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:20:16 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:20:16 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/newsarchive.htm to 192.198.54.130:80[Fri May 19 08:27:19 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/[Fri May 19 08:27:19 2006] [debug] proxy_util.c(1378): [client 64.34.145.194] proxy: http: found workerfor[Fri May 19 08:27:19 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:27:19 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:27:19 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/ to 192.198.54.130:80[Fri May 19 08:44:53 2006] [debug] proxy_util.c(1378): [client 204.248.22.161] proxy: http: found workerfor, referer:[Fri May 19 08:50:25 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/rural/mederrors/[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1378): [client 170.163.32.2] proxy: http: found workerfor, referer:?imgurl=-graphics.jpg&imgrefurl=nid=vKuIcgUlFtYkKM:&tbnh=86&tbnw=100&hl=en&start=19&prev=/images%3Fq%3Dmedication%2Berrors%26svnum%3D10%26hl%3Den%26lr%3D[Fri May 19 08:50:25 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1951): proxy: connected /rural/mederrors/ to 192.198.54.130:80[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1378): [client 170.163.32.2] proxy: http: found workerfor, referer:[Fri May 19 08:50:25 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/rural/mederrors/images/logo-partners.jpg[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1378): [client 170.163.32.2] proxy: http: found workerfor, referer:[Fri May 19 08:50:25 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1951): proxy: connected /rural/mederrors/images/logo-partners.jpg to 192.198.54.130:80[Fri May 19 08:50:25 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/rural/mederrors/images/logo-contact.jpg[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1378): [client 170.163.32.2] proxy: http: found workerfor, referer:[Fri May 19 08:50:25 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1951): proxy: connected /rural/mederrors/images/logo-contact.jpg to 192.198.54.130:80[Fri May 19 08:50:26 2006] [debug] proxy_util.c(1378): [client 170.163.32.2] proxy: http: found workerfor, referer:[Fri May 19 08:54:32 2006] [error] [client 24.252.13.39] request failed: error reading the headers[Fri May 19 09:16:43 2006] [debug] proxy_util.c(1378): [client 137.197.145.122] proxy: http: found workerfor, referer:[Fri May 19 09:16:43 2006] [debug] proxy_util.c(1378): [client 137.197.145.122] proxy: http: found workerfor, referer:[Fri May 19 09:32:06 2006] [debug] proxy_util.c(1378): [client 138.163.0.38] proxy: http: found workerfor, referer:[Fri May 19 09:32:19 2006] [debug] proxy_util.c(1378): [client 138.163.0.44] proxy: http: found workerfor, referer:We installed 2.2.2 and turned debugging on but it does not appear to give us any further information.Here is a portion of the error log that contains the errors:[Fri May 19 04:02:28 2006] [error] [client 69.60.122.139] proxy: error reading status line from remote server 192.198.54.130[Fri May 19 04:02:28 2006] [error] [client 69.60.122.139] proxy: Error reading from remote server returned by /community/ruralmeded/model.htm[Fri May 19 05:02:27 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/calm.htm[Fri May 19 05:02:27 2006] [debug] proxy_util.c(1378): [client 72.30.133.212] proxy: http: found workerfor[Fri May 19 05:02:27 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 05:02:27 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 05:02:27 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/calm.htm to 192.198.54.130:80[Fri May 19 05:23:12 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/security.htm[Fri May 19 05:23:12 2006] [debug] proxy_util.c(1378): [client 72.30.101.219] proxy: http: found workerfor[Fri May 19 05:23:12 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 05:23:12 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 05:23:12 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/security.htm to 192.198.54.130:80[Fri May 19 05:59:07 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/[Fri May 19 05:59:07 2006] [debug] proxy_util.c(1378): [client 24.96.199.46] proxy: http: found workerfor, referer:[Fri May 19 05:59:07 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 05:59:07 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 05:59:07 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/ to 192.198.54.130:80[Fri May 19 06:06:19 2006] [debug] proxy_util.c(1378): [client 65.214.39.180] proxy: http: found workerfor, referer:[Fri May 19 06:06:19 2006] [debug] proxy_util.c(1378): [client 65.214.39.180] proxy: http: found workerfor, referer:[Fri May 19 06:37:19 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/nphl.htm[Fri May 19 06:37:19 2006] [debug] proxy_util.c(1378): [client 66.249.72.77] proxy: http: found workerfor[Fri May 19 06:37:19 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:37:19 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:37:19 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/nphl.htm to 192.198.54.130:80[Fri May 19 06:47:07 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/rural/mederrors/faq.htm[Fri May 19 06:47:07 2006] [debug] proxy_util.c(1378): [client 68.142.249.93] proxy: http: found workerfor[Fri May 19 06:47:07 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:47:07 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:47:07 2006] [debug] proxy_util.c(1951): proxy: connected /rural/mederrors/faq.htm to 192.198.54.130:80[Fri May 19 06:48:37 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/rural/mederrors/[Fri May 19 06:48:37 2006] [debug] proxy_util.c(1378): [client 66.249.72.77] proxy: http: found workerfor[Fri May 19 06:48:37 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:48:37 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:48:37 2006] [debug] proxy_util.c(1951): proxy: connected /rural/mederrors/ to 192.198.54.130:80[Fri May 19 06:49:29 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/nphl.htm[Fri May 19 06:49:29 2006] [debug] proxy_util.c(1378): [client 24.96.199.46] proxy: http: found workerfor, referer:[Fri May 19 06:49:29 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:49:29 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:49:29 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/nphl.htm to 192.198.54.130:80[Fri May 19 06:49:32 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/sambol.htm[Fri May 19 06:49:32 2006] [debug] proxy_util.c(1378): [client 24.96.199.46] proxy: http: found workerfor, referer:[Fri May 19 06:49:32 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:49:32 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:49:32 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/sambol.htm to 192.198.54.130:80[Fri May 19 06:49:42 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/inthenews.htm[Fri May 19 06:49:42 2006] [debug] proxy_util.c(1378): [client 24.96.199.46] proxy: http: found workerfor, referer:[Fri May 19 06:49:42 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:49:42 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:49:42 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/inthenews.htm to 192.198.54.130:80[Fri May 19 06:52:30 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/sambol.htm[Fri May 19 06:52:30 2006] [debug] proxy_util.c(1378): [client 66.249.72.77] proxy: http: found workerfor[Fri May 19 06:52:30 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:52:30 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:52:30 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/sambol.htm to 192.198.54.130:80[Fri May 19 06:52:33 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/newsarchive.htm[Fri May 19 06:52:33 2006] [debug] proxy_util.c(1378): [client 66.249.72.77] proxy: http: found workerfor[Fri May 19 06:52:33 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:52:33 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:52:33 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/newsarchive.htm to 192.198.54.130:80[Fri May 19 06:59:06 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/rural/mederrors/partners.htm[Fri May 19 06:59:06 2006] [debug] proxy_util.c(1378): [client 66.249.72.77] proxy: http: found workerfor[Fri May 19 06:59:06 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 06:59:06 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 06:59:06 2006] [debug] proxy_util.c(1951): proxy: connected /rural/mederrors/partners.htm to 192.198.54.130:80[Fri May 19 07:51:38 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/Community/ruralmeded/admissions/admissions_and_medical_errors.htm[Fri May 19 07:51:38 2006] [debug] proxy_util.c(1378): [client 66.249.72.77] proxy: http: found workerforors.htm[Fri May 19 07:51:38 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URLors.htm[Fri May 19 07:51:38 2006] [debug] proxy_util.c(1858): proxy: connectingors.htm to 192.198.54.130:80[Fri May 19 07:51:38 2006] [debug] proxy_util.c(1951): proxy: connected /Community/ruralmeded/admissions/admissions_and_medical_errors.htm to 192.198.54.130:80[Fri May 19 08:00:19 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/coping.htm[Fri May 19 08:00:19 2006] [debug] proxy_util.c(1378): [client 65.38.102.153] proxy: http: found workerfor[Fri May 19 08:00:19 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:00:19 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:00:19 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/coping.htm to 192.198.54.130:80[Fri May 19 08:00:19 2006] [debug] proxy_util.c(1378): [client 65.38.102.153] proxy: http: found workerfor, referer:[Fri May 19 08:14:21 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/calm.htm[Fri May 19 08:14:21 2006] [debug] proxy_util.c(1378): [client 65.214.44.82] proxy: http: found workerfor[Fri May 19 08:14:21 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:14:21 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:14:21 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/calm.htm to 192.198.54.130:80[Fri May 19 08:20:15 2006] [debug] proxy_util.c(1378): [client 69.86.74.229] proxy: http: found workerfor, referer:[Fri May 19 08:20:15 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/system.htm[Fri May 19 08:20:15 2006] [debug] proxy_util.c(1378): [client 69.86.74.229] proxy: http: found workerfor, referer:[Fri May 19 08:20:15 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:20:15 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:20:15 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/system.htm to 192.198.54.130:80[Fri May 19 08:20:16 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/newsarchive.htm[Fri May 19 08:20:16 2006] [debug] proxy_util.c(1378): [client 69.86.74.229] proxy: http: found workerfor, referer:[Fri May 19 08:20:16 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:20:16 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:20:16 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/newsarchive.htm to 192.198.54.130:80[Fri May 19 08:27:19 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/bioterrorism/[Fri May 19 08:27:19 2006] [debug] proxy_util.c(1378): [client 64.34.145.194] proxy: http: found workerfor[Fri May 19 08:27:19 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:27:19 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:27:19 2006] [debug] proxy_util.c(1951): proxy: connected /bioterrorism/ to 192.198.54.130:80[Fri May 19 08:44:53 2006] [debug] proxy_util.c(1378): [client 204.248.22.161] proxy: http: found workerfor, referer:[Fri May 19 08:50:25 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/rural/mederrors/[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1378): [client 170.163.32.2] proxy: http: found workerfor, referer:?imgurl=-graphics.jpg&imgrefurl=nid=vKuIcgUlFtYkKM:&tbnh=86&tbnw=100&hl=en&start=19&prev=/images%3Fq%3Dmedication%2Berrors%26svnum%3D10%26hl%3Den%26lr%3D[Fri May 19 08:50:25 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1951): proxy: connected /rural/mederrors/ to 192.198.54.130:80[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1378): [client 170.163.32.2] proxy: http: found workerfor, referer:[Fri May 19 08:50:25 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/rural/mederrors/images/logo-partners.jpg[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1378): [client 170.163.32.2] proxy: http: found workerfor, referer:[Fri May 19 08:50:25 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1951): proxy: connected /rural/mederrors/images/logo-partners.jpg to 192.198.54.130:80[Fri May 19 08:50:25 2006] [debug] mod_proxy_http.c(54): proxy: HTTP: canonicalising URL //192.198.54.130/rural/mederrors/images/logo-contact.jpg[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1378): [client 170.163.32.2] proxy: http: found workerfor, referer:[Fri May 19 08:50:25 2006] [debug] mod_proxy_http.c(1662): proxy: HTTP: serving URL[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1858): proxy: connectingto 192.198.54.130:80[Fri May 19 08:50:25 2006] [debug] proxy_util.c(1951): proxy: connected /rural/mederrors/images/logo-contact.jpg to 192.198.54.130:80[Fri May 19 08:50:26 2006] [debug] proxy_util.c(1378): [client 170.163.32.2] proxy: http: found workerfor, referer:[Fri May 19 08:54:32 2006] [error] [client 24.252.13.39] request failed: error reading the headers[Fri May 19 09:16:43 2006] [debug] proxy_util.c(1378): [client 137.197.145.122] proxy: http: found workerfor, referer:[Fri May 19 09:16:43 2006] [debug] proxy_util.c(1378): [client 137.197.145.122] proxy: http: found workerfor, referer:[Fri May 19 09:32:06 2006] [debug] proxy_util.c(1378): [client 138.163.0.38] proxy: http: found workerfor, referer:[Fri May 19 09:32:19 2006] [debug] proxy_util.c(1378): [client 138.163.0.44] proxy: http: found workerfor, referer:oops...sorry, didn't mean to post twice.Does anybody still observe segfaults with 2.2.2 or is it "only" the "error readingstatus line" that is left?not me...nope... no segfaults.Same problem, here :Configuration : Apache 2.2.2 (with worker MPM) on Solaris 9 proxying to an IIS 6.0 back-end server.No core dump produced.As I understand, truss (see below) shows that the process uses a file (25, in the example below) handle to communicate with the back-end server but does not check whether it is still usable or not.This file handle seems to be used from the connection pool ("As of Apache 2.1, the ability to use pooled connections to a backend server is available").Result from truss :3313/19: 3.9623 read(25, 0xFDA7B997, 1) Err#131 ECONNRESET3313/19: 3.9624 write(11, 0xFDA79938, 126) = 1263313/19: [ M o n J u n 2 6 1 4 : 3 7 : 3 6 2 0 0 6 ] [ d e b u3313/19: g ] p r o x y _ u t i l . c ( 2 1 4 1 ) : p r o x y : H T3313/19: T P : c o n n e c t i o n c o m p l e t e t o 1 3 6 . 13313/19: 7 3 . 2 1 . 2 4 3 : 8 8 8 8 ( e i c i t e s t v s 0 9 )\n3313/19: 3.9628 writev(25, 0xFDA7B780, 3) Err#32 EPIPE3313/19: iov_base = 0x003F2278 iov_len = 10573313/19: P O S T / E i c i - A D M / A u t h e n t i c a t i o n / u s...3313/1: 3.9628 Received signal #13, SIGPIPE, in read() [ignored]3313/19: 3.9635 write(11, 0xFDA795B8, 123) = 1233313/19: [ M o n J u n 2 6 1 4 : 3 7 : 3 6 2 0 0 6 ] [ i n f o3313/19: ] [ c l i e n t 1 3 6 . 1 7 3 . 2 1 . 2 4 3 ] ( 3 2 ) B r3313/19: o k e n p i p e : c o r e _ o u t p u t _ f i l t e r : w3313/19: r i t i n g d a t a t o t h e n e t w o r k\n3313/19: 3.9639 read(25, 0x003D6208, 8000) = 03313/19: 3.9641 write(11, 0xFDA77910, 158) = 1583313/19: [ M o n J u n 2 6 1 4 : 3 7 : 3 6 2 0 0 6 ] [ e r r o3313/19: r ] [ c l i e n t 1 3 6 . 1 7 3 . 2 2 . 1 3 7 ] p r o x y3313/19: : e r r o r r e a d i n g s t a t u s l i n e f r o m3313/19: r e m o t e s e r v e r e i c i t e s t v s 0 9 , r e f3313/19: e r e r : h t t p : / / e i c i a p p q u a / G E D A /\n3313/19: 3.9656 write(11, 0xFDA77888, 186) = 1863313/19: [ M o n J u n 2 6 1 4 : 3 7 : 3 6 2 0 0 6 ] [ e r r o3313/19: r ] [ c l i e n t 1 3 6 . 1 7 3 . 2 2 . 1 3 7 ] p r o x y3313/19: : E r r o r r e a d i n g f r o m r e m o t e s e r v3313/19: e r r e t u r n e d b y / E i c i - A D M / A u t h e n t3313/19: i c a t i o n / u s e r i n f o s e c . a s p , r e f e r e r3313/19: : h t t p : / / e i c i a p p q u a / G E D A /\n3313/19: 3.9658 write(11, 0xFDA798D0, 111) = 1113313/19: [ M o n J u n 2 6 1 4 : 3 7 : 3 6 2 0 0 6 ] [ d e b u3313/19: g ] p r o x y _ u t i l . c ( 1 8 1 6 ) : p r o x y : H T3313/19: T P : h a s r e l e a s e d c o n n e c t i o n f o r3313/19: ( e i c i t e s t v s 0 9 )\n3313/19: 3.9661 close(25) = 03313/19: 3.9664 read(24, 0x003EE268, 8000) Err#11 EAGAIN3313/19: 3.9666 writev(24, 0xFDA7BBF0, 2) = 7293313/19: iov_base = 0x003F4280 iov_len = 1883313/19: H T T P / 1 . 1 5 0 2 P r o x y E r r o r\r\n D a t e :(In reply to)readingSame problem here:No sefaults, but error reading status lineConfig: Solaris 8, MPM Worker, HTTPd 2.2.2 mod_proxy to IIS 6 on WindowsNo error with MPM Prefork and HTTPd 2.0.54With the following 'Murxaround' it's working now:- Added 'keeplive Off' in httpd.conf -> No effect- Set keep-alive timeout to 1 day in IIS -> No effect- Added 'SetEnv force-proxy-request-1.0 1' 'SetEnv proxy-nokeepalive 1' in httpd.confIt's working now without errors since 30 hours...Don't know if first steps were necessary, I let them in placeAs workaround, I made the following modification in modules/proxy/proxy_util.c (from source files) and changed line 1972 :BEFORE : if (APR_STATUS_IS_EOF(socket_status))AFTER : if (APR_STATUS_IS_EOF(socket_status) || APR_STATUS_IS_ECONNRESET(socket_status))configuremakemake installRestart ApacheNo more message "proxy: error reading status line from remote server"Hope this helps...(In reply to)That works perfectly. Apache no longer reports and error, when IIS uses a RSTpacket to close an idle back end connection, and the request is serviced correcly.Many thanksJamesThat fix (Olivier BOEL,) looks right to me, and works for two of you. I'm applying it to /trunk/.Others, if you're in a position to recompile mod_proxy, please apply the patch and report back.Apache 2.2.3 compiled with mod_proxy used as rev.proxy for Owa.Works like a charm. I no longer see the '502 Proxy error' when clients try toconnect, while before this error showed 20 times per hour. A big improvement. Istill have some minor error logging issues with ActiveSync but these may beconfig issues as well.Is this going to be adressed in an official patch?Many thanks,DannyThe error message from this bug also appears when:a large and therefore slow incoming request over mobile GPRS line takes morethan about 15 secs to complete.The following workaround works well:add the following two lines to file httpd.conf:SetEnv force-proxy-request-1.0 1SetEnv proxy-nokeepalive 1(In reply to)I can repro what I believe is this problem as well on a 2.2.3 server on linuxdoing a reverse proxy to IIS using the prefork MPM. I'm going to apply thepatch locally and see if it clears up. Will report back tomorrow.So far so good. :)Will keep watching it throughout the day.(In reply to)I've seen absolutely zero instances of this problem since applying the patch. I'm going to go ahead and declare it fixed. Furthermore, the patch does appearcorrect to me, especially given the details given in.+1 (emeritus) for backport to the 2.2 branch.Just to add, the SetEnv workaround also seems to work for httpd 2.0.50 which I'mstill using. I did not try compiling the code fix since we need to use officialreleases only. However, if the SetEnv works, then the code fix likely works, incase it needs to be backported as well.Thanks for all the great info and the workaround. Hope the fix gets into thenext release.Fix unfortunately does'nt work with my configuration.httpd-2.2.3configure option:--enable-proxy--enable-proxy-balancer--enable-proxy-http--with-mpm=workerFound expected code at modules/proxy/proxy_util.c:1972 and patch it (configure/make/make install/restart)Still have :proxy: error reading status line from remote server www.myserver.comproxy: Error reading from remote server returned by /myurl.htmI've tried to add the following 2 directives in httpd.conf:SetEnv force-proxy-request-1.0 1SetEnv proxy-nokeepalive 1Restarted apache. same errors occur.I've tried debug Loglevel, but I don't get any other messages when it comes to that error.Any ideas ?In 2.2.4-dev and trunk***has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ***Just like to point out that I'm seeing this all the time still. I have upgradedto httpd-2.2.4, switched to using prefork instead of worker, added the SetEnvconfiguration options mentioned previously, tried various backend servercombinations of KeepAlive etc.Running httpd-2.2.4, prefork on Solaris 10 Sparc (T1000), caching and proxyingto httpd-2.2.4 prefork on Solaris 10 AMD64.Thu Feb 08 11:37:44 2007] [error] [client 88.110.18.62] proxy: error readingstatus line from remote server www1.mydomain.co.uk,referer:[Thu Feb 08 11:37:44 2007] [error] [client 88.110.18.62] proxy: Error readingfrom remote server returned by /forums/postlist.php, referer:Did you definitely upgrade mod_proxy?If you ran your 2.2.4 configure without *explicitly* compiling mod_proxy, thenit wont have compiled it, so it'll still be using your previous mod_proxy. Justsetting, for example --enable-mods-shared=all is not sufficient.I have to say that I am still seeing these errors as well.I'm using RedHat and I built and installed a new rpm for http-2.2.4 so I'm sure that I compiled in the new mod_proxy. I verified that the source fix is there in the .c file as well.[Fri Feb 09 10:18:21 2007] [error] [client xxx] proxy: error reading status line from remote server 127.0.0.1[Fri Feb 09 10:18:21 2007] [error] [client xxx] proxy: Error reading from remote server returned by /search/guide/queryI am running a cluster of Ruby/Mongrel servers behind a proxy balancer.--AndyI saw this error in my new environment. The cause of my error was by theExceptionNotifier.exception_recipients = $exception_recipients line in theenvironment.rbwhich calls the smtp.rb. This was hanging due to environment restrictions. I addedActionMailer::Base.server_settings = { :address => 'somemachine', :domain => 'somedoamin.com', :port => 25, :user_address => '', :user_name => 'someuser', :password => 'somepassword', :authentication=>:login }to the production.rb and the problem was resolve. Didn't happen much since wewrite such good code. ;)I have this problem with apache-2.2.4 (and 2.0.59) on Windows 2003 SP1. Itproxies (via a rewriterule - which btw. means I can't set the ttl etc. timingsenabled for ProxyPass in v2.2 :( )I reproduce it, via a stresstest setup.I have tried to set: SetEnv force-proxy-request-1.0 1 SetEnv proxy-nokeepalive 1in the virtualhost - but it has no effect.I have also tried to disable keepalive on the local IIS backend (and keeping theSetEnv's ) but to no effect either.in LogLevel debug I get this:[Wed Apr 18 16:03:12 2007] [error] [client x.x.x.x] proxy: error reading statusline from remote server www.sitename.dk[Wed Apr 18 16:03:12 2007] [error] [client x.x.x.x] proxy: Error reading fromremote server returned by /apps/file.dll/forside[Wed Apr 18 16:03:12 2007] [debug] proxy_util.c(1816): proxy: HTTP: has releasedconnection for (*)Anything I can try - I'm out of options :(p.s. I have set max ephemeral ports to 5000 and set tcptimedwaitdelay to 30secs, to avoid running into the 10048 OS error (every so often) - in apache 2.2this has the unfortunate side effect of disabling the site for the standardtimeout seconds, since mod_proxy has this new "feature" - and I can't set thedisable-time, as I'm using mod_proxy via mod_rewrite :(I have set max ephemeral ports to 65534 ofcourse - 5000 is the default :)(In reply to)I don't think that your problem is the same problem as described by many othershere. It is also no surprise to me that force-proxy-request-1.0 andproxy-nokeepalive show no effect in your situation as you are using mod_rewriteto do reverse proxying to a single server (a balancer would be a differentstory). In this case httpd NEVER reuses the backend connections. For eachrequest to the backend a new one will be opened.Above (the star) proves that you are using the 'reverse worker' which neverreuses backend connections. So it looks like your fresh backend connections get broken for some other reasons.I'm seeing this pretty frequently with Apache 2.2.4 on Solaris 10 using preformMPM. The proxy is forwarding through a firewall, so I have disabled KeepAlive onboth internal and external servers. The internal runs Apache 2.0.55. I cannotreplicate the error myself, so testing solutions involves trying something andthen waiting to see if the error still pops up.Not sure if this is an issue, but I actually have a server name in "...statusline from remote server (null)".(In reply to)Nevermind, it is the firewall inbetween that is dropping the connections. Thisis not an Apache bug for me. Thanks.I also have this bug running Apache 2.2.4 on Solaris 9, worker MPM.The SetEnv settings do not make it go away for me.My setup is Apache doing reverse proxy + caching in front of a Tomcat server (nofirewalls in between, running on the same machine in fact).Problem seems to happen under high load. An interesting fact is that it happensaround the time the pages should expire (I control this from Tomcat by settingthe "Expires" header). So maybe something is wrong with some header processing?Running the same setup but using mod_jk instead of mod_proxy works fine for me.I believe I'm running into this problem as well. My situation is similar to post#46 except I'm on apache 2.2.3 (64bit). I noticed the problem immediately afterdeploying my rails app to a new server; my mongrels completely freeze afterlittle bit of inactivity. Some information about my setup:Linux web001 2.6.16.27-0.9-smp #1 SMP Tue Feb 13 09:35:18 UTC 2007 x86_64 x86_64x86_64 GNU/Linux<VirtualHost *:80> ServerName demo DocumentRoot '/demo/public' <Location /> Order deny,allow Allow from all </Location> <Directory '/demo/public'> Options FollowSymLinks AllowOverride None </Directory> RewriteEngine On # Check for maintenance file and redirect all requests RewriteCond %{DOCUMENT_ROOT}/system/maintenance_demo.html -f RewriteCond %{SCRIPT_FILENAME} !maintenance_demo.html RewriteRule ^.*$ /system/maintenance_demo.html [PT] # Redirect all non-static requests to cluster RewriteCond %{DOCUMENT_ROOT}/%{REQUEST_FILENAME} !-f RewriteRule ^/(.*)$ balancer://demo_balancer%{REQUEST_URI} [P,QSA,L]</VirtualHost><Proxy balancer://demo_balancer> BalancerMemberBalancerMember</Proxy>May 21 07:46:25 web001 httpd2-prefork[12751]: [error] [client x.x.x.x] proxy:error reading status line from remote server localhost, referer:May 21 07:46:25 web001 httpd2-prefork[12751]: [error] [client x.x.x.x] proxy:Error reading from remote server returned by /, referer:Using the SetEnv solution in the virtual host did not work.Paul(In reply to)First please try with 2.2.4 as it contains a fix. Second, if things do not workwithSetEnv proxy-nokeepalive 1(regardles of 2.2.3 / 2.2.4) then this looks similar to #48 for me and it lookslike your fresh backend connections gets broken for some other reasons that arenot caused by the bug of this PR.***has been marked as a duplicate of this bug. ***I too am having this issue.Running apache 2.2.4 on linux 64-bit, reverse proxy to IIS 6 backend running on Windows 2003.Apache Errors:[error] [client x.x.x.x] proxy: error reading status line from remote server y.y.y.y, referer:.[error] [client x.x.x.x] proxy: Error reading from remote server returned by /URL/blahWe are using mod_proxy. Errors seem very random, and tough to pin-point. When we use AJP, problem is gone.We have very high volume with this setup.Both env's are set:SetEnv force-proxy-request-1.0 1SetEnv proxy-nokeepalive 1We have tried no keepalives on the IIS side, large keepalives, but no difference.There is a PIX firewall in between, but we are not losing packets with that.Any ideas are indeed welcome.Hi all,I have the same problem. I've just installed Apache 2.2.4 on the frontend(reverse proxy) server, on the backend server there is Apache 2.2.3 version(Debian Etch package).I still do have these problems:[Sun Jul 01 22:21:26 2007] [error] [client xx.xx.xx.xx] proxy: error readingstatus line from remote server backend.xxxx.com[Sun Jul 01 22:21:26 2007] [error] [client xx.xx.xx.xx] proxy: Error readingfrom remote server returned by /articles/According to Changelog of 2.2.4 this should be fixed if I'm not mistaken.Hi allWe are using apache 2.2.4 and proxying to jboss 4.0.2 server. Our users reported a problem of 2 trade failures. When we checked apache logs, we noticed a slowdown in response time (5 sec and 7 sec respectively)Seems like request was 'waiting' 'in' apache because it hit our jboss server after 5 and 7 second delay ( usual delay time is in millisecond level).On checking in error logs we saw this error ->[Tue Jun 05 14:14:16 2007] [error] [client xxx.xxx.19.41] proxy: error reading status line from remote server 10.10.xxx.xx[Tue Jun 05 14:14:16 2007] [error] [client xxx.xxx.19.41] proxy: Error reading from remote server returned by /isCxxx/Heartbeat.do[Tue Jun 05 14:14:16 2007] [error] [client xxx.xxx.19.41] File does not exist: /xxxx/opt/apache2/htdocs/maintenance.htmlOn deeper analysis, we found that this error message comes 2-3 times an hour in our logs consistently. But we were not noticing it because it was not causing any problems.However, during the time when trades were rejected, we have 20-25 instances of this error. So it looks like due to this error response time gets increased.I am ready to provide more information if required.TapanI have created a very simple test for this.a Fresh Ubuntu Install with the latest Gutsy Apache2.2.4 on itI have done it as a prefork and as a worker both failhere is the error log[Thu Aug 23 19:20:09 2007] [debug] mod_proxy_http.c(1662): proxy: HTTP: servingURL[Thu Aug 23 19:20:09 2007] [debug] proxy_util.c(1798): proxy: HTTP: has acquiredconnection for (192.168.0.110)[Thu Aug 23 19:20:09 2007] [debug] proxy_util.c(1859): proxy: connectingto 192.168.0.110:2054[Thu Aug 23 19:20:09 2007] [debug] proxy_util.c(1955): proxy: connected / to192.168.0.110:2054[Thu Aug 23 19:20:09 2007] [debug] proxy_util.c(2050): proxy: HTTP: fam 2 socketcreated to connect to 192.168.0.110[Thu Aug 23 19:20:09 2007] [debug] proxy_util.c(2146): proxy: HTTP: connectioncomplete to 192.168.0.110:2054 (192.168.0.110)[Thu Aug 23 19:22:38 2007] [error] [client 192.168.0.1] proxy: error readingstatus line from remote server 192.168.0.110[Thu Aug 23 19:22:38 2007] [error] [client 192.168.0.1] proxy: Error readingfrom remote server returned by /ceservice.cetix[Thu Aug 23 19:22:38 2007] [debug] proxy_util.c(1816): proxy: HTTP: has releasedconnection for (192.168.0.110)[Thu Aug 23 19:22:38 2007] [debug] mod_proxy_balancer.c(521):proxy_balancer_post_request for (balancer://demo_balancer)Here is the sites-available file...Pretty straight forward. NameVirtualHost *<Proxy balancer://demo_balancer> BalancerMember# BalancerMember</Proxy><VirtualHost *> ServerAdmin webmaster@localhostSetEnv force-proxy-request-1.0 1SetEnv proxy-nokeepalive 1RewriteEngine onRewriteCond %{REQUEST_METHOD} ^(TRACE|TRACK)RewriteRule .* - [F]RewriteLog "/var/log/apache2/rewrite.log"RewriteLogLevel 9RewriteRule ^/(.*.cetix) balancer://demo_balancer [P,QSA,L] DocumentRoot /var/www/ <Directory /> Options FollowSymLinks AllowOverride None </Directory> <Directory /var/www/> Options Indexes FollowSymLinks MultiViews AllowOverride None Order allow,deny allow from all # This directive allows us to have apache2's default start page # in /apache2-default/, but still have / go to the right place #RedirectMatch ^/$ /apache2-default/ </Directory> ScriptAlias /cgi-bin/ /usr/lib/cgi-bin/ <Directory "/usr/lib/cgi-bin"> AllowOverride None Options +ExecCGI -MultiViews +SymLinksIfOwnerMatch Order allow,deny Allow from all </Directory> ErrorLog /var/log/apache2/error.log # Possible values include: debug, info, notice, warn, error, crit, # alert, emerg. LogLevel debug CustomLog /var/log/apache2/access.log combined ServerSignature On Alias /doc/ "/usr/share/doc/" <Directory "/usr/share/doc/"> Options Indexes MultiViews FollowSymLinks AllowOverride None Order deny,allow Deny from all Allow from 127.0.0.0/255.0.0.0 ::1/128 </Directory></VirtualHost>~This is forwarding to another web service on a local network. I can talkdirectly to the web service just fine.I need to get this resolved any advice is great.Jeff***has been marked as a duplicate of this bug. ***I have compiled the latest httpd 2.2.6 with static modules i.e. ./configure --prefix=/usr/local/apache --enable-modules="asis alias cache deflate disk_cache expires file_cache mem_cache headers mime mimemagic include log_config proxy proxy_http rewrite setenvif status vhost_alias" --with-mpm=workerWe have IIS 6.0 as backend webservers and a BigIP F5 for load balancing in front of them. The Apache sits in a different geographical location and connects to the IIS webservers thru a VPN connection. The virtual host config is as follows:<VirtualHost xx.xx.xx.xx> ServerName www.abc.com ServerAlias abc.com ProxyRequests Off ProxyBadHeader Ignore SetEnv force-proxy-request-1.0 1 SetEnv proxy-nokeepalive 1 ProxyPass /smax=5 max=20 ttl=120 retry=1 keepalive=Off #ProxyPass /ProxyPassReverse /</VirtualHost>[Mon Sep 10 17:36:59 2007] [error] ap_proxy_connect_backend disabling worker for (www.ABC.com)[Mon Sep 10 17:36:59 2007] [error] (110)Connection timed out: proxy: HTTP: attempt to connect to xx.xx.xx.xx:80 (www.ABC.com) failed[Mon Sep 10 17:37:00 2007] [error] (110)Connection timed out: proxy: HTTP: attempt to connect to xx.xx.xx.xx:80 (www.ABC.com) failed[Mon Sep 10 17:37:00 2007] [error] ap_proxy_connect_backend disabling worker for (www.ABC.com)[Mon Sep 10 17:37:00 2007] [error] (110)Connection timed out: proxy: HTTP: attempt to connect to xx.xx.xx.xx:80 (www.ABC.com) failed[Mon Sep 10 17:37:01 2007] [error] (110)Connection timed out: proxy: HTTP: attempt to connect to xx.xx.xx.xx:80 (www.ABC.com) failed[Mon Sep 10 17:37:01 2007] [error] ap_proxy_connect_backend disabling worker for (www.ABC.com)[Mon Sep 10 17:38:21 2007] [error] [client 200.124.142.132] (70007)The timeout specified has expired: proxy: error reading status line from remote server www.ABC.com, referer:[Mon Sep 10 17:38:21 2007] [error] [client 200.124.142.132] proxy: Error reading from remote server returned by /welcome/welcome.aspx, referer:These errors are appearing randomly and I was monitoring a ping between Apache to the F5 and there were no hiccups in the VPN connection. The problem gets exaggerated when I use multiple Apache's in front and load balance between them.Please let me know if there is any fix available to the problem or If I should not be using Apache to reverse proxy a website being loadbalanced by F5.Thanks in anticipation:ASError reading status line can appear due to network timeout and is often observed when the 'Timeout' setting is set to low value. This is a normal situation.If it is a normal situation, perhaps it shouldn't be logged to the error log?It is a normal situation in terms of httpd functioning. The causes of 'error reading status line' are:(70007)The timeout specified has expired -- indicates network timeout, configured by Timeout directive(104)Connection reset by peer -- TCP RST was sent from server(70014)End of file found -- TCP FIN was sent from serverAll these messages appearing in error.log indicate some problem with network on server side or with server itself and do not mean errors in proxy software. I think the bug should be closed.I think this bug should NOT be closed because the proxy server should betransparent and it is not as long this problem remains. Due to this bug, thesystem behaves differently when the proxy server is in between the client andthe server. Direct connections to the server do not produce this error messagein clients. When the workarounds proposed are in place, the error is notreproduced either.Now the proxy module produces an error message, thus moving problems on the TCPnetwork layer to the http layer. I think it is not practical to start proxyingthe TCP layer messages, so instead the http proxy should react to TCP layerproblems in the same way a http client would.(In reply to)What data are you privy to that causes you to believe that there isn't a bugcausing the proxy module to believe one of those conditions is true when it, infact, is false?(In reply to)And what data can be previed to the belief that there is a bug? Do you think there can be a bug in apr functions that are called from ap_rgetline_core or do you think that there can be a bug in ap_rgetline_core itself? If this could be the case, then we would see the bug not only in "reading status line" but everywhere!I'm not familiar with the intracacies of APR, but what I do know is that Iexperience problems with Apache acting as a proxy server that I do notexperience when it is not the proxy server. I may be ignorant of the code, butto me that points to something about the proxy. I haven't seen anything reportedin this bug that points to another cause other than what appears to be blindfaith that there cannot be anything wrong with the proxy code.(In reply to)He is correct in several cases:(70007)The timeout specified has expired -- indicates network timeout.This can happen if your backend takes a long time preparing the response andthis is a wanted behaviour. You can adjust the timeout either via TimeOut,ProxyTimeout or a specific worker timeout.(104)Connection reset by peer -- TCP RST was sent from server(70014)End of file found -- TCP FIN was sent from serverThese can happen either because of network problems or problems on the backendserver or because of a race condition in the proxy code.Currently there is a race condition that the backend server can close thekeepalive connection right after httpd checked the state of this TCP connectionand httpd starts sending a request and waiting on a response on a connectionthat is in fact dead. But this should happen rarely and browsers try to resentthe request in such situations provided the request was idempotent (so no POSTrequest or GET request with parameters).All other cases (except for the race condition) also lead to error messages inthe browser (maybe a different one that the one sent by the proxy, but an errormessage).So the race condition is the only reason why I think that it is justified tokeep this PR open.Createdbackend connection race condition fix (untested)This should fix such race condition.(In reply to)I think it's missing a cleanup before the retry in the APR_ECONNRESET/APR_EOFcase. But otherwise looks reasonable, if this race condition is indeed the culprit.It would be good to get some feedback from folks affected by this problem. Doesthe patch fix it for you?(In reply to)As explained before this patch is not correct:1. Once you sent the request body it is no longer available.2. The RFC does not allow to resend non idempotent requests.The only fix I can think of is to send a test request over the connection firstthat does not trigger (a significant action) on backend side.OPTIONS *would be a candidate for this. If retrieve an 200 OK here we can be pretty surethat the race will not occur(In reply to)This can be fixed.Why does this applies to our situation when the request does not reach backend server?(In reply to)Not easily as buffering large request entities is a pain.With your loop you don't know. Once you sent a single byte of the request bodyor even only the GET request with parameters you do not really now what thebackend server did. Didn't it receive it or did it receive it, processed it andjust in the moment as it wanted to send the reply the network broke down or theprocess that processed the request died.(In reply to)bodyandtheIf after FIN was recieved RST is following couldn't we assume that our request is sure rejected by remote server's TCP/IP stack?Besides, I suppose that any client will suffer from such Byzantine problem. Where is the difference between mod_proxy and browser behaviour addressing this situation?(In reply to)Ummmm, yeah - thanks Aleksey! Big autodopeslap here. I've been watching thisbug for a while and it never occurred to me to 'de-optimize' the globalTimeout. I guess I assumed the ProxyTimeout would have superceded Timeout in aVirtualHost (if I'm reading the code correctly, the recent commit fordoes just that, but I'm still on 2.2.6). Anyway, a new Timeout in the affectedVirtualHost fixed this for me (finally - yay!)I filedasking for a more specific error message.***has been marked as a duplicate of this bug. ***It also happens with us. Any thoughts on when the patch existing inwill be commited in SVN?(In reply to)It is really wrong. Don't use it.Any thoughts on how to fix it then?Try to use the patch from(). Patch:svn diff -r645812:645813(In reply to)So as I understand it, there is a bug regarding a race condition in mod_proxy that is still causing this error. And the only patch in place is the one above in, which I noticed is also in trunk. However, this patch, while perhaps a good one to have in place, doesn't solve the race condition. And the race condition is the real issue here.This is a serious problem that is affecting the use of Apache 2.2.8 on many customers. The frequency of the error with some sites is high enough that it puts Apache mod_proxy in jeopardy of being unusable, especially for non-idempotent requests. It may be less than 5% of requests at most but that is a lot for a major traffic site.Are there any new developments on getting this race condition fixed?Can someone please point me to the area in the code where this race condition exists and describe the complications around it that have prevented it from being fixed thus far?(In reply to)Backported to 2.2.x as().Any chance of getting this fix ported to 2.0?I'm having the same/similar issue when using mod_proxy to an https target. (2.0.63/worker/linux)Using the SetEnv hack seems to resolve the issue... but this is not a desirable workaround.(In reply to)Should never say never but the probability is very low. There is low developer interest in 2.0.x these days and so expect only security fixes for 2.0.x.Upgrade to 2.2.x. It has many interesting new features in the proxy module.***has been marked as a duplicate of this bug. ***CreatedPatch against trunkCan you please try this patch and addsetenv proxy-initial-not-pooled 1to your virtualhost configuration.Note that it comes with a performance penalty.(In reply to)At first, I introduced.The problem has been improved, but not perfect. I was watching "netstat", but sometimes there were "ESTABLISHED" connection ghosts after backend closed them, and in this case "Proxy error" occurred at 100%.(In reply to)I tried this just now.It looks working well perfectly. But it has really very large performance problem. Especially, it is very serious with clients which does not have HTTP/1.1 keep-alive like IE on https.(In reply to)Here is the debug log related to the "Proxy error" on. It occurred after the backend closes all connection by keep-alive timeout and there were connection ghost.[Fri Aug 01 11:33:07 2008] [info] Initial (No.1) HTTPS request received for child 2 (server example.com:443)[Fri Aug 01 11:33:07 2008] [debug] mod_proxy_http.c(55): proxy: HTTP: canonicalising URL //192.168.1.100/example.gif[Fri Aug 01 11:33:07 2008] [debug] proxy_util.c(1488): [client 192.168.3.125] proxy: http: found workerfor[Fri Aug 01 11:33:07 2008] [debug] mod_proxy.c(966): Running scheme http handler (attempt 0)[Fri Aug 01 11:33:07 2008] [debug] mod_proxy_http.c(1874): proxy: HTTP: serving URL[Fri Aug 01 11:33:07 2008] [debug] proxy_util.c(2044): proxy: HTTP: has acquired connection for (192.168.1.100)[Fri Aug 01 11:33:07 2008] [debug] proxy_util.c(2102): proxy: connectingto 192.168.1.100:80[Fri Aug 01 11:33:07 2008] [debug] proxy_util.c(2200): proxy: connected /example.gif to 192.168.1.100:80[Fri Aug 01 11:33:07 2008] [error] [client 192.168.3.125] (104)Connection reset by peer: proxy: error reading status line from remote server 192.168.1.100[Fri Aug 01 11:33:07 2008] [debug] mod_proxy_http.c(1395): [client 192.168.3.125] proxy: NOT Closing connection to client although reading from backend server 192.168.1.100 failed.[Fri Aug 01 11:33:07 2008] [error] [client 192.168.3.125] proxy: Error reading from remote server returned by /apps/example.gif[Fri Aug 01 11:33:07 2008] [debug] proxy_util.c(2062): proxy: HTTP: has released connection for (192.168.1.100)[Fri Aug 01 11:33:07 2008] [info] [client 192.168.3.125] Connection closed to child 2 with unclean shutdown (server example.com:443)***has been marked as a duplicate of this bug. ***With the latest fixes in 2.2.10 this is now fixed and reduces itself to a configuration problem.***has been marked as a duplicate of this bug. ***Was this problem really fixed?I updated my system to 2.2.11, and add new conf lines;<VirtualHost *:80> ServerName www.example.com DocumentRoot /www <Location /sub/> ProxyPassProxyPassReverseSetEnv force-proxy-request-1.0 1 SetEnv proxy-nokeepalive 1 </Location></VirtualHost>But, I still have same problem, and the error log is;[Mon May 18 09:30:48 2009] [error] [client ***.***.***.***] (104)Connection reset by peer: proxy: error reading status line from remote server 192.168.60.34[Mon May 18 09:30:48 2009] [error] [client ***.***.***.***] proxy: Error reading from remote server returned by /sub/index.htmlDoes anyone know how to fix this?Sorry!I just forgot to add "SetEnv proxy-initial-not-pooled 1".Now it seems to working well, but really too slow....I was using apache 2.2.3 and internal web server is IIS 6 hosted on windows 2003.Initially i have triedSetEnv force-proxy-request-1.0 1SetEnv proxy-nokeepalive 1SetEnv proxy-initial-not-pooled 1With this change the frequency of getting the proxy error is reduced.And now upgraded to 2.2.15 and got the same result after adding "SetEnv" hack.I have configured the proxy time out to 900Timeout 900httpd keepalive option on and off IIS Keep alive time is also 900.The error is consistent if the request made is time consuming. and if approx 10 simultaneous request has send to the server.I was behind this problem for more than 8 months and still no hope :(Appreciate your help!--kirant400Our hoster company is using an Apache 2.2.9 as Reverse-Proxy. The web server is IIS 6 on windows 2003.We tried alsoSetEnv force-proxy-request-1.0 1SetEnv proxy-nokeepalive 1but we get the error like kirant400 if the request is time consuming.Now our hoster and I changed some settings and we have NOT this issue anymore.1. Unchanged "force-proxy-request-1.0 1"2. Deactivated "proxy-nokeepalive" on apache3. Apache Keep Alive set to 9004. IIS Keep Alive set to 900Can someone try and confirm this.Thanks,ibramibram, please use the users support list to discuss your issue with verbatim details.Hi,The SetEnv Parameters worked well with my case and I am no longer getting this error but I faced another issue which is HTTP/1.1 417 Expectation Failed.This is due to the apache client application is using HTTP/1.1 and the Proxy was forced to use HTTP/1.0 as per the setEnv Parameters.I found a solution for this problem in this URL:I applied this solution and now my httpd.conf in the proxy server is having the below lines:SetEnv force-proxy-request-1.0 1SetEnv proxy-nokeepalive 1RequestHeader unset Expect earlyThank you for the support.***has been marked as a duplicate of this bug. ***Hi , Error message shown below: Proxy Error The proxy server received an invalid response from an upstream server. The proxy server could not handle the request GET /. Reason: Error reading from remote server Apache/2.2.15 (CentOS) Server at calmacprod.cigniti.com Port 86NameVirtualHost *:85<VirtualHost *:85> ServerName calmacprod.cigniti.comProxyRequests OffProxyPreserveHost On<Proxy *> Order allow,deny Allow from all</Proxy>ProxyPass /rmProxyPassReverse /rmProxyPass /ProxyPassReverse /<Location /jira> Order allow,deny Allow from all</Location>#SSLProxyEngine onSSLEngine onSSLCertificateFile /etc/httpd/ssl/131fd0633495ecf7.crtSSLCertificateKeyFile /etc/httpd/ssl/calmacprod.cigniti.com.keyErrorLog logs/jira.cigniti</VirtualHost>NameVirtualHost *:86<VirtualHost *:86> ServerName calmacprod.cigniti.comProxyRequests OffProxyPreserveHost On<Proxy *> Order deny,allow Allow from all</Proxy>ProxyPass /ProxyPassReverse /<Location /confluence> Order allow,deny Allow from all</Location>SSLEngine onSSLCertificateFile /etc/httpd/ssl/131fd0633495ecf7.crtSSLCertificateKeyFile /etc/httpd/ssl/calmacprod.cigniti.com.keyErrorLog logs/confluence.cigniti</VirtualHost>ReWriteEngine OnReWriteCond %{HTPS} offRatlassian/installation/confluence/jre//bin/java -Djava.util.log ging.config.file=/atlassian/installation/confluence/conf/logging.properties -Djava.util.logging.manager=org.apac he.juli.ClassLoaderLogManager -Xms1024m -Xmx1024m -XX:MaxMetaspaceSize=256m -XX:+UseG1GC -Djava.awt.headless=tru e -Xloggc:/atlassian/installation/confluence/logs/gc-2015-10-27_01-21-59.log -XX:+UseGCLogFileRotation -XX:Numbe rOfGCLogFiles=5 -XX:GCLogFileSize=2M -XX:-PrintGCDetails -XX:+PrintGCTimeStamps -XX:-PrintTenuringDistribution - Djava.endorsed.dirs=/atlassian/installation/confluence/endorsed -classpath /atlassian/installation/confluence/bi n/bootstrap.jar:/atlassian/installation/confluence/bin/tomcat-juli.jar -Dcatalina.base=/atlassian/installation/c onfluence -Dcatalina.home=/atlassian/installation/confluence -Djava.io.tmpdir=/atlassian/installation/confluence /temp org.apache.catalina.startup.Bootstrap startpostgres 13603 30198 0 01:23 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(47601) idlepostgres 14165 30198 0 01:46 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(48008) idlepostgres 20868 30198 0 04:18 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50767) idlepostgres 20956 30198 0 04:19 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50796) idlepostgres 20957 30198 0 04:19 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50797) idlepostgres 21001 30198 0 04:20 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50817) idlepostgres 21002 30198 0 04:20 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50818) idlepostgres 21011 30198 0 04:20 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50823) idlepostgres 21012 30198 0 04:20 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50824) idlepostgres 21013 30198 0 04:20 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50825) idlepostgres 21014 30198 0 04:20 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50830) idlepostgres 21015 30198 0 04:20 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50831) idlepostgres 21016 30198 0 04:20 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50832) idlepostgres 21017 30198 0 04:20 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50833) idlepostgres 21018 30198 0 04:20 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50834) idlepostgres 21019 30198 0 04:20 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50835) idlepostgres 21020 30198 0 04:20 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50836) idlepostgres 21021 30198 0 04:20 ? 00:00:00 postgres: confluence1 confluencedb 127.0.0.1(50837) idleroot 21040 20801 0 04:21 pts/0 00:00:00 grep confluenceroot 28153 1 1 Aug11 ? 1-02:47:44 /atlassian/installation/confluence/jre//bin/java -Djava.util.l ogging.config.file=/atlassian/installation/confluence/conf/logging.properties -Djava.util.logging.manager=org.ap ache.juli.ClassLoaderLogManager -Xms1024m -Xmx1024m -XX:MaxMetaspaceSize=256m -XX:+UseG1GC -Djava.awt.headless=t rue -Xloggc:/atlassian/installation/confluence/logs/gc-2015-08-11_08-37-19.log -XX:+UseGCLogFileRotation -XX:Num berOfGCLogFiles=5 -XX:GCLogFileSize=2M -XX:-PrintGCDetails -XX:+PrintGCTimeStamps -XX:-PrintTenuringDistribution -Djava.endorsed.dirs=/atlassian/installation/confluence/endorsed -classpath /atlassian/installation/confluence/ bin/bootstrap.jar:/atlassian/installation/confluence/bin/tomcat-juli.jar -Dcatalina.base=/atlassian/installation /confluence -Dcatalina.home=/atlassian/installation/confluence -Djava.io.tmpdir=/atlassian/installation/confluen ce/temp org.apache.catalina.startup.Bootstrap start[root@production ~]# cd /atlassian/installation/confluence/bin/[root@production bin]#[root@production bin]# ./stop-confluence.shexecuting using dedicated userIf you encounter issues starting up Confluence, please see the Installation guide at.com/display/DOC/Confluence+Installation+Guide[root@production bin]# ./startup.shIf you encounter issues starting up Confluence, please see the Installation guide at[root@production logs]# tail -f -n 1000 catalina.out"I/O dispatcher 9" #90 daemon prio=5 os_prio=0 tid=0x00007f698c093000 nid=0x6e8a runnable [0x00007f69879f8000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method) at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269) at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked <0x00000000cb0bc1a8> (a sun.nio.ch.Util$2) - locked <0x00000000cb0bc1b8> (a java.util.Collections$UnmodifiableSet) - locked <0x00000000cb0bc160> (a sun.nio.ch.EPollSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:256) at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:105) at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:584) at java.lang.Thread.run(Thread.java:745)"I/O dispatcher 8" #89 daemon prio=5 os_prio=0 tid=0x00007f698c009000 nid=0x6e89 runnable [0x00007f6987af9000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method) at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269) at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked <0x00000000cb15b550> (a sun.nio.ch.Util$2) - locked <0x00000000cb15b560> (a java.util.Collections$UnmodifiableSet) - locked <0x00000000cb15b508> (a sun.nio.ch.EPollSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:256) at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:105) at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:584) at java.lang.Thread.run(Thread.java:745)"I/O dispatcher 7" #88 daemon prio=5 os_prio=0 tid=0x00007f698c007800 nid=0x6e88 runnable [0x00007f6987bfa000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method) at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269) at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked <0x00000000cb0dca28> (a sun.nio.ch.Util$2) - locked <0x00000000cb0dca38> (a java.util.Collections$UnmodifiableSet) - locked <0x00000000cb0dc9e0> (a sun.nio.ch.EPollSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:256) at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:105) at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:584) at java.lang.Thread.run(Thread.java:745)"I/O dispatcher 6" #87 daemon prio=5 os_prio=0 tid=0x00007f698c006000 nid=0x6e87 runnable [0x00007f6967ffe000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method) at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269) at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked <0x00000000cb0ff338> (a sun.nio.ch.Util$2) - locked <0x00000000cb0ff348> (a java.util.Collections$UnmodifiableSet) - locked <0x00000000cb0ff2f0> (a sun.nio.ch.EPollSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:256) at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:105) at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:584) at java.lang.Thread.run(Thread.java:745)"I/O dispatcher 5" #86 daemon prio=5 os_prio=0 tid=0x00007f698c00f800 nid=0x6e86 runnable [0x00007f6987cfb000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method) at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269) at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked <0x00000000cb0bc438> (a sun.nio.ch.Util$2) - locked <0x00000000cb0bc448> (a java.util.Collections$UnmodifiableSet) - locked <0x00000000cb0bc3f0> (a sun.nio.ch.EPollSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:256) at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:105) at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:584) at java.lang.Thread.run(Thread.java:745)"I/O dispatcher 4" #85 daemon prio=5 os_prio=0 tid=0x00007f698c00e000 nid=0x6e85 runnable [0x00007f6987dfc000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method) at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269) at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked <0x00000000cb0ff5d8> (a sun.nio.ch.Util$2) - locked <0x00000000cb0ff5e8> (a java.util.Collections$UnmodifiableSet) - locked <0x00000000cb0ff590> (a sun.nio.ch.EPollSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:256) at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:105) at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:584) at java.lang.Thread.run(Thread.java:745)"I/O dispatcher 3" #84 daemon prio=5 os_prio=0 tid=0x00007f698c00c000 nid=0x6e84 runnable [0x00007f6987efd000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method) at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269) at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked <0x00000000cb1dbc08> (a sun.nio.ch.Util$2) - locked <0x00000000cb1dbc18> (a java.util.Collections$UnmodifiableSet) - locked <0x00000000cb1dbbc0> (a sun.nio.ch.EPollSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:256) at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:105) at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:584) at java.lang.Thread.run(Thread.java:745)"I/O dispatcher 2" #83 daemon prio=5 os_prio=0 tid=0x00007f698c002000 nid=0x6e83 runnable [0x00007f697fefd000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method) at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269) at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked <0x00000000cb0dccb8> (a sun.nio.ch.Util$2) - locked <0x00000000cb0dccc8> (a java.util.Collections$UnmodifiableSet) - locked <0x00000000cb0dcc70> (a sun.nio.ch.EPollSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:256) at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:105) at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:584) at java.lang.Thread.run(Thread.java:745)"I/O dispatcher 1" #82 daemon prio=5 os_prio=0 tid=0x00007f698c001000 nid=0x6e82 runnable [0x00007f6987ffe000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method) at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269) at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked <0x00000000cb15b7e0> (a sun.nio.ch.Util$2) - locked <0x00000000cb15b7f0> (a java.util.Collections$UnmodifiableSet) - locked <0x00000000cb15b798> (a sun.nio.ch.EPollSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:256) at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:105) at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:584) at java.lang.Thread.run(Thread.java:745)"httpclient-io:thread-1" #81 daemon prio=5 os_prio=0 tid=0x00007f69ec224800 nid=0x6e81 runnable [0x00007f6a08113000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method) at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269) at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked <0x00000000ca3cece0> (a sun.nio.ch.Util$2) - locked <0x00000000ca3cecf0> (a java.util.Collections$UnmodifiableSet) - locked <0x00000000ca3cec98> (a sun.nio.ch.EPollSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:340) at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:189) at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase.doExecute(CloseableHttpAsyncClientBase.java:67) at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase.access$000(CloseableHttpAsyncClientBase.java:38) at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:57) at java.lang.Thread.run(Thread.java:745)"Timer-3" #54 daemon prio=5 os_prio=0 tid=0x00007f69f00b3000 nid=0x6e63 in Object.wait() [0x00007f6a5818f000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:502) at java.util.TimerThread.mainLoop(Timer.java:526) - locked <0x00000000c8453ee0> (a java.util.TaskQueue) at java.util.TimerThread.run(Timer.java:505)"Timer-2" #53 daemon prio=5 os_prio=0 tid=0x00007f69f0029800 nid=0x6e62 in Object.wait() [0x00007f6a0b7fe000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:502) at java.util.TimerThread.mainLoop(Timer.java:526) - locked <0x00000000c80fb128> (a java.util.TaskQueue) at java.util.TimerThread.run(Timer.java:505)"FelixStartLevel" #52 daemon prio=5 os_prio=0 tid=0x00007f69f4439000 nid=0x6e61 in Object.wait() [0x00007f6a0b5fc000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:502) at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:279) - locked <0x00000000c809bda8> (a java.util.ArrayList) at java.lang.Thread.run(Thread.java:745)"FelixDispatchQueue" #51 daemon prio=5 os_prio=0 tid=0x00007f69f400c000 nid=0x6e60 in Object.wait() [0x00007f6a0b6fd000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:502) at org.apache.felix.framework.util.EventDispatcher.run(EventDispatcher.java:1063) - locked <0x00000000c82ffca0> (a java.util.ArrayList) at org.apache.felix.framework.util.EventDispatcher.access$000(EventDispatcher.java:54) at org.apache.felix.framework.util.EventDispatcher$1.run(EventDispatcher.java:101) at java.lang.Thread.run(Thread.java:745)"com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread-#2" #44 daemon prio=5 os_prio=0 tid=0x00007f6a63788800 nid=0x6e57 runnable [0x00007f6a58290000] java.lang.Thread.State: RUNNABLE at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:170) at java.net.SocketInputStream.read(SocketInputStream.java:141) at org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:143) at org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:112) at org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:71) at org.postgresql.core.PGStream.ReceiveChar(PGStream.java:269) at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1700) at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255) - locked <0x00000000d5180430> (a org.postgresql.core.v3.QueryExecutorImpl) at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:555) at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:403) at org.postgresql.jdbc2.AbstractJdbc2Connection.execSQLQuery(AbstractJdbc2Connection.java:347) at org.postgresql.jdbc2.AbstractJdbc2Connection.execSQLQuery(AbstractJdbc2Connection.java:339) at org.postgresql.jdbc2.AbstractJdbc2Connection.getTransactionIsolation(AbstractJdbc2Connection.java:868) at com.mchange.v2.c3p0.impl.NewPooledConnection.<init>(NewPooledConnection.java:107) at com.mchange.v2.c3p0.WrapperConnectionPoolDataSource.getPooledConnection(WrapperConnectionPoolDataSource.java:198) at com.mchange.v2.c3p0.WrapperConnectionPoolDataSource.getPooledConnection(WrapperConnectionPoolDataSource.java:171) at com.mchange.v2.c3p0.impl.C3P0PooledConnectionPool$1PooledConnectionResourcePoolManager.acquireResource(C3P0PooledConnectionPool.java:137) at com.mchange.v2.resourcepool.BasicResourcePool.doAcquire(BasicResourcePool.java:1014) at com.mchange.v2.resourcepool.BasicResourcePool.access$800(BasicResourcePool.java:32) at com.mchange.v2.resourcepool.BasicResourcePool$AcquireTask.run(BasicResourcePool.java:1810) at com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread.run(ThreadPoolAsynchronousRunner.java:547)"com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread-#1" #43 daemon prio=5 os_prio=0 tid=0x00007f6a6b55b800 nid=0x6e56 runnable [0x00007f6a58390000] java.lang.Thread.State: RUNNABLE at java.lang.ClassLoader.findLoadedClass0(Native Method) at java.lang.ClassLoader.findLoadedClass(ClassLoader.java:1035) at java.lang.ClassLoader.loadClass(ClassLoader.java:406) - locked <0x00000000c00bf930> (a java.lang.Object) at java.lang.ClassLoader.loadClass(ClassLoader.java:411) - locked <0x00000000c0010630> (a java.lang.Object) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.util.ResourceBundle$RBClassLoader.loadClass(ResourceBundle.java:503) at java.util.ResourceBundle$Control.newBundle(ResourceBundle.java:2640) at java.util.ResourceBundle.loadBundle(ResourceBundle.java:1501) at java.util.ResourceBundle.findBundle(ResourceBundle.java:1465) at java.util.ResourceBundle.findBundle(ResourceBundle.java:1419) at java.util.ResourceBundle.findBundle(ResourceBundle.java:1419) at java.util.ResourceBundle.getBundleImpl(ResourceBundle.java:1361) at java.util.ResourceBundle.getBundle(ResourceBundle.java:890) at sun.util.resources.LocaleData$1.run(LocaleData.java:164) at sun.util.resources.LocaleData$1.run(LocaleData.java:160) at java.security.AccessController.doPrivileged(Native Method) at sun.util.resources.LocaleData.getBundle(LocaleData.java:160) at sun.util.resources.LocaleData.getDateFormatData(LocaleData.java:124) at java.text.DateFormatSymbols.initializeData(DateFormatSymbols.java:697) at java.text.DateFormatSymbols.<init>(DateFormatSymbols.java:146) at sun.util.locale.provider.DateFormatSymbolsProviderImpl.getInstance(DateFormatSymbolsProviderImpl.java:85) at java.text.DateFormatSymbols.getProviderInstance(DateFormatSymbols.java:359) at java.text.DateFormatSymbols.getInstanceRef(DateFormatSymbols.java:349) at java.text.SimpleDateFormat.<init>(SimpleDateFormat.java:603) at java.text.SimpleDateFormat.<init>(SimpleDateFormat.java:580) at org.postgresql.core.Logger.<init>(Logger.java:26) at org.postgresql.jdbc2.AbstractJdbc2Connection.<init>(AbstractJdbc2Connection.java:105) - locked <0x00000000c74afe48> (a java.lang.Class for org.postgresql.jdbc2.AbstractJdbc2Connection) at org.postgresql.jdbc3.AbstractJdbc3Connection.<init>(AbstractJdbc3Connection.java:29) at org.postgresql.jdbc3g.AbstractJdbc3gConnection.<init>(AbstractJdbc3gConnection.java:21) at org.postgresql.jdbc4.AbstractJdbc4Connection.<init>(AbstractJdbc4Connection.java:31) at org.postgresql.jdbc4.Jdbc4Connection.<init>(Jdbc4Connection.java:24) at org.postgresql.Driver.makeConnection(Driver.java:393) at org.postgresql.Driver.connect(Driver.java:267) at com.mchange.v2.c3p0.DriverManagerDataSource.getConnection(DriverManagerDataSource.java:134) at com.mchange.v2.c3p0.WrapperConnectionPoolDataSource.getPooledConnection(WrapperConnectionPoolDataSource.java:182) at com.mchange.v2.c3p0.WrapperConnectionPoolDataSource.getPooledConnection(WrapperConnectionPoolDataSource.java:171) at com.mchange.v2.c3p0.impl.C3P0PooledConnectionPool$1PooledConnectionResourcePoolManager.acquireResource(C3P0PooledConnectionPool.java:137) at com.mchange.v2.resourcepool.BasicResourcePool.doAcquire(BasicResourcePool.java:1014) at com.mchange.v2.resourcepool.BasicResourcePool.access$800(BasicResourcePool.java:32) at com.mchange.v2.resourcepool.BasicResourcePool$AcquireTask.run(BasicResourcePool.java:1810) at com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread.run(ThreadPoolAsynchronousRunner.java:547)"com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread-#0" #42 daemon prio=5 os_prio=0 tid=0x00007f6a54206800 nid=0x6e55 waiting for monitor entry [0x00007f6a58492000] java.lang.Thread.State: BLOCKED (on object monitor) at org.postgresql.jdbc2.AbstractJdbc2Connection.<init>(AbstractJdbc2Connection.java:104) - waiting to lock <0x00000000c74afe48> (a java.lang.Class for org.postgresql.jdbc2.AbstractJdbc2Connection) at org.postgresql.jdbc3.AbstractJdbc3Connection.<init>(AbstractJdbc3Connection.java:29) at org.postgresql.jdbc3g.AbstractJdbc3gConnection.<init>(AbstractJdbc3gConnection.java:21) at org.postgresql.jdbc4.AbstractJdbc4Connection.<init>(AbstractJdbc4Connection.java:31) at org.postgresql.jdbc4.Jdbc4Connection.<init>(Jdbc4Connection.java:24) at org.postgresql.Driver.makeConnection(Driver.java:393) at org.postgresql.Driver.connect(Driver.java:267) at com.mchange.v2.c3p0.DriverManagerDataSource.getConnection(DriverManagerDataSource.java:134) at com.mchange.v2.c3p0.WrapperConnectionPoolDataSource.getPooledConnection(WrapperConnectionPoolDataSource.java:182) at com.mchange.v2.c3p0.WrapperConnectionPoolDataSource.getPooledConnection(WrapperConnectionPoolDataSource.java:171) at com.mchange.v2.c3p0.impl.C3P0PooledConnectionPool$1PooledConnectionResourcePoolManager.acquireResource(C3P0PooledConnectionPool.java:137) at com.mchange.v2.resourcepool.BasicResourcePool.doAcquire(BasicResourcePool.java:1014) at com.mchange.v2.resourcepool.BasicResourcePool.access$800(BasicResourcePool.java:32) at com.mchange.v2.resourcepool.BasicResourcePool$AcquireTask.run(BasicResourcePool.java:1810) at com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread.run(ThreadPoolAsynchronousRunner.java:547)[root@production logs]#Please Advise,Vinay.Is it still supposed to be open? I see the same behaviour in 2.4.20, Using the "proxy-nokeepalive" to disable keepalives definitely is not the best way to solve this and causes other issues.I am seeing this with Jetty(9.3.3.v20150827) as backend.(In reply to vin01 from)If you're hitting a keepalive race, you can also set smax=0 and a ttl lower then your backend keepalive timeout. There's also proxy-initial-not-pooled. Either way, a lot more detail is needed in a report.	104.0	id=53540	11	True	False	alexandru.ersenie	1
id=43586	REOPENED	None	Ant	Optional Tasks (	1.7.0	Other other	P1 normal	Ant Notifications List	2007-10-10 08:22 UTC by	Rahul Mahboobani	2010-03-25 15:15 UTC (	5 users	I am unable to run my JUNIT tests in a forked ANT v1.7.0 I am running on Linux.It keeps crashing with ->java.io.FileNotFoundException: /home/PRTI/apache-ant-1.7.0/junitvmwatcher527107969.properties I have read a "fix" in the fixed list () saying -><junittask/> created junitvmwatcher*.properties files but did not close and delete them. I cannot find any nightly builds at all onIs there any build of ant v1.7.0 that FIXES this bug, please?Please help, this is a blocker.	this isnt a bug in ant; I can assure you it isnt possible for ant to be releasedwith junit not working on linux, as it would fail its own tests.you are mixing ant versions. -Look at the classpath you run the tests on, make sure there arent ant 1.6.xjars on there-delete any RPM/deb installed ant versionsThere are no ant 1.6.x jars in my classpath. But, I have lots of jars (commons, jboss, axis, etc.etc.), could it be that one of these is carrying old version ant classes inside, causing this?It will be very very tedious for me to look into each and every jar and find out, so any advice greatly appreciatedI am running on Linux, and I assure you I have no other versions of ant, I just took the 1.7.0 tar.gz from apache and installed it. I have checked thoroughly but this vmwatcher thing is blocking me entirely.Please could you suggest a workaround?I have run out of time, this is urgent, please help.funnily enough, I had to track down duplicate classes in a different programlast week, and now know how to do it. It's pretty easy. If you add the jvm option -verbose:class to <junit> you can find where classesget loaded on; capture the output and search through it fororg.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner<junit fork="yes"> <jvmarg value="-verbose:class"/>Thanks for your advice, but the output only shows the following once -> [junit] [Loaded org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner] [junit] [Loaded org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner$TeeOutputStream]I do not see any duplicate classes, neither does the above say which jar its loaded from.I do have junit/swingui/TestRunner.class in junit.jar, which is in my classpath. (if I remove junit.jar, ant junit task cannot find junit.framework.Test so it fails)I do not see any other duplicate classes related to ant, all the classes are coming from my apache-ant-1.7.0/lib.Please could you advise some more, there has to be something! I am thinking of deleting everything and starting from scratch, but I don't see how that could help.(In reply to)I also have the same issue, followed the advice on mailing list, but problem is not solved.I also built new version of Ant from SVN repository ANT_17_BRANCH (revision no. 583390), now using new version of Ant junitvmwatcherxxx.properties error is gone, but it hangs while running junit tests unless I terminate jave.exe using takmanager.Well, I reverted back to Ant v1.6.5 and everything works fine.Yes, if you are still using JUnit 3.x, but my tests are using JUnit 4.x, so for me there is no option but to use Ant 1.7. I am facing this problem on Windows XP.I too ran into this problem and did a bit of digging. As Steve pointed out, the source of the problem is having an older version ofthe Ant libraries in the classpath. I ran my tests with Ant's debug switch [-d] and noticed that theJava13CommandLauncher is constructing a classpath with the Ant 1.7 librariesappended to the classpath as opposed to being prepended. That is a [relativelyminor] bug IMHO.Unless there is a valid use case I am overlooking, I believe it would make thetool a bit more robust to promote the ant libs to the front of the list.Regards,jbHello together,i have a similar Problem only under Windows.Software:- Apache Ant version 1.7.0 compiled on December 13 2006- JUnit4.1- java version "1.5.0_11"Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_11-b03)Java HotSpot(TM) Client VM (build 1.5.0_11-b03, mixed mode)Same Message:[junit] java.io.FileNotFoundException: ***\junitvmwatcher1492363537.properties (Das System kann die angegebene Datei nicht finden)It seems that the junit-task fails if the classpath contains more than about 12250 classes. A test with about 12200 classes runs without failures.RegardsI have stumbled on what I believe may be another common cause of this "bug". This can happen if Ant's junit task is run with fork="yes" in default forkmode, and you pass in a jvmarg element which has an empty string as value. We have a common Ant framework script used by multiple projects, with properties driving the specific project's behavior. E.g. our build uses this fragment: <jvmarg value="junit.jvmarg.project"/> <jvmarg value="junit.jvmarg.internal"/>where the latter is always defined (by the framework) and the former can be supplied by users. When we split the jvmargs into these two elements, of course there was a reason, and at the time both properties had values. Subsequent usages sometimes broke, because the client project did not provide a property definition of "junit.jvmarg.project" with a non-empty value (the project's definition being seen first, the framework's definition having the empty string as default value).That was a recipe for the issue reported in this bug. The specific Ant/Junit problem can be seen when running Ant with the -debug option, producing output like this (---scrubbed---): [junit] Executing '/usr/java/jdk1.6.0_07/jre/bin/java' with arguments: [junit] '' [junit] '-Dcom.foo=NONE [junit] '-classpath' [junit] --- classpath elements here --- [junit] 'org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner' [junit] --- Test class here --- [junit] 'filtertrace=true' [junit] 'haltOnError=false' [junit] 'haltOnFailure=false' [junit] 'formatter=org.apache.tools.ant.taskdefs.optional.junit.SummaryJUnitResultFormatter' [junit] 'showoutput=false' [junit] 'outputtoformatters=true' [junit] 'logtestlistenerevents=true' [junit] 'formatter=org.apache.tools.ant.taskdefs.optional.junit.XMLJUnitResultFormatter,/---Test xml file here --- [junit] 'crashfile=/---some-path-here---/junitvmwatcher602965515.properties' [junit] 'propsfile=/---some-path-here---/junit1771756651.properties' [junit] [junit] The ' characters around the executable and arguments are [junit] not part of the command.Note the empty string on the line immediately after the Executing. That is the empty string contributed by the first jvmarg element, from the empty-string-valued "junit.jvmarg.project" property. The line with "-Dcom.foo=NONE" was contributed by the second jvmarg element, the "junit.jvmarg.internal" having this (phony) value. The JVM crashed because of this empty (1st) argument. If we define a non-empty value for "junit.jvmarg.project" the problem goes away (the empty line being replaced by something the JVM will swallow, like "-Djunk=ignore". The underlying Ant problem (yes I think there is one) is that it doesn't check for empty arguments like this before running the JVM. I think it should do so, because the consequences are so pernicious and difficult to debug, and their seem to be no benefit. Specifically, I suggest the CommandlineJava class (instances of which are used by JUnitTask.executeAsForked() to represent the forked JVM command to run) should, somewhere in the conversion of the constituents of the command to a List of command elements (Strings), if there is an empty string it should perhaps be skipped over. Or, more narrowly, te Java13CommandLauncher inner class could deal with the issue since it's that launcher that seems to be the last place to do so. Furthermore, it seems that some improved error handling could be used here. There are no failures detected by the Execute nor JUnitTask classes in running the JVM which summarily exits. But (as is seen at the top of this bug) neither is there valid information about a JVM "crash" in the junitvmwatcher*.properties file (which doesn't exist). We simply see an exception from Ant with no hint to the uninitiated (and a lot of debugging for those who may be).Just as another point of reference, we encountered this problem as well running - Ant 1.7.0 - Maven Ant Tasks 2.0.9 - Java 1.6.0_07 - Windows XPThe only apparent workaround was to NOT fork the junits.That's interesting. I could imagine stripping empty jvm args everywhere makes sense, in which case it is a change we'd make to the Java task, which would stop this problem (and similar) arising.I have the same problem. I use jdk_1.5.0_11 apache-ant-1.0.7 and solaris 10 SPARC.The same build process don't fail in windows XP.I need set fork="true" because i need use more than one jvm to same script.When will be solve this bug?? It's important for me.ThanksThis bug bit me because in my <junit> element I had a <test> element that used a name attribute that was incorrect. The resulting FNFE error, that junitjvmwatcherXXXXX.properties was not found, was a huge red herring. Comments I found about it being caused by versionitis took me way down wrong paths.Thanks toabove, I finally saw this in my verbose ant output:(---scrubbed---):[junit] Executing '/System/Library/Frameworks/JavaVM.framework/Versions/1.5.0/Home/bin/java' with arguments: [junit] '-ea' [junit] '-classpath' [junit] <classpath here> [junit] 'org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner' [junit] 'null' [junit] 'filtertrace=true' [junit] 'haltOnError=true' [junit] 'haltOnFailure=true' [junit] 'formatter=org.apache.tools.ant.taskdefs.optional.junit.SummaryJUnitResultFormatter' [junit] 'showoutput=false' [junit] 'outputtoformatters=true' [junit] 'logtestlistenerevents=true'...Note the 'null' about 5 lines down. It took me a while to figure out that it should have held the name of the test class and that the name I had provided was wrong. Far too costly in time to figure that out! But maybe someone else can benefit from this info. Thank you, Gerry Plummer. Your post led me to the solution.Createdstrips null and empty args from command lineI've attached a patch based on comments by Gerry and Steve which prevents this somewhat cryptic error when running tests in a forked JVM: [junit] Exception in thread "main" java.lang.NoClassDefFoundError: [junit] Caused by: java.lang.ClassNotFoundException: [junit] at java.net.URLClassLoader$1.run(URLClassLoader.java:200) [junit] at java.security.AccessController.doPrivileged(Native Method) [junit] at java.net.URLClassLoader.findClass(URLClassLoader.java:188) [junit] at java.lang.ClassLoader.loadClass(ClassLoader.java:307) [junit] at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) [junit] at java.lang.ClassLoader.loadClass(ClassLoader.java:252) [junit] at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320) [junit] Could not find the main class: . Program will exit.The fix is indeed to strip null/empty args. I considered throwing a BuildException but figured that might break too many existing builds that have other option arguments in positions other than the first.Createddetect invalid test name values (null, empty string, "null") and failThis patch adds fail-fast behavior with a hopefully helpful error message when the build script declares a <test/> element with an invalid test name. This addresses the problem described by A Wilson in.(In reply to)It's in with svnwith some minor modifications* I've created an AntUnit test from your JUnit test (a matter of taste, really)* String#isEmpty is a JDK 1.6 method while Ant is supposed to be JDK 1.4 compatible.I'll look into your patchlater.(In reply to)that would apply to	19.0	id=42558	12	False	False	mangst	1
id=1541	REOPENED	None	Xerces-J	DOM (	1.3.1	PC All	P1 major	Xerces-J Developers Mailing List	2001-04-26 11:10 UTC by	Anthony Roy	2004-11-16 19:05 UTC (	1 user	Even in very small documents, a TreeWalkerImpl created to find nodes of a specific name can take arbitrarily long amounts of time. Also, in larger documents, similar filters can cause StackOverflowError's.I have included source for a small test file below that can be used to reproduce the bug. In it, an xml document is created and passed to the TreeWalker. A filter is created that matches only nodes with a given name. TreeWalker.nextNode() begins to take arbitrarily long amounts of time for even small documents (~60 seconds for a 3/3/2 (~20) node document). This only occurs when there is a low number of nodes that match the filter.In addition, a wider filter (for example two tags) can cause a StackOverflowError on large documents (e.g. 14/13/11 (2000+) nodes).Running the following code will result in the output:rootheadernullapproximately 68.098 secondsChanging the size of the document to 14/13/11, and setting the nodefilter to use FILTER_TAGS_B will result in the following output:rootheaderjava.lang.StackOverflowErrorCode:import org.w3c.dom.Document;import org.w3c.dom.Node;import org.w3c.dom.traversal.TreeWalker;import org.apache.xerces.dom.TreeWalkerImpl;import org.w3c.dom.traversal.NodeFilter;import org.apache.xerces.parsers.DOMParser;import org.xml.sax.InputSource;import java.io.StringReader;public class TestWalker { // this many children of root node private static final int NUM_BRANCH = 3; // this many children of each branch node private static final int NUM_TWIG = 3; // this many children for each twig node private static final int NUM_LEAF = 2; private static final String[] FILTER_TAGS_A = {"rootheader"}; private static final String[] FILTER_TAGS_B = {"rootheader", "rootfooter"}; public TestWalker() { try { runTest(parseXML(createXML())); } catch (StackOverflowError e) { System.out.println(e); } } // end constructor() // Creates a TreeWalker with a simple name filter and loops // through each node returned until the document is finished. private void runTest(Document doc) { // Create TreeWalker with inline filter. // The filter simply looks for a node matching the filter node. TreeWalker walker = new TreeWalkerImpl(doc, Node.ELEMENT_NODE, new NodeFilter() { protected String tag[] = FILTER_TAGS_A; public short acceptNode (Node n) { String testName = n.getNodeName(); for (int i=0; i < tag.length; i++) { if (testName.equals(tag[i])) return FILTER_ACCEPT; } return FILTER_SKIP; } // end acceptNode(Node) }, true); // Loop until done. long startTime; long endTime; startTime = System.currentTimeMillis(); Node node = walker.nextNode(); while (node != null) { System.out.println(node.getNodeName()); node = walker.nextNode(); } endTime = System.currentTimeMillis(); System.out.println("null"); double runTime = (double)(endTime - startTime); System.out.println("approximately " + (runTime/1000.0) + " seconds"); } // end runTest() // Parses given String and returns a Document private Document parseXML(String xmlSrc) { // Double check accuracy of created xml: // System.out.println("Source:\n" + xmlSrc); DOMParser parser = new DOMParser(); InputSource in = new InputSource(new StringReader(xmlSrc)); try { parser.parse(in); } catch (Exception e) {e.printStackTrace();System.exit(1);} return parser.getDocument(); } // end parseXML() // Creates a simple Document based on global parameters. // The root has NUM_BRANCH children, each has NUM_TWIG children, // and each of those has NUM_LEAF children. private String createXML() { StringBuffer xmlSrc = new StringBuffer(); xmlSrc.append("<?xml version=\"1.0\"?>\n"); xmlSrc.append("<root><rootheader>Root</rootheader>\n"); for (int x=0; x < NUM_BRANCH; x++) { xmlSrc.append("\t<branch><branchheader>Branch"+x+"</branchheader>\n"); xmlSrc.append("\t\t<resource>Resource"+x+"</resource>\n"); for (int y=0; y<NUM_TWIG; y++) { xmlSrc.append("\t\t<twig><twigheader>Twig"+x+"."+y+ "</twigheader>\n"); for (int z=0; z<NUM_LEAF; z++) xmlSrc.append("\t\t\t<leaf>Leaf"+x+"."+y+"."+z+"</leaf>\n"); xmlSrc.append("\t\t</twig>\n"); } xmlSrc.append("\t</branch>\n"); } xmlSrc.append("<rootfooter>The End</rootfooter>\n"); xmlSrc.append("</root>"); return xmlSrc.toString(); } // end createXML() // Usage: java TestWalker public static void main(String[] args) { TestWalker bob = new TestWalker(); } // end main} // end class TestWalker	Fixed in CVS today. Can you pick up latest Xerces and try it out?CreatedRevised test case showing new failure modes and unresolved failuresCreatedPatch to resolve problems illustrated by TestWalker.javaUnfortunately, Elena's fix created a new problem. Specifically,TreeWalker.firstChild() now returns the nextSibling() of the current node if thecurrent node is a leaf node. I attached a test case (id=250) to illustrate.The test case also takes forever when traversing backwards (eg viapreviousNode()).The cause of the original problem (and the remaining problem withpreviousNode()) is that getNextSibling(Node) and getPreviousSibling(Node) cantraverse all the way up the tree, even when they are called fromgetFirstChild(Node) and getLastChild(Node). I've attached a patch (id=251)which solves the problem by allowing getFirstChild(Node) and getLastChild(Node)to specify the parent node above which getNextSibling(Node) andgetPreviousSibling(Node) should not traverse.Even with my patch, there is still a problem with running out of stack spacewhen traversing very large documents. I see this if I set NUM_BRANCH = 10000 inthe test case. To solve this the class would need to be rewritten to not userecursion.Dean thank you for testing the code and submitting the patch: I did not realize that getFirstChild() is also used in DOM firstChild() method.. I will take a look at your patch.Applied the patch. Might need to revisit in the future.	6.0	id=1541	7	True	False	elena	1
id=57795	REOPENED	None	Apache httpd-2	mod_proxy_wstunnel (	2.4.9	Macintosh All	P1 major	Apache HTTPD Bugs Mailing List	2015-04-07 10:27 UTC by	bob	2017-02-15 18:44 UTC (	2 users	We use mod_proxy_wstunnel on port 80 because we want all traffic over the same port. Http as well as the websocket. In this configuration once connected everything works ok. But sometimes the websocket does not connect and most of the time the websocket does not disconnect. For example on browser refresh, the server does not notice the disconnection and the server needs to ping to know the client is no longer connected.This seems to be a mod_proxy_wstunnel issue because websockets always connect to- and disconnect from the application-server port. The difference between a working configuration and a not working configuration is that there is mod_proxy_wstunnel in between.	I am sorry but i am not able to reproduce the problem.I tried it with 2 different servers and 3 different browsers.Please disregard bugreport. If it happens again i'll file a new one, hopefully with additional information.Using Chrome or Safari on IOS, open a Websocket to Tomcat8 over Apache wstunnel (http connector). We use SockJS in an AngularJS WebApp, on iPad.If you normally close the browser's tab or if you kill (swipe up) the App, then the Javascript App will apparently not fire a websocket close(). In that case, the socket remains open forever between Apache and Tomcat. Apache does not 'see' that the client went away.Other browsers (on desktop OS's) are sending the close, but IOS (and perhap's Android??) not.In such a situation, the subsequent websockets handshakes are routed to this "half closed" socket, and we run into error and a not working XHR fallback. It just completely blocks the service.Running the same use case without Apache and working directly with Tomcat runs fine. Tomcat itself is capable to react to a 'disapeared' client and does close the websocket.Which version of Apache HTTP Server are you using?This (maybe relative) fix was done in 2.4.10: *) mod_proxy_wstunnel: Don't pool backend websockets connections, because we need to handshake every time..There are also more fixes up to latest 2.4.12.I just checked and we use Apache 2.4.7 on Ubuntu 14.04.1 LTS. The description telling "don't pool connections" is the same as our thoughts. We came to the conclusion that the connection is pooled and therefore subsequent handshakes on this connection are logically not working.But you also need to handle a client going away. I mean, not pooling is a good point, but you also need to send a close() to tomcat in case the client goes away without closing the WebSocket.I think the simple use of disablereuse=on for the ProxyPass directive is not enough.Latest versions do close both client and backend connections when done (on any side).(In reply to Gaël Oberson from)Wait, the client is gone away when it closes its connection, mod_proxy_wstunnel has no knowledge of the application (close) semantic.It's exactly what I point out. An IOS Chrome or Safari may be killed (just swipe up) by the user. In that case, je JavaScript thread is instantly terminated, and thus the SockJS client has no chance to close() the socket.Therefore, Apache needs to handle this and become the ability to "be aware" that the client just went away.If you run this scenario without Apache, and directly use Tomcat, you will see that Tomcat has this ability. When the IOS Browser is killed, Tomcat closes the Socket immediately and releases the resources.Thus, it should be possible for mod_proxy_wstunnel to do the same. Otherwise, this will lead to a ressource leak.I don't known much about IOS' browsers but it seems to me that if a socket has no owner it should be closed...And if the application is being killed, it can't even signal the application by any mean, so how Tomcat knows?Thus I think you are hitting the pooled connection issue, and latest version should fix it, can you give it a try? BTW, Tomcat is (running) the application, so it knows about the close semantic (should it be anything other than socket closing), whereas mod_proxy (the *tunnel*) does not, it can't known about any possible/existing websocket application passing through.(In reply to Yann Ylavic from)s/application/browser/It does appear that mod_proxy_wstunnel does not handle half-open connections well. If you setup Tomcat for websockets and then have a client pull their network cable Tomcat will get a "connection forcibly closed by the remote host" error on the session. Do the same test going through mod_proxy_wstunnel to get to Tomcat and the server isn't notified of the connection loss. Worse, the Session.isOpen() method returns true and sendMessage doesn't throw any exceptions! I don't know if this is due to TCP/IP keepalive or what. Maybe the proxy server responds that the connection is still alive, but fails to check with the client.	10.0	id=2529	10	True	False	thomas2.maesing	1
id=53540	REOPENED	None	JMeter	HTTP (	2.7	All All	P1 normal	JMeter issues mailing list	2012-07-12 10:37 UTC by	Alex	2014-10-22 22:08 UTC (	2 users	CreatedCache Manager ProblemIf the resource being accessed in the main http request (with HTTP Cache Manager enabled of course) gets "304 not modified" as an answer, the resources embedded will no longer be retrieved. If we had any javascript objects, or specific dynamic files embedded, these will therefore not be downloaded. Example:Thread Manager with two repetitions;Simple HTTP Request (GET) containing some 104 embedded resourcesFirst request: response code 200 for all resourcesSecond request: response code 304 not modified only for the main request, additional embedded resources are not downloadedThis is very inconvenient, as retrieving each resource separately is not an option, because it cannot be done using concurrent pools, therefore no "cached content" tests can be performed....or is there a solution out there?ThanksAlex	I just implemented a workaround. I added a dynamic parameter at the end of the request, so that the main request will always be newly loaded. It works this way, but it only works because this is a configuration error in Apache...it shouldn't be actually allowed. lucky me:)Bugzilla is not a support forum.Please subscribe to the JMeter user list and ask there; thanks.I see this as bug, not as an enhancement. Stopping on the main request does not dive in the embedded resources, therefore not retrieving possibly new content.If you still think it is a "support" issue, close it.I cannot find what the spec says about this:- If resource is coming from Cache , should embedded resources be checked or not ?After further thinking, I agree that embedded resources should be checked.As in a browser main page would be rendered and each resoutce checked.Sebb, milamber, any thoughts ?JMeter does not cache the page contents, only summary details, so this would require saving more data.Rather than saving the whole page, one could perhaps save just the parsed URLs.This would be the cheapest option.Maybe there should be a Cache Manager option to save the entire page, as that would then be displayable in the Listeners, and would allow post-processors.This would likely have to be a disk cache to avoid memory issues - is there some ASF software to do this?EhCache provides this.	7.0	id=3601	8	True	False	dhananjay.bhandari	1
id=37496	REOPENED	None	Batik - Now in Jira	SVG DOM (	1.5	Other other	P2 normal	Batik Developer's Mailing list	2005-11-14 17:09 UTC by	M.H.	2011-06-29 10:10 UTC (	0 users	On some generated SVGs I get the following exception when trying to include the SVG in a PDF via FOP. The SVG can be displayed correctly with an SVG viewer, so the SVG isn't corrupt. Unfortunately, I can't use the latest Batik as Apache FOP is tied to Batik 1.5 (I tried but then FOP 0.20.5 doesn't work anymore). So I hope to find a workaround for this bug with Batik 1.5:svg graphic could not be built: file:/E:/iComps/amc/reports/7whvGXAN4boGDqySrxtwcfNAqK8eQwthsMhNqZ0ZzrQ=/C_PerfCons_S.svg:28The attribute 'transform' of the element <path> is invalid [de.icomps.amc.bl.CaseReportClientMulti:de.icomps.xml.FOP] org.apache.batik.bridge.BridgeException: file:/E:/iComps/amc/reports/7whvGXAN4boGDqySrxtwcfNAqK8eQwthsMhNqZ0ZzrQ=/C_PerfCons_S.svg:28The attribute 'transform' of the element <path> is invalid at org.apache.batik.bridge.SVGUtilities.convertTransform(SVGUtilities.java:852) at org.apache.batik.bridge.AbstractGraphicsNodeBridge.createGraphicsNode(AbstractGraphicsNodeBridge.java:92) at org.apache.batik.bridge.SVGShapeElementBridge.createGraphicsNode(SVGShapeElementBridge.java:50) at org.apache.batik.bridge.GVTBuilder.buildGraphicsNode(GVTBuilder.java:182) at org.apache.batik.bridge.GVTBuilder.buildComposite(GVTBuilder.java:148) at org.apache.batik.bridge.GVTBuilder.buildGraphicsNode(GVTBuilder.java:188) at org.apache.batik.bridge.GVTBuilder.buildComposite(GVTBuilder.java:148) at org.apache.batik.bridge.GVTBuilder.buildGraphicsNode(GVTBuilder.java:188) at org.apache.batik.bridge.GVTBuilder.buildComposite(GVTBuilder.java:148) at org.apache.batik.bridge.GVTBuilder.build(GVTBuilder.java:120) at org.apache.batik.bridge.SVGImageElementBridge.createSVGImageNode(SVGImageElementBridge.java:328) at org.apache.batik.bridge.SVGImageElementBridge.createGraphicsNode(SVGImageElementBridge.java:118) at org.apache.batik.bridge.GVTBuilder.buildGraphicsNode(GVTBuilder.java:182) at org.apache.batik.bridge.GVTBuilder.buildComposite(GVTBuilder.java:148) at org.apache.batik.bridge.GVTBuilder.build(GVTBuilder.java:76) at org.apache.fop.render.pdf.PDFRenderer.renderSVGDocument(PDFRenderer.java:590) at org.apache.fop.render.pdf.PDFRenderer.renderSVGArea(PDFRenderer.java:549) at org.apache.fop.svg.SVGArea.render(SVGArea.java:98) at org.apache.fop.render.pdf.PDFRenderer.renderForeignObjectArea(PDFRenderer.java:533) at org.apache.fop.layout.inline.ForeignObjectArea.render(ForeignObjectArea.java:89) at org.apache.fop.render.AbstractRenderer.renderLineArea(AbstractRenderer.java:516) at org.apache.fop.layout.LineArea.render(LineArea.java:519) at org.apache.fop.render.AbstractRenderer.renderBlockArea(AbstractRenderer.java:485) at org.apache.fop.layout.BlockArea.render(BlockArea.java:117) at org.apache.fop.render.AbstractRenderer.renderBlockArea(AbstractRenderer.java:485) at org.apache.fop.layout.BlockArea.render(BlockArea.java:117) at org.apache.fop.render.AbstractRenderer.renderAreaContainer(AbstractRenderer.java:451) at org.apache.fop.layout.AreaContainer.render(AreaContainer.java:88) at org.apache.fop.render.AbstractRenderer.renderAreaContainer(AbstractRenderer.java:451) at org.apache.fop.layout.AreaContainer.render(AreaContainer.java:88) at org.apache.fop.render.AbstractRenderer.renderAreaContainer(AbstractRenderer.java:451) at org.apache.fop.layout.AreaContainer.render(AreaContainer.java:88) at org.apache.fop.render.AbstractRenderer.renderAreaContainer(AbstractRenderer.java:451) at org.apache.fop.layout.AreaContainer.render(AreaContainer.java:88) at org.apache.fop.render.AbstractRenderer.renderAreaContainer(AbstractRenderer.java:451) at org.apache.fop.layout.ColumnArea.render(ColumnArea.java:71) at org.apache.fop.render.AbstractRenderer.renderSpanArea(AbstractRenderer.java:100) at org.apache.fop.layout.SpanArea.render(SpanArea.java:94) at org.apache.fop.render.AbstractRenderer.renderBodyAreaContainer(AbstractRenderer.java:368) at org.apache.fop.layout.BodyAreaContainer.render(BodyAreaContainer.java:137) at org.apache.fop.render.AbstractRenderer.renderRegions(AbstractRenderer.java:529) at org.apache.fop.render.pdf.PDFRenderer.renderPage(PDFRenderer.java:904) at org.apache.fop.render.pdf.PDFRenderer.render(PDFRenderer.java:880) at org.apache.fop.apps.StreamRenderer.queuePage(StreamRenderer.java:302) at org.apache.fop.layout.AreaTree.addPage(AreaTree.java:108) at org.apache.fop.fo.pagination.PageSequence.makePage(PageSequence.java:415) at org.apache.fop.fo.pagination.PageSequence.format(PageSequence.java:338) at org.apache.fop.apps.StreamRenderer.render(StreamRenderer.java:262) at org.apache.fop.fo.FOTreeBuilder.endElement(FOTreeBuilder.java:223) at org.apache.xml.serializer.ToXMLSAXHandler.endElement(ToXMLSAXHandler.java:261) at org.apache.xalan.templates.ElemLiteralResult.execute(ElemLiteralResult.java:1399) at org.apache.xalan.transformer.TransformerImpl.executeChildTemplates(TransformerImpl.java:2411) at org.apache.xalan.templates.ElemLiteralResult.execute(ElemLiteralResult.java:1374) at org.apache.xalan.transformer.TransformerImpl.executeChildTemplates(TransformerImpl.java:2411) at org.apache.xalan.transformer.TransformerImpl.applyTemplateToNode(TransformerImpl.java:2281) at org.apache.xalan.transformer.TransformerImpl.transformNode(TransformerImpl.java:1367) at org.apache.xalan.transformer.TransformerImpl.transform(TransformerImpl.java:709) at org.apache.xalan.transformer.TransformerImpl.transform(TransformerImpl.java:1284) at org.apache.xalan.transformer.TransformerImpl.transform(TransformerImpl.java:1262)	You can download the FOP maintenance branch [1]. This is the branch where FOP0.20.5 came from and the code there has been adjusted to compile with Batik 1.6.We simply haven't released that code, but it's said to be stable. If thatdoesn't help you might want to attach a sample so we can easily reproduce yourproblem.[1] svn co***has been marked as a duplicate of this bug. ***This likely isn't an issue any longer.Unfortunately, it occured today again - with FOP 1.0 (that has Batik 1.7):java.lang.RuntimeException: SVG graphic could not be built. Reason: java.lang.RuntimeException: SVG error: file:/c:/temp/0b81065d-1d31-43b4-a94f-57d5453942c9/C_Single_L.svg:-1The attribute "transform" of the element <text> is invalid at org.apache.fop.events.EventExceptionManager.throwException(EventExceptionManager.java:72) at org.apache.fop.events.DefaultEventBroadcaster$1.invoke(DefaultEventBroadcaster.java:175) at $Proxy1.svgNotBuilt(Unknown Source) at org.apache.fop.render.pdf.PDFImageHandlerSVG.handleImage(PDFImageHandlerSVG.java:98) at org.apache.fop.render.intermediate.AbstractIFPainter.drawImage(AbstractIFPainter.java:227) at org.apache.fop.render.intermediate.AbstractIFPainter.drawImage(AbstractIFPainter.java:183) at org.apache.fop.render.intermediate.AbstractIFPainter.drawImageUsingImageHandler(AbstractIFPainter.java:148) at org.apache.fop.render.intermediate.AbstractIFPainter.drawImageUsingDocument(AbstractIFPainter.java:297) at org.apache.fop.render.pdf.PDFPainter.drawImage(PDFPainter.java:204) at org.apache.fop.render.intermediate.IFRenderer.renderForeignObject(IFRenderer.java:1189) at org.apache.fop.render.AbstractRenderer.renderViewport(AbstractRenderer.java:745) at org.apache.fop.render.AbstractPathOrientedRenderer.renderViewport(AbstractPathOrientedRenderer.java:709) at org.apache.fop.render.intermediate.IFRenderer.renderViewport(IFRenderer.java:860) at org.apache.fop.render.AbstractRenderer.renderInlineArea(AbstractRenderer.java:634) at org.apache.fop.render.intermediate.IFRenderer.renderInlineArea(IFRenderer.java:912) at org.apache.fop.render.AbstractRenderer.renderLineArea(AbstractRenderer.java:609) at org.apache.fop.render.AbstractRenderer.renderBlocks(AbstractRenderer.java:544) at org.apache.fop.render.AbstractRenderer.renderBlock(AbstractRenderer.java:581) at org.apache.fop.render.intermediate.IFRenderer.renderBlock(IFRenderer.java:976) at org.apache.fop.render.AbstractRenderer.renderBlocks(AbstractRenderer.java:534) at org.apache.fop.render.AbstractRenderer.renderBlock(AbstractRenderer.java:581) at org.apache.fop.render.intermediate.IFRenderer.renderBlock(IFRenderer.java:976) at org.apache.fop.render.AbstractRenderer.renderBlocks(AbstractRenderer.java:534) at org.apache.fop.render.AbstractPathOrientedRenderer.renderReferenceArea(AbstractPathOrientedRenderer.java:637) at org.apache.fop.render.AbstractRenderer.renderBlock(AbstractRenderer.java:568) at org.apache.fop.render.intermediate.IFRenderer.renderBlock(IFRenderer.java:976) at org.apache.fop.render.AbstractRenderer.renderBlocks(AbstractRenderer.java:534) at org.apache.fop.render.AbstractRenderer.renderBlock(AbstractRenderer.java:581) at org.apache.fop.render.intermediate.IFRenderer.renderBlock(IFRenderer.java:976) at org.apache.fop.render.AbstractRenderer.renderBlocks(AbstractRenderer.java:534) at org.apache.fop.render.AbstractRenderer.renderFlow(AbstractRenderer.java:432) at org.apache.fop.render.AbstractPathOrientedRenderer.renderFlow(AbstractPathOrientedRenderer.java:665) at org.apache.fop.render.AbstractRenderer.renderMainReference(AbstractRenderer.java:411) at org.apache.fop.render.AbstractRenderer.renderBodyRegion(AbstractRenderer.java:345) at org.apache.fop.render.AbstractRenderer.renderRegionViewport(AbstractRenderer.java:292) at org.apache.fop.render.intermediate.IFRenderer.renderRegionViewport(IFRenderer.java:731) at org.apache.fop.render.AbstractRenderer.renderPageAreas(AbstractRenderer.java:265) at org.apache.fop.render.AbstractRenderer.renderPage(AbstractRenderer.java:230) at org.apache.fop.render.intermediate.IFRenderer.renderPage(IFRenderer.java:580) at org.apache.fop.area.RenderPagesModel.addPage(RenderPagesModel.java:114) at org.apache.fop.layoutmgr.AbstractPageSequenceLayoutManager.finishPage(AbstractPageSequenceLayoutManager.java:312) at org.apache.fop.layoutmgr.PageSequenceLayoutManager.finishPage(PageSequenceLayoutManager.java:167) at org.apache.fop.layoutmgr.PageSequenceLayoutManager.activateLayout(PageSequenceLayoutManager.java:109) at org.apache.fop.area.AreaTreeHandler.endPageSequence(AreaTreeHandler.java:238) at org.apache.fop.fo.pagination.PageSequence.endOfNode(PageSequence.java:120) at org.apache.fop.fo.FOTreeBuilder$MainFOHandler.endElement(FOTreeBuilder.java:349) at org.apache.fop.fo.FOTreeBuilder.endElement(FOTreeBuilder.java:177) at org.apache.xalan.transformer.TransformerIdentityImpl.endElement(TransformerIdentityImpl.java:1102) at org.apache.xerces.parsers.AbstractSAXParser.endElement(Unknown Source) at org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanEndElement(Unknown Source) at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source) at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source) at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source) at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source) at org.apache.xerces.parsers.XMLParser.parse(Unknown Source) at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source) at org.apache.xalan.transformer.TransformerIdentityImpl.transform(TransformerIdentityImpl.java:485)	4.0	id=43586	13	False	False	peterreilly	1
id=2529	REOPENED	None	Xerces-J	Schema-Structures (	1.4.1	PC All	P1 blocker	Xerces-J Developers Mailing List	2001-07-09 23:11 UTC by	None	2005-03-20 17:06 UTC (	0 users	Hi,I have found the following problem:A relative URI in a import namespace statement like <xsd:import namespace="" schemaLocation="xlink.xsd"/> is resolved to the URI ofthe XML Source to be parsed and not to the containingXML Schema.RegardsThomas	This is not a bug, this is a default behavior. To overwrite this behavior, implement your own EntityResolver and register it with a parser.Look also at:I have reopened the bug because I have a further question:The resolveEntity(String publicId, String systemId) method is called with the relative systemId.How should this method obtain the absolute URI of the XML Schema which containsthis relative URI?For implementing your suggestion the method should be called with aresolved absolute URI.RegardsThomasThe resolveEntity method will always be called with an absolute URI when you gave the parser a chance to have some absolute base URI for creating absolute URI's from relative ones.If you use parse(String systemId) you should supply an absolute URI.If you use parse(InputSource input) you can use setSystemId at the InputSource to set the absolute URI.What you will most likely do in such EntityResolver is either keeping some hashtables of public and system identifiers to 'real' locations and than return an InputSource created from that 'real' location. For doing this xerces contains an nice base class org.apache.readers.XMLCatalogHandler and the XCatalog that can load such mappings from a file using an xml based format.You can also use the EntityResolver to handle your own vnd.mycompany.myscheme:/.. URI scheme(s) or do both, eg. if you created the InputStream using some application specific procedure that doesn´t map to the URI schemes known to xerces like file:// http://.For resolving relative to absolute URI´s have a look at the java.net.URI class in JDK1.4 andOkay, with your comment it seems to be a problem with the JAXP Implementation.I use the methodpublic void parse(java.io.InputStream is, DefaultHandler dh, java.lang.String systemId)of javax.xml.parsers.SAXParser.The method is called with this systemId:file:/D:/htdocs/xmledi/xml/ssregw_paper_template.xmlAnd the resolveEntity(String publicId, String systemId) Method than is called with the relative URI "ssregw_paper.xsd" which is contained in the InputStream.I have attched the Java sources.Createdclass used for validationCreatedclass used as DefaultHandler	6.0	id=37496	4	False	False	cam	1
id=42387	REOPENED	None	Batik - Now in Jira	GVT (	1.7	PC Linux	P2 normal	Batik Developer's Mailing list	2007-05-10 10:32 UTC by	Thomas Behr	2011-05-18 12:31 UTC (	2 users	After applying the patch submitted for, rendering GraphicsNodes created from a SVGDocument containing gradients will render all gradient-affected areas black.	Createdtest caseA very simple SVG with a gradient to reproduceThanks for reporting and providing the test case! :-)Using the current trunk code I'm unable to reproduce the issue. I believe it was fixed by. Please reopen if you feel that's not the case. ;-)*** This bug has been marked as a duplicate of***CreatedA test case to demonstrate the bugI checked out the most recent batik source code (10.12.2010) and am still facing the problem of not being able to render the gradients when attempting to store my graphics as an SVG image.I have attached a test case which recreates the problem. The image that I am using for the test case can be downloaded from the following link.On the screen, the image is rendered perfectly, but when stored to a file and the file is viewed in a browser (or Inkscape software), the gradients are rendered black.I also encountered this problem when using batik SVG library. I'm creating SVG images and I have to draw SVG icons inside other SVG images. All SVG icons which have gradient patterns are displayed with black areas.I would be really happy if this bug would be fixed. Is it technically difficult?	5.0	id=42137	5	False	False	cam	1
id=3601	REOPENED	None	Xerces-J	Serialization (	1.4.3	PC All	P1 blocker	Xerces-J Developers Mailing List	2001-09-14 01:12 UTC by	Dhananjay	2005-08-25 10:17 UTC (	2 users	Hi, I downloaded Xerces-J-src.1.4.3.zip from-j/ site. I am trying to compile this xerces. But, I dont want to use any of the makefiles that come with source code. I have my own make scripts, in thode I collect names of all the java files in the xerces hierarchy and then run sj on each of those. I am using sj version 3.00.042. I noticed a weird thing abt the compilation of apache/xerces/validators/schema/TraverseSchema.java file. sj just aborts if I try to compile this file. When i took a look at this file, I found that this file is very big (9000 + lines) and it has nested class definitions. is it possible to simplify the this file, or even if it is not, then at least some kind of workaround.Thanks,Dhananjay.	I don't think this has anything to do with Java/Xerces. Maybe some problem inyour thode script. All the source files of xerces-j compile perfectly well. Ya,TraverseSchema is indeed a very big file but, it compiles right.In the new design of xerces2, this file is broken into several small traverses.Maybe your thode scriot works now with this. But, this does not seems to be aBUG in xerces.I am not using any script to compile the code. It is simple sj cpmmand that i am using, and I could reproduce the behaviour with different machines.To reproduce it : 1. Remove all the classes from the build dir. 2. Remove the reference of xerces.jar file in the classpath, if at all it is present. 3. AMek sure that you have sj 3.00.042 verison. 4. Use the ant script with build.compiler set to "Semantic" or sj class wrapper. Since, you have also mentioned that in the new design of xerces 2, the traverseschema.java file is split. Which version is that, and is that version product ready. Meaning, Can I use that release as a production-ready release.	2.0	id=44553	5	False	False	johannes.neubauer	1
id=44553	REOPENED	None	Batik - Now in Jira	SVG DOM (	1.7	PC Linux	P2 normal	Batik Developer's Mailing list	2008-03-06 22:15 UTC by	Cameron McCormack	2008-10-06 23:12 UTC (	1 user	The above URL shows that a path with a single quadratic bezier segment gets a bounding box that includes only the control points, not the whole shape.	Same problem here. Generating svg from java2D with SVGGenerator produces some bounding boxes that are too small (clipPath) for simple paths.I just fixed the quadradic bounds bug (rev 702147).BTW the problem was not using the control points itwas an error in the math calculating the tight bbox(if I used the ctrl points the bounds would have goneto 100,10).To Johannes, I check the similar math for the quadradic caseand it appears correct. Can you post an example likeCameron's? I should also say that I don't think the SVGGraphics2Dclasses do any bbox calculations they just use thevalues that are given to them (i.e. you only get aclip-path if someone sets a clip path). So I'm notsure your bug is really very closely related to thisone. Anyway feel free to reopen (or create a new bug)if you can provide more details.CreatedThe cut text is highlighted with a red sphereYou're right this might be a different bug, anyway I added an attatchement with the generated SVG (enhanced with a little sphere around the problem in inkscape). there is a clipPath set which is too small. It is generated from a JLabel with [Pseudo]-html. CSS-Option was off.	4.0	id=51765	8	False	False	m.s.ganzeboom	1
id=51765	REOPENED	None	Batik - Now in Jira	SVG Rasterizer (	1.7	PC All	P2 normal	Batik Developer's Mailing list	2011-09-05 11:55 UTC by	None	2012-10-04 09:59 UTC (	1 user	Hello,The text-decoration:underline attribute does not work when used inside a flowSpan. I attach a svg file demonstrating the problem.Just rasterize it and you'll see a line appear way above the text.Thanks for your consideration.	CreatedSvg describing underline problemCreatedPng describing underline problemCreatedFixes the incorrect positioning of underline, overline and line-through in flowing text tagsAs the previously attached test case illustrates perfectly, the underline is positioned incorrect. As it seems, it always gets positioned at the top of the defined flowRegion. This is also true for the overline and line-through text decorations.After two to three days of analysing I found that the cause was in the GlyphLayout class in the code painting the text decorations (paintDecorations(...)). The protected methods getUnderlineShape(), getOverlineShape() and getStrikethroughShape() (i.e. SVG calls it line-through), all use the offset property to determine the offset of the text decoration in relation to the text. With regular <text> and <tspan> tags somehow this property gets the right offset values set (probably through manual x, y tag attributes or derived from parent tags) and the text decoration is showed correct. However, with flowing text tags (<flowPara>, <flowSpan>) the offset values do not get set at all (both offset.x and offset.y stay 0.0). That is probably because flowing text elements have their own <flowRegion> defined in which the text flows 'freely'. As a result the offset determined for the text decoration only reaches to the top of the defined flow region. Hence, the text decoration is always shown at the top. To fix this, one could either try to let the GlyphLayout.offset property values get set and take into account the possible multiline flowing text or instead of using the offset property, determine the text decoration offset with help of the available glyph vector. This vector contains all the glyphs (i.e. typographic representation of characters), that are subject to the text decoration. Those glyphs have bounds from which their positions can be determined. And with the other components (line thickness and what not) making up the total offset the correct position of the text decoration can be determined.The latter solution I chose to implement and is attached in the patch. It is implemented such that for regular <text> and <tspan> tags the GlyphLayout.offset property (i.e. the original way) is still used, but for flowing text (i.e. flowPara, flowSpan) the glyph vector is used. That is why the patch also contains changes in FlowTextPainter.java and FlowGlyphLayout.java, because I had to change that instances of both of those classes get loaded when Batik is dealing with flowing text instead of regular text.Looking forward to seeing your comments on this fix and possibly how to commit it to the repository if requested.In reflect to my previous post. I would say that the status of this bug has changed to Resolved: Worksforme.Comment onFixes the incorrect positioning of underline, overline and line-through in flowing text tagsThis patch does not fix everything regarding the positioning of the text decorations. Especially I later noticed that the line-through text decoration was not positioned correctly. That was because I used the max y position of the geometric bounds of the glyph vector. When the text descends beyond the baseline (as it does with characters like 'g', 'j', etc.) the line-through appeared still too low.The patch marked as [CORRECT] fixes this by not using the max y position of the geometric bounds of the glyph vector, but employing the y position of the first glyph in the glyph vector. That is basically the baseline of the glyph vector as I figure it. Results in tests confirm that (see patch test case svg).CreatedFixes the incorrect positioning of underline, overline and line-through in flowing text tagsThis is the correct patch (previous one removed). The text-decorations were still positioned incorrectly if text descended. In this patch the usage of the max y position of the geometric bounds of the glyph vector has been changed in employing the y position of the first glyph in the glyph vector. That is basically the baseline of the glyph vector as I figured it. Results in tests confirm that (see patch test case svg).CreatedSVG test case file to test the patch for this bug(In reply to)Thanks for the patch submission, Mario! :-)Again, having a patch available doesn't mean the issue's fixed: a committer [1] familiar with the code (Thomas?) still needs to review it and integrate it into the mainstream code. Best way to flag this is using the "PatchAvailable" in the keywords, as has been done. I'm therefore reopening the issue.[1]	8.0	id=42387	6	False	False	helder.magalhaes	1
id=24095	REOPENED	None	Apache httpd-2	mpm_winnt (	2.2.13	PC All	P2 critical	Apache HTTPD Bugs Mailing List	2003-10-24 12:21 UTC by	Jean Daniel TOULY	2009-10-19 13:28 UTC (	2 users	We're using the following configuration :APACHE 2.047PHP 4.3.3HP PROLIANT DL380WINDOWS 2000 ADVANCED SERVER (Last SPs and FIXs)No LOCAL PROXY Local NETWORK (just a switch 100Mb between the server and the client)No ANTIVIRUS on the local machineDescription :This problem has been described in 14704 and 12340. None of the solutions seems to workMaxRequestsPerChild is set to 0.The CHILD PROCESS dies and then is recreated. You can see that in the log and with the PROCESS folder of the TASK MANGER. Once restarted, APACHE become instable and sometimes needs to be kill manually.The dealy to reproduce the problem is not fixed. We use a ROBOT to send HTTP requests (WAPT) to reproduce it more quickly.We've reproduce the problem with APACHE 2.0.40 and PHP 2.3 on two diffrents PCs.Nothing in the server's event log. Nothing int he PHP error LOG.Here's what we have in ERROR.LOG :[Fri Oct 24 12:45:31 2003] [debug] .\server\mpm\winnt\child.c(669): Child 2644: Worker thread 29 starting.[Fri Oct 24 12:45:42 2003] [notice] Parent: child process exited with status 3221225477 -- Restarting.[Fri Oct 24 12:45:42 2003] [notice] Parent: Created child process 4488[Fri Oct 24 12:45:42 2003] [debug] .\server\mpm\winnt\mpm_winnt.c(483): Parent: Sent the scoreboard to the child[Fri Oct 24 12:45:42 2003] [notice] Child 4488: Child process is running[Fri Oct 24 12:45:42 2003] [info] Parent: Duplicating socket 256 and sending it to child process 4488[Fri Oct 24 12:45:42 2003] [debug] .\server\mpm\winnt\mpm_winnt.c(404): Child 4488: Retrieved our scoreboard from the parent.[Fri Oct 24 12:45:42 2003] [debug] .\server\mpm\winnt\mpm_winnt.c(601): Parent: Sent 1 listeners to child 4488[Fri Oct 24 12:45:42 2003] [debug] .\server\mpm\winnt\mpm_winnt.c(560): Child 4488: retrieved 1 listeners from parent[Fri Oct 24 12:45:42 2003] [notice] Child 4488: Acquired the start mutex.[Fri Oct 24 12:45:42 2003] [notice] Child 4488: Starting 50 worker threads.[Fri Oct 24 12:45:42 2003] [debug] .\server\mpm\winnt\child.c(669): Child 4488: Worker thread 0 starting.	Same problem for me: Apache 2.0.49, PHP5RC1:[Wed Apr 21 14:53:37 2004] [notice] Parent: child process exited with status 3221225477 -- Restarting.[Wed Apr 21 14:53:37 2004] [notice] Parent: Created child process 456[Wed Apr 21 14:53:37 2004] [debug] mpm_winnt.c(479): Parent: Sent the scoreboard to the child[Wed Apr 21 14:53:38 2004] [notice] Child 456: Child process is running[Wed Apr 21 14:53:38 2004] [debug] mpm_winnt.c(400): Child 456: Retrieved our scoreboard from the parent.[Wed Apr 21 14:53:38 2004] [info] Parent: Duplicating socket 188 and sending it to child process 456[Wed Apr 21 14:53:38 2004] [debug] mpm_winnt.c(597): Parent: Sent 1 listeners to child 456[Wed Apr 21 14:53:38 2004] [debug] mpm_winnt.c(556): Child 456: retrieved 1 listeners from parent[Wed Apr 21 14:53:38 2004] [notice] Child 456: Acquired the start mutex.[Wed Apr 21 14:53:38 2004] [notice] Child 456: Starting 100 worker threads.I had 2.0.48 running with PHP5RC1 fine, not sure what happened in the mean time. Zonealarm is not installed, virus scanner is running but cannot be disabled.There's not much anyone can do with this - it could be a PHP bug, it could be anhttpd bug. To track a problem down further, we need either:a) a backtrace from the crash, orb) a reliable reproduction case.From:Hi,We are experiencing problems with Apache 2.2.9 on Windows 2000 ServerSP4. Our site handles around 100,000 hits per day.The problems started when we upgraded from 2.0.47.The server restarts several times a day with the following message (fromerror.log):[notice] Parent: child process exited with status 3221225477 On some occassions, httpd.exe (the child process) is using 100% CPU andcannot be killed, and the server has to be rebooted.The access logs for the relevant times have nothing obvious in common,and there are no entries in the windows event logs.We have been unable to reproduce the fault on demand.Though there are many postings about the error message, none of themseem to apply to our situation, because we are not using perl, python,php, mysql, or ldap authentication.Apache is configured to serve html pages using mod-include, and we areusing mod-proxy to pass requests to IIS for the dynamic pages.Here are the relevant portions of the config file:LoadModule alias-module modules/mod-alias.soLoadModule authz-default-module modules/mod-authz-default.soLoadModule authz-host-module modules/mod-authz-host.soLoadModule authz-user-module modules/mod-authz-user.soLoadModule autoindex-module modules/mod-autoindex.soLoadModule cgi-module modules/mod-cgi.soLoadModule dir-module modules/mod-dir.soLoadModule include-module modules/mod-include.soLoadModule log-config-module modules/mod-log-config.soLoadModule mime-module modules/mod-mime.soLoadModule proxy-module modules/mod-proxy.soLoadModule proxy-http-module modules/mod-proxy-http.so<Directory /> Options FollowSymLinks Includes AllowOverride None</Directory><Directory "c:/foo"> Options Indexes FollowSymLinks +Includes AllowOverride None Order allow,deny Allow from all</Directory>ErrorLog "logs/error.log"LogLevel warn<IfModule log-config-module> LogFormat "%h %l %u %t "%r" %>s %b "%{Referer}i""%{User-Agent}i"" combined LogFormat "%h %l %u %t "%r" %>s %b" common <IfModule logio-module> # You need to enable mod-logio.c to use %I and %O LogFormat "%h %l %u %t "%r" %>s %b "%{Referer}i""%{User-Agent}i" %I %O" combinedio </IfModule> CustomLog "logs/access.log" common</IfModule><Directory "C:/Apache2.2/cgi-bin"> AllowOverride None Options None Order allow,deny Allow from all</Directory>DefaultType text/plain<IfModule mime-module> TypesConfig conf/mime.types AddType application/x-compress .Z AddType application/x-gzip .gz .tgz AddOutputFilter INCLUDES .html</IfModule>#Added because of error: winnt-accept: Asynchronous AcceptEx failed. inerror.logEnableMMAP offEnableSendfile offWin32DisableAcceptEx#Added because of error: [warn] Server ran out of threads to serverequests. Consider raising the ThreadsPerChild setting<IfModule mpm-winnt-module> ThreadsPerChild 250 MaxRequestsPerChild 0</IfModule>AddCharset UTF-8 .utf8AddDefaultCharset UTF-8#IP addresses, ports, and file locations have been anonymized.<VirtualHost 1.2.3.4:80> ServerName www.dummy-host.com ServerAdminDocumentRoot "c:/foo" DirectoryIndex index.html "/site-files/pages/Home.html"=09 ErrorLog C:/Logs/foo-error.log CustomLog "|C:/Logs/bin/rotatelogs.exeC:/Logs/foo-access-%Y%m%d.log 86400" combined =20 ProxyPass /someservice/ProxyPass /someotherservice/=20 <directory "c:/foo/redirectme">=20 RedirectMatch 301 /.*</directory> =20</VirtualHost>NameVirtualHost 1.2.3.5:80<VirtualHost 1.2.3.5:80> ServerName www.anotherdummyhost.com ServerAdminErrorLog C:/Logs/bar-error.log CustomLog "|C:/Logs/bin/rotatelogs.exeC:/Logs/bar-access-%Y%m%d.log 86400" combined ProxyPass /someservice/ProxyPass /someotherservice/<Proxy *> Order Deny,Allow Allow from all </Proxy>=20</VirtualHost><VirtualHost 1.2.3.5:80> ServerName www.yetanotherdummyhost.com ServerAdminErrorLog C:/Logs/foobar-error.log CustomLog "|C:/Logs/bin/rotatelogs.exeC:/Logs/foobar-access-%Y%m%d.log 86400" combined RedirectMatch permanent .*</VirtualHost>I got a stack back trace from the most recent crashes, and the faultingthread details are shown below:Application exception occurred: App: (pid=3D896) When: 19/11/2008 @ 17:41:21.875 Exception number: c0000005 (access violation)State Dump for Thread Id 0xe88eax=3D07aec068 ebx=3D00000000 ecx=3D00000000 edx=3D00001f50 esi=3D07aec068edi=3D00000000eip=3D6eec72df esp=3D0498fd1c ebp=3D0498fd2c iopl=3D0 nv up ei pl z=r napo nccs=3D001b ss=3D0023 ds=3D0023 es=3D0023 fs=3D0038 gs=3D0000efl=3D00000246function: apr-allocator-free 6eec72d0 55 push ebp 6eec72d1 8bec mov ebp,esp 6eec72d3 51 push ecx 6eec72d4 53 push ebx 6eec72d5 56 push esi 6eec72d6 8b750c mov esi,[ebp+0xc]ss:04d59c12=3D00000000 6eec72d9 57 push edi 6eec72da 8b7d08 mov edi,[ebp+0x8]ss:04d59c12=3D00000000 6eec72dd 33db xor ebx,ebxFAULT ->6eec72df 8b470c mov eax,[edi+0xc]ds:003c9ee6=3D???????? 6eec72e2 85c0 test eax,eax 6eec72e4 7406 jz apr-socket-data-get+0x3c(6eecc2ec) 6eec72e6 50 push eax 6eec72e7 e8b4f8ffff call apr-thread-mutex-lock(6eec6ba0) 6eec72ec 8b07 mov eax,[edi]ds:00000000=3D???????? 6eec72ee 8b4f04 mov ecx,[edi+0x4]ds:003c9ee6=3D???????? 6eec72f1 8b5708 mov edx,[edi+0x8]ds:003c9ee6=3D???????? 6eec72f4 89450c mov [ebp+0xc],eaxss:04d59c12=3D00000000 6eec72f7 894d08 mov [ebp+0x8],ecxss:04d59c12=3D00000000 6eec72fa 8b06 mov eax,[esi]ds:07aec068=3D00000000 6eec72fc 8b4d08 mov ecx,[ebp+0x8]ss:04d59c12=3D00000000*!apr-bucket-flush-create=200498FD60 6EE6105C 0700C358 00000000 0498FEDC 6FF09FF1!apr-brigade-cleanup=200498FD70 6FF09FF1 0700C358 07003200 07003200 062F5290!apr-brigade-destroy=200498FEDC 6FF19CC2 062F56D8 0700C358 07022378 0498FF0C!ap-get-request-note=200498FEF0 6FF0E9DC 062F56D8 07003200 07022378 07022378 !ap-pass-brigade=200498FF0C 6FF0E933 07022378 062F5290 0498FF34 6FF0A87C !ap-die=200498FF1C 6FF0A87C 07022378 00000000 0042A6F0 062F5290 !ap-die=200498FF34 6FF04D21 062F5290 062F5290 062F5290 000000DE!ap-get-request-note=200498FF4C 6FF04FD3 062F5290 004E3BC0 0498FF7C 6FF1D2DC!ap-run-process-connection=200498FF5C 6FF1D2DC 062F5290 026FA780 00000000 026FA570!ap-process-connection=200498FF7C 780085BC 062F5288 00000000 78008532 026FA570!ap-regkey-value-remove=200498FFB4 7C57B396 026FA570 00000000 78008532 026FA570 !endthreadex=200498FFEC 00000000 00000000 00000000 00000000 00000000 kernel32!lstrcmpiWApplication exception occurred: App: (pid=3D3392) When: 19/11/2008 @ 18:06:06.796 Exception number: c0000005 (access violation)State Dump for Thread Id 0x4fceax=3D0047c1a0 ebx=3D02af92d8 ecx=3D00000000 edx=3D0047c1a0 esi=3D004fd3c8edi=3D05b0592ceip=3D6ee6101e esp=3D03b0fd58 ebp=3D03b0fd60 iopl=3D0 nv up ei ng n=z acpo cycs=3D001b ss=3D0023 ds=3D0023 es=3D0023 fs=3D0038 gs=3D0000efl=3D00000297function: apr-brigade-cleanup 6ee61007 57 push edi 6ee61008 8b7004 mov esi,[eax+0x4]ds:00846086=3D???????? 6ee6100b 8d7804 lea edi,[eax+0x4]ds:00846086=3D???????? 6ee6100e 3bf7 cmp esi,edi 6ee61010 7426 jz apr-memcache-multgetp+0x368(6ee69b38) 6ee61012 8b0e mov ecx,[esi]ds:004fd3c8=3D00000000 6ee61014 8b5604 mov edx,[esi+0x4]ds:008c72ae=3D00000000 6ee61017 890a mov [edx],ecxds:0047c1a0=3D00000000 6ee61019 8b4604 mov eax,[esi+0x4]ds:008c72ae=3D00000000 6ee6101c 8b0e mov ecx,[esi]ds:004fd3c8=3D00000000FAULT ->6ee6101e 894104 mov [ecx+0x4],eaxds:003c9ee6=3D???????? 6ee61021 8b4618 mov eax,[esi+0x18]ds:008c72ae=3D00000000 6ee61024 8b5608 mov edx,[esi+0x8]ds:008c72ae=3D00000000 6ee61027 50 push eax 6ee61028 ff520c call dword ptr [edx+0xc]ds:00846086=3D???????? 6ee6102b 56 push esi 6ee6102c ff561c call dword ptr [esi+0x1c]ds:008c72ae=3D00000000 6ee6102f 8b37 mov esi,[edi]ds:05b0592c=3D004fd3c8 6ee61031 83c408 add esp,0x8 6ee61034 3bf7 cmp esi,edi 6ee61036 75da jnz apr-posix-perms2mode+0x1c2(6ee67012) 6ee61038 5f pop edi*03B0FEDC 6FF19CC2 05AF8D78 05B05928 05B07930 03B0FF0C!ap-get-request-note=2003B0FEF0 6FF0E9DC 05AF8D78 05AFCC80 05B07930 05B07930 !ap-pass-brigade=2003B0FF0C 6FF0E933 05B07930 05AF8930 03B0FF34 6FF0A87C !ap-die=2003B0FF1C 6FF0A87C 05B07930 00000000 0042A6F0 05AF8930 !ap-die=2003B0FF34 6FF04D21 05AF8930 05AF8930 05AF8930 000000BD!ap-get-request-note=2003B0FF4C 6FF04FD3 05AF8930 004E1958 03B0FF7C 6FF1D2DC!ap-run-process-connection=2003B0FF5C 6FF1D2DC 05AF8930 02AF7200 00000000 0071F730!ap-process-connection=2003B0FF7C 780085BC 05AF8928 00000000 00000000 0071F730!ap-regkey-value-remove=2003B0FFB4 7C57B396 0071F730 00000000 00000000 0071F730 !endthreadex=2003B0FFEC 00000000 00000000 00000000 00000000 00000000 kernel32!lstrcmpiWApplication exception occurred: App: (pid=3D2988) When: 19/11/2008 @ 22:38:41.250 Exception number: c0000005 (access violation)State Dump for Thread Id 0x67ceax=3D004981c0 ebx=3D00000000 ecx=3D00000000 edx=3D00001f50 esi=3D004981c0edi=3D00000000eip=3D6eec72df esp=3D03b8fd1c ebp=3D03b8fd2c iopl=3D0 nv up ei pl z=r napo nccs=3D001b ss=3D0023 ds=3D0023 es=3D0023 fs=3D0038 gs=3D0000efl=3D00000246function: apr-allocator-free 6eec72d0 55 push ebp 6eec72d1 8bec mov ebp,esp 6eec72d3 51 push ecx 6eec72d4 53 push ebx 6eec72d5 56 push esi 6eec72d6 8b750c mov esi,[ebp+0xc]ss:03f59c12=3D???????? 6eec72d9 57 push edi 6eec72da 8b7d08 mov edi,[ebp+0x8]ss:03f59c12=3D???????? 6eec72dd 33db xor ebx,ebxFAULT ->6eec72df 8b470c mov eax,[edi+0xc]ds:003c9ee6=3D???????? 6eec72e2 85c0 test eax,eax 6eec72e4 7406 jz apr-socket-data-get+0x3c(6eecc2ec) 6eec72e6 50 push eax 6eec72e7 e8b4f8ffff call apr-thread-mutex-lock(6eec6ba0) 6eec72ec 8b07 mov eax,[edi]ds:00000000=3D???????? 6eec72ee 8b4f04 mov ecx,[edi+0x4]ds:003c9ee6=3D???????? 6eec72f1 8b5708 mov edx,[edi+0x8]ds:003c9ee6=3D???????? 6eec72f4 89450c mov [ebp+0xc],eaxss:03f59c12=3D???????? 6eec72f7 894d08 mov [ebp+0x8],ecxss:03f59c12=3D???????? 6eec72fa 8b06 mov eax,[esi]ds:004981c0=3D00000000 6eec72fc 8b4d08 mov ecx,[ebp+0x8]ss:03f59c12=3D????????*!apr-bucket-flush-create=2003B8FD60 6EE6105C 07593278 00000000 03B8FEDC 6FF09FF1!apr-brigade-cleanup=2003B8FD70 6FF09FF1 07593278 07586110 07586110 05BB0B48!apr-brigade-destroy=2003B8FEDC 6FF19CC2 05BB0F90 07593278 07583228 03B8FF0C!ap-get-request-note=2003B8FEF0 6FF0E9DC 05BB0F90 07586110 07583228 07583228 !ap-pass-brigade=2003B8FF0C 6FF0E933 07583228 05BB0B48 03B8FF34 6FF0A87C !ap-die=2003B8FF1C 6FF0A87C 07583228 00000000 0042A6F0 05BB0B48 !ap-die=2003B8FF34 6FF04D21 05BB0B48 05BB0B48 05BB0B48 000000BF!ap-get-request-note=2003B8FF4C 6FF04FD3 05BB0B48 004E1AF0 03B8FF7C 6FF1D2DC!ap-run-process-connection=2003B8FF5C 6FF1D2DC 05BB0B48 0237F668 00000000 023421B8!ap-process-connection=2003B8FF7C 780085BC 05BB0B40 00000000 00000000 023421B8!ap-regkey-value-remove=2003B8FFB4 7C57B396 023421B8 00000000 00000000 023421B8 !endthreadex=2003B8FFEC 00000000 00000000 00000000 00000000 00000000 kernel32!lstrcmpiWApplication exception occurred: App: (pid=3D952) When: 20/11/2008 @ 09:43:00.781 Exception number: c0000005 (access violation)State Dump for Thread Id 0xb8ceax=3D646e6957 ebx=3D0047c270 ecx=3D00b4fec8 edx=3D00b4fecc esi=3D00000000edi=3D0047c270eip=3D6ff09c0c esp=3D00b4fd6c ebp=3D00b4fedc iopl=3D0 nv up ei ng n=z acpe cycs=3D001b ss=3D0023 ds=3D0023 es=3D0023 fs=3D0038 gs=3D0000efl=3D00000293function: ap-get-request-note 6ff09bf4 8b4314 mov eax,[ebx+0x14]ds:00846156=3D???????? 6ff09bf7 8945cc mov [ebp+0xcc],eaxss:00f19dc2=3D???????? 6ff09bfa e932020000 jmp ap-get-request-note+0x1251(6ff09e31) 6ff09bff 8b4dd4 mov ecx,[ebp+0xd4]ss:00f19dc2=3D???????? 6ff09c02 8d55f0 lea edx,[ebp+0xf0]ss:00f19dc2=3D???????? 6ff09c05 51 push ecx 6ff09c06 8d4dec lea ecx,[ebp+0xec]ss:00f19dc2=3D???????? 6ff09c09 52 push edx 6ff09c0a 51 push ecx 6ff09c0b 53 push ebxFAULT ->6ff09c0c ff5010 call dword ptr [eax+0x10]ds:64ab083d=3D???????? 6ff09c0f 83c410 add esp,0x10 6ff09c12 83f80b cmp eax,0xb 6ff09c15 0f845a020000 je ap-get-request-note+0x1295(6ff09e75) 6ff09c1b 3d68fd0a00 cmp eax,0xafd68 6ff09c20 0f844f020000 je ap-get-request-note+0x1295(6ff09e75) 6ff09c26 3dd9fc0a00 cmp eax,0xafcd9 6ff09c2b 0f8444020000 je ap-get-request-note+0x1295(6ff09e75) 6ff09c31 3d57fd0a00 cmp eax,0xafd57 6ff09c36 0f8439020000 je ap-get-request-note+0x1295(6ff09e75) 6ff09c3c 3d24fd0a00 cmp eax,0xafd24 6ff09c41 0f842e020000 je ap-get-request-note+0x1295(6ff09e75)*00B4FF1C 6FF0A87C 03982C78 00000000 0042A6F0 03975C80 !ap-die=2000B4FF34 6FF04D21 03975C80 03975C80 03975C80 0000000E!ap-get-request-note=2000B4FF4C 6FF04FD3 03975C80 00425430 00B4FF7C 6FF1D2DC!ap-run-process-connection=2000B4FF5C 6FF1D2DC 03975C80 005A4DE8 00000000 0051F810!ap-process-connection=2000B4FF7C 780085BC 03975C78 00000000 00000000 0051F810!ap-regkey-value-remove=2000B4FFB4 7C57B396 0051F810 00000000 00000000 0051F810 !endthreadex=2000B4FFEC 00000000 00000000 00000000 00000000 00000000 kernel32!lstrcmpiWIt would be much appreciated if anyone could provide some assistance insolving this problem. Please let me know if you need more information.Regards,Simon=20We are using the following configApache 2.2.13Windows 2003 Server NO PHP!!!MaxRequestsPerChild is set to 0.The CHILD PROCESS dies and then is recreated. You can see that in the log and with the PROCESS folder of the TASK MANGER. Once restarted, APACHE become instable and sometimes needs to be kill manually.[Mon Oct 19 17:40:23 2009] [notice] Parent: child process exited with status 3221225477 -- Restarting.httpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerName[Mon Oct 19 17:40:23 2009] [notice] Apache/2.2.9 (Win32) mod_jk/1.2.26 mod_ssl/2.2.9 OpenSSL/0.9.8h configured -- resuming normal operations[Mon Oct 19 17:40:23 2009] [notice] Server built: Jun 13 2008 04:04:59[Mon Oct 19 17:40:23 2009] [notice] Parent: Created child process 2464httpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerNamehttpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerName[Mon Oct 19 17:40:24 2009] [notice] Child 2464: Child process is running[Mon Oct 19 17:40:24 2009] [notice] Child 2464: Acquired the start mutex.[Mon Oct 19 17:40:24 2009] [notice] Child 2464: Starting 500 worker threads.[Mon Oct 19 17:40:24 2009] [notice] Child 2464: Starting thread to listen on port 8443.[Mon Oct 19 17:40:24 2009] [notice] Child 2464: Starting thread to listen on port 443.[Mon Oct 19 17:40:24 2009] [notice] Child 2464: Starting thread to listen on port 80.[Mon Oct 19 17:43:17 2009] [notice] Parent: child process exited with status 3221225477 -- Restarting.httpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerName[Mon Oct 19 17:43:17 2009] [notice] Apache/2.2.9 (Win32) mod_jk/1.2.26 mod_ssl/2.2.9 OpenSSL/0.9.8h configured -- resuming normal operations[Mon Oct 19 17:43:17 2009] [notice] Server built: Jun 13 2008 04:04:59[Mon Oct 19 17:43:17 2009] [notice] Parent: Created child process 3180httpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerNamehttpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerName[Mon Oct 19 17:43:18 2009] [notice] Child 3180: Child process is running[Mon Oct 19 17:43:18 2009] [notice] Child 3180: Acquired the start mutex.[Mon Oct 19 17:43:18 2009] [notice] Child 3180: Starting 500 worker threads.[Mon Oct 19 17:43:18 2009] [notice] Child 3180: Starting thread to listen on port 8443.[Mon Oct 19 17:43:18 2009] [notice] Child 3180: Starting thread to listen on port 443.[Mon Oct 19 17:43:18 2009] [notice] Child 3180: Starting thread to listen on port 80.[Mon Oct 19 17:44:18 2009] [notice] Parent: child process exited with status 3221225477 -- Restarting.httpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerName[Mon Oct 19 17:44:18 2009] [notice] Apache/2.2.9 (Win32) mod_jk/1.2.26 mod_ssl/2.2.9 OpenSSL/0.9.8h configured -- resuming normal operations[Mon Oct 19 17:44:18 2009] [notice] Server built: Jun 13 2008 04:04:59[Mon Oct 19 17:44:18 2009] [notice] Parent: Created child process 2508httpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerNamehttpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerName[Mon Oct 19 17:44:19 2009] [notice] Child 2508: Child process is running[Mon Oct 19 17:44:19 2009] [notice] Child 2508: Acquired the start mutex.[Mon Oct 19 17:44:19 2009] [notice] Child 2508: Starting 500 worker threads.[Mon Oct 19 17:44:19 2009] [notice] Child 2508: Starting thread to listen on port 8443.[Mon Oct 19 17:44:19 2009] [notice] Child 2508: Starting thread to listen on port 443.[Mon Oct 19 17:44:19 2009] [notice] Child 2508: Starting thread to listen on port 80.[Mon Oct 19 17:45:57 2009] [notice] Parent: child process exited with status 3221225477 -- Restarting.httpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerName[Mon Oct 19 17:45:57 2009] [notice] Apache/2.2.9 (Win32) mod_jk/1.2.26 mod_ssl/2.2.9 OpenSSL/0.9.8h configured -- resuming normal operations[Mon Oct 19 17:45:57 2009] [notice] Server built: Jun 13 2008 04:04:59[Mon Oct 19 17:45:57 2009] [notice] Parent: Created child process 5112httpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerNamehttpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerName[Mon Oct 19 17:45:57 2009] [notice] Child 5112: Child process is running[Mon Oct 19 17:45:57 2009] [notice] Child 5112: Acquired the start mutex.[Mon Oct 19 17:45:57 2009] [notice] Child 5112: Starting 500 worker threads.[Mon Oct 19 17:45:58 2009] [notice] Child 5112: Starting thread to listen on port 8443.[Mon Oct 19 17:45:58 2009] [notice] Child 5112: Starting thread to listen on port 443.[Mon Oct 19 17:45:58 2009] [notice] Child 5112: Starting thread to listen on port 80.[Mon Oct 19 17:48:13 2009] [notice] Parent: child process exited with status 3221225477 -- Restarting.httpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerName[Mon Oct 19 17:48:13 2009] [notice] Apache/2.2.9 (Win32) mod_jk/1.2.26 mod_ssl/2.2.9 OpenSSL/0.9.8h configured -- resuming normal operations[Mon Oct 19 17:48:13 2009] [notice] Server built: Jun 13 2008 04:04:59[Mon Oct 19 17:48:13 2009] [notice] Parent: Created child process 4664httpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerNamehttpd.exe: Could not reliably determine the server's fully qualified domain name, using 10.1.1.33 for ServerName[Mon Oct 19 17:48:14 2009] [notice] Child 4664: Child process is running[Mon Oct 19 17:48:14 2009] [notice] Child 4664: Acquired the start mutex.[Mon Oct 19 17:48:14 2009] [notice] Child 4664: Starting 500 worker threads.[Mon Oct 19 17:48:14 2009] [notice] Child 4664: Starting thread to listen on port 8443.[Mon Oct 19 17:48:14 2009] [notice] Child 4664: Starting thread to listen on port 443.[Mon Oct 19 17:48:14 2009] [notice] Child 4664: Starting thread to listen on port 80.	4.0	id=24095	10	True	False	trawick	1
id=35652	REOPENED	None	Apache httpd-2	Runtime Config (	2.0.54	All other	P2 minor	Apache HTTPD Bugs Mailing List	2005-07-07 19:46 UTC by	Alan Pinstein	2011-08-06 23:36 UTC (	0 users	During a recent install of awstats there was a config problem which manifested itself via this error message:Permission denied: /opt/showcase/awstats/.htaccess pcfg_openfile: unable to check htaccess file, ensure it is readableThe error message is a little misleading, as it implies that .htaccess exists and is not readable. There is no .htaccess file in that location. However, /opt/showcase/awstats was chmod 700 and thus httpd couldn't CHECK to see if the .htaccess file was there.The error message would be more helpful if it said:Permission denied: <path> pcfg_openfile: unable to check for existence of htaccess file or read it if it does exist; make sure <path> is readable by httpd.Something along those lines. Since the message implied that .htaccess existed, it really threw me for a loop trying to figure out the problem.	Well, it is not readable if the containing directory is not readable; the erroris not inaccurate per se, I don't think the more verbose error message is muchbetter. The 2.1 FAQ has a section on EPERM errors which might help:Hmm. While I agree that the data in the FAQ entry would've probably helped me solve the problem faster, the actual error I got did not suceed in leading me to that FAQ entry, so that's kindof a moot point.Also, simply check out this google query:,+ensure+it+is+readable%22+awstats&ie=UTF-8&oe=UTF-8to see how many people are having this same problem. I don't think it's unusually stupid of me to have had so much trouble tracking it down, even if it is something that'd be obvious to an expert.It makes perfect sense to check perms along the path if an EXISTING file cannot be read. However, it doesn't really make sense that a NON-EXISTING FILE cannot be read due to permissions. That's why the error message is confusing. While I certainly understand not wanting to get more granular with the error messages (because it'd take more code and performance to supply the more granular info), is there a good reason not to inlcude a link to the FAQ entry in the error message? Or to put some more text in the error message to remind people of what to check in case of permission denied?The simple fact that the answer to this question lies in an FAQ IMHO is ample evidence to support the improvement of the error message! If the error message were better, the FAQ entry would not be necessary...The message is a bit longer now in trunk[Sat Aug 06 19:34:20.087359 2011] [core:crit] [pid 17061:tid 2860051312] (13)Permission denied: [client 127.0.0.1:45326] /tmp/a/.htaccess pcfg_openfile: unable to check htaccess file, ensure it is readable and that '/tmp/a/' is executable.	3.0	id=35652	5	False	False	jorton	1
id=37290	REOPENED	None	Apache httpd-2	mod_dir (	2.0.54	Other other	P2 minor	Apache HTTPD Bugs Mailing List	2005-10-28 15:37 UTC by	Gabor M.	2005-11-02 08:04 UTC (	0 users	ScriptAlias /lists /usr/lib/cgi-bin/mailmanDirectoryIndex listinfolog: attempt to invoke directory as script: /usr/lib/cgi-bin/mailman/	This has always been true, and is easily worked-around usingAlias /lists /usr/lib/cgi-bin/mailman<Directory /usr/lib/cgi-bin/mailman>Options ExecCGISetHandler cgi-script</Directory>I'm not sure if there is a good reason for not allowing mod_autoindex to act onScriptAliased directories (it could be considered a security feature) or if itis just a side-effect of the way ScriptAlias works.(In reply to)[error] [client] attempt to invoke directory as script: /usr/lib/cgi-bin/mailman/Indeed, I believe that worked in 1.3 but SetHandler is more agressive in 2.x(grabbing requests even when they don't map to files). So you need to wrap theSetHandler in <FilesMatch .+> ... </FilesMatch>. (Or just map the extensionsyou need using AddHandler.)File Extensions?We don't use no stinking file extensions!*** This bug has been marked as a duplicate of***mailman use scripts without extensions :(I'm not sure that the original "bug" here is really a dupe. ScriptAlias andDirectoryIndex don't work together for a different reason than SetHandler andDirectoryIndex don't work together.And Zoltan, the <FilesMatch> trick will work regardless of whether or not thefiles have extensions. It is only AddHandler that requires extensions.	7.0	id=37290	5	False	True	slive	1
id=36710	REOPENED	None	Apache httpd-2	mod_cgi (	2.0.54	PC Windows 2000	P2 blocker	Apache HTTPD Bugs Mailing List	2005-09-19 15:58 UTC by	Stefan Krude	2007-01-09 11:12 UTC (	0 users	When a batch file is executed via CGI, the output of programs this batch filecalls is not captured and sent to the client.Example:contents of test.cmd--------------------@echo offecho Content-type: text/plainecho.echo pinging 192.168.0.160ping.exe 192.168.0.160echo done--------------------The host is pinged, thus the ping.exe is executed (I can see the ICMP packetswith Ethereal, but the output of the ping program does not appear in the browserwindow (only the text 'pinging 192.168.2.160' and 'done').	Please ask in a user support forum, if you have difficulty with the numerous examples of CGI scripts that capture the output of external programs.Sorry, I *asked* inbut got no answer. And I haveextensively googled - without result.What 'numerous examples of CGI scripts that capture the output of externalprograms' do you refer to?I don't think my example CGI script is wrong (it runs with IIS which I want toreplace by Apache) and I don't see any mod_cgi directive that could be relatedto my problem.I didn't see that. Maybe if you said windows I just assumed it lay outside my expertise and I had nothing to say. You just capture the output from your child command, and print it back to stdout. So in *X your script could be something like: echo Content-type: text/plain echo echo pinging 192.168.0.160 /path/to/ping.exe 192.168.0.160 | while read line do echo $line done echo doneYes, this is what in *X will work, but in *X the problem doesn't exist, scriptoutput and output from progs called by the script is captured properly.Unfortunately Windows' cmd.exe doesn't know 'while' and 'read' and this would bean odd workaround anyway.I think if capturing from programs invoced by a script doesn't work withWindows, it must be a bug.(In reply to)The bug is actually not an apache bug. Under windows a command script will notecho the output of any program it calls if @echo off is called.I suggest changing the script to:@echo offecho Content-type: text/plainecho.echo pinging 192.168.0.160@echo onping.exe 192.168.0.160@echo offecho doneand leave it there. If that doesn't work, reopen this bug.Of course that doesn't help. @echo off switches off the echoing of the commandline, not the output of any program the script is calling.And as I said before: My scripts (that script with ping.exe is only a testscript) are running well with Microsoft IIS, but I want to migrate to Apache andwould expect that the scripts will work as well as with IIS (if not better ;-).I would suspect that PATH isn't set as expected. May that be?what does echo %PATH% put out? Is there anything in the error log that ping.exeis not found? (Can't test myself right now, sorry).No, the ping is executed, pls. look at my first posting:The problem is *NOT* that the .exe isn't called but that it's output isn'tcaputered and sent to the web client. The output of cmd.exe (the scripting host)*IS* captured but the output of all of it's child processes is not.oh well. Anyway; perhaps I've missed that also - there's still the question ifthere's anything in the error log.(In reply to)Sorry, I hastily threew that together earlier without thinking and I can seeyour problem. It appears that ping opens in a NEW command session rather thanthe Apache cmd session and therefore does not behave as expected.In other words, take this up with M$.This is a bug reporting forum, not a user support forum.Your problem is that on win32, ping never terminates unless you tell it to.Fix your script to perform it's action and -terminate- and perhaps you willhave a working CGI.BillSorry, but that's rubbish. Of course ping.exe will terminate, but ping.exe isnot the matter, it was just an example. Take for another example any hello worldprogram. It's output will not be seen by the client if this program is called bycmd.exe running as an Apache CGI.Can anybody understand the problem?Isn't it a bug if the output of an scripted CGI never reaches the user?Isn't it a bug if the same script runs well with Microsoft IIS?Isn't it a bug if (nearly) the same script runs well with Apache/Unix?Please change your script to invokeping.exe -n 4 192.168.0.160and I think your problem will be solved (just tested locally).Yep, this still is an Issue on Windows XP with Apache 2.2.3.I am Not a Windows Guy, so this took me about a week to figure out.Some additional observations: 1) Internal cmd.exe commands (e.g. "echo", "dir")work fine 2) Re-directing the output of an external program (such as "ping.exe"or "netstat.exe") to a file yields a 0-length file. 3) If you set the "Allowservice to interact with desktop" checkbox in "AdministrativeTools->Services->Apache->properties->Log On", you can see the stdout from yourcommands in a desktop window which closes when the command completes.Also NB that running the script from a command line yields the expected results.The only work-around I could find was to drop back to apache 1.3.34, which workscorrectly. See also	14.0	id=36710	18	False	True	sk_np	1
id=39275	REOPENED	None	Apache httpd-2	mpm_worker (	2.5-HEAD	All All	P2 normal	Apache HTTPD Bugs Mailing List	2006-04-11 21:09 UTC by	Chris Darroch	2009-10-08 15:47 UTC (	2 users	Per this thread on the httpd-dev mailing list:I have been seeing something similar with 2.2.0 using the workerMPM, where with the following settings, I get over 10 child processesinitializing immediately (e.g., up to 15), and then they drop back to10. I see the "server reached MaxClients" message as well rightafter httpd startup, although nothing is connecting yet.<IfModule mpm_worker_module> StartServers 10 MaxClients 150 MinSpareThreads 25 MaxSpareThreads 100 ThreadsPerChild 10</IfModule>In my case, the problem relates to how long the child_init phasetakes to execute. I can "tune" this by raising DBDMin (and DBDKeep)so that mod_dbd attempts to open increasingly large numbers ofDB connections during child_init. With DBDMin set to 0 or 1,all is well; no funny behaviour. Up at DBDMin and DBDKeep at 3,that's when (for me) things go pear-shaped.In server/mpm/worker/worker.c, after make_child() creates achild process it immediately sets the scoreboard parent slot's pidvalue. The main process goes into server_main_loop() and beginsexecuting perform_idle_server_maintenance() every second; thislooks at any process with a non-zero pid in the scoreboard andassumes that any of its worker threads marked SERVER_DEAD are,in fact, dead.However, if the child processes are starting "slowly" becauseap_run_child_init() in child_main() is taking its time, thenstart_threads() hasn't even been run yet, so the threads aren'tmarked SERVER_STARTING -- they're just set to 0 as the defaultvalue. But 0 == SERVER_DEAD, so the main process sees a lotof dead worker threads and begins spawning new child processes,up to MaxClients/ThreadsPerChild in the worst case. In this case,when no worker threads have started yet, but all possible childprocesses have been spawned (and are working through theirchild_init phases), then the following is true and the"server reached MaxClients" message is printed, even thoughthe server hasn't started accepting connections yet: else if (idle_thread_count < min_spare_threads) { /* terminate the free list */ if (free_length == 0) {I considered wedging another thread status into thescoreboard, between SERVER_DEAD (the initial value) andSERVER_STARTING. The make_child() would set all the threadslots to this value and start_threads() would later flip themto SERVER_STARTING after actually creating the worker threads.That would have various ripple effects on other bits ofhttpd, though, like mod_status and other MPMs, etc. So insteadI tried adding a status field to the process_score scoreboardstructure, and making the following changes to worker.c such thatthis field is set by make_child to SERVER_STARTING and thenchanged to SERVER_READY once the start thread that runsstart_threads() has done its initial work.During this period, while the new child process is runningap_run_child_init() and friends, perform_idle_server_maintenance()just counts that child process's worker threads as all beingeffectively in SERVER_STARTING mode. Once the process_score.statusfield changes to SERVER_READY, perform_idle_server_maintenance()begins to look at the individual thread status values.	Createdadds a per-process status fieldCreatedadds process_score statusI should add that this patch is more food-for-thought than a clear fix.For one thing, other MPMs like event and prefork aren't considered.For another, hard and graceful restarts may not play well with the per-processstatus field, since at least with graceful restarts, new processes cangradually take over thread slots in the scoreboard from an exiting process.I'll do some additional testing and perhaps a better solution will present itself.Comments welcome!svn rev. 399099 should take care of it.Greg, svn rev. 399099 helps fine during server start. However, the same problemoccurs during a graceful restart.Createda module which introduces a delay in child_init() for testingCreatedbackport for 2.0.xThis is the same fix for 2.0.x. It has the same problem as the 2.2.xbackport, as well as the fix in trunk: startup is fixed, but gracefulrestart isn't.	7.0	id=38325	10	False	False	nick	1
id=39807	REOPENED	None	Apache httpd-2	Core (	2.2.2	PC Linux	P2 enhancement	Apache HTTPD Bugs Mailing List	2006-06-13 22:22 UTC by	Tyler "Crackerjack" MacDonald	2006-06-19 07:02 UTC (	0 users	I recently had some filesystem corruption that resulted in the file size of a.GIF to be reported as 65,536 terabytes, even though it's only 782 bytes:-rw-r--r-- 1 kirin kirin 656 Mar 21 2005 btn_profile_on.gif-rw-r--r-- 1 kirin kirin 866 Mar 21 2005 btn_register.gif-rw-r--r-- 1 kirin kirin 772 Mar 21 2005 btn_register_on.gif-rw-r--r-- 1 kirin kirin 72057594037928718 Mar 21 2005 btn_search.gif-rw-r--r-- 1 kirin kirin 709 Mar 21 2005 btn_search_on.gif-rw-r--r-- 1 kirin kirin 1045 Mar 21 2005 btn_users.gif-rw-r--r-- 1 kirin kirin 915 Mar 21 2005 btn_users_on.gif# cp btn_search.gif btn_search2.gif# ls -l btn_search2.gif-rw-r--r-- 1 root root 782 Jun 13 15:19 btn_search2.gifWhen apache tries to serve the corrupt file, it enters into a loop indefault_handler that causes it to use up all available memory.) bt#0 apr_bucket_alloc (size=52, list=0x8210390) at buckets/apr_buckets_alloc.c:136#1 0xa7f8c790 in apr_bucket_simple_copy (a=0x8210670, b=0xaf9e12f0) at buckets/apr_buckets_simple.c:22#2 0xa7f8a36c in apr_bucket_shared_copy (a=0x8210670, b=0xaf9e12f0) at buckets/apr_buckets_refcount.c:38#3 0x0806d811 in default_handler (r=0x8320320) at core.c:3678#4 0x080740f7 in ap_run_handler (r=0x8320320) at config.c:157#5 0x080771e1 in ap_invoke_handler (r=0x8320320) at config.c:371#6 0x08081c48 in ap_process_request (r=0x8320320) at http_request.c:258#7 0x0807eeee in ap_process_http_connection (c=0x820c550) at http_core.c:172#8 0x0807ae77 in ap_run_process_connection (c=0x820c550) at connection.c:43#9 0x08085c24 in child_main (child_num_arg=<value optimized out>) at prefork.c:640#10 0x08085f1a in make_child (s=<value optimized out>, slot=0) at prefork.c:736#11 0x08085fda in startup_children (number_to_start=5) at prefork.c:754#12 0x08086a44 in ap_mpm_run (_pconf=0x80a70a8, plog=0x80d5160, s=0x80a8f48) at prefork.c:975#13 0x08061dcf in main (argc=134893856, argv=0x8153390) at main.c:717I'm not sure if httpd would go into such a tailspin if the file *was* actuallythat huge or not, but it would be nice to see it be more resilient to largefiles and filesystem corruption.	The out of memory is from trying to split the huge file into buckets ofAP_MAX_SENDFILE size to sendfile each section of the file. I don't believethere is anything we can do in this case, except to call the OOM abort functionin APR, which has already been done in trunk.If you had a 64bit machine/OS/httpd (or where sizeof(apr_off_t) <=sizeof(apr_size_t)), I believe it would work since it would attempt tosendfile() it all in one bucket, rather than millions AP_MAX_SENDFILE size buckets.I don't think it's unreasonable to expect that the server should either handlethis situation correctly or at least fail gracefully without attempting to eatall your RAM.It would be possible to handle this quite correctly by storing an "insanelylarge file" in a bucket with unknown length. This could be done either as amodification to the file bucket or, I suppose, as a new bucket type. httpdcould handle such a bucket without problems; apr_brigade_insert_file could dowhatever is necessary to insert it.But I'm not entirely convinced it would be worth all the effort to fix thisproperly unless someone really wants to serve insanely large files with 32-bithttpd binaries. By my calculations apr_brigade_file_insert() will be using 36Mbof RAM in bucket structures per petabyte of file.The server has effectively DOSed itself. A 500 response would be a sensible alternative.How to detect when to bail out? Maybe a LimitResponseSize directive (with a default value of AP_MAX_SENDFILE) would make sense as a sanity check?CreatedAdd a "LimitResponseFileSize" directive for default_handler to use.This patch implements Nick's suggestion to have a config directive that limitsthe maximum size of a file we will send.I named the directive "LimitResponseFileSize" instead of "LimitResponseSize",because the directive only affects default_handler; other handlers could stillsend more information than this in a response.Might not be obvious, we have a keyword rather than changing subject ;-)ahh, didn't catch that. thanks. :-)Please let's not add yet-another-config-directive just to paper over a bug.1) this doesn't catch all the places where such files are added to brigades2) there is more code added there than would be necessary to actually correctlyhandle such files3) enforcing a 16Mb default response size is... unwise ;)I didn't think it would be that easy either... nothing's that easy. :)You're suggesting a "bucket with unknown length"... I'm not really sure whatthis means or how it'd work but it *sounds* interesting...Fair enoughcorrectly handle such filesWell that patch took me 10 minutes to whip up... so I guess that means you cansupply a patch to fix this properly in 2 minutes? :)People shouldn't have filed larger than that on their webservers! HTTP is not afile transfer protocol!... kidding... I actually had no idea the limit was set that low. But that meansthat httpd is creating a bunch of these structures whenever it sends out anylarge file, not just an insanely large one... hmm...I'm going to stop thinking about this now.Well, OK :) I *thought* this would be a 2 minute patch but modifying the FILEbucket to do this actually isn't possible, so my point (2) is out of the window. I still think that adding config directives for this is the wrong approach,however.It's actually annoyingly difficult to fix this in APR-util becauseapr_brigade_insert_file() cannot assume that the *whole* passed-in file must beinserted; so a new function with those semantics would be needed.	9.0	id=39275	7	False	False	gregames	1
id=39658	REOPENED	None	Apache httpd-2	mod_proxy (	2.2.2	All All	P2 normal	Apache HTTPD Bugs Mailing List	2006-05-25 11:36 UTC by	Chetan Sabnis	2012-03-20 21:00 UTC (	1 user	mod_proxy_ajp appears to not comply to the AJP 1.3 spec when sending over theSSL key size as an attribute in the AJP 1.3 Request. Instead of sending the keysize as a string, it sends it as an integer. Both the tomcat and mod_proxy_ajpdocumentation implies that it is a string:This bug ends up manifesting itself in Jetty 5.1.11 (latest stable) when aconnection comes in over SSL to Apache HTTP 2.2.2 and is tunneled to Jetty. Unlike Tomcat, their AJP implementation is not resilient against non-compliantbehavior. It completely rejects the request. The 2.2.2 following patch includes my fix. I have tested this against Tomcat5.0.24, Tomcat 5.5.9, and Jetty 5.1.11 over SSL. All work for proxying therequest, but I have not verified that the key size is available and present inthe respective servlet containers.diff httpd-2.2.2/modules/proxy/ajp_header.chttpd-2.2.2-css/modules/proxy/ajp_header.c392c392< || ajp_msg_append_uint16(msg, (unsigned short) atoi(envvar))) {---	You are correct about the documentation of the protocol, but Tomcat, mod_jk andas noticed mod_proxy_ajp handle this as an integer. So its a bug in thedocumentation of the protocol and a bug in Jetty which implements this accordingto the buggy documentation. So please open up a bug report at Jetty.so we should keep this PR open to fix our doc, right?Yes of course. Pushed the wrong button :-).Thanks for the quick feedback. For anyone interested in following this on theJetty side of things I have submitted a bug and patch for Jetty here.Committed to trunk as().	5.0	id=39658	5	False	False	rpluem	1
id=40102	REOPENED	None	Apache httpd-2	mod_rewrite (	2.2-HEAD	PC All	P2 normal	Apache HTTPD Bugs Mailing List	2006-07-24 11:08 UTC by	Michael Minicki	2012-04-18 07:07 UTC (	3 users	SCRIPT_NAME is passed incorrectly to PHP depending on where you placethe mod_rewrite rule. If it is placed in .htaccess or under <Directory>, the SCRIPT_NAME is initialized correctly but when you place the same rule under <VirtualHost> it is an empty string (or any other bogus value).I wasn't sure if it was an Apache or PHP bug and I have posted it on PHP bugtracker first. They claim it's an Apache bug:The rule I'm using is: RewriteEngine On RewriteCond %{SCRIPT_FILENAME} !-f RewriteCond %{SCRIPT_FILENAME} !-d RewriteRule ^(.*)$ /index.php/$1Or (it's not relevant - both have the same result): RewriteEngine On RewriteRule !\.(js|ico|gif|jpg|png|css|swf)$ index.phpApache2 PHP SAPI. Apache version is 2.0.58 but I guess it may not berelevant as it was reported by number of people (though I don't know what their setups are). Tested on Gentoo Linux.	It seems that if you place the rules under VirtualHost more things are mangled - ie. PATH_INFO. For example, for virtual host of test.nebula.intranet and root dir the variables are initialized as follows:$_SERVER['DOCUMENT_ROOT'] = '/var/www/localhost/htdocs/test'$_SERVER['SCRIPT_FILENAME'] = '/var/www/localhost/htdocs/test/index.php'$_SERVER['REQUEST_URI'] = '/'$_SERVER['SCRIPT_NAME'] = ''$_SERVER['PATH_INFO'] = '/index.html' // OMG!And now with deeper URL where PATH_INFO should be initialized - (SCRIPT_NAME is still wrong):$_SERVER['DOCUMENT_ROOT'] = '/var/www/localhost/htdocs/test'$_SERVER['SCRIPT_FILENAME'] = '/var/www/localhost/htdocs/test/index.php'$_SERVER['REQUEST_URI'] = '/archive/2006/05'$_SERVER['SCRIPT_NAME'] = ''$_SERVER['PATH_INFO'] = '/archive/2006/05'SCRIPT_NAME should be '/index.php' in both of those examples.It still doesn't work on apache 2.0.59.What happens if you run the PHP script as CGI?As a CGI script for URL of:values are set to:$_SERVER['DOCUMENT_ROOT'] = string(0) $_SERVER['SCRIPT_FILENAME'] = string(40) /var/www/localhost/htdocs/test/index.cgi$_SERVER['REQUEST_URI'] = string(16) /archive/2006/05$_SERVER['SCRIPT_NAME'] = string(40) /var/www/localhost/htdocs/test/index.cgi$_SERVER['PATH_INFO'] = string(16) /archive/2006/05Should document root be empty? I don't have much experience with PHP working as a CGI script. I have added a handler to <VirtualHost> (AddHandler cgi-script .cgi), added ExecCGI to Options under <Directory> and changed RewriteRule to "RewriteRule ^(.*)$ /index.cgi/$1".The values are the same in every case when run as a CGI. It does not matter if the rule is placed under <VirtualHost>, <Directory> or in .htaccess file. It seems it's a SAPI problem.If CGI gets the expected SCRIPT_NAME, I infer the problem isn't in mod_rewrite, but somewhere within mod_php. That is not an apache product. Please report the bug to its maintainers.FWIW, if you want to tell the mod_php folks how to fix it, this is my guess:(1) SCRIPT_NAME is set by ap_add_cgi_vars in Apache's "util_script".(2) PHP presumably calls that somewhere - unless it's reinvented that wheel.(3) The bug *looks like* a case of PHP calling it too early - specifically *before* the rewrite happened - so of course there was no SCRIPT_NAME.(4) So a fix would be for mod_php to call it later. Or maybe even call it twice, if the early call is unavoidable.Thank you, Nick. I will post your remarks on PHP bug tracker.Here is the way it works:1) ap_process_http_connection() is called when processing HTTP request;2) ap_process_http_connection() calls ap_read_request() in order to create request struct;3) ap_read_request() calls ap_getword_white(), which returns the original URI, not the file used to handle the request (this is how it works here, with PHP 5.2 and Apache 2.0.55);4) ap_read_request() sets request->uri to the result of ap_getword_white();5) finally PHP request handler is called, which in turn calls ap_add_cgi_vars() to get the variables.The way it works is the very same in both cases and it doesn't depend on the place where mod_rewrite directives were set, so I don't see how PHP could call ap_add_cgi_vars() too early or too late.I can reproduce this bug with a simple shell script:----------#!/bin/shcat <<HEADERSStatus: 200 OkContent-Type: text/plainHEADERSexport----------I added the following the <VirtualHost> to test that:SetHandler cgi-script<Directory /var/www> Options +ExecCGI</Directory>And the rewrite rules:RewriteEngine OnRewriteCond %{SCRIPT_FILENAME} !-fRewriteCond %{SCRIPT_FILENAME} !-dRewriteRule ^(.*)$ /test.sh/$1When the script is executed "export" shows that SCRIPT_NAME is empty. This appends only when the URL is rewritten to a one which has a path info component:Triggers the bug:RewriteRule ^(.*)$ /test.sh/$1Does not bug:RewriteRule ^(.*)$ /test.sh(In reply to)[...]In per-server context, mod_rewrite acts by default as an URI-to-filename translator. If you add the [PT] flag, mod_rewrite will copy the rewrite result back to r->uri and does not map the request to the filesystem, i.e. another URI-to-filename translator will map the request to the filesystem. In that case (if you use the PT-flag) SCRIPT_NAME seems to be set correctly.BTW: I'm wondering how your conditions will work in per-server context since w/o a URI-to-filename translation SCRIPT_FILENAME cannot contain the physical path of the request, which is needed in oder to check for existing files or dirs. See the docs.Using apache 2.2.10 (but I think this problem is in trunk, too):- If e.g. php is used as a moduleOriginal request_uri /bar ; rewrite result /foo.php In that case (mod_php), and if there is no internal redirect, SCRIPT_NAME contains in my environment w/o the PT-flag the value of r->uri (/bar) and with the PT-flag /foo.php. ENV PATH_INFO - rewrite result /foo.php/path - is '/path' and works correct with and w/o the PT-flag.- If a directive is used which triggers an internal redirect - such as php cgi setup via the Action directive, SCRIPT_NAME will contain the value of the cgi prog.Original request_uri /bar ; rewrite result /foo.php ; the result of the Action directive is r->uri /php-script/php with path_info /bar:ScriptAlias /php-script/ "/path/to/php/"AddHandler cgi-php .phpAction cgi-php /php-script/php--> SCRIPT_NAME=/php-script/php (correct with and w/o PT-flag)W/o the PT-Flag, PATH_INFO contains /bar (a subrequest for /bar will pass /foo.php to the cgi prog), REDIRECT_URL contains the original request_uri /bar, too (IMHO correct, because mod_rewrite doesn't change r->uri w/o the PT-flag, the only way Aliases can work). But that means that there is no ENV which points to the result of mod_rewrite (/foo.php).With the PT-flag set, PATH_INFO and REDIRECT_URL both contain /foo.php, which is correct.- Directly executed CGIs such as /cgi/printenv.pl with a config like (not Alias'd nor ScriptAlias'd in another way)<Directory "/var/www/cgi"> Options +ExecCGI AddHandler cgi-script .pl</Directory>r->uri: /bar ; rewrite result /cgi/printenv.plW/o the PT-flag: regardless if PATH_INFO was specified, SCRIPT_NAME contained /bar.With the PT-flag: regardless if PATH_INFO was specified, SCRIPT_NAME contained /cgi/printenv.plI tried to reproduce an empty SCRIPT_NAME with a rule in per-servr context, but that seems to fail in my environment, I don't know why. Instead, SCRIPT_NAME contained the unchanged r->uri of the initial request if there was no internal redirect or the PT-flag was not set. I think this is correct because mod_rewrite acts in per-server context w/o specifying the PT-flag like an Alias. To my understanding, SCRIPT_NAME represents the physical web view and if it's Alias'd it seems to me complicated if not impossible to get a physical web view.	11.0	id=39807	8	False	True	chip	1
id=42137	REOPENED	None	Batik - Now in Jira	SVG DOM (	1.7	PC Linux	P2 major	Batik Developer's Mailing list	2007-04-16 15:05 UTC by	Archie Cobbs	2009-03-30 15:53 UTC (	0 users	The DOM specification for mutation events:specifies that the DOMNodeRemovedFromDocument and DOMNodeInsertedIntoDocumentmutation events, unlike the others, should NOT bubble.However, Batik is bubbling these events.Example of an event I received in a test program:event=org.apache.batik.dom.events.DOMMutationEvent@ff45detarget=org.apache.batik.dom.svg.SVGOMSVGElement@6210fbcurrentTarget=org.apache.batik.dom.svg.SVGOMGElement@aa37a6 phase=3 bubbles=truetype=DOMNodeRemovedFromDocumentNote that the "target" is different from the "currentTarget" and that"bubbles=true". Both of these things should not happen for a non-bubbling event.	However, DOM Level 3 Events defines them to bubble:Since the implementation is based on DOM 3 now, I think I'll leave it as bubbling.Well I'll be darned. OK, nevermind then.(In reply to)There seems to be an inconsistency in the DOM 3 events spec; the table of event types in section 1.5.1still lists DOMNodeRemovedFromDocument and DOMNodeInsertedIntoDocument as non-bubbling.I thinkthis is an error in the specification. The definition was not meant to be changed from DOM Level 2, as far as I recall anyway.OK, reopened. DOM Level 3 Events is being worked on again now, and DOM mutation events are being reworked AIUI. I'll leave this bug open until that reworking is done.	5.0	id=40102	10	False	False	nick	1
id=40513	REOPENED	None	Apache httpd-2	mod_ssl (	2.2.12	All Linux	P2 major	Apache HTTPD Bugs Mailing List	2006-09-14 20:45 UTC by	Patrick Rutkowski	2013-04-22 20:08 UTC (	0 users	I installed apache 2.2 with mod_ssl today on a local computer for developmentpurposes and decided to have a peek at the logs just for fun. When I saw"Seeding PRNG with 0 bytes of entropy" I was amused and at the same timesomewhat frightened.Below is a copy of my config file and what follows after that are the relevantlog entries for a single fetch of "" via Safari.==============================================================================ServerRoot "/opt/apache2"Listen 80Listen 443User opt-wwwGroup opt-wwwLoadModule perl_module modules/mod_perl.soLoadModule logio_module modules/mod_logio.so<Directory /> Options None AllowOverride None</Directory><FilesMatch "^\.ht"> Order allow,deny Deny from all Satisfy All</FilesMatch>SSLRandomSeed startup file:/dev/random 512SSLRandomSeed connect file:/dev/random 512SSLSessionCache shmcb:/opt/apache2/ssl/ssl_scache(512000)SSLSessionCacheTimeout 300SSLMutex file:/opt/apache2/ssl/ssl_mutexLogLevel infoLogFormat "%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \"%r\" %b" ssl_log_formatLogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\" %I %O"NameVirtualHost *:80<VirtualHost *:80> ServerAdminDocumentRoot /opt/var/www/localhost/htdocs ServerName localhost ServerAlias mini Loglevel debug ErrorLog /opt/var/www/localhost/logs/error_log TransferLog /opt/var/www/localhost/logs/access_log ScriptAlias /cgi-bin/ "/opt/var/www/localhost/cgi-bin/"</VirtualHost><VirtualHost *:443> SSLEngine on SSLCertificateFile /opt/var/www/localhost-ssl/ssl/server.crt SSLCertificateKeyFile /opt/var/www/localhost-ssl/ssl/server.key ServerAdminDocumentRoot /opt/var/www/localhost-ssl/htdocs ServerName localhost ServerAlias mini Loglevel info ErrorLog /opt/var/www/localhost-ssl/logs/error_log TransferLog /opt/var/www/localhost-ssl/logs/access_log CustomLog /opt/var/www/localhost-ssl/logs/ssl_log ssl_log_format <Directory /opt/var/www/localhost-ssl/htdocs> Options Indexes </Directory> BrowserMatch ".*MSIE.*" \ nokeepalive ssl-unclean-shutdown \ downgrade-1.0 force-response-1.0</VirtualHost>==============================================================================There's no imminent danger since this is just a local dev box but I would like to get to the bottom of this 0 PRNG seed weirdness anyway; just blame human curiosity.(Note: The offending log entry is on the 2nd line of error_log)================================================================================access_log==::1 - - [14/Sep/2006:12:52:11 -0400] "GET / HTTP/1.1" 200 209 "-" "Mozilla/5.0 (Macintosh; U; PPC Mac OS X; en) AppleWebKit/418.8 (KHTML, like Gecko) Safari/419.3" 569 1359::1 - - [14/Sep/2006:12:52:11 -0400] "GET /favicon.ico HTTP/1.1" 200 - "-" "Mozilla/5.0 (Macintosh; U; PPC Mac OS X; en) AppleWebKit/418.8 (KHTML, like Gecko) Safari/419.3" 320 362==error_log==[Thu Sep 14 12:52:11 2006] [info] [client ::1] Connection to child 5 established (server localhost:443)[Thu Sep 14 12:52:11 2006] [info] Seeding PRNG with 0 bytes of entropy[Thu Sep 14 12:52:11 2006] [info] Client requested a 'session-resume' but we have no such session.[Thu Sep 14 12:52:11 2006] [info] Initial (No.1) HTTPS request received for child 5 (server localhost:443)[Thu Sep 14 12:52:11 2006] [info] Subsequent (No.2) HTTPS request received for child 5 (server localhost:443)[Thu Sep 14 12:52:16 2006] [info] [client ::1] (70007)The timeout specified has expired: SSL input filter read failed.[Thu Sep 14 12:52:16 2006] [info] [client ::1] Connection closed to child 5 with standard shutdown (server localhost:443)==ssl_log==[14/Sep/2006:12:52:11 -0400] ::1 TLSv1 RC4-SHA "GET / HTTP/1.1" 209[14/Sep/2006:12:52:11 -0400] ::1 TLSv1 RC4-SHA "GET /favicon.ico HTTP/1.1" -==============================================================================	Can you get a truss output from the child process doing this? It should onlyhappen AFAICT if a read() from /dev/random fails.This PR has been in state NEEDINFO for > 4 years. Closing.I have the same message on Linux x86_64 platform, httpd 2.2.24:strace for single child process:# grep random log22:22:12 open("/dev/urandom", O_RDONLY) = 14 <0.002871>22:22:12 open("/dev/urandom", O_RDONLY) = 14 <0.000039>22:22:12 open("/dev/urandom", O_RDONLY) = 14 <0.000028>22:22:12 open("/dev/urandom", O_RDONLY) = 14 <0.000035>22:22:12 open("/dev/urandom", O_RDONLY) = 14 <0.000030>22:22:12 open("/dev/urandom", O_RDONLY) = 14 <0.000030>so it opens /dev/urandom fine and later:22:22:42 write(2, "[Tue Apr 09 22:22:42 2013] [debug] mod_headers.c(756): headers: ap_headers_output_filter()\n", 91) = 91 <0.000035>22:22:42 setsockopt(13, SOL_TCP, TCP_CORK, [1], 4) = 0 <0.000018>22:22:42 writev(13, [{"HTTP/1.1 200 OK\r\nDate: Tue, 09 Apr 2013 20:22:42 GMT\r\nServer: Apache\r\nLast-Modified: Wed, 27 Mar 2013 10:32:25 GMT\r\nETag: \"92d1bb54-13252-4d8e58dcfc4d5\"\r\nAccept-Ranges: bytes\r\nContent-Length: 78418\r\nK"..., 284}], 1) = 284 <0.000021>22:22:42 sendfile(13, 14, [0], 78418) = 23076 <0.000045>22:22:42 setsockopt(13, SOL_TCP, TCP_CORK, [0], 4) = 0 <0.000015>22:22:42 poll([{fd=13, events=POLLOUT}], 1, 300000) = 1 ([{fd=13, revents=POLLOUT}]) <0.010243>22:22:42 sendfile(13, 14, [23076], 55342) = 30660 <0.000036>22:22:42 poll([{fd=13, events=POLLOUT}], 1, 300000) = 1 ([{fd=13, revents=POLLOUT}]) <0.029560>22:22:42 sendfile(13, 14, [53736], 24682) = 24682 <0.000026>22:22:42 read(13, 0x8838ef8, 8000) = -1 EAGAIN (Resource temporarily unavailable) <0.000017>22:22:42 write(9, "some.host xx.xx.170.48 - - [09/Apr/2013:22:22:42 +0200] \"GET /pages_low/51_low.jpg HTTP/1.1\" 200 78702 \""..., 297) = 297 <0.000027>22:22:42 write(11, "some.host xx.xx.170.48 - - [09/Apr/2013:22:22:42 +0200] \"GET /pages_low/51_low.jpg HTTP/1.1\" 200 78702 \""..., 280) = 280 <0.000020>22:22:42 gettid() = 29778 <0.000023>22:22:42 open("/proc/29778/attr/current", O_WRONLY) = 15 <0.000027>22:22:42 write(15, "changehat 000000001f74f58e^", 27) = 27 <0.000021>22:22:42 close(15) = 0 <0.000017>22:22:42 gettid() = 29778 <0.000014>22:22:42 open("/proc/29778/attr/current", O_WRONLY) = 15 <0.000020>22:22:42 write(15, "changehat 000000001f74f58e^HANDLING_UNTRUSTED_INPUT", 51) = 51 <0.000052>22:22:42 close(15) = 0 <0.000015>(change hat due to apache-mod_apparmor usage)22:22:42 times({tms_utime=156, tms_stime=41, tms_cutime=0, tms_cstime=0}) = 1444949832 <0.000015>22:22:42 close(14) = 0 <0.000017>22:22:42 poll([{fd=13, events=POLLIN}], 1, 2000) = 0 (Timeout) <2.007569>22:22:44 times({tms_utime=156, tms_stime=41, tms_cutime=0, tms_cstime=0}) = 1444950033 <0.000016>22:22:44 shutdown(13, SHUT_WR) = 0 <0.000039>22:22:44 poll([{fd=13, events=POLLIN}], 1, 2000) = 1 ([{fd=13, revents=POLLIN|POLLHUP}]) <0.023552>22:22:44 read(13, "", 512) = 0 <0.000029>22:22:44 close(13) = 0 <0.000033>22:22:44 read(7, 0x7fff40e8c897, 1) = -1 EAGAIN (Resource temporarily unavailable) <0.000021>22:22:44 semop(5931027, {{0, -1, SEM_UNDO}}, 1) = 0 <1.202430>22:22:46 epoll_wait(10, {?} 0x881c4a8, 4, 10000) = 1 <0.058998>22:22:46 accept(6, {sa_family=AF_INET, sin_port=htons(39947), sin_addr=inet_addr("89.69.21.174")}, [16]) = 13 <0.000023>22:22:46 fcntl(13, F_GETFD) = 0 <0.000018>22:22:46 fcntl(13, F_SETFD, FD_CLOEXEC) = 0 <0.000018>22:22:46 semop(5931027, {{0, 1, SEM_UNDO}}, 1) = 0 <0.000031>22:22:46 write(2, "[Tue Apr 09 22:22:46 2013] [info] [client 89.69.21.174] Connection to child 54 established (server localhost:443)\n", 114) = 114 <0.000039>22:22:46 write(2, "[Tue Apr 09 22:22:46 2013] [info] Seeding PRNG with 0 bytes of entropy\n", 71) = 71 <0.000032>22:22:46 fcntl(13, F_GETFL) = 0x2 (flags O_RDWR) <0.000023>22:22:46 fcntl(13, F_SETFL, O_RDWR|O_NONBLOCK) = 0 <0.000016>22:22:46 write(2, "[Tue Apr 09 22:22:46 2013] [debug] ssl_engine_kernel.c(1872): OpenSSL: Handshake: start\n", 88) = 88 <0.000023>22:22:46 write(2, "[Tue Apr 09 22:22:46 2013] [debug] ssl_engine_kernel.c(1880): OpenSSL: Loop: before/accept initialization\n", 106) = 106 <0.000022># grep "Seeding PRNG with 0 bytes of entropy" /var/log/httpd/error_log | wc -l4395# grep "Seeding PRNG" /var/log/httpd/error_log | grep -v "Seeding PRNG with 0 bytes of entropy"#so every message had 0 bytes in it.Updated bug fields to match this case since original reporter is not responding.In ssl_rand_seed apRandSeed->nelts is 0 so it does nothing.Turns out that I was missing some configuration likeSSLRandomSeed startup file:/dev/urandom 256SSLRandomSeed connect builtinbut on the other hand I would expect defaults to be sane and using entropy source without requiring manual configuration.I think this is not a problem on common platforms. According to the openssl RAND_seed man page, 'On systems that provide "/dev/urandom", the randomness device is used to seed the PRNG transparently'. So, SSLRandomSeed is really only needed on platforms that do not have /dev/urandom.But this should be updated in the docs, and maybe we want to reduce the loglevel of the message to debug if SSLRandomSeed is not set.I can confirm this issue on a clean SLES 11SP2 64-bit, with apache2-2.2.12-1.28.1.Without the lines:SSLRandomSeed startup file:/dev/urandom 256SSLRandomSeed connect builtinin the apache config, the error log reports the following: [info] Init: Seeding PRNG with 0 bytes of entropySystem info: uname -a:Linux test-host-name 3.0.13-0.27-default #1 SMP Wed Feb 15 13:33:49 UTC 2012 (d73692b) x86_64 x86_64 x86_64 GNU/Linuxcat /etc/SuSE-release:SUSE Linux Enterprise Server 11 (x86_64)VERSION = 11PATCHLEVEL = 2modssl-version from Apache error_log:mod_ssl/2.2.12 compiled against Server: Apache/2.2.12, Library: OpenSSL/0.9.8h/dev/urandom is present on this host:crw-rw-rw- 1 root root 1, 9 Apr 10 11:44 /dev/urandom(When the two lines first mentioned are present in the config, PRNG is behaving as one should expect, with entropy =! 0)Regards,Vegard Fremstad	6.0	id=40217	11	False	False	rpluem	1
id=40217	REOPENED	None	Apache httpd-2	mod_dav (	2.2.2	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2006-08-08 21:32 UTC by	Anthony Atkins	2014-08-15 06:12 UTC (	1 user	I have the following block in my httpd.conf, outside of any Virtual Host directive:<FilesMatch"^\.(perms.xml|home.xml|htaccess|davaccess|htaccess.ssl|localUsers|localGroups|ftpaccess)$"> Order allow,deny Deny from all Satisfy All</FilesMatch>This block is intended to hide files that control access for apache and anotherapp that reads the filesystem. When accessing the space using a web browser,all files matching the pattern above are hidden as expected.When using a DAV client such as WebDrive, the files are returned in thedirectory listing. This causes problems when attempting to copy a foldercontaining hidden files to a new location, as the DAV client is aware of thehidden file and tries to copy it. It seems like one or more dav permissions is not correctly limited in a generalFilesMatch block, when in fact all permissions for both DAV and non-DAV accessshould be removed by the above block.	(In reply to)directive:>"^\.(perms.xml|home.xml|htaccess|davaccess|htaccess.ssl|localUsers|localGroups|ftpaccess)$">From my point of view this is not what filesmatch is designed for. You canprevent access to these files, but not prevent showing them. Think of Unixfilesystem permissions:You may have files in a directory on which you have no permissions. As long asyou have read permissions on the directory you can see them.What do you mean by hidden? Do you have configured mod_autoindex and they don'tshow up in the mod_autoindex generated listings?As stated above I would see this as works as designed.I disagree that FilesMatch is not intended to hide files, mod_autoindex does notdisplay files that match the above patter. Moreover, you get a "403 Forbidden"when trying to read a file that matches the FilesMatch pattern from any webbrowser. Again, everything *but* DAV honors the FilesMatch directive as specified.Anyway, if you don't think FilesMatch should be used in this way, how would yousuggest preventing a DAV client from seeing access control files?(In reply to)In order to prevent files from showing up in a mod_autoindex listing and thushiding them, you should use IndexIgnore. I guess it is just becausemod_autoindex fails to detect the mime type of the file that it does not displayit. But this not really intentional.Forbidden"In this case you try to access the file itself. That is what filesmatch preventsand should prevent. This is also prevented in the dav case. You can *see* thefiles in Webdrive but you *cannot* access them.specified.What do you mean by everything? I see only mod_autoindex and the behaviour thereis not intentional.This is not possible. Again think of the Unix filesystem permissions here. Ifyou want to prevent someone from *seeing* files in a directory you have torevoke read permissions on this directory. The same is true for mod_dav. Seeinga file is not a property or permission of the file, but of the directory orbetter the collection in the dav case. But of course you can prevent people fromaccessing the access control files via filesmatch.IndexIgnore doesn't affect the DAV functionality. I explicitly added all of theaccess control files to an IndexIgnore block and they still show up. Try it and see.specified.is not intentional.>GET access to the file itself is limited by the FilesMatch directive, as is thedisplay of the file in the index created by mod_auto_index. That's what I meantby "everything".If you're saying you can't limit GET access to a file, you're mistaken. Apachecan definitely be configured not to display directories or files for which ititself has read access.Okay, didn't mean to sound so harsh. I reread your note, I understand yourpoint about the containing directory. But to extend your analogy, if I have thepermissions necessary to read a directory, but I'm not allowed to read a file byan ACL, then I won't see the file in the directory listing, period. You needboth permissions to be able to see a file in a directory listing, which is mypoint. DAV is apparently honoring the directory permissions and ignoring thefile permissions.(In reply to)I did not say this I said the opposite.But this is only true if the IndexOption ShowForbidden is not set. So I mustcorrect myself a little bit regarding the 'non intentional' statement. Dependingon the configuration setting of mod_autoindex this behaviour is intentional.(In reply to)This is just plain wrong on a standard Unix file system. If you have thepermission to read the directory you see the file in the directory listing. Iadmit that I don't know if this behaviour changes if you are using POSIX ACLS.BTW: The same seems to be true with NTFS on Windows.The default behavior of mod_autoindex is not to show files that are restrictedby a FilesMatch block.Also, I specifically used the example of ACLs because those are a better matchfor what FilesMatch does than regular UNIX perms. Anyway, I'm going to try a demonstration, I suspect that it's just one of themethods that's not working as expected.Have you tried to replicate this?Okay, I just did some testing. The files are not revealed when using the GETmethod, but are revealed when using the PROPFIND method with a depth of 1. Soit seems like all this talk about mod_autoindex is beside the point. It's thePROPFIND handling in mod_dav that exhibits the unexpected behavior.I'm no DAV expert, but I do believe there is a bug here. If you look at thePROPFIND multistatus response for such a directory, I believe it should beshowing a different status and omitting the get* elements for access-restrictedfiles. Instead, it is showing them just like any other file.[collision with joshua]I completely agree with Ruediger that the correct behaviour should be to revealthe existence/non-existence of a resource by name in mod_dav.mod_dav does not however apply any access control checks during a PROPFIND walk,which is certainly a bug: you can do a Depth: infinity walk straight throughprotected areas, and discover properties of protected resources in a Depth: 1even if infinity is disabled. Fixing this was somewhat more complicated thatjust adding a subreq call in the walker because of the error handling IIRC fromworking on this for mod_dav 1.0.Returning simply a name in a 200 propstat for protected resources is probablycorrect; not sure how DAV clients will react to such resources though.I have a pretty good idea how DAV clients react, having testing mod_dav withWebDrive, Web Folders, and the DAV support built into OS X.In our case, all the files we hide (.davaccess), etc. start with a leading dot. If the client program is configured to hide "dot files", then the name/icon ofthe file will not be displayed to the end user. If the client program isconfigured to display "dot files", the name/icon will be displayed. All threeclients base their directory listings on the results of the PROPFIND.If a user attempts to copy a directory containing restricted files, the clientwill attempt to request the restricted files and fail with an error.This is precisely why I reported this problem.Anthony, the question isn't how clients now react, it is how would they react ifthe PROPFIND response was correct (including the restricted files, but with thecorrect status and without the extra details).Ah, got it. I could probably arrange for a perl script to pretend to be Apacheand return an edited replay of a normal propfind. I'll let you know what Ifigure out.CreatedHack to exclude ms office 2010 generated temporary filesThis patch excludes temporary files generated by ms office 2010 from the webdav directory listing.	16.0	id=40513	11	False	True	jorton	1
id=42732	REOPENED	None	Apache httpd-2	All (	2.2.4	HP HP-UX	P2 major	Apache HTTPD Bugs Mailing List	2007-06-25 05:43 UTC by	destroyedlolo	2007-09-18 06:37 UTC (	0 users	Hello,I running on an HP-UX 11.11 712 box, and I have compiled apache 2.2.4 with GCC.I have took mod_dbd.c from truck due to several bugs in the version provided w/2.2.4.I'm authenticating my users against a PostgreSQL database w/ following directives :AuthType BasicAuthName "mariage"AuthBasicProvider dbdRequire valid-userAuthDBDUserPWQuery "select passwd from test.comptes where id = %s"Unfortunately, I have to enter 4 or 5 time my username password before beingaccepted by apache. In error.log, I got many [Mon Jun 25 12:08:28 2007] [error] [client 164.129.228.100] user myuser : authentication failure for "/~laurent/": Password MismatchIt's not a problem of password as I typed exactly the same one (copy/past), andevery time I'm restarting apache, the password is asked another 4 or 5 timebefore being accepted.I can spend some time for debugging if needed, but I duno where to start.Best regards,Laurent	Laurent, This is not the place to obtain support for your issues. Rather go to"" and use opne of those methods to getsome help. This forum is for the reporting of bugs in the product. If indeedthere is a bug found, please come back here an re-open this issue.Thanks,TonyIt's not a misconfiguration but really a bug : I did several tests and generatedsome log on the database.when the password is rejected, Apache did initiated the connection but the querydidn't run.DEBUG: bind <unnamed> to authn_dbd_1DEBUG: StartTransactionCommandDEBUG: StartTransactionDEBUG: name: unnamed; blockState: DEFAULT; state: INPROGR, xid/subid/cid:3116/1/0, nestlvl: 1, children: <>LOG: execute authn_dbd_1: PREPARE authn_dbd_1 (varchar) AS select passwd fromtest.comptes where id = $1DETAIL: parameters: $1 = 'Test'DEBUG: CommitTransactionCommandDEBUG: CommitTransactionDEBUG: name: unnamed; blockState: STARTED; state: INPROGR, xid/subid/cid:3116/1/1, nestlvl: 1, children: <>When the password is accepted, Apache initiate the connection and then a queryis issued on the database.DEBUG: bind <unnamed> to authn_dbd_1DEBUG: StartTransactionCommandDEBUG: StartTransactionDEBUG: name: unnamed; blockState: DEFAULT; state: INPROGR, xid/subid/cid:3120/1/0, nestlvl: 1, children: <>LOG: execute authn_dbd_1: PREPARE authn_dbd_1 (varchar) AS select passwd fromtest.comptes where id = $1DETAIL: parameters: $1 = 'Test'DEBUG: CommitTransactionCommandDEBUG: CommitTransactionDEBUG: name: unnamed; blockState: STARTED; state: INPROGR, xid/subid/cid:3120/1/1, nestlvl: 1, children: <>DEBUG: bind <unnamed> to authn_dbd_1DEBUG: StartTransactionCommandDEBUG: StartTransactionDEBUG: name: unnamed; blockState: DEFAULT; state: INPROGR, xid/subid/cid:3121/1/0, nestlvl: 1, children: <>LOG: execute authn_dbd_1: PREPARE authn_dbd_1 (varchar) AS select passwd fromtest.comptes where id = $1DETAIL: parameters: $1 = 'Test'DEBUG: CommitTransactionCommandDEBUG: CommitTransactionDEBUG: name: unnamed; blockState: STARTED; state: INPROGR, xid/subid/cid:3121/1/1, nestlvl: 1, children: <>DEBUG: bind <unnamed> to authn_dbd_1DEBUG: StartTransactionCommandDEBUG: StartTransactionDEBUG: name: unnamed; blockState: DEFAULT; state: INPROGR, xid/subid/cid:3122/1/0, nestlvl: 1, children: <>LOG: execute authn_dbd_1: PREPARE authn_dbd_1 (varchar) AS select passwd fromtest.comptes where id = $1DETAIL: parameters: $1 = 'Test'DEBUG: CommitTransactionCommandDEBUG: CommitTransactionDEBUG: name: unnamed; blockState: STARTED; state: INPROGR, xid/subid/cid:3122/1/1, nestlvl: 1, children: <>DEBUG: bind <unnamed> to authn_dbd_1DEBUG: StartTransactionCommandDEBUG: StartTransactionDEBUG: name: unnamed; blockState: DEFAULT; state: INPROGR, xid/subid/cid:3123/1/0, nestlvl: 1, children: <>LOG: execute authn_dbd_1: PREPARE authn_dbd_1 (varchar) AS select passwd fromtest.comptes where id = $1DETAIL: parameters: $1 = 'Test'DEBUG: CommitTransactionCommandDEBUG: CommitTransactionDEBUG: name: unnamed; blockState: STARTED; state: INPROGR, xid/subid/cid:3123/1/1, nestlvl: 1, children: <>DEBUG: bind <unnamed> to authn_dbd_1DEBUG: StartTransactionCommandDEBUG: StartTransactionDEBUG: name: unnamed; blockState: DEFAULT; state: INPROGR, xid/subid/cid:3124/1/0, nestlvl: 1, children: <>LOG: execute authn_dbd_1: PREPARE authn_dbd_1 (varchar) AS select passwd fromtest.comptes where id = $1DETAIL: parameters: $1 = 'Test'DEBUG: CommitTransactionCommandDEBUG: CommitTransactionDEBUG: name: unnamed; blockState: STARTED; state: INPROGR, xid/subid/cid:3124/1/1, nestlvl: 1, children: <>LOG: connection received: host=[local]DEBUG: forked new backend, pid=18697 socket=7LOG: connection authorized: user=www database=wwwDEBUG: postgres child[18697]: starting with (DEBUG: postgresDEBUG: -v196608DEBUG: -yDEBUG: wwwDEBUG: )DEBUG: InitPostgresDEBUG: StartTransactionDEBUG: name: unnamed; blockState: DEFAULT; state: INPROGR, xid/subid/cid:3125/1/0, nestlvl: 1, children: <>DEBUG: CommitTransactionDEBUG: name: unnamed; blockState: STARTED; state: INPROGR, xid/subid/cid:3125/1/0, nestlvl: 1, children: <>DEBUG: StartTransactionCommandDEBUG: StartTransactionDEBUG: name: unnamed; blockState: DEFAULT; state: INPROGR, xid/subid/cid:3126/1/0, nestlvl: 1, children: <>LOG: statement: SELECT *, TO_CHAR(expiration, 'YYYYMMDD') AS expiration FROMhebergement.site WHERE id='Test';DEBUG: parse tree:DETAIL: {QUERY :commandType 1 :querySource 0 :canSetTag true :utilityStmt <> :resultRelation 0 :into <> :intoOptions <> :intoOnCommit 0 :intoTableSpaceName ...According to PostgreSQL log, it seems the connection hasn't a "working" status(expired ?), and this time, apache is doing several attempts, all failing andfinally create a new connection and do the query, successfully this time.Best regards,LaurentHi Laurent,I fixed my 2.2.4 dbd authn problems by using the mod_dbd.c from trunk. See thismessage:Regards, Phil.Sorry for my silence (I was in paternity leave ;-D).I compiled this patch but I'm not able anymore to restart 2.2.4 (nothing relatedto this patch, httpd crash at startup on my system depending on my configurationsetting).Anyway, I've installed 2.2.6 and the problem is still here. Can I test thispatch w/ 2.2.6 ?ByeLaurent	4.0	id=41143	5	False	False	davi	1
id=41143	REOPENED	None	Apache httpd-2	mod_proxy (	2.2-HEAD	All All	P2 normal	Apache HTTPD Bugs Mailing List	2006-12-10 15:03 UTC by	Davi Arnaut	2007-06-29 07:02 UTC (	0 users	The misplaced ap_getline may discard a valid header after a too long header, ap_getline already discards extra data.The patch (side effect) also reduces the stack usage considerably (by 8KB).	Createdpatch against the 2.2.x branchFixed in 2.2.4Are you sure? which commit fixed it? I fail to see how this was solved. The code inquestion is still present on the 2.2.x branch and trunk.(In reply to)ap_getline already discardsI am no sure if this is true in the case that the header is larger than 16KB,because in this case ap_rgetline_core will be left after the secondap_get_brigade (in the case we only have the ap_core_input_filter in the inputfilter chain, not checked the SSL case) which only read 16KB from the socket.(In reply to)Bear with me:ap_proxy_read_headers() calls ap_getline() with a buffer of 8192 bytes. ap_getline()creates a brigade and calls ap_rgetline() to get a line of protocol input (and to copythe line to the buffer). ap_rgetline_core() calls ap_get_brigade(AP_MODE_GETLINE)which returns a line (brigade) of 8200 bytes (for the sake of example). If all datais in the first bucket, the line size could overflow the buffer, then APR_ENOSPCis returned. ap_getline sees the APR_ENOSPC and returns the buffer size.We have two problems here. First, if the data of the first bucket overflows the buffer,the data is not copied but ap_getline returns the size of the buffer (meaning that thedata was copied). Second, ap_proxy_read_headers checks if the returned len is >= thanthe buffer size-1, if so, it calls ap_getline again, but the line was already read byap_get_brigade and discarded.Makes sense now?(In reply to)ap_getline()to copydataBut ap_get_brigade(AP_MODE_GETLINE) returns 8192 bytes at max and it is notguaranteed that this data contains a LF. It is only guaranteed thatap_get_brigade(AP_MODE_GETLINE) does not read *past* a LF. Seeap_core_input_filter and apr_brigade_split_line for why.I agree that current code does it wrong, but I fear that there are other caseswhere your patch will do it wrong.(In reply to)No! ap_get_brigade(AP_MODE_GETLINE) may return much more than 8192 bytes.Suppose a brigade has two buckets of APR_BUCKET_BUFF_SIZE (8000) bytes eachand the line break is on the second bucket, at position 4000. In this case, apr_brigade_split_line() will read the first bucket, and will proceed to read thenext one since it's size is not >= HUGE_STRING_LEN. It will find a LF on the secondone and return a brigade of 12000 bytes.Would you care to elaborate? Also, i'm not sure too if it's the best fix, but it can'tget any worse (famous last words).I forgot to mention, but but if my memory serves well I spotted this issue while playing withlarge cookies. It's not a critical real-world issue, yet :-)Thanks for you comments.(In reply to)Ok. You are correct with this. Sorry, I misread the loop.secondSo apr_brigade_split_line() will not return more than maxbytes-1 + the size ofthe following bucket. So the brigade returned by ap_get_brigade(AP_MODE_GETLINE)may not contain the LF if the header is long enough (I would suppose that 16KB =2 * 8000 (APR_BUCKET_BUFF_SIZE) + 384 should be sufficient to get there).I admit that this is a constructed edge case that might not happen in real life :-).it can'tIt is not getting worse, only different :-).	9.0	id=40987	5	False	True	rpluem	1
id=38325	REOPENED	None	Apache httpd-2	mod_auth (	2.5-HEAD	All All	P2 normal	Apache HTTPD Bugs Mailing List	2006-01-20 04:15 UTC by	None	2014-02-17 13:49 UTC (	1 user	It's not possible to determine AUTH_TYPE if the resource on which authenticationis defined is interpreted by a handlerI experienced this same problem a year ago -Currently, my problem is determining the AUTH_TYPE in PHP scripts using the CGIPHP server APII define authentication on a PHP resource - the Gallery PHP web gallery, forinstance -I then want to set the active Gallery user if appropriate HTTP authenticationhas taken place. HTTP authentication is necessary to implement a WebDAVinterface -Using mod_php I can check the values of the REMOTE_USER & AUTH_TYPE environmentvariables - using php-cgi I must check the value of the REDIRECT_REMOTE_USERenvironment variable - but there's no REDIRECT_AUTH_TYPE variable, so it'simpossible to determine the AUTH_TYPE in this caseUsing php-cgi the REMOTE_USER & AUTH_TYPE variables correspond to authenticationdefined on the php-cgi interpreterThis bug presents a serious problem for implementing HTTP authentication inGallery, since many installations use the CGI PHP server API. I haven't yetfound a work-aroundMany thanks!Jack	Using CGI you have AUTH_TYPE. What you have using PHP is none of Apache's business.This is a problem with any resource where Apache is configured to use a handleror interpreter. Apache is configured to use a handler with 'Action my-handler/path/handlercgi' & 'AddHandler my-handler foo' or 'SetHandler my-handler'. Thisis not a problem only with PHPAuthentication defined on the *handler* (/path/handlercgi) using AuthType isdescribed by AUTH_TYPE & REMOTE_USER environment variablesAuthentication defined on the *actual resource* using AuthType is described byREDIRECT_REMOTE_USER - but there is no REDIRECT_AUTH_TYPE environment variable!I just encountered this problem yet again, trying to determine the AUTH_TYPE inMoin where the Moin CGI is handled by CGIWrapAuthentication defined on CGIWrap is described by AUTH_TYPE & REMOTE_USER,authentication defined on the Moin CGI is described by REDIRECT_REMOTE_USER -but since there is no REDIRECT_AUTH_TYPE, it is impossible for Moin to tell whatAUTH_TYPE was defined on the actual Moin CGIThanks again - JackLook, if you want to define environment variables REDIRECT_*, that's your business. It only becomes Apache's business if Apache claims to support a standard that defines them. CGI - which apache supports - certainly doesn't. Apache doesn't define environment variable MORON_USER, either. A third-party module is free to define any environment variableit wants, but that's none of Apache's business.Fair enough - maybe it would be better to use AUTH_TYPE & REMOTE_USER variablesdefined in CGI. I use REDIRECT_AUTH_TYPE only for symmetry withREDIRECT_REMOTE_USER which Apache already definesMy point is when authentication is defined on a resource using AuthType,authentication which takes place should be described in AUTH_TYPE & REMOTE_USERvariables - as per CGI & usual Apache behaviorBut in the common case that mod_actions defines a handler or interpreter,AUTH_TYPE & REMOTE_USER are blank - even though authentication takes placeIn this case Apache defines REDIRECT_REMOTE_USER - for good reason: Otherwise itwould be impossible to work around blank REMOTE_USER & determine theauthenticated usernameBut since Apache doesn't also define REDIRECT_AUTH_TYPE it is impossible to workaround blank AUTH_TYPEUse AUTH_TYPE & REMOTE_USER or REDIRECT_AUTH_TYPE & REDIRECT_REMOTE_USER - justplease let AUTH_TYPE be determined when authentication is defined on a resource& mod_actions defines a handler or interpreter. This problem has stung meseveral times with different configurationsThis small patch extends Apache's current REDIRECT_REMOTE_USER approach toAUTH_TYPE -What's the status here?Jack is in so far right, that Apache itself creates those standards in-compliant REDIRECT_* variables...a) I never understood why,... and why it simply doesn't set the "normal" ones like "REMOTE_USER" in the redirection case,... any ideas?I mean this behaviour causes a lot of trouble as all applications need to support this Apache weirdness...b) If there was a good reason for the REDIRECT_* variables, then I agree that it's a problem not to have that counterpart for AUTH_TYPE.Cheers,Chris.btw: bumping this to current apache versions, so that it gets attention again.	5.0	id=42732	4	False	False	tony	1
id=43533	REOPENED	None	Apache httpd-2	mod_include (	2.0.61	Macintosh Mac OS X 10.4	P2 normal	Apache HTTPD Bugs Mailing List	2007-10-02 03:54 UTC by	Noah Williamsson	2008-11-18 04:07 UTC (	1 user	Over the past years Apache has crashed several times every hour.I've been running Apache 2 since 2.0.40 and now we're up at 2.0.61 and stillexperiencing these crashes so I guess it's about time I report this bug.Our site heavily relies on SSI, in particular <!--#include virtual=..-->(they're often nested aswell), and mod_rewrite. Output is compressed withmod_deflate. Often the crashdumps looks like this:crashdump[26541]: crashdump[26541]: Thread 20 Crashed:crashdump[26541]: 0 httpd 0x000047a0 bndm + 120(mod_include.c:317)crashdump[26541]: 1 httpd 0x0000a100 find_start_sequence+ 136 (mod_include.c:2388)crashdump[26541]: 2 httpd 0x0000bad0 send_parsed_content+ 1288 (mod_include.c:3054)crashdump[26541]: 3 httpd 0x0000dbf8 includes_filter +1468 (mod_include.c:3591)crashdump[26541]: 4 httpd 0x00068e30 ap_pass_brigade +236 (util_filter.c:512)crashdump[26541]: 5 httpd 0x0004a5c8 default_handler +1888 (core.c:3648)crashdump[26541]: 6 httpd 0x00060038 ap_run_handler + 136(config.c:152)crashdump[26541]: 7 httpd 0x00060d8c ap_invoke_handler +424 (config.c:364)crashdump[26541]: 8 httpd 0x00051f98 ap_run_sub_req + 104(request.c:1855)crashdump[26541]: 9 httpd 0x00005a68 handle_include + 968(mod_include.c:782)crashdump[26541]: 10 httpd 0x0000cc80 send_parsed_content+ 5816 (mod_include.c:3309)crashdump[26541]: 11 httpd 0x0000dbf8 includes_filter +1468 (mod_include.c:3591)crashdump[26541]: 12 httpd 0x00068e30 ap_pass_brigade +236 (util_filter.c:512)crashdump[26541]: 13 httpd 0x0004a5c8 default_handler +1888 (core.c:3648)crashdump[26541]: 14 httpd 0x00060038 ap_run_handler + 136(config.c:152)crashdump[26541]: 15 httpd 0x00060d8c ap_invoke_handler +424 (config.c:364)crashdump[26541]: 16 httpd 0x0001d980 ap_process_request +148 (http_request.c:249)crashdump[26541]: 17 httpd 0x000145a4ap_process_http_connection + 136 (http_core.c:255)crashdump[26541]: 18 httpd 0x0006bcdcap_run_process_connection + 136 (connection.c:43)crashdump[26541]: 19 httpd 0x0006c318ap_process_connection + 132 (connection.c:178)crashdump[26541]: 20 httpd 0x0003b408 process_socket + 196(worker.c:523)crashdump[26541]: 21 httpd 0x0003befc worker_thread + 552(worker.c:843)crashdump[26541]: 22 libapr-0.0.dylib 0x003796f0 dummy_worker + 68(thread.c:105)crashdump[26541]: 23 libSystem.B.dylib 0x9002bd08 _pthread_body + 96--------------------------crashdump[4103]: crashdump[4103]: Thread 15 Crashed:crashdump[4103]: 0 httpd 0x000047a0 bndm + 120(mod_include.c:317)crashdump[4103]: 1 httpd 0x0000a100 find_start_sequence+ 136 (mod_include.c:2388)crashdump[4103]: 2 httpd 0x0000bad0 send_parsed_content+ 1288 (mod_include.c:3054)crashdump[4103]: 3 httpd 0x0000dbf8 includes_filter +1468 (mod_include.c:3591)crashdump[4103]: 4 httpd 0x00068e30 ap_pass_brigade +236 (util_filter.c:512)crashdump[4103]: 5 httpd 0x0004a5c8 default_handler +1888 (core.c:3648)crashdump[4103]: 6 httpd 0x00060038 ap_run_handler + 136(config.c:152)crashdump[4103]: 7 httpd 0x00060d8c ap_invoke_handler +424 (config.c:364)crashdump[4103]: 8 httpd 0x00051f98 ap_run_sub_req + 104(request.c:1855)crashdump[4103]: 9 httpd 0x00005a68 handle_include + 968(mod_include.c:782)crashdump[4103]: 10 httpd 0x0000cc80 send_parsed_content+ 5816 (mod_include.c:3309)crashdump[4103]: 11 httpd 0x0000dbf8 includes_filter +1468 (mod_include.c:3591)crashdump[4103]: 12 httpd 0x00068e30 ap_pass_brigade +236 (util_filter.c:512)crashdump[4103]: 13 httpd 0x0004a5c8 default_handler +1888 (core.c:3648)crashdump[4103]: 14 httpd 0x00060038 ap_run_handler + 136(config.c:152)crashdump[4103]: 15 httpd 0x00060d8c ap_invoke_handler +424 (config.c:364)crashdump[4103]: 16 httpd 0x0001d980 ap_process_request +148 (http_request.c:249)crashdump[4103]: 17 httpd 0x000145a4ap_process_http_connection + 136 (http_core.c:255)crashdump[4103]: 18 httpd 0x0006bcdcap_run_process_connection + 136 (connection.c:43)crashdump[4103]: 19 httpd 0x0006c318ap_process_connection + 132 (connection.c:178)crashdump[4103]: 20 httpd 0x0003b408 process_socket + 196(worker.c:523)crashdump[4103]: 21 httpd 0x0003befc worker_thread + 552(worker.c:843)crashdump[4103]: 22 libapr-0.0.dylib 0x003796f0 dummy_worker + 68(thread.c:105)crashdump[4103]: 23 libSystem.B.dylib 0x9002bd08 _pthread_body + 96------------------I saw someone else having a similar problem here. There was an open bug aboutrandom crashes in crc32 and bndm, but with Apache 2.2.I've tried to log the PID along with the HTTP requests into a logfile andgrepping out the PID after the crash but I guess the thread crashes before thelog is written since I'm not finding anything interesting there.I've not been able to reproduce this but it keeps happening several times eachhour. Since we've got a fair share of traffic to this host I can't really gdbApache and wait for a crash and see what's going on. It might be a memory corruption since we're getting crashes in other placesaswell. It's not likely to be a hardware problem since the box have beenreplaced two times already (for performance reasons) and the crashes havecontinued on the new hardware aswell.Here's one of the other crashes we're getting quite frequently but not asfrequently as with bndm():crashdump[14604]: crashdump[14604]: Thread 7 Crashed:crashdump[14604]: 0 httpd 0x0000a6c0 find_directive + 284(mod_include.c:2552)crashdump[14604]: 1 httpd 0x0000bf14 send_parsed_content+ 2380 (mod_include.c:3118)crashdump[14604]: 2 httpd 0x0000dbf8 includes_filter +1468 (mod_include.c:3591)crashdump[14604]: 3 httpd 0x00068e30 ap_pass_brigade +236 (util_filter.c:512)crashdump[14604]: 4 httpd 0x0004a5c8 default_handler +1888 (core.c:3648)crashdump[14604]: 5 httpd 0x00060038 ap_run_handler + 136(config.c:152)crashdump[14604]: 6 httpd 0x00060d8c ap_invoke_handler +424 (config.c:364)crashdump[14604]: 7 httpd 0x00051f98 ap_run_sub_req + 104(request.c:1855)crashdump[14604]: 8 httpd 0x00005a68 handle_include + 968(mod_include.c:782)crashdump[14604]: 9 httpd 0x0000cc80 send_parsed_content+ 5816 (mod_include.c:3309)crashdump[14604]: 10 httpd 0x0000dbf8 includes_filter +1468 (mod_include.c:3591)crashdump[14604]: 11 httpd 0x00068e30 ap_pass_brigade +236 (util_filter.c:512)crashdump[14604]: 12 httpd 0x0004a5c8 default_handler +1888 (core.c:3648)crashdump[14604]: 13 httpd 0x00060038 ap_run_handler + 136(config.c:152)crashdump[14604]: 14 httpd 0x00060d8c ap_invoke_handler +424 (config.c:364)crashdump[14604]: 15 httpd 0x00051f98 ap_run_sub_req + 104(request.c:1855)crashdump[14604]: 16 httpd 0x00005a68 handle_include + 968(mod_include.c:782)crashdump[14604]: 17 httpd 0x0000cc80 send_parsed_content+ 5816 (mod_include.c:3309)crashdump[14604]: 18 httpd 0x0000dbf8 includes_filter +1468 (mod_include.c:3591)crashdump[14604]: 19 httpd 0x00068e30 ap_pass_brigade +236 (util_filter.c:512)crashdump[14604]: 20 httpd 0x0004a5c8 default_handler +1888 (core.c:3648)crashdump[14604]: 21 httpd 0x00060038 ap_run_handler + 136(config.c:152)crashdump[14604]: 22 httpd 0x00060d8c ap_invoke_handler +424 (config.c:364)crashdump[14604]: 23 httpd 0x0001d980 ap_process_request +148 (http_request.c:249)crashdump[14604]: 24 httpd 0x000145a4ap_process_http_connection + 136 (http_core.c:255)crashdump[14604]: 25 httpd 0x0006bcdcap_run_process_connection + 136 (connection.c:43)crashdump[14604]: 26 httpd 0x0006c318ap_process_connection + 132 (connection.c:178)crashdump[14604]: 27 httpd 0x0003b408 process_socket + 196(worker.c:523)crashdump[14604]: 28 httpd 0x0003befc worker_thread + 552(worker.c:843)crashdump[14604]: 29 libapr-0.0.dylib 0x003796f0 dummy_worker + 68(thread.c:105)------This problem has occured on Mac OS X 10.3 and up to 10.4.10, both Serverversions. We've been running a broad list of different versions of Apache 2,from .40 to .61.This problem occurs with the threaded worker.Does anyone have any idea what's going on here?	I recently worked with a user to track down an issue with identical backtraces -the root cause was PR 36780; (optimistically?) marking as a duplicate.*** This bug has been marked as a duplicate of***Reporter says the patch fordidn't help, so re-opening.Noah:1) can you reproduce this with prefork rather than worker?2) can you get a coredump and do some debugging with gdb?What is needed out of the core dump is:# gdb /path/to/httpd core.dump(gdb) source /patch/to/httpd/src/.gdbinitthen select a frame with a apr_bucket_brigade * in scope, e.g.send_parsed_content, e.g. "up 2" from that first "Thread 20", and do:(gdb) dump_brigade bbto dump the contents of the brigade.(In reply to)I'll try.Is it crucial for the debugging part or just interesting to know whether itmight be a threading related problem or not?I've enabled coredumps and I'll see if I can find some time to look into it.This is what Mac OS X's crashdump process have to say.Thread 2 Crashed:0 httpd 0x00004760 bndm + 120 (mod_include.c:317)1 httpd 0x0000a0c0 find_start_sequence + 136(mod_include.c:2388)2 httpd 0x0000ba90 send_parsed_content + 1288(mod_include.c:3054)3 httpd 0x0000dbb8 includes_filter + 1468(mod_include.c:3591)4 httpd 0x00068e20 ap_pass_brigade + 236 (util_filter.c:512)5 httpd 0x0004a588 default_handler + 1888 (core.c:3648)6 httpd 0x00060028 ap_run_handler + 136 (config.c:152)7 httpd 0x00060d7c ap_invoke_handler + 424 (config.c:364)8 httpd 0x00051f58 ap_run_sub_req + 104 (request.c:1855)9 httpd 0x00005a28 handle_include + 968 (mod_include.c:782)10 httpd 0x0000cc40 send_parsed_content + 5816(mod_include.c:3309)11 httpd 0x0000dbb8 includes_filter + 1468(mod_include.c:3591)12 httpd 0x00068e20 ap_pass_brigade + 236 (util_filter.c:512)13 httpd 0x0004a588 default_handler + 1888 (core.c:3648)14 httpd 0x00060028 ap_run_handler + 136 (config.c:152)15 httpd 0x00060d7c ap_invoke_handler + 424 (config.c:364)16 httpd 0x0001d940 ap_process_request + 148(http_request.c:249)17 httpd 0x00014564 ap_process_http_connection + 136(http_core.c:255)18 httpd 0x0006bcdc ap_run_process_connection + 136(connection.c:43)19 httpd 0x0006c318 ap_process_connection + 132(connection.c:178)20 httpd 0x0003b3c8 process_socket + 196 (worker.c:523)21 httpd 0x0003bebc worker_thread + 552 (worker.c:843)22 libapr-0.0.dylib 0x003796f0 dummy_worker + 68 (thread.c:105)23 libSystem.B.dylib 0x9002bd08 _pthread_body + 96Now, if I'm running GDB, I'm getting this.# gdb -q /opt/FrontA/apache/bin/httpd core.9148 Reading symbols for shared libraries ........... done#0 0x900148a8 in read ()(gdb) source /data/FrontA/software/httpd-2.0.61/.gdbinit(gdb) thread 2[Switching to thread 2 (core thread 1)]#0 0x9001f88c in select ()(gdb) bt#0 0x9001f88c in select ()#1 0x003851ec in ?? ()#2 0x00386728 in ?? ()#3 0x00372e2c in ?? ()#4 0x001af150 in ?? ()#5 0x001aba20 in ?? ()#6 0x0004ac48 in core_input_filter (f=0x133d060, b=0x11d4bd0,mode=AP_MODE_GETLINE, block=APR_BLOCK_READ, readbytes=0) at core.c:3802#7 0x00068d04 in ap_get_brigade (next=0x133d060, bb=0x11d4bd0,mode=AP_MODE_GETLINE, block=APR_BLOCK_READ, readbytes=0) at util_filter.c:475#8 0x0004a83c in net_time_filter (f=0x137ff70, b=0x11d4bd0,mode=AP_MODE_GETLINE, block=APR_BLOCK_READ, readbytes=0) at core.c:3705#9 0x00068d04 in ap_get_brigade (next=0x137ff70, bb=0x11d4bd0,mode=AP_MODE_GETLINE, block=APR_BLOCK_READ, readbytes=0) at util_filter.c:475#10 0x0005b51c in ap_rgetline_core (s=0x1327c58, n=8192, read=0xf0101b40,r=0x1327c40, fold=0, bb=0x11d4bd0) at protocol.c:230#11 0x0005beac in read_request_line (r=0x1327c40, bb=0x11d4bd0) at protocol.c:586#12 0x0005cb70 in ap_read_request (conn=0x133ce10) at protocol.c:877#13 0x00014648 in ap_process_http_connection () at apr_xml.c:737#14 0x0006bcdc in ap_run_process_connection (c=0x133ce10) at connection.c:43#15 0x0006c318 in ap_process_connection (c=0x133ce10, csd=0x1321200) atconnection.c:176#16 0x0003b3c8 in process_socket (p=0x11973a0, sock=0x1321200, my_child_num=7,my_thread_num=0, bucket_alloc=0x185fc18) at worker.c:522#17 0x0003bebc in worker_thread (thd=0x416c60, dummy=0x469760) at worker.c:842#18 0x003796f0 in ?? ()#19 0x9002bd08 in _pthread_body ()GDB and crashdump sees different addresses for "0x00014648 inap_process_http_connection". Is this normal? After that it seems that the stackframes differs wildly, at least in my eyes.gdb -v saysGNU gdb 6.1-20040303 (Apple version gdb-384) (Mon Mar 21 00:05:26 GMT 2005)Apache was compiled using:./configure --enable-so --enable-mods-shared=auth-anon auth-digest file-cachecache disk-cache mem-cache deflate mime-magic expires headers usertrack unique-id proxy proxy-http ssl vhost-alias cgi info speling rewrite--with-mpm=prefork --mandir=/usr/share/man --prefix=/opt/FrontA/apache-2.0.61-prefork --enable-pool-debugFive years back or so when I last used GDB I recall it had issues with threads,is that still so?Maybe I should try to see if the problem occurs with MPM prefork unless you'vegot some input on this thing?I tried doing a bunch of backtraces on other core files aswell but theirbacktraces all look like the one above.You need to have #include virtual on file which changes during request. I guess as file is mmapped and if it is not complete or changes during request you get these crashes.Reproducible. RHEL4 x86_64 Stock 2.2.10, compiled:CFLAGS='-O0 -g' ./configure --with-mpm=prefork --enable-maintainer-modeCreate shtml file with include virtual, so that it includes complex shtml-file.# cat html/test-43533.shtml <html><body><!--#include virtual="inc-43533.shtml" --></body></html># cat html/inc-43533-orig.shtml <!--#config timefmt="%M" --><!--#if expr="$date_local > 00 && $date_local < 10" --> <!--#include virtual="/foo2.html" --><!--#endif --><!--#if expr="$date_local > 09 && $date_local < 20" --> <!--#include virtual="/foo2.html" --><!--#endif --><!--#if expr="$date_local > 19 && $date_local < 30" --> <!--#include virtual="/foo2.html" --><!--#endif --><!--#if expr="$date_local > 29 && $date_local < 40" --> <!--#include virtual="/foo2.html" --><!--#endif --><!--#if expr="$date_local > 39 && $date_local < 50" --> <!--#include virtual="/foo2.html" --><!--#endif --><!--#if expr="$date_local > 49 && $date_local < 60" --> <!--#include virtual="/foo2.html" --><!--#endif -->Create job that changes file continuously:# (while sleep 1;do cp html/inc-43533-orig.shtml html/inc-43533.shtml;done)&Start prefork (or worker) apache with -X option. Request test-43533.shtml with ab and wait it to crash.Below several points.Program received signal SIGBUS, Bus error.[Switching to Thread 182897648736 (LWP 13275)]0x0000000000450d71 in find_directive (ctx=0x65f7a8, data=0x2a959131de "if expr=\"$date_local > 39 && $date_local < 50\" -->\n <!--#include virtual=\"/foo2.html\" -->\n<!--#endif -->\n<!--#if expr=\"$date_local > 49 && $date_local < 60\" -->\n <!--#include virtual=\"/foo2.html\" --"..., len=217, store=0x7fbfffee20, store_len=0x7fbfffee18) at mod_include.c:27262726 while (p < ep && !apr_isspace(*p)) {(gdb) where#0 0x0000000000450d71 in find_directive (ctx=0x65f7a8, data=0x2a959131de "if expr=\"$date_local > 39 && $date_local < 50\" -->\n <!--#include virtual=\"/foo2.html\" -->\n<!--#endif -->\n<!--#if expr=\"$date_local > 49 && $date_local < 60\" -->\n <!--#include virtual=\"/foo2.html\" --"..., len=217, store=0x7fbfffee20, store_len=0x7fbfffee18) at mod_include.c:2726#1 0x0000000000452068 in send_parsed_content (f=0x65f5e8, bb=0x65f768) at mod_include.c:3300#2 0x0000000000453075 in includes_filter (f=0x65f5e8, b=0x65f768) at mod_include.c:3651#3 0x00000000004490c4 in ap_pass_brigade (next=0x65f5e8, bb=0x65f768) at util_filter.c:526#4 0x0000000000434b44 in default_handler (r=0x65da78) at core.c:3740#5 0x000000000043bc88 in ap_run_handler (r=0x65da78) at config.c:157#6 0x000000000043c52f in ap_invoke_handler (r=0x65da78) at config.c:372#7 0x0000000000439128 in ap_run_sub_req (r=0x65da78) at request.c:1876#8 0x000000000044e027 in handle_include (ctx=0x654128, f=0x653e90, bb=0x654c40) at mod_include.c:1737#9 0x00000000004527b8 in send_parsed_content (f=0x653e90, bb=0x6540e8) at mod_include.c:3432#10 0x0000000000453075 in includes_filter (f=0x653e90, b=0x6540e8) at mod_include.c:3651#11 0x00000000004490c4 in ap_pass_brigade (next=0x653e90, bb=0x6540e8) at util_filter.c:526#12 0x0000000000434b44 in default_handler (r=0x64fa08) at core.c:3740#13 0x000000000043bc88 in ap_run_handler (r=0x64fa08) at config.c:157#14 0x000000000043c52f in ap_invoke_handler (r=0x64fa08) at config.c:372#15 0x000000000045bd5f in ap_process_request (r=0x64fa08) at http_request.c:258#16 0x0000000000458f84 in ap_process_http_connection (c=0x64bbd8) at http_core.c:190#17 0x0000000000444d25 in ap_run_process_connection (c=0x64bbd8) at connection.c:43#18 0x0000000000445156 in ap_process_connection (c=0x64bbd8, csd=0x64b9e8) at connection.c:178#19 0x00000000004747a6 in child_main (child_num_arg=0) at prefork.c:650#20 0x0000000000474876 in make_child (s=0x5b9160, slot=0) at prefork.c:690#21 0x0000000000474e0a in ap_mpm_run (_pconf=0x5b0138, plog=0x5f2348, s=0x5b9160) at prefork.c:966#22 0x0000000000423bea in main (argc=6, argv=0x7fbffff7d8) at main.c:740Program received signal SIGBUS, Bus error.[Switching to Thread 182897648736 (LWP 15546)]0x00000000004508d4 in bndm (t=0x6559b8, h=0x2a95913247 "\n<!--#if expr=\"$date_local > 49 && $date_local < 60\" -->\n <!--#include virtual=\"/foo2.html\" -->\n<!--#endif -->\n", hl=112) at mod_include.c:25202520 d &= T[(unsigned char) *p--];#0 0x00000000004508d4 in bndm (t=0x6559b8, h=0x2a95913247 "\n<!--#if expr=\"$date_local > 49 && $date_local < 60\" -->\n <!--#include virtual=\"/foo2.html\" -->\n<!--#endif -->\n", hl=112) at mod_include.c:2520#1 0x00000000004509bf in find_start_sequence (ctx=0x659778, data=0x2a95913247 "\n<!--#if expr=\"$date_local > 49 && $date_local < 60\" -->\n <!--#include virtual=\"/foo2.html\" -->\n<!--#endif -->\n", len=112) at mod_include.c:2561#2 0x0000000000451d20 in send_parsed_content (f=0x6595b8, bb=0x659738) at mod_include.c:3238#3 0x0000000000453075 in includes_filter (f=0x6595b8, b=0x659738) at mod_include.c:3651...Program received signal SIGBUS, Bus error.[Switching to Thread 182897648736 (LWP 16265)]0x0000000000450d71 in find_directive (ctx=0x65b788, data=0x2a959131de "", len=217, store=0x7fbfffee20, store_len=0x7fbfffee18) at mod_include.c:27262726 while (p < ep && !apr_isspace(*p)) {Program received signal SIGBUS, Bus error.[Switching to Thread 182897648736 (LWP 16695)]0x000000000045102b in find_arg_or_tail (ctx=0x6637c8, data=0x2a959130e7 " -->\n<!--#endif -->\n<!--#if expr=\"$date_local > 19 && $date_local < 30\" -->\n <!--#include virtual=\"/foo2.html\" -->\n<!--#endif -->\n<!--#if expr=\"$date_local > 29 && $date_local < 40\" -->\n <!--#includ"..., len=464) at mod_include.c:28232823 while (p < ep && apr_isspace(*p)) {Program received signal SIGBUS, Bus error.[Switching to Thread 182897648736 (LWP 17474)]0x0000003f31072584 in memcpy () from /lib64/tls/libc.so.6(gdb) where#0 0x0000003f31072584 in memcpy () from /lib64/tls/libc.so.6#1 0x0000002a955731ff in apr_brigade_flatten (bb=0x657830, c=0x65baa8 "/foo2.html", len=0x7fbfffed98) at buckets/apr_brigade.c:252#2 0x0000002a955732a6 in apr_brigade_pflatten (bb=0x657830, c=0x65ba88, len=0x65ba90, pool=0x65b9e8) at buckets/apr_brigade.c:294#3 0x00000000004523e1 in send_parsed_content (f=0x6575a8, bb=0x657728) at mod_include.c:3370...Program received signal SIGBUS, Bus error.[Switching to Thread 182897648736 (LWP 17906)]0x00000000004516ca in find_argument (ctx=0x659778, data=0x2a9591306d "/foo2.html\" -->\n<!--#endif -->\n<!--#if expr=\"$date_local > 09 && $date_local < 20\" -->\n <!--#include virtual=\"/foo2.html\" -->\n<!--#endif -->\n<!--#if expr=\"$date_local > 19 && $date_local < 30\" -->\n "..., len=586, store=0x7fbfffee20, store_len=0x7fbfffee18) at mod_include.c:30283028 if (intern->quote && *p == '\\') {(gdb) print *(*b->list->next)->type$16 = {name = 0x2a9558b054 "MMAP", num_func = 5, is_metadata = APR_BUCKET_DATA, destroy = 0x2a95575042 <mmap_bucket_destroy>, read = 0x2a95574f98 <mmap_bucket_read>, setaside = 0x2a9557519a <mmap_bucket_setaside>, split = 0x422698, copy = 0x421cb8}(In reply to)Nice catch. It could very well be just that.We do have frequent updates of those files (via FTP) on the server and they're rewritten each time they're updated, not moved in place.	6.0	id=43220	6	False	False	rpluem	1
id=43220	REOPENED	None	Apache httpd-2	mod_proxy (	2.2.3	PC Linux	P2 critical	Apache HTTPD Bugs Mailing List	2007-08-27 07:53 UTC by	Brian Wheeler	2008-10-22 07:27 UTC (	1 user	We've got a tomcat 5.5.23 server hiding behind an apache 2.2.3 server usingmod_proxy. the configuration is: ProxyPass /confluence/ ajp://feta.dlib.indiana.edu:8079/confluence/ ProxyPassReverse /confluence/ ajp://feta.dlib.indiana.edu:8079/confluence/When we access the application directly (i.e. on port 8070) the data comes backok, but when using the proxy, it is truncated. The data we're sending back is >2M, so most requests are ok, and sometimes it does work for larger requests. The test url through the proxy for this is:or directlyFeta and wiki are different IPs, but both the tomcat and the apache instance arelistening on all IPs. The content-length header coming back to the browserseems to be correct -- when it failed on a wget request, wget retried.	Please provide the output of your error_log file. Sorry for being confusedregarding your size observations.You mean requests below 2 GB are fine and request above 2 GB only work sometimes?The proxy is cutting off data less than 2 Meg, not G. The test URL is ~8M andon some requests I may only get 500K. However, sometimes (using wget, but neverfirefox, it seems) I will get the whole file, though wget had to retry andmanaged to get all of the content.I went to the page:and clicked on the boot.iso link. The original file is 8,161,280 bytes. Thesemessages were generated in the log:[Mon Aug 27 16:37:47 2007] [error] ajp_check_msg_header() got bad signature d3e6[Mon Aug 27 16:37:47 2007] [error] ajp_ilink_receive() received bad header[Mon Aug 27 16:37:47 2007] [error] ajp_read_header: ajp_ilink_receive failed[Mon Aug 27 16:37:47 2007] [error] (120007)APR does not understand this errorcode: proxy: send body failed to 156.56.241.30:8079 (feta.dlib.indiana.edu)and only 663,544 bytes were transferred.If I go towhich is the tomcat instance without the proxy, clicking on the boot.iso linkgives me the entire file as expected.(In reply to)TheseIt seems that your Tomcat is sending wrong data. So please:- Increase the loglevel of your Apache to debug.- Try sniffing the ajp traffic between your Apache and your Tomcat and attachthe sniff file.I've done the proxy from my workstation's apache (2.2.4 vs 2.2.3 of theproduction server) and it still does the same thing. Here's the error messagesgenerated when setting LogLevel debug and requesting the file. I will attach adump from wireshark of the transaction.[Tue Aug 28 08:20:19 2007] [debug] mod_proxy_ajp.c(44): proxy: AJP:canonicalising URL//feta.dlib.indiana.edu:8079/confluence/download/attachments/25/boot.iso[Tue Aug 28 08:20:19 2007] [debug] proxy_util.c(1378): [client 127.0.0.1] proxy:ajp: found worker ajp://feta.dlib.indiana.edu:8079/confluence/ forajp://feta.dlib.indiana.edu:8079/confluence/download/attachments/25/boot.iso,referer:[Tue Aug 28 08:20:19 2007] [debug] mod_proxy.c(777): Running scheme ajp handler(attempt 0)[Tue Aug 28 08:20:19 2007] [debug] mod_proxy_http.c(1652): proxy: HTTP:declining URLajp://feta.dlib.indiana.edu:8079/confluence/download/attachments/25/boot.iso[Tue Aug 28 08:20:19 2007] [debug] mod_proxy_ajp.c(507): proxy: AJP: serving URLajp://feta.dlib.indiana.edu:8079/confluence/download/attachments/25/boot.iso[Tue Aug 28 08:20:19 2007] [debug] proxy_util.c(1798): proxy: AJP: has acquiredconnection for (feta.dlib.indiana.edu)[Tue Aug 28 08:20:19 2007] [debug] proxy_util.c(1859): proxy: connectingajp://feta.dlib.indiana.edu:8079/confluence/download/attachments/25/boot.iso tofeta.dlib.indiana.edu:8079[Tue Aug 28 08:20:19 2007] [debug] proxy_util.c(1955): proxy: connected/confluence/download/attachments/25/boot.iso to feta.dlib.indiana.edu:8079[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(218): Into ajp_marshal_into_msgb[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(284): ajp_marshal_into_msgb:Header[0] [Host] = [localhost][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(284): ajp_marshal_into_msgb:Header[1] [User-Agent] = [Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.5)Gecko/20070718 Fedora/2.0.0.5-1.fc7 Firefox/2.0.0.5][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(284): ajp_marshal_into_msgb:Header[2] [Accept] =[text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(284): ajp_marshal_into_msgb:Header[3] [Accept-Language] = [en-us,en;q=0.5][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(284): ajp_marshal_into_msgb:Header[4] [Accept-Encoding] = [gzip,deflate][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(284): ajp_marshal_into_msgb:Header[5] [Accept-Charset] = [ISO-8859-1,utf-8;q=0.7,*;q=0.7][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(284): ajp_marshal_into_msgb:Header[6] [Keep-Alive] = [300][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(284): ajp_marshal_into_msgb:Header[7] [Connection] = [keep-alive][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(284): ajp_marshal_into_msgb:Header[8] [Referer] =[][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(284): ajp_marshal_into_msgb:Header[9] [Cookie] = [JSESSIONID=AEAB20432F03EE55B61C3DF792948B91;wikiuserUserID=1; wikiuserUserName=Admin;wikiuserToken=0a2ca7abde82342684d88f18c80498bf][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(284): ajp_marshal_into_msgb:Header[10] [Max-Forwards] = [10][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(424): ajp_marshal_into_msgb: Done[Tue Aug 28 08:20:19 2007] [debug] mod_proxy_ajp.c(188): proxy: APR_BUCKET_IS_EOS[Tue Aug 28 08:20:19 2007] [debug] mod_proxy_ajp.c(193): proxy: data to read(max 8186 at 4)[Tue Aug 28 08:20:19 2007] [debug] mod_proxy_ajp.c(208): proxy: got 0 bytes of data[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 04[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 04[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(484): ajp_unmarshal_response:status = 200[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(495): ajp_unmarshal_response:Number of headers is = 5[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(557): ajp_unmarshal_response:Header[0] [Last-Modified] = [Mon, 27 Aug 2007 13:52:44 GMT][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(557): ajp_unmarshal_response:Header[1] [ETag] = ["1188222764000"][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(557): ajp_unmarshal_response:Header[2] [Content-Disposition] = [inline; filename="boot.iso"][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(557): ajp_unmarshal_response:Header[3] [Content-Type] = [application/x-cd-image;charset=UTF-8][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(567): ajp_unmarshal_response:ap_set_content_type done[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(557): ajp_unmarshal_response:Header[4] [Content-Length] = [8161280][Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(643): ajp_read_header:ajp_ilink_received 03[Tue Aug 28 08:20:19 2007] [debug] ajp_header.c(653): ajp_parse_type: got 03[Tue Aug 28 08:20:19 2007] [error] ajp_check_msg_header() got bad signature 5528[Tue Aug 28 08:20:19 2007] [error] ajp_ilink_receive() received bad header[Tue Aug 28 08:20:19 2007] [error] ajp_read_header: ajp_ilink_receive failed[Tue Aug 28 08:20:19 2007] [debug] mod_proxy_ajp.c(395): (120007)APR does notunderstand this error code: ajp_read_header failed[Tue Aug 28 08:20:19 2007] [error] (120007)APR does not understand this errorcode: proxy: send body failed to 156.56.241.30:8079 (feta.dlib.indiana.edu)[Tue Aug 28 08:20:19 2007] [debug] proxy_util.c(1816): proxy: AJP: has releasedconnection for (feta.dlib.indiana.edu)Createdpcap dump from wireshark of truncated transactionThis is the proxy request from my workstation to the tomcat server viamod_proxy.Thanks for the feedback. According to my first analysis of the pcap dump Tomcatsents the data correctly. The debug messages from httpd are not sufficient forme for further analysis. It seems that some debug messages get written to themain server error log and not to the error log of the virtual host which youadded. Can you please also add the corresponding entries from the main server log?I'm not sure where to look, since I didn't put the redirect into a virtual host. I set "LogLevel" to debug -- is there a different setting I should have used? What error message are you looking for? Maybe I can grep around and see whereit went.Sorry for creating confusion. I assume that you use a virtual host and that thisvirtual host contains an ErrorLog directive. I guess you put excerpts from thisfile into your report. There should be also an ErrorLog directive outside of anyvirtual host and I need the appropriate messages of this file. Of course thisrequires that you set LogLevel to debug outside of the virtual host.If this is not the case could you please repeat your debugging session andattach the messages from both logs plus the dump.And it does not matter that the ProxyPass is outside of the virtual host. Itonly matters that there is one with a separate ErrorLog directive.CreatedError LogFull error log with truncated outputI've attached the full error log from a startup and a few hits ending with thetruncated output download. This should be all the logging messages there are.Thanks for the quick response, but I see no failing request in your error_log.Your request to /confluence/download/attachments/25/boot.iso results in a 304(not modified). Please clear the cache of your browser and try again.Forgot one thing: I am sorry but I need a network dump AND the error_log for thesame failing request as I need to analyse this data together. Sorry for theinconvenience.Createdpacket dump for truncated request (part 1 of 2)Here's a packet dump from the startup of apache to after the truncated request. I had to split it due to size.Createdpacket dump for truncated request (part 2 of 2)This is part 2 of 2.CreatedError log for truncated transactionThis is the error log in questionThanks for the update. I am still missing the debug message I expected :-(. Thisis not your fault. I do not know why it does not show up. I need to think about it.Are there any thoughts on this issue? Brian, did you resolve this?I resolved it in a round-about sort of way: I used the http proxy instead ofthe ajp proxy.We're seeing this problem too. The tomcat configuration works fine with Apache1.3, but we recently decided to move to Apache2 and it's broken for large filessent with AJP.This is with Apache 2.2.4 on gentoo, tomcat 5.5.25, so slightly more up to datethan Brian's case, and it's still there.This is preventing us moving to Apache2 at the moment...Some more testing on our systems:* truncation happens reliably on any ajp response page longer than 1 MB, andintermittently on responses in the 100 - 500 kB range (the longer the response,the more likely to have a problem)* problem is there for apache 2.2.6 server (latest download) as well.* However, the problem is only there when the tomcat server is on a linux box.Running the identical tomcat server on a Mac OS X box resulted in no ajptruncation problems.* We've tried this with apache-tomcat-5.5.20 and 5.5.25, same issue.* on the apache side in debug mode we're seeing the following error messageswhen it happens: [error] ajp_check_msg_header() got bad signature 3d22 [error] ajp_ilink_receive() received bad header [error] ajp_read_header: ajp_ilink_receive failed...except the '3d22' may be some other random 4-hex number* on the tomcat side we're seeing an IOException (5.5.25): at org.apache.coyote.ajp.AjpAprProcessor.flush(AjpAprProcessor.java:1199) atorg.apache.coyote.ajp.AjpAprProcessor$SocketOutputBuffer.doWrite(AjpAprProcessor.java:1284) at org.apache.coyote.Response.doWrite(Response.java:560) atorg.apache.catalina.connector.OutputBuffer.realWriteBytes(OutputBuffer.java:361) at org.apache.tomcat.util.buf.ByteChunk.append(ByteChunk.java:352)...though presumably tomcat is seeing the IO error because apache has closed theconnection?However, the problem is gone when we use tomcat 6.0.14. Maybe time to switch tothat... It would be nice to know what was causing this issue though!What AJP connector did you use with 5.5.x? Despite one of my first comments Inow think that the Tomcat connector sends wrong data, but I did not have thetime to check this further and prove it.. It would be useful to know whether youused the "classic" blocking pure java ajp connector or the APR ajp connector.(In reply to)Here's the server.xml line: <Connector port="8009" enableLookups="false" redirectPort="8443"protocol="AJP/1.3" />- i.e. no mention of APR. On the other hand, the tomcat stack trace started in:org.apache.coyote.ajp.AjpAprProcessorwhich sounds like an APR version of the connector. I'm afraid I'm not familiarenough with that to know if that answers the question or not!Ah, if APR = Apache Portable Runtime, then no, we're not using that at themoment, just pure java on the tomcat side.(In reply to)This should use the APR connector if the APR libraries are present. If not thepure java connector is used.Ups. My fault I should have read your answer more closely. So yes, you used theAPR (= Apache Portable Runtime) connector in the case where the error occurred.Maybe you only used the APR connector on Linux and not on Mac OS (as the APRlibraries were possibly not present on Mac OS).Yup, APR's the problem! Those tricky sysadmin's, I hadn't realized it was set upon our server.I removed the /usr/lib/libtcnative* libraries, restarted the tomcat-5.5.20server, and the truncation problem was gone. Restored them, and the truncationsare back.So, who's responsible for the APR AJP stuff?!It turns out we didn't have the latest 'tomcat-native' package for gentoo. Theone that was causing problems was version 1.1.7. We've now updated totomcat-native-1.1.10 and the truncation problem is gone!It seems likely this was the same problem Brian was having - should probablyconfirm this, but if so this bug can probably be considered resolved. Thanks.No further feedback. I assume it was a bug in tcnative and upgrading it fixedthe problem.(In reply to)Hmm, I'm afraid we are still seeing this here with tcnative 1.12.When starting tomcat with "-Djava.library.path=/usr/local/apr/lib" we get:ajp_check_msg_header() got bad signature 420[error] ajp_ilink_receive() received bad header[error] ajp_read_header: ajp_ilink_receive failed[error] (120007)APR does not understand this errorcode: proxy: send body failed to 127.0.0.1:8009When starting tomcat without this env variable it runs fine.- Redhat EL4 x86_64 U6, kernel 2.6.9-67.0.7.ELsmp x86_64- java version "1.5.0_13"Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_13-b05)Java HotSpot(TM) 64-Bit Server VM (build 1.5.0_13-b05, mixed mode) - Apache Tomcat/5.5.25- ./httpd -VServer version: Apache/2.2.8 (Unix)Server built: Mar 31 2008 15:17:07Server's Module Magic Number: 20051115:11Server loaded: APR 1.2.12, APR-Util 1.2.12Compiled using: APR 1.2.12, APR-Util 1.2.12Architecture: 64-bitServer MPM: Worker threaded: yes (fixed thread count) forked: yes (variable process count)- # ll /usr/local/apr/lib/insgesamt 2316-rw-r--r-- 1 root root 1502768 31. Mär 15:24 libtcnative-1.a-rwxr-xr-x 1 root root 953 31. Mär 15:24 libtcnative-1.lalrwxrwxrwx 1 root root 23 31. Mär 15:24 libtcnative-1.so -> libtcnative-1.so.0.1.12lrwxrwxrwx 1 root root 23 31. Mär 15:24 libtcnative-1.so.0 -> libtcnative-1.so.0.1.12-rwxr-xr-x 1 root root 850307 31. Mär 15:24 libtcnative-1.so.0.1.12drwxr-xr-x 2 root root 4096 31. Mär 15:24 pkgconfigCan you confirm that this is actually a bug in HTTPD...Since we're no longer using it, I can't directly confirm it is an httpd issue, but since it doesn't appear when using mod_jk or when connecting to the tomcat instance directly, it does kind of point that way.	31.0	id=43533	5	False	False	jorton	1
id=44031	REOPENED	None	Apache httpd-2	mod_mime (	2.0.55	PC Linux	P2 major	Apache HTTPD Bugs Mailing List	2007-12-06 04:12 UTC by	Michal Jurosz	2007-12-06 07:22 UTC (	0 users	I configure my virtual server:# bug.mj41.cz<VirtualHost *:80> ServerName bug.mj41.cz # etc <Location /dir> RemoveHandler .php ForceType text/plain </Location> <Location /dir/subdir> RemoveHandler .php ForceType text/plain </Location></VirtualHost>I put the same 'index.php' to directories'/''/dir/''/dir/subdir'(inside DocumentRoot).Result forandare ok.But what about result of?'dir/subdir/index.php' output is"php runs"so it is not same as that of 'dir/index.php' which is"<?phpecho "php runs\n";?>".Server runs Apache/2.0.52 (CentOS).	Use <Directory> not <Location> for local contents. Otherwise you WILL getunexpected results, as you just found out.Please use a user support forum for configuration questions.Thanks for response. I tried Directory too (My configuration tested with 'denyfrom all'). Same effect.Another ideas? Thanks.BTW: I posted this to apache-httpd-users (), but nobody answered.	2.0	id=44031	4	False	False	nick	1
id=43649	REOPENED	None	Apache httpd-2	mod_autoindex (	2.2.21	PC FreeBSD	P2 trivial	Apache HTTPD Bugs Mailing List	2007-10-18 08:11 UTC by	Jose Kahan	2012-01-02 21:28 UTC (	0 users	mod_autoindex is generating invalid XHTML markup. The xhtml namespace was missing in emit_preamble(). I'm including a patch against both 2.2.6 and trunk that fixes it.Bug report 34519 is quite similar, but mentions other enhancements (and didn't include a patch). I decided to make a new bug report as I was not sure if it was OK to reply to one of the points given in the previous report.	Createdmod_autoindex was not adding the xhtml namespacePatch against 2.2.6Createdmod_autoindex was not adding the xhtml namespacePatch against trunkCommitted to trunk as().Thanks for the patch.Proposed for backport as().Here's an idea:These autoindexes should not only be valid markup but promote open standards(valid markup) by linking back to W3C's Validator. If you are open to the ideait would be ideal if Apache were to include W3C valid markup logos in the /iconsdirectory so as to distribute load of serving those icons on all the new validautoindex URIs.In turn we can promote adoption of Apache 2.2, as apparently many are still 1.3,on the Validator's results page eg=We could give an alternate markup for Apache instances >2.2.N to all users orperhaps just for sites we detect the Server: header as still Apache 1.3 or 2.0 <p> <a href=""><img src="/icons/valid-xhtml10" alt="Valid XHTML 1.0 Strict" height="31" width="88" /></a> </p>I don't like it.to 2.2 as<>mod_autoindex is generating invalid XHTML 1.0 Transitional for fancy list.Configuration:IndexOptions Charset=utf-8 XHTML FancyIndexing FoldersFirst VersionSortIndexHeadInsert "<meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" />"HeaderName secret.htmlReadmeName secret.htmlThere is problem with hr markups:document type does not allow element "hr" here; missing one of "button", "ins", "del" start-tagYou can read it here:Its should be outside pre markup. They are created in lines 1603-1608 and 1814-1819. Problem can be solved with replece specifed lines:1604 -> ap_rputs("</pre><hr", r);1608 -> ap_rputs("><pre>", r);insert ap_rputs("</pre>\n", r); under 18131819 ap_rputs(">\n", r);delete 1821, 1822, 1823I don't test it, because I am a new FreeBSD and Apache user, and I not able to do a lot of things now :/	8.0	id=43649	10	False	True	jose	1
id=45023	REOPENED	None	Apache httpd-2	mod_deflate (	2.2.8	All All	P2 regression	Apache HTTPD Bugs Mailing List	2008-05-16 23:53 UTC by	Noah	2017-02-13 10:17 UTC (	9 users	On my server, I recently added the DEFLATE filter for all application/x-javascript files, and noticed that apache has stopped sending 304 NOT MODIFIED on ALL my js files *always*.Observe this header trace:-----------------------------------------GET /main.js HTTP/1.1Host: xxxxxxxxxxxxxUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.14) Gecko/20080404 Firefox/2.0.0.14Accept: */*Accept-Language: en-us,en;q=0.5Accept-Encoding: gzip,deflateAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7Keep-Alive: 300Connection: keep-aliveIf-Modified-Since: Mon, 12 May 2008 16:09:00 GMTIf-None-Match: "44d0ac3fd1f00"-gzipCache-Control: max-age=0HTTP/1.x 200 OKDate: Sat, 17 May 2008 06:09:25 GMTServer: ApacheLast-Modified: Mon, 12 May 2008 16:09:00 GMTEtag: "44d0ac3fd1f00"-gzipAccept-Ranges: bytesCache-Control: max-age=86400, must-revalidate, privateExpires: Sun, 18 May 2008 06:09:25 GMTVary: Accept-EncodingContent-Encoding: gzipContent-Length: 9797Keep-Alive: timeout=2, max=299Connection: Keep-AliveContent-Type: application/x-javascript-----------------------------------------It seems to me, that a 304 NOT MODIFIED would have been the appropriate response here.Before I added DEFLATE to javascript files, the above response was always 304 NOT MODIFIED (except the first one).Now with deflate, it is *always* 200 OK.Is this by design, or is this a bug?One thing to mention is - I have both mod_deflate and mod_expires statically compiled into httpd.	If we remove "-gzip" from Etag for If-None-Match, a server returns 304.mod_deflate adds "-gzip" and sends it to us, but condition check of If-None-Match runs before mod_deflate ...???This is actually a fix of. Yes, we know the core code testing for conditional responses still needs work.This bug nearly defeats the purpose of mod_deflate. I turned mod_deflate on with the intent of saving bandwidth when sending JavaScript and CSS files. Instead, I'm using *more* bandwidth for every request but the first. (Previously, subsequent requests would result in small, no-content 304 responses. Now the compressed JS and CSS files are sent every time.)BTW, the change that appends "-gzip" to the etag (, I think) is a bit wacky in its own right in that it appends outside the double quotes. IOW, if the original header is: Etag: "5954c6-10f4-449d11713aac0"the modified header ends up as: Etag: "5954c6-10f4-449d11713aac0"-gzipwhen it probably should be: Etag: "5954c6-10f4-449d11713aac0-gzip"with the "-gzip" inside the quotes. I altered the code to do this, but it had no affect on this 304 bug. I just wanted to note it here so the situation is avoided when this bug is fixed for real.There is a workaround, If you are serving from a location say /jsYou can use a configuration like below to switch ETag: $1-gzip to $1 <Location /js>RequestHeader edit "If-None-Match" "^(.*)-gzip$" "$1"Header edit "ETag" "^(.*[^g][^z][^i][^p])$" "$1-gzip"</Location>(Perhaps this should be done from mod_deflate as input filterwith the condition that if Client requests with Accept-Encoding: gziponly then modify If*match headers to their un-gzip values.)(Also the current value of ETag (+gzip) is not RFC compliant, since RFC mandatesa quoted string as the value of ETag. As the noted by john, -gzip should be inside quotes.)CreatedA POC slightly better than the above conf lines,Adds an input filter ETAG that can be used like below, along with DEFLATEto let apache recognize ($1)-gzip as $1AddInputFilter ETAG .txtAddOutputFilterByType DEFLATE text/plainBoth the workaround inand the patch inappear to work. Neither are real "fixes" for the bug, however. (But I'm glad I have something to work with until a real fix arrives, so thanks rahul!)Createduse ap_hook_post_read_request instead of input filterIncorporate suggested changes by NickHello!I'm also facing the same problem -- I had been scratching my head for sometime before I found this bug report. I have to reduce my page size urgently and not having gzip on is not really an option for me.I'm not really comfortable having a customized compiled version of apache2 on my system and the regular expression "hack" will also slow things down (will it?).Any alternative ideas? When might this bug be fixed?Thanks,SidharthI had a look at ap_hook_post_read_request attachment...seems like I can get away with just compiling mod_deflate.cI'm using Apache 2.2.8. Can anyone help me on where I can get the correct source for the corresponding mod_deflate and how I can compile mod_deflate?Thanks for your help!SidharthHi, Sorry for flooding this mailing list.In this thread there is Rahul's mod_deflate patch and there are some patches in this bug thread:(mostly by Nick Kew)Which one is the best solution?Thanks,SidharthYou will need both. This patch expects the other to have been applied (this is already applied on trunk).Also please use thefor questions.Thanks for your response Rahul.Unfortunately I am new to all of this. I applied Nick's patch to my mod_deflate.c but I got patching errors...his patch is assuming a slightly different mod_deflate.c from mine.Would really appreciate if you were able to direct me to the place (or even better just gave me the patched mod_deflate.c version) that I could just compile.[ASIDE: I am using apxs2 -c mod_deflate.cwith LDFLAGS="-lz" set in /usr/bin/aprconfigI notice that my original mod_deflate.so depends on libpthread. When I apply your patch to my file and compile, I don't get dependency with libpthread.]Please tell me if I would need to do any additional transformations to the file you would provide me (assuming you can :-) ).Thanks a Ton!Sidharth[Attaching the mod_deflate that I have. This I obtained by issuingsudo apt-get install apache2-src on ubuntu. This is version 2.2.8 Apache]Createdmod_deflate as found in apache2-src package on ubuntu version 2.2.8-1ubuntu0.3This bug was added in 2.2.8 in a failed attempt to address.Since a real fix will require extensive changes, I have reverted thechange inNote: if you are patching a released version of the source code,then extract the patch from the original changeand use patch -R to apply it in reverse.Meanwhile, the original bug will still be tracked as issue 39727.I'm still experiencing this with 2.2.11-2ubuntu2 -- but adding these 2 lines are suggested by rahul works:RequestHeader edit "If-None-Match" "^(.*)-gzip$" "$1"Header edit "ETag" "^(.*[^g][^z][^i][^p])$" "$1-gzip"Was this ever resolved in trunk/2.4?Alias /deflate /home/covener/SRC/httpd-trunk/built/htdocs/index.html<Location /deflate>SetOutputFilter DEFLATE</location>$ wget --header="Accept-Encoding: gzip" -S-O/dev/null 2>&1 |grep ETag ETag: "58c5-4b26c6f28ce80-gzip"$ wget --header='If-None-Match: "58c5-4b26c6f28ce80-gzip"' --header="Accept-Encoding: gzip" -S-O/dev/null 2>&1--2013-06-23 13:39:47--Resolving localhost (localhost)... 127.0.0.1Connecting to localhost (localhost)|127.0.0.1|:80... connected.HTTP request sent, awaiting response... HTTP/1.1 200 OK Date: Sun, 23 Jun 2013 17:39:47 GMT Server: Apache/2.5.0-dev (Unix) OpenSSL/1.0.1c Last-Modified: Wed, 23 Nov 2011 20:04:58 GMT ETag: "58c5-4b26c6f28ce80-gzip" Accept-Ranges: bytes Vary: Accept-Encoding Content-Encoding: gzip Content-Length: 206 Keep-Alive: timeout=5, max=100 Connection: Keep-Alive Content-Type: text/htmlcovener@cov-w520:~/SRC/httpd-trunk$ wget --header='If-None-Match: "58c5-4b26c6f28ce80"' --header="Accept-Encoding: gzip" -S-O/dev/null 2>&1|grep HTTP/1.1 HTTP/1.1 304 Not ModifiedIt's a copout, but I've made this configurable inwith a default for now of maintaining the 2.4 behavior.***has been marked as a duplicate of this bug. ***Eric, I am graceful for the new directive, but why not implementing a better ETAG verification instead of altering the ETAG generation?.If you get an etag with a "-gzip" suffix, you just verify the etag without that suffix. You could link this to the "accept-encoding" header, maybe.(In reply to Jesús Cea from)My immediate concern is the unnecessary difference between 2.2 and 2.4, allowing the 2.2 behavior in 2.4 is a lot simpler to tackle then adding some third behavior (I admitted before it was a copout)(In reply to Eric Covener from)Hi Eric,will your workaround ofmake it into some Apache 2.4 release? I saw that it is integrated in 2.5, but it is not mentioned in the 2.4 docs.I tried the DeflateAlterETag directive, but it does not seem to available in Ubuntu 14.04 LTS (Apache 2.4.7).Since you only added a directive, this cannot break any existing server configurations, so it should be available in 2.4, too!Am I correct to read inthat the If-None-Match header can contain mulitple ETAG values?If so, maybe an alternative version of rahuls workaround could be:RequestHeader edit "If-None-Match" '^"((.*)-gzip)"$' '"$1", "$2"'With above line, I think there is no need to modify the outgoing Header and the modefied RequestHeader works in with or without deflation	22.0	id=45023	23	False	True	takashi.asfbugzilla	1
id=44503	REOPENED	None	Apache httpd-2	mod_ssl (	2.2.4	Sun OpenBSD	P2 blocker	Apache HTTPD Bugs Mailing List	2008-02-27 18:09 UTC by	Brendon Matthews	2016-04-18 11:44 UTC (	0 users	I have installed a standard apache 2.2.4 package on OpenBSD 4.2, and have found a major problem.I set up 4 virtual hosts on the same IP address:*.80 -> no sslXXX.XXX.XXX.XXX:443 -> SSL enabledXXX.XXX.XXX.XXX:40002 -> SSL enabledXXX.XXX.XXX.XXX:40003 -> SSL enabledAll 3 have been configured correctly to use SSL, and each one uses its own dummy SSL certificate.The server starts fine, and all 3 virtual hosts work as expected. Testing each one individually on a web browser shows that they all work fine.However, after a period of time under a moderate load the 3rd SSL site starts to have great difficulty figuring out which host it's supposed to be using, and ends up dropping out of SSL to the default virtual host on port 80. But this seems to happen AFTER the browser has already established the initial handshake with SSL.This results in errors similar to the following in the default host's error log: [Tue Feb 26 16:35:00 2008] [error] [client XXX.XXX.XXX.XXX] Invalid method in request \x16\x03\x01The SSL virtual host shows no errors, even with the LogLevel set to debug.The web browser comes up with an error message: "Unrecognized SSL message, plaintext connection?"If i disable all SSL virtual hosts and enable each one individually i get no errors, even under a heavy load, so the configuration seems to be fine. There just seems to be a major problem with the mod_ssl module making it unable to handle more than one SSL host for very long.	Could this be another manifestation of?I have just implemented the workaround suggested in, but the problem still persists. But from the looks of it, this could be related.It looks like it's only happening when 2 or more requests come in at around the same time, suggesting some kind of synchronization issue between requests?I wondered if perhaps it was something to do with the SSL Session Cache:SSLSessionCache shm:logs/ssl_scache(512000)Once apache is started i can see the following in the logs:[Fri Feb 29 09:04:29 2008] [info] Shared memory session cache initialisedHowever, it looks like the shared memory file is never created.There is no file named ssl_scache in my logs directory, and there are no shared memory segments showing up either:# ipcs Message Queues:T ID KEY MODE OWNER GROUPShared Memory:T ID KEY MODE OWNER GROUPSemaphores:T ID KEY MODE OWNER GROUPWhatever value i use for SSLSessionCache appears to make no difference to the stability of the server.I get the same sort of problem with the SSLMutex directive:SSLMutex file:logs/ssl_mutex- No file is ever createdI hope all this helps.Please give a minimal configuration which reproduces this error.Configs are as follows:<VirtualHost *:80> DocumentRoot "/var/apache2/htdocs" ServerName default.myhost.com ServerAdmin</VirtualHost><VirtualHost 192.168.1.16:443>DocumentRoot /var/web/testServerName test.myhost.comServerAdminSSLEngine onSSLCertificateFile /etc/apache2/server.crtSSLCertificateKeyFile /etc/apache2/server.keyCustomLog logs/testsite.ssl_request_log \ "%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \"%r\" %b"SetEnvIf User-Agent ".*MSIE.*" nokeepalive \ ssl-unclean-shutdown downgrade-1.0 force-response-1.0</VirtualHost><VirtualHost 192.168.1.16:40002>DocumentRoot /var/web/test2ServerName test2.myhost.comServerAdminSSLEngine onSSLCertificateFile /etc/apache2/server2.crtSSLCertificateKeyFile /etc/apache2/server2.keyCustomLog logs/testsite2.ssl_request_log \ "%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \"%r\" %b"SetEnvIf User-Agent ".*MSIE.*" nokeepalive \ ssl-unclean-shutdown downgrade-1.0 force-response-1.0</VirtualHost><VirtualHost 192.168.1.16:40003>DocumentRoot /var/web/test3ServerName test3.myhost.comServerAdminSSLEngine onSSLCertificateFile /etc/apache2/server3.crtSSLCertificateKeyFile /etc/apache2/server3.keyCustomLog logs/testsite3.ssl_request_log \ "%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \"%r\" %b"SetEnvIf User-Agent ".*MSIE.*" nokeepalive \ ssl-unclean-shutdown downgrade-1.0 force-response-1.0</VirtualHost>NOTE: I get the same results when i configure 3 different IP addresses using port 443, but this is an easier configuration to test.*** This bug has been marked as a duplicate of***Reopen if still an issue.	6.0	id=44503	5	False	False	nick	1
id=45049	REOPENED	None	Apache httpd-2	mod_cache (	2.2.8	PC All	P2 normal	Apache HTTPD Bugs Mailing List	2008-05-20 13:11 UTC by	nick pace	2015-09-01 18:25 UTC (	3 users	Hi, I’m running apache 2.2.8 with mod_cache + mod_mem_cache. I’ve configured all content to be cached. Content is served from a backend server. I’ve found if a “non cached page” is requested and the socket to apache is terminated before all content is read from backend server and dispatched to client, mod_mem_cache caches partial content! When I make a subsequent request for the same page, the partial content is returned from cache. In apache 2.0 the configuration value CacheForceCompletion details the cache will only be populated with complete content. This config value has now been removed! Is there a flag to control such behaviour? I require all content stored in cache to be 100% and never partial. At present to no avail I’ve found nothing detailing the above issues! So, I had a look at mod_mem_cache and plugged in the below code as a temporary solution which works a treat. /* * FD cacheing is not enabled or the content was not * suitable for fd caching. */ if (mobj->m == NULL) { mobj->m = malloc(mobj->m_len); if (mobj->m == NULL) { return APR_ENOMEM; } obj->count = 0; } cur = (char*) mobj->m + obj->count;// START CHANGE if (r->connection->aborted==1) { // Connection aborted, do not cache this page! ap_log_error(APLOG_MARK, APLOG_INFO, 0, r->server, "socket aborted!“);obj->count = 0;return APR_ENOMEM; }// END CHANGE /* Iterate accross the brigade and populate the cache storage */ for (e = APR_BRIGADE_FIRST(b); e != APR_BRIGADE_SENTINEL(b); e = APR_BUCKET_NEXT(e)) {I need to understand if I’ve missed something or the behaviour I’m experiencing is a feature! Thanks again...I've already posted a number of questions on other sites but no one has been able to answer. Cheers, Nick	Just want to briefly confirm this one (using debian etch's 2.2.3)Ran into it in a reverse proxy setup and did now switch to using mod_disk_cache + tmpfs. That seems to have solved the problem for us. It's a nasty show-stopper and opens gates for denial-of-service (or call it corrupt-service?) attacks. Think a warning should be added to the docs.Will drop back when I have found time to do some synthetic tests so I may give more detail.If possible, please add the change as a unified diff (diff -u).This behavior was still around in 2.2.x. Fixed in.(In reply to Edward Lu from)Sorry, not actually fixed yet - only proposed for integration.	4.0	id=26153	7	True	True	chip	1
id=45801	REOPENED	None	Apache httpd-2	mod_ssl (	2.2.9	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2008-09-13 13:39 UTC by	thawn	2013-08-07 11:15 UTC (	3 users	I would like to achieve the following behavior:On a http server, a subdirectory should only be accessible via httpsIf http is tried, the user should be automatically redirected to the https pageFrom the intranet the subtree is accessible without authenticationFrom the internet the subtree needs authenticationMy config:<Location /opendb># Network Access Control Order Deny,Allow Deny from all Allow from 192.168.0 Allow from 127.0.0.1# Authentication AuthType Basic AuthName "Open Media Database" AuthUserFile /var/svn/conf/svnbackupusers Require valid-user# Allow Network Access and/or Basic Auth Satisfy Any# Require HTTPS and redirect if HTTP is used SSLRequireSSL SSLOptions +StrictRequire ErrorDocument 403 /bin/httpsredirect.php</Location>The behavior i get is the following:from the intranet (192.168.0.x) the page behaves as expected, redirecting the user to the https page without authenticationhowever from the internet, the user is asked to authenticate, but is not redirected to the https page (resulting in plain text transfer of username and password)	This cannot work due to the order how both access checks are processed. The IP address check comes first and if it fails the SSL requirements will not be checked anymore. Thus no redirect happens.Try the following:RewriteCond %{HTTPS} =OffRewriteRule ^/opendb($|/.*)<Location /opendb># Network Access Control Order Deny,Allow Deny from all Allow from 192.168.0 Allow from 127.0.0.1# Authentication AuthType Basic AuthName "Open Media Database" AuthUserFile /var/svn/conf/svnbackupusers Require valid-user# Allow Network Access and/or Basic Auth Satisfy Any</Location>(In reply to)My fault. It needs to beRewriteRule ^/opendb($|/.*)[R,L]of course.I do not agree that this is a simple misconfiguration. Therefore I disagree with having this bug assigned the status resolved.Reasons:1. If I use the suggested configuration, I get double log ins one to the http page and one to the https page. This is absolutely inacceptible because the reason for using https in the first place was to have the authentication transmitted ssl encrypted.2. I use Satisfy Any, so if the check for the ip address fails, apache should still check for SSL and authentication.3. The description of SSLOption StrictRequire says:"This forces forbidden access when SSLRequireSSL or SSLRequire successfully decided that access should be forbidden. Usually the default is that in the case where a ``Satisfy any'' directive is used, and other access restrictions are passed, denial of access due to SSLRequireSSL or SSLRequire is overridden (because that's how the Apache Satisfy mechanism should work.) But for strict access restriction you can use SSLRequireSSL and/or SSLRequire in combination with an ``SSLOptions +StrictRequire''. Then an additional ``Satisfy Any'' has no chance once mod_ssl has decided to deny access."Which, to my understanding, means that if StrictRequire is on, it should not be possible to access the page without ssl at all, period. This is a feature which is security sensitive and should not fail to work!Especially point 3 is, in my opinion a strong argument to reopen the bug.The desire to redirect from http to https is a common one. Getting it right is important for security. One can use mod_rewrite to do this, but not if SSLRequireSSL is specified. One of the reasons for SSLRequireSSL is "for defending against configuration errors that expose stuff that should be protected". It's handy to have a simple, one-line configuration which provides that security.My suggestion: create an optional parameter so that one could put in a config file "SSLRequireSSL Redirect". This would issue a redirect to non-SSL connections before any other access controls or authentication would be tested.I think the bug here is that ssl_hook_Access runs as APR_HOOK_MIDDLE while it should run at APR_HOOK_FIRST (or even REALLYFIRST). ssl_hook_Access provides information (in the ssl-access-forbidden request note) that is used later by other hooks if StrictRequire is set. Therefore it is important that ssl_hook_Access is always run.Another example: With this test config:SSLOptions +StrictRequire<Directory /opt/apache22/htdocs/test/strictrequire> AuthBasicProvider file AuthName "strict require test" AuthType basic AuthUserFile conf/users Require user admin Satisfy any Deny from all allow from 10.56.51.0/24 SSLRequire %{HTTP_REFERER} == "foo"</Directory>If I make a request where neither SSLRequire nor the ip restriction is fulfilled, it depends on the load order of mod_ssl and mod_authz_host if I get a "Forbidden" or a "Authorization Required". Different behavior depending on the load order is always a bug, IMHO.SSLRequire and SSLRequireSSL are equivalent with respect to this bug because they are both checked in ssl_hook_Access.	5.0	id=45049	9	False	False	covener	1
id=46024	REOPENED	None	Apache httpd-2	Build (	2.2.10	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2008-10-16 06:58 UTC by	Yannick LE NY	2008-10-16 09:52 UTC (	0 users	I found nothing about this problem in Apache bugzilla.When I do :./configure --helpi can not see the option :--enable-staticI found nothing about that this option was removed inorPlease can you add --enable-static as optionin configure file when I do ./configure --help orif this option is not available now, please add this changein the apache 2.2.x changes or changelog file and explain why this option is removed.Use ./buildconf in httpd-2.2.10 folder and next do configure don't help Error message when I do ./configure :configure: WARNING: you should use --build, --host, --targetconfigure: WARNING: invalid host type:checking for chosen layout... Apachechecking for working mkdir -p... yeschecking build system type... config.sub: missing argumentTry `config.sub --help' for more information.configure: error: /bin/sh build/config.sub failed./script.03_apache_httpd.linux32: line 65: --enable-static: command not foundmake: *** Pas de cibles spÃ©cifiÃ©es et aucun makefile n'a Ã©tÃ© trouvÃ©. ArrÃªt.Environment :Apache 2.2.10Redhat Linux ES4.0GNU Make 3.80autoconf (GNU Autoconf) 2.59automake (GNU automake) 1.9.2	--enable-static / --enable-shared was removed when the build system was refactored from scratch in httpd 2.0. It is no longer present. Yo can now use --enable-modules / --enable-mods-shared to specify which modules should be build static or dynamic.***has been marked as a duplicate of this bug. ***(In reply to)Thank you for this answer.Please add this VERY USEFUL information in.Strange because the command-line --enable-static and --enable-shared works fine with Apache 2.0.59 and not in Apache 2.2.10.if this option is not available now, please add this changein the apache 2.2.x changes or changelog file and explain why this option isremoved and write that there are new options that replace the old options.Please propose your deisred documentation patch or at least bullet points(in ***one*** bug incident, just use this one) for all the flags that now"do nothing" or have changed.As pointed out these ***never did anything*** from httpd 2.0.0 through today.The culprit of your confusion is *autoconf*. This will affect all the variouspackages you build, one by one as the maintainers migrate and roll out newpackages. 2.0.63, and 2.2.9-.10 are affected here.Autoconf had a strict rule that unrecognized flags should not cause failure.The maintainers in the 2nd-to-the-most-recent release of autoconf have changedthis underlying assumption. So all unrecognized flags scream out where theywere silently ignored before.	4.0	id=45801	9	False	True	rpluem	1
id=46195	REOPENED	None	Apache httpd-2	mod_proxy (	2.2.8	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2008-11-12 05:19 UTC by	Chris Hills	2012-01-08 18:01 UTC (	2 users	When using ProxyPass with a URL containing a literal IPv6 address, for example 'ProxyPass /google/', the web server returns a 400 Bad Request.A workaround is to add a dummy entry to /etc/hosts, for example, '2001:4860:0:1001::68 dummy-google-ipv6', and adding 'ProxyPass /google/' to the configuration.	*** This bug has been marked as a duplicate of***When trying to configure the following (2.2.17 on Win Server 2008): ProxyPass /ProxyPassReverse /it produces the following error:[Wed Nov 17 07:34:51 2010] [error] [client 95.87.153.77] proxy: DNS lookup failure for: [::1]]:802 returned by /Diagnosis for the record: apr_sockaddr_info_get is being passed flags of 0 in proxy_util.c at line 2134 of proxy_util.c (2.2.17). Passing it APR_IPV6_ADDR_OK fixes it.Don't have time right now to figure out whether this is a correct or a coincidental fix, and apply it.(In reply to)It passes family APR_UNSPEC, i.e. APR_IPV6_ADDR_OK should only change which addresses are returned first, not if they are returned at all. If apr_sockaddr_info_get() doesn't return v6 addresses without APR_IPV6_ADDR_OK that would be a bug in apr.On the other hand, apr passes AI_ADDRCONFIG to getaddrinfo() in this case, so glibc's rather broken algorithm for checking IPv6 support could also be at fault (at least on Linux).	4.0	id=46024	4	False	False	rpluem	1
id=46428	REOPENED	None	Apache httpd-2	mod_rewrite (	2.2.9	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2008-12-21 10:32 UTC by	Alexey Vlasov	2012-09-18 18:31 UTC (	0 users	Proxy request:On server1 -# cat .htaccessRewriteEngine OnRewriteRule (AAA.*)[L,P]# telnet server1.com 80GET /AAA%20AAA HTTP/1.1HOST:server1.comanswer from server2:QUERY_STRING q=AAAREQUEST_URI /phpinfo.php?q=AAASCRIPT_NAME /phpinfo.phpUsual requst:# cat .htaccessRewriteEngine OnRewriteRule (AAA.*) /phpinfo.php?q=$1 [L]Similar request but the answer is:QUERY_STRING q=AAA AAAREQUEST_URI /AAA%20AAASCRIPT_NAME /phpinfo.php	Fixed in- thanks for the report.Backported to 2.2.x as.By the way this patch didn't solve that problem I described first. You should add the following:--- modules/mappers/mod_rewrite.c.orig 2012-09-18 22:30:31.170185072 +0400+++ modules/mappers/mod_rewrite.c 2012-09-17 23:48:43.428422154 +0400@@ -769,6 +769,7 @@ else if (r->args[len-1] == '&') { r->args[len-1] = '\0'; }+ r->args = ap_escape_uri(r->pool, r->args); rewritelog((r, 3, NULL, "split uri=%s -> uri=%s, args=%s", olduri, r->filename, r->args ? r->args : "<none>")); }	3.0	id=46195	6	False	False	nick	1
id=40987	REOPENED	None	Apache httpd-2	mod_proxy (	2.2.3	Other Linux	P2 enhancement	Apache HTTPD Bugs Mailing List	2006-11-16 05:48 UTC by	Florian Dufour	2006-11-17 03:18 UTC (	0 users	Hi,I'm using Apache 2.2.3 as a reverse proxy : ProxyPass /another/ProxyPassReverse /another/By default, according to the documentation, ProxyVia is set to Off.Effectively, the Via: header is left untransformed for the client : Via: 1.1 anothersiteIf I set ProxyVia to On, it's ok again : Via: 1.1 anothersite, 1.1 mysiteHowever, if I set ProxyVia to Block, the Via: header is left untransformedwhereas it should be deleted : Via: 1.1 anothersite	I think this works as documented:"If set to Block, every proxy request will have all its Via: header linesremoved. No new Via: header will be generated."Mind that is says that it removes Via from every *request* not from the *response*.Thus I will mark it as invalid. Feel free to reopen the bug if you thinkdifferently or if you think that ProxyVia's behaviour should be changed. Then itis an enhancement of course :-).Thank you for using httpd.OK, I think I didn't understand the documentation correctly. Thanks for youranswer !IMHO it could be a real enhancement to also block Via: header in the response.It's weird to delete the request's one whithout modifying the response's one ;what would it mean ? If Apache is part of a proxy chain, it would mean that ithides the proxies before itself, but not the ones after. Maybe it lacks of adirective hiding the proxies before AND after.If you don't mean, I reopen the bug, in order to discuss about this*enhancement* ;-)	2.0	id=46650	9	False	False	poirier	1
id=46671	REOPENED	None	Apache httpd-2	mpm_worker (	2.2.11	PC FreeBSD	P2 critical	Apache HTTPD Bugs Mailing List	2009-02-07 12:01 UTC by	Toor	2011-07-03 18:07 UTC (	1 user	FreeBSD 7.0 with PAE.cimpiled with libpth (with pthread more problems, but i didn't fix it)After start Apache working up to some hours and got Segmentation fault error in logs.[Sat Feb 07 19:46:50 2009] [notice] Apache/2.2.11 (Unix) PHP/5.2.8 configured -- resuming normal operations[Sat Feb 07 20:23:28 2009] [notice] child pid 51670 exit signal Segmentation fault (11)[Sat Feb 07 20:39:34 2009] [notice] caught SIGTERM, shutting down[Sat Feb 07 20:40:02 2009] [notice] Apache/2.2.11 (Unix) PHP/5.2.8 configured -- resuming normal operations[Sat Feb 07 20:41:25 2009] [notice] child pid 55560 exit signal Segmentation fault (11)After each segfault one of server died and didn't start. Its look in server -status like this:BEFORE:R__R_C___R__RR___RR______R_RR_R____W____C___C_____R___________R_____R___RR__C___C______CCR__R___R___RR____C_R___C______WR___RR_W_C____C__RR___RCC___R_RR_RR_______R_____R_____CC__RC______C____R__RRW__RR__C_____R__C_R_R___C_C__R___R_R_W____WR____RC__R_C_C_W__W_______R_______RW__RW___C_______RR_R_R____R_R_R___WR_WCC___RR________C__R____R__R______R____R___R________C_RRCC______R__R__R__C___R_____W____R_W__C_C_____C_RC_______RRR_R_C___RC_R______R__W___RW_R_____CW_R_RR_C_R_W_R___R____R____R_R__C___C___W__R__________C__RR____________R____WC______C____C__R_R_CW______C____R__C__RRWC__R_______C___________W___C___R____R__R_R_RR____RWC_W__RR__R_AFTER first sighfault:R__R_C___R__RR___RR______R_RR_R____W____C___C_____R___________R_____R___RR__C___C______CCR__R___R___RR____C_R___C______WR___RR_W_C____C__RR___RCC___R_RR_RR_______R_____R_____CC__RC______C____R................................................................__RRW__RR__C_____R__C_R_R___C_C__R___R_R_W____WR____RC__R_C_C_W__W_______R_______RW__RW___C_______RR_R_R____R_R_R___WR_WCC___RR________C__R____R__R______R____R___R________C_RRCC______R__R__R__C___R_____W____R_W__C_C_____C_RC_______RRR_R_C___RC_R______R__W___RW_R_____CW_R_RR_C_R_W_R___R____R____R_R__C___C___W__R__________C__RR____________R____WC______C____C__R_R_CW______C____R__C__RRWC__R_______C___________W___C___R____R__R_R_RR____RWC_W__RR__R_AFTER second sighfault:R__R_C___R__RR___RR______R_RR_R____W____C___C_____R___________R_____R___RR__C___C______CCR__R___R___RR____C_R___C______WR___RR_W_C____C__RR___RCC___R_RR_RR_______R_____R_____CC__RC______C____R................................................................__RRW__RR__C_____R__C_R_R___C_C__R___R_R_W____WR____RC__R_C_C_W__W_______R_______RW__RW___C_______RR_R_R____R_R_R___WR_WCC___RR_................................................................_______C__R____R__R______R____R___R________C_RRCC______R__R__R__C___R_____W____R_W__C_C_____C_RC_______RRR_R_C___RC_R______R__W___RW_R_____CW_R_RR_C_R_W_R___R____R____R_R__C___C___W__R__________C__RR____________R____WC______C____C__R_R_CW______C____R__C__RRWC__R_______C___________W___C___R____R__R_R_RR____RWC_W__RR__R_When maximum of allowed server (ServerLimit) is reached (with dead lines)- apache died. For each of segfault I got a apache core.Core was generated by `httpd'.Program terminated with signal 11, Segmentation fault.Reading symbols from /lib/libm.so.5...done.Loaded symbols for /lib/libm.so.5Reading symbols from /usr/local/lib/libaprutil-1.so.3...done.Loaded symbols for /usr/local/lib/libaprutil-1.so.3Reading symbols from /usr/local/lib/libexpat.so.1...done.Loaded symbols for /usr/local/lib/libexpat.so.1Reading symbols from /usr/local/lib/libapr-1.so.3...done.Loaded symbols for /usr/local/lib/libapr-1.so.3Reading symbols from /lib/libcrypt.so.4...done.Loaded symbols for /lib/libcrypt.so.4Reading symbols from /lib/libthr.so.3...done.Loaded symbols for /lib/libthr.so.3Reading symbols from /lib/libc.so.7...done.Loaded symbols for /lib/libc.so.7Reading symbols from /usr/local/libexec/apache2/mod_authz_host.so...done.Loaded symbols for /usr/local/libexec/apache2/mod_authz_host.soReading symbols from /usr/local/libexec/apache2/mod_env.so...done.Loaded symbols for /usr/local/libexec/apache2/mod_env.soReading symbols from /usr/local/libexec/apache2/mod_headers.so...done.Loaded symbols for /usr/local/libexec/apache2/mod_headers.soReading symbols from /usr/local/libexec/apache2/mod_setenvif.so...done.Loaded symbols for /usr/local/libexec/apache2/mod_setenvif.soReading symbols from /usr/local/libexec/apache2/mod_mime.so...done.Loaded symbols for /usr/local/libexec/apache2/mod_mime.soReading symbols from /usr/local/libexec/apache2/mod_status.so...done.Loaded symbols for /usr/local/libexec/apache2/mod_status.soReading symbols from /usr/local/libexec/apache2/mod_cgi.so...done.Loaded symbols for /usr/local/libexec/apache2/mod_cgi.soReading symbols from /usr/local/libexec/apache2/mod_negotiation.so...done.Loaded symbols for /usr/local/libexec/apache2/mod_negotiation.soReading symbols from /usr/local/libexec/apache2/mod_dir.so...done.Loaded symbols for /usr/local/libexec/apache2/mod_dir.soReading symbols from /usr/local/libexec/apache2/mod_actions.so...done.Loaded symbols for /usr/local/libexec/apache2/mod_actions.soReading symbols from /usr/local/libexec/apache2/mod_alias.so...done.Loaded symbols for /usr/local/libexec/apache2/mod_alias.soReading symbols from /usr/local/libexec/apache2/libphp5.so...done.Loaded symbols for /usr/local/libexec/apache2/libphp5.soReading symbols from /usr/local/lib/mysql/libmysqlclient_r.so.15...done.Loaded symbols for /usr/local/lib/mysql/libmysqlclient_r.so.15Reading symbols from /usr/local/lib/libfreetype.so.9...done.Loaded symbols for /usr/local/lib/libfreetype.so.9Reading symbols from /usr/local/lib/libpng.so.5...done.Loaded symbols for /usr/local/lib/libpng.so.5Reading symbols from /lib/libz.so.4...done.Loaded symbols for /lib/libz.so.4Reading symbols from /usr/local/lib/libjpeg.so.9...done.Loaded symbols for /usr/local/lib/libjpeg.so.9Reading symbols from /usr/local/lib/libcurl.so.4...done.Loaded symbols for /usr/local/lib/libcurl.so.4Reading symbols from /usr/lib/libssl.so.5...done.Loaded symbols for /usr/lib/libssl.so.5Reading symbols from /lib/libcrypto.so.5...done.Loaded symbols for /lib/libcrypto.so.5Reading symbols from /usr/local/lib/libxml2.so.5...done.Loaded symbols for /usr/local/lib/libxml2.so.5Reading symbols from /usr/local/lib/libiconv.so.3...done.Loaded symbols for /usr/local/lib/libiconv.so.3Reading symbols from /usr/local/lib/php/extensions/no-debug-zts-20060613/eaccelerator.so...done.Loaded symbols for /usr/local/lib/php/extensions/no-debug-zts-20060613/eaccelerator.soReading symbols from /usr/local/lib/php/extensions/no-debug-zts-20060613/imagick.so...done.Loaded symbols for /usr/local/lib/php/extensions/no-debug-zts-20060613/imagick.soReading symbols from /usr/local/lib/libMagickWand.so.2...done.Loaded symbols for /usr/local/lib/libMagickWand.so.2Reading symbols from /usr/local/lib/libMagickCore.so.2...done.Loaded symbols for /usr/local/lib/libMagickCore.so.2Reading symbols from /usr/lib/libbz2.so.3...done.Loaded symbols for /usr/lib/libbz2.so.3Reading symbols from /usr/lib/libgomp.so.1...done.Loaded symbols for /usr/lib/libgomp.so.1Reading symbols from /libexec/ld-elf.so.1...done.Loaded symbols for /libexec/ld-elf.so.1#0 0x28163188 in sem_init () from /lib/libthr.so.3[New Thread 0x28301100 (LWP 100564)](gdb) thread apply all bt fullThread 1 (Thread 0x28301100 (LWP 100564)):#0 0x28163188 in sem_init () from /lib/libthr.so.3No symbol table info available.#1 0x282fc5b9 in GOMP_ordered_start () from /usr/lib/libgomp.so.1No symbol table info available.#2 0x282fd145 in omp_get_nested () from /usr/lib/libgomp.so.1No symbol table info available.#3 0x280c63f4 in ?? () from /libexec/ld-elf.so.1No symbol table info available.#4 0x280cefc0 in ?? ()No symbol table info available.#5 0xbf7fe9f8 in ?? ()No symbol table info available.#6 0x282f9bc1 in ?? () from /usr/lib/libgomp.so.1No symbol table info available.#7 0x280c6560 in ?? () from /libexec/ld-elf.so.1No symbol table info available.#8 0x280c63f4 in ?? () from /libexec/ld-elf.so.1No symbol table info available.#9 0xbf7fe9f8 in ?? ()No symbol table info available.#10 0x280a43bf in dlsym () from /libexec/ld-elf.so.1No symbol table info available.So, I tried to change AcceptMutex, change "worker" options, but it doesn't helpMy option is now:ServerLimit 64StartServers 10MaxClients 1024MinSpareThreads 75MaxSpareThreads 1024ThreadsPerChild 64#ThreadLimit 100MaxRequestsPerChild 0KeepAlive OffMaxKeepAliveRequests 600KeepAliveTimeout 3	***has been marked as a duplicate of this bug. ***CreatedCrash core dumpYou are using a threaded MPM not only with mod_php, but with a number of libraries known not to be thread-safe. When you do that, occasional segfaults are to be expected.(In reply to)It's possible, but I talk not about segfault, but about behaviour after segfault. As described in doc, apache child must restart (or just die) after segfault, but his not. So, after this happend many times apache have no more limit for run new childs and crashes as result of limit excedeed (not because segfault happend).I'm getting the exact same error with almost the exact same backtrace, but I'm using prefork MPM. This doesn't seem possible. This is FreeBSD 8.1.	5.0	id=46428	4	False	False	jorton	1
id=46685	REOPENED	None	Apache httpd-2	mod_access (	2.2.3	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2009-02-09 11:18 UTC by	Clément Hermann	2011-06-27 18:43 UTC (	0 users	Hi,When protecting files (in a <files> or <filesmatch> block), the file which will be requested via the DirectoryIndex Directive will trigger a 401 response, but without WWW-Authenticate header.eg :DirectoryIndex index.php<files index.php> [Authtype basic stuff] require valid-user</files>When requesting, the response is 401 and WWW-Authenticate header is sent.When requesting, the response is 401 but no WWW-Authenticate header is sent, so no way to access the file.One can work around this with a rewrite rule or a redirectmatch.Regards,Clément Hermann (nodens)	I can't reproduce this alleged bug (it sends me the WWW-Authenticate as expected). Are you sure it isn't a PHP problem, or something elsewhere in your config (e.g. messing with mod_rewrite)?If you're sure it's a bug, please reopen and supply a minimal config to provoke it, excluding extras like PHP that could confuse the issue.I can reproduce it like so:DirectoryIndex index.html<Files "index.html"> Order allow,deny Allow from all AuthType Basic AuthName "Some Auth" AuthUserFile "/var/www/htpasswd" Require valid-user</Files>Live HTTP headers:GET / HTTP/1.1Host: localhostUser-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-us,en;q=0.5Accept-Encoding: gzip, deflateAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7Keep-Alive: 115Connection: keep-aliveHTTP/1.1 401 Authorization RequiredDate: Mon, 27 Jun 2011 18:39:39 GMTServer: Apache/2.2.17 (Fedora)Content-Length: 476Connection: closeContent-Type: text/html; charset=iso-8859-1----------------------------------------------------------Here are the modules enabled:Loaded Modules: core_module (static) mpm_prefork_module (static) http_module (static) so_module (static) auth_basic_module (shared) auth_digest_module (shared) authn_file_module (shared) authn_alias_module (shared) authn_anon_module (shared) authn_dbm_module (shared) authn_default_module (shared) authz_host_module (shared) authz_user_module (shared) authz_owner_module (shared) authz_groupfile_module (shared) authz_dbm_module (shared) authz_default_module (shared) ldap_module (shared) authnz_ldap_module (shared) include_module (shared) log_config_module (shared) logio_module (shared) env_module (shared) ext_filter_module (shared) mime_magic_module (shared) expires_module (shared) deflate_module (shared) headers_module (shared) usertrack_module (shared) setenvif_module (shared) mime_module (shared) dav_module (shared) status_module (shared) autoindex_module (shared) info_module (shared) dav_fs_module (shared) vhost_alias_module (shared) negotiation_module (shared) dir_module (shared) actions_module (shared) speling_module (shared) userdir_module (shared) alias_module (shared) substitute_module (shared) rewrite_module (shared) proxy_module (shared) proxy_balancer_module (shared) proxy_ftp_module (shared) proxy_http_module (shared) proxy_ajp_module (shared) proxy_connect_module (shared) cache_module (shared) suexec_module (shared) disk_cache_module (shared) cgi_module (shared) version_module (shared)	2.0	id=46671	5	False	False	nick	1
id=47220	REOPENED	None	Apache httpd-2	mod_substitute (	2.2.17	All All	P2 enhancement	Apache HTTPD Bugs Mailing List	2009-05-19 06:44 UTC by	Marc Stern	2014-03-27 19:55 UTC (	0 users	It would be great to have access to environment variables, like Substitute s/%{HTTP_HOST}/www.mycompany.com/inor Substitute s/www.mycompany.com/%{HTTP_HOST}/n	*** This bug has been marked as a duplicate of***This was not a duplicate. You may indicate it is 'resolved by' your contribution,but this ticket indicated a specific request with a specific scope.I've tried the patch specifically to substitute in environment variables (%{env:var_name}) but it seems to only allow access to local variables created by the use of 'define' (${var_name}).Please could you confirm how your code is to be used in this manner?	3.0	id=46685	4	False	False	nick	1
id=46765	REOPENED	None	Apache httpd-2	mod_proxy (	2.2.11	PC Windows XP	P2 normal	Apache HTTPD Bugs Mailing List	2009-02-25 05:03 UTC by	Chursin Evg	2009-05-20 00:22 UTC (	1 user	Createdtraffic dumps by WiresharkPortable*Apache 2.11/WinXP*recently i try to setup "Bitnami REDMINE Stack" on two computers.on first PC (my notebook) it was successful and just run out of the box.on second PC (odinary computer) i get error 502 proxy ERROR==BEG= redmine.conf ===ProxyPass /redmine balancer://redmineclusterProxyPassReverse /redmine balancer://redminecluster<Proxy balancer://redminecluster> BalancerMemberBalancerMember</Proxy>==EOF= redmine.conf =====BEG= apache-error.log ===[Mon Feb 16 14:14:52 2009] [error] [client 127.0.0.1] (70014)End of file found: proxy: error reading status line from remote server 127.0.0.1, referer:[Mon Feb 16 14:14:52 2009] [error] [client 127.0.0.1] proxy: Error reading from remote server returned by /redmine/, referer:==EOF= apache-error.log ===the same problem was described atbut no solution foundso i'v download Wireshark portable and sniff both cases ( good one and one leads to error) see "traffic-capture-1(bad).pcap" and "traffic-capture-2(good).pcap"it seems to syncronization problem or buffer overflowi use latest Apache from apache.orgi check system for virusesbut i cannot figureout reason of dublication of message header:(	it seems not intresting to anyone for fix this bug :-(i change httpd.conf to=====ProxyPass /redmine#ProxyPassReverse /redmine===and error reproduced.so, i decide that problem in mod_proxy, not in balancer.Fixed in trunk, and proposed for backport.Seefor the patch applied to trunk.Also seefor the compiled mod_proxy.so 2.2.12-dev that corresponds to this patch,simply unpack the zip into apache2.2\modules\.CreatedMy httpd.exe config and logon answer toas i undestend, you fix somthing elsebecause error is still here and nothing changed :(	7.0	id=45407	12	False	False	mkevac	1
id=46709	REOPENED	None	Apache httpd-2	mod_proxy (	2.5-HEAD	PC Linux	P2 regression	Apache HTTPD Bugs Mailing List	2009-02-12 15:38 UTC by	Ian Reynolds	2013-08-12 21:56 UTC (	0 users	Our environment uses a HTTP 1.1 mod_proxy server with 1.0 back-end webs. This is for various compatibility reasons, but in order to make this arrangement work in versions of Apache after 2.2.10 due to this change:*) mod_proxy_http: Do not forward requests with 'Expect: 100-continue' to known HTTP/1.0 servers. Return 'Expectation failed' (417) instead. [Ruediger Pluem]This change breaks .NET web service clients that use Expect: 100-continue by default when initiating a session. To Replicate, attempt to connect to a proxy setup similar to the one mentioned above with any web service url hosted by a back-end web. The connection will fail due to the Expectation failed response. Regardless of what the RFC declares, this worked previous to the change. We have been patching manually just by applying the below simple patch:--- mod_proxy_http.c.orig 2009-02-12 18:35:55.000000000 -0500+++ mod_proxy_http.c 2009-02-12 18:35:26.000000000 -0500@@ -703,10 +703,11 @@ * According to RFC 2616 8.2.3 we are not allowed to forward an * Expect: 100-continue to an HTTP/1.0 server. Instead we MUST return * a HTTP_EXPECTATION_FAILED- */+ * Disabled for .NET clients if (r->expecting_100) { return HTTP_EXPECTATION_FAILED; }+ */ p_conn->close++; } else { buf = apr_pstrcat(p, r->method, " ", url, " HTTP/1.1" CRLF, NULL);	If you need the 100-response, why do you tell it to talk HTTP/1.0 to the backend?The behaviour you describe is not a bug, and the change you identify is a bug fix. It may very well be that you have a legitimate need for different behaviour, but you should at least tell us why you can't just use HTTP/1.1. The place for that would be the mailinglist, not bugzilla.I'll leave this open, in case you want to clarify the above and turn this into an enhancement request.CreatedAdd support to completely ignore an "Expect: 100-continue" if env "ignorecontinue" is setWe had a very similar issue, also with .NET clients.As we have only relatively few clients affected by the problem (more detailed description below), and those clients always come from the same IP, we were able to solve the problem by the attached patch and setting the new "ignorecontinue" environment variable for those clients.Now to the problem itself:In our case, the relevant URL is protected by HTTP basic auth. .Net clients send the following header in our case:POST /url HTTP/1.1Content-Type: text/xmlContent-Length: 1113Expect: 100-continueConnection: Keep-AliveHost: host.do.main<<<< CLIENT ----At this point, apache httpd (2.2.3 with patches from RedHat 5.4, but I saw no relevant changes up to the current 2.2.14) reacts like requested by the client:It responds immediately with an error code, in this case:HTTP/1.1 401 Authorization RequiredDate: Wed, 04 Nov 2009 10:24:52 GMTServer: ApacheWWW-Authenticate: Basic realm="Our Gateway"Last-Modified: Thu, 17 Sep 2009 08:52:08 GMTETag: "23-473c221659e00"Accept-Ranges: bytesContent-Length: 35Keep-Alive: timeout=15, max=10Connection: Keep-AliveContent-Type: text/html<pre>This is an error page</pre><<<< SERVER ----Problem here is that .Net can't handle this the way it should, the .Net client actually keeps going by sending the body of the request, immediately followed by the authenticated copy of the first request<?xml version="1.0" encoding="UTF-8"?><Company_BXML...POST /url HTTP/1.1Content-Type: text/xmlAuthorization: Basic <zensiert>Content-Length: 1113Expect: 100-continueHost: host.do.main<?xml version="1.0" encoding="UTF-8"?><Company_BXML...<<<< CLIENT ----Which httpd interprets as being a request of type:<?xml....>POST Which is obviously not a valid request, so httpd returns code 400For some clients, it seems to help to simply disable keepalive (setenv nokeepalive), but for some others, this only results in no authenticated request being sent at all.So we tried several workarounds:1) Use any combination of the typical MSIE workaround settings, including unclean ssl shutdown, downgrade-1.0, force-response-1.0. This resulted in varying errors, mostly having just a single request come in, for which code 401 was returned, then no other request following.2) Return 417 if we knew it was a problematic customer and the Expect header was set. This also just resulted in no further request coming in (no matter wether nokeepalive was set or not).Finally, I tried patching the httpd to allow to simply ignore the Expect header from the known-bad clients. The resulting patch is attached. Please note that I'm in no way an apache httpd expert, so the formating might be non-standard and - more importantly - the logging I added might be wrong (ap_log_rerror might be more correct than ap_log_error).Anyway, I think the patch might be off use for others as well, and I would be glad to see something similar in a future httpd release. I don't think anyone would thing copyright might apply to this simple patch, but just in case anyone is insane enough to do so, I hereby grant all rights related to this patch to the apache foundation. I also release the "code" under the apache license, the GNU (L)GPL (v2 or higher) or the BSD copyright (without the advertisement clause), at the recipients choice.describes PR 47087. Are you in a position to test-drive that patch?*** This bug has been marked as a duplicate of***I see the same (version 2.2.22)... having a HTTP 1.0 only backend with force-proxy-request-1.0 set.The client sends an "Expect: 100-continue" Apache (correctly follows the RFC) and returns 417.I don't say that this is a bug in Apache, since AFAIU it does what the RFC says but rather from the clients...And even if one would unset force-proxy-request-1.0 it wouldn't really make things better, since while it would work then, the RFC would be more or less broken ... or at least the functionality of "Expect: 100-continue" wouldn't work.The only solution right now seems to strip the header, which is IMHO not very nice though, as Expect may get further use (even if right now only 100-continue is defined).Therefore, can re-opening this with the question whether you can reconsider the patch from.It would allow to only ignore 100-continue, and not any other future values for Expect: .	4.0	id=46709	6	False	False	nick	1
id=47630	REOPENED	None	APR	APR (	HEAD	PC Windows Vista	P2 normal	Apache Portable Runtime bugs mailinglist	2009-08-03 21:10 UTC by	Dan	2016-12-14 07:53 UTC (	3 users	Overview:APR currently treats any sort of NTFS reparse point as a symlink (see, for instance, fillin_fileinfo). However, an NTFS reparse point may be one of several things: * A "symbolic link" (very similar to n*x symbolic links) * A "hard link" * A "directory junction", also called a "mounted folder" (very similar to a mount point)It's currently unclear to me if APR should treat hard links as APR_LNK or APR_REG (I guess APR_REG), but APR should definitely treat junctions as APR_DIR (the same as you would treat a mount point on Linux, etc.).Determining if a reparse point is a directory junction described in the MSDN documentation: "Determining Whether a Directory Is a Mounted Folder"Repro steps:1. Use any APR API to determine the file type of a mounted folder ("directory junction").Expected result: APR_DIRActual result: APR_LNKSee: fillin_fileinfo in /apr/apr/trunk/file_io/win32/filestat.c. There may also be other places that need to be updated; I'm not familiar enough with the APR source.	Symbolic links on Unix are polymorphic, just as you described for Junctions.This behavior is by design.Unlike a unix mount point, a Junction is not a directory. Unlike a unixmount point, rm is not used to remove the intersection of the file systems.And like Unix symlinks, a Junction may refer to a nonexistent or invalid object.A Junction is never a hard link; NTFS has hard links, these are well definedobjects and are invisible to the consumer after creation except by comparingfile ID (e.g. inode) for equality.APR_LNK must be dereferenced to discover the target resource. Omitting theapr_file_info flag APR_FINFO_LINK should resolve the referenced APR_DIR. Is this not what you observe?(In reply to)No, I think reparse points are polymorphic. A junction is a specific type/usage of a reparse point. Reparse points are used to implement a lot of things; see.As far as I can tell, junctions are almost /exactly/ the same as unix mount points, perhaps minus the restrictions on reparse points (like you can have only 31 reparse points in a given path). Another term for a junction is a "mounted folder". The MSDN documentation states that "Because mounted folders are directories, you can rename, remove, move, and otherwise manipulate them, as you would other directories." ()The only case that I am aware of where a junction can refer to a nonexistent or invalid target is if the volume that the junction refers to fails. I'm not sure how unix could do anything different. This is what the documentation says (from same page referenced earlier):"If a volume fails, any volumes that have been assigned to mounted folders on that volume can no longer be accessed through those mounted folders. For example, suppose you have two volumes, C: and D:, and that D: is associated with the mounted folder C:\MountD\. If volume C: fails, volume D: can no longer be accessed through the path C:\MountD\."Understand that a unix mount point *is a directory*. That's what the filesystemsees. The driver handles those mounts, not the filesystem.Unix also allows symlinks to point to other directories.Similarly, Windows allows any driver to register a mount without the existenceof a Junction. The Junction is a filesystem entity, not a driver entity.Junctions allow the user to point any directory at another. junction.exe dir1 alias1is completely valid. rd dir1 and you will find alias1 is broken. There is nodistinction in the filesystem driver between a junction to another directory on the same volume, or a junction to another volume."Mount points" are only one use of junctions, and they are not the same asa driver registering it's mount point in NTFS space.This behavior is identical to the behavior of directory symlinks on Unix.You did not answer my question of whether you are seeing these entities asdirectories when APR_FINFO_LINK is omitted from the apr_file_info_get flags.(In reply to)But... for all intents and purposes, a junction is just as much of a directory: The MSDN documentation states that "Because mounted foldersare directories, you can rename, remove, move, and otherwise manipulate them,as you would other directories."()NTFS is not a driver? Anyway, I don't think the implementation matters (it does not matter where in the driver stack it is implemented, nor does it matter whether a mount point is recorded in fstab or filesystem metadata); I think that windows junctions are the moral equivalent of unix mount points. For the purposes of creating a portable runtime, they should be treated the same.Windows also allows symlinks (distinct from junctions) to point to other directories. Windows junctions and windows symlinks-to-directories are two different things. Just like unix mount points and unix symlinks are different things.I don't know how that is accomplished; if you happen to know how, I would be interested in that.As of Linux 2.4.0 (I don't know about other unices), it is possible to remount part of the file heirarchy somewhere else ("mount --bind olddir newdir"). I don't know where in the driver stack that is implemented, but again, for the purposes of creating a portable runtime, I don't think that matters.Sorry, I have not had time to test that. But I don't think it is relevant to this bug; the point of this bug is that NTFS junctions are the moral equivalent of unix mount points and should be treated as such by the APR.For instance, if you go into the Disk Management tool (right-click Computer, choose "manage", then drill into Storage-->Disk Management in the navigation heirarchy), and you choose to mount a partition somewhere (this is the windows GUI version of "mount"), it will create a junction. (Not a directory symlink, and not some sort of special non-junction mount point that you refer to above.)If I do find a problem with APR_FINFO_LINK and apr_file_info_get in the future, I will be sure to open a separate bug for that issue.I am studying that relationship to determine how we will finally implementapr_file_symlink_create() et al. Most likely, in XP/2003 directory symlinkswill continue to be junctions, while Vista/2008 with upgraded NTFS file systemswould gain symlinks. The 1/2 hour of testing I did today was not promising.But Junctions have no relationship to unix mount points. Just like symlinks,junctions and mount points can be renamed. Unlike mount points, the changeis immediate. It is not at the driver layer. In the unix case, the mountis at a much lower level, and that rename causes the system to fail (in fact,it isn't possible).You are missing the distinction between kernel layer/driver driven mounts, suchas unix and also windows drivers, and userspace controllable filesystem aliases.A Junction is a symlink-style alias, and for security must be represented assuch, because any user can manipulate them. End of discussion.Ok, this is clearly confusion on your point as to the flexibility of Junctionsand where they sit on the driver stack. Meaning no disrespect, I am closingthis bug as invalid.Closing. If a problem exists in resolving Junctions w/o APR_FINFO_LINK flag,feel free to reopen and recycle this incident.(In reply to)I think the layer is irrelevant. I don't understand "unlike mount points, the change is immediate"--you can unmount /a/b and remount it as /a/c, just as "immediately", right? (so what if you have to type two commands vs. one)What does "mount --bind olddir newdir" do? And NTFS is not userspace. If you want to fool around with the reparse point information, you have to send IOCTLs via DeviceIoControl--to the kernel-mode driver.Ironically, any user can create a junction, provided it has access to the things it is linking, but for windows symlinks, you must have [elevated] administrative privileges (unless you tweak security policy).However, I don't understand why you think that is a security problem. On linux, normal users can mount things if those rights are granted (like cdrom). With junctions, it's as if there is a magic fstab file that grants the "user" option to any places that the user has rights to. And a user has rights to manipulate directories, so if the APR treated junctions as directories, why would that be a security problem?No disrespect taken. But what does the driver stack have to do with anything? Even if there are some behavior and implementation details that are different between windows and unices, what is important to building a portable runtime is that windows junctions are the moral equivalent to unix mount points. Differences in implementation don't matter, and even some differences in behavior don't matter, because APR treats mount points as directories, and directories can and do change, by the actions of users or superusers.For a portable program, where you would use a mount point on linux, you would use a junction on windows. The Apache portable runtime should reflect this.No, you would not.A modestly privilaged unix user cannot change mounts.Any authenticated win32 user can edit junctions.Therefore this is far less kernel-oriented and far more symlink oriented.And you've offered no argument for breaking the APR *portability* associationof junctions and symlinks, other than "I don't think it should work this way."Other than to open up a raft of new security issues, what is your technicalargument that the existing behavior is broken?(In reply to)Unless granted those privileges. Such as for cdrom.Any win32 user can edit directories.What is the specific security concern? I.e., a win32 user can create, remove, or change a junction, so if the APR treated junctions as directories, this would cause... what? Data loss vulnerability? Spoofing vulnerability? Tampering vulnerability? DoS vulnerability? Repudiation vulnerability? Elevation of privilege vulnerability? How? Like, "Alice creates a Junction in a directory that Bob has write access to, and he deletes the junction, so..."On windows, in my experience, if you want to "play tricks" with your filesystem such as store a bunch of files on some other drive but make them show up somewhere in your C: drive, you use a junction. Mountvol creates a junction. If you use the disk managment GUI to mount a volume anywhere other than as a separate drive letter, it creates a junction.This is analogous to how mount points are used in unix. We even call them "mounted folders". Wherever you use "mount" on unix, you use "mountvol" on windows.If you are using this method to play tricks with your directory structure, and then use a program that uses the APR and does not happen to omit APR_FINFO_LINK when getting info about a directory, then the trick does not work with that program.junction path/to/cgi-bin path/to/htdocs/revealscriptis just one example, I'm not going to repeat 20 years of security issues surrounding symlinks for your benefit.Junctions are alias/symlinks. A junction cannot point at an unmounted volume.E.g. although my checkouts to develop apr on httpd look like build/ httpd-2.2/ apr-1.3/ apr-util-1.3/ apr-iconv-1.3/so that svn up httpd-* apr-* bring all sources up-to-date, build/ httpd-2.2/ srclib/ apr <junction to apr-1.3> apr-util <junction to apr-util-1.3> apr-iconv <junction to apr-iconv-1.3>Please don't tell me that this facility doesn't expose the same issues assymlinks raise. That's why APR was designed to handle symlinks in this manner.The symlink protection is in there to help determine when aliases exist.Precisely. This is exactly the same impact on unix of a symlinked directory. Ergo the portability behavior is 100% correct, and this discussion really is finished.As a few follow up observations;MKLINK /J by default requires no security escalation, while MKLINK for filesor directories is usually denied to all but administrator by default. Thisreaffirms the conclusion that Junction must be recognized as a symlink wheninspecting with lstat rather than stat or by open resource handle.An arbitrary example of where junctions are effectively and frequently used as symlinks can be found at;which notes that Vista et al default to creating one such Junction on the bootdrive by default; C:\Documents and Settings [C:\Users]Reviewing the behavior tonight to validate that Junctions and Dir-Symlinksboth resolve to the directory when APR_FINFO_LINK is omitted, along with File-Symlinks as well.A variant of this trips SVN badly, and can cause surprises for Apache httpd installs too.If SVN is running on an NTFS volume that has the Windows Data Deduplication active, it breaks miserably once files in the working set get deduplicated (and replaced by reparse points).What happens is this:1. Windows triggers deduplication after some time (default 5 days) and replaces a file with a reparse point for dedup2. When SVN scans the repository, it finds a change to the file and claims that the special file status has changed AND claims to not support symlinks, so it breaks.3. It is mislead into believing this is a symlink, due to this issue in APR, which turns an IO_REPARSE_TAG_DEDUP into a 'symlink', while its not.So any SVN checkout on a data deduplicated volume is doomed and will totally break once Dedup kicks in. Especially nice as it happens with a delay, so once it happens the only cure is a totally fresh checkout. You cannot do anything with the checkout anymore.So please reopen this and provide some better semantics for all the non-symlinky Reparse Points.The SVN mailing list stated this is a bug in APR:The same issue trips Apache httpd when testing for Options +FollowSymlinks, reparse Points are treated as symlinks and a valid Apache install on a deduped volume may break randomly, when the content dir gets elected for deduplication.I agree that non-Junction reparse points should be evaluated on a case-by-casebasis, you make a good distinction for IO_REPARSE_TAG_DEDUP, which is a systemgenerated (safe, non-userland) equivalence.But I disagree this bug is the correct bug to edit, the subject makes the case for junctions to be treated as non-symlinks and that's simply inappropriate forall of the reasons listed in comments above.Would you please re-close and create a new ticket, rather than recycle an oldtangentially-related ticket?Reviewed the originaland it did raise in a general sense the specific issue in(although we disagreed on the scope of examples provided,the dir link was agreed as an APR_LNK, junction was debatable but should still bepresented as APR_LNK for the security considerations outlined in #10.)But this report called out that we did not distinguished by reparse type, whichis the root of, so let us continue to use this ticket to explicitlyset the behavior type-by-type (which could be patched by the dev to satisfy theirown opinion). I've changed the title slightly to reflect this wider scope.Sorry for the confusion I introduced in my previous comment.Agreed. The scope of this ticket went a bit toward the link/junction part, as that was the most common type encountered in the wild at the time of the ticket creation.The reparse point tag list at MSDN lists quite a few tags, which might be encountered, although most will be rare in smaller infrastructures.Slightly explaining the values:The docs are not really that informative sadly, but the tags seem to be like this, even if probably not complete:IO_REPARSE_TAG_CSV - Cluster Shared Volumes (transparently mounting a SMB Share to share storage between cluster nodes), kind of a mount pointIO_REPARSE_TAG_DEDUP - Data Deduplication in Windows Server 2012IO_REPARSE_TAG_DFS - Seem to be related to Distributed-Filesystem ReplicationIO_REPARSE_TAG_DFSR IO_REPARSE_TAG_HSM - Hierarchical Storage Manager redirectionsIO_REPARSE_TAG_HSM2 IO_REPARSE_TAG_SYMLINK - Win7 SymlinksIO_REPARSE_TAG_MOUNT_POINT - Seems the one used for junctions and filesystem mount pointsIO_REPARSE_TAG_NFS - Probably the NFS Filesystem filter driverIO_REPARSE_TAG_SIS - Single Instance Storage, seems to be similar to the Dedup one ()IO_REPARSE_TAG_WIM - seems to be mountpoints for WIM images (e.g. similar to a mounted disc image), seeAll the dedup and installable filesystem style stuff should probably be treated as normal files/directories. The SYMLINK and MOUNT_POINT things are probably correctly treated as APR_LINK.I wonder if we can get additional clues as to IO_REPARSE_TAG_MOUNT_POINT whetherthis is a junction or other facility?I agree that we should devolve anything other than junction mount points andIO_REPARSE_TAG_SYMLINK from APR_LNK into an APR_DIR or APR_FILE as appropriate.I don't think we can comprehensively cover every possible and future resource type,and we should add user-controlled, potentially harmful symlinks as we understandthat list in more detail over time.This article has some info how to distinguish a directory junction and a volume mount point.which points at the structure defintions here:There is discussion of 1.6 and perhaps 2.0-RC releases in the very near future. This should get a closer look and patch before year end, and certainly before a tag. It is a significant behavior change, even if it is a 'bug fix'.	18.0	id=46765	8	False	False	wizard2k	1
id=47693	REOPENED	None	Apache httpd-2	All (	2.5-HEAD	All All	P2 normal	Apache HTTPD Bugs Mailing List	2009-08-13 05:23 UTC by	None	2009-09-29 01:38 UTC (	1 user	When a GET request is received by Apache, it first opens the file to determine the length, then closes it, then reopens it to serve the file.An interesting use case is to replace a file (using an atomic rename) and have Apache give consistent results: always return either the old file or the new file.However, the current implementation will give inconsistent results when the length of the file changes between the first and second time Apache opens the files.If the new file is longer, Apache returns the content of the new file truncated to the length of the first file. If the new file is shorter, Apache returns an error response.	That's all true, but are there are any feasible ways to fix the behavior? For that matter, what is the right behavior if a file changes in the middle of a request?The server could ensure consistency by making a complete copy of every file before beginning to serve it, but that's not reasonable for any significant load. There might be some operating systems where the server could get an exclusive lock on the file before beginning to serve it, but not on most OSes.Practically speaking, the solution is not to change files under a running server. Or serve the content from a database or other source with true atomic changes.Perhaps it won't be feasible in terms of internal state management inside the server, but the solution would be to use the following sequence:open the file, get it's length, read the file, close the fileinstead of:open the file, get it's length, close the file, open the file, read the file, close the file.Note that the interference with file that I'm talking about is not an arbitrary modification to the file, but only the case where the file is replaced by another file. This is a typical filesystem operation for doing a "clean" / "atomic" modification to a file: write a new copy of the file content to a temporary file, then replace the target file by the temporary file.Reopening to get feedback on the feasibility of the proposed solution...	2.0	id=47392	8	False	False	poirier	1
id=47650	REOPENED	None	Apache httpd-2	support (	2.5-HEAD	All All	P2 trivial	Apache HTTPD Bugs Mailing List	2009-08-05 13:39 UTC by	None	2015-09-23 10:59 UTC (	0 users	On line 157 of /httpd/httpd/trunk/support/httxt2dbm.c, dbmval.dsize = (c - line); should be dbmval.dsize = (c - value);, matching the allocated space in the preceeding line.	I have made the changes to the httxt2dmb.c file as described by Jon and now it needs testing... This is my first patch do i check it in to CVS and if so where does it needs to go to be unit tested,Thanks for your help guysBetter late than never...... so thanks for this report.This has been fixed on trunkand will be available in 2.4.4.fixed in 2.4.4Not fixed in 2.2.31	5.0	id=47580	6	False	False	rpluem	1
id=45407	REOPENED	None	APR	APR (	HEAD	PC Linux	P2 normal	Apache Portable Runtime bugs mailinglist	2008-07-16 04:49 UTC by	Marko Kevac	2009-10-08 15:46 UTC (	5 users	Auto reconnect option in apr_dbd_mysql is set to 1:#if MYSQL_VERSION_ID >= 50013 my_bool do_reconnect = 1;#endifThis forces mysql_ping(), which is called by dbd_mysql_check_conn(), to reconnect if connection to MySQL server was lost.But lost connection also means that all prepared statements are lost. And because statements are prepared only when dbd_construct() is called, one can not use prepared statements any more.I think we should set do_reconnect to 0 by default.	dbd_construct() is from mod_dbdAnyway. There are no way to know if mysql_ping() reconnected to MySQL server or if connection was just fine.So we don't know if we should prepare statements one more time.I think dbd_construct() should remember if it successfully prepared that statement in the past (i.e. if the statement was generally good). When _pselect() fails and we had a good prepared statement, then this means that prepared statement was lost due to reconnect, so it should be prepared again.Does that make sense?Well, this will probably work, but I think that it is not so good solution.First, when pselect() fails, it gives error 2013 (CR_SERVER_LOST), but connection exists and select() works well. This is a little bit confusing and it is not clear for user whether he should reconnect or he just needs to prepare statements one more time.Second, thing you are talking about should be implemented not only in mod_dbd, but also in modules, that prepare statements by themself, but use mod_dbd for pools and easy API. It's not good design I think.What do you think?I think this is the key here. It is obvious that if select() works (or if apr_dbd_check_conn() is OK) that connectivity of that particular prepared statement is the problem (i.e. because the connection for which it was prepared was lost). So, the logical thing to do it to prepare the statement again. No?It is obvious if you know about this. But if someone sees error 'connection lost' and he does not that this can possibly be error with prepared statement, his first step will be to reconnect or invalidate reslist element.Anyway, that's not the point. Let's say that we know that if pselect() returns 'connection lost' error, we should prepare statements one more time.There are two types of prepared statements.First, ones from DBDPrepare (mod_dbd). Second, statements that were prepared with apr library in modules that use mod_dbd or in modules that don't use mod_dbd at all.mod_dbd does not know anything about statements that were prepared in other modules. So feature 'if pselect gives error, prepare one more time' should be implemented in mod_dbd and in all modules, that use apr_dbd_mysql.This 'lot of coding in a lot of places' can be avoided with just turning off reconnect. Maybe not by default, but there should be option.In this case, after connection lost, mysql_ping will not reconnect and it will return error. apr_reslist element will be invalidated and new connection will be created and statements will be prepared (in dbd_construct). It is not large overhead, becaule mysql_ping reconnects anyway.I think this is what we should do. MySQL DBD driver understand some options, we can teach it to understand this too.CreatedIntroduce reconnection option to MySQL DBD driverDoes the patch work for you? In your connection string you should say reconnect=0.Yes. It works as it should. Thank you.I think that information about problem with reconnect, prepared statements and explanation how to avoid it should be in mod_dbd documentation on httpd.apache.org. What should I do to initiate documentation change process? Open bug?Excellent, thanks for testing.Yes, open a bug for httpd to get this documented.I'll commit the patch to the trunk and if there are no objections by other developers, I'll backport it to 1.3.x.This fix should fixas well, as far as I read it. Or am I misunderstanding something?Maybe. I think this is something for folks playing with mod_dbd and MySQL to test.I have run into this problem on RHEL 4, running Apache 2.2.10 and MySQL 5.0.67. The patch doesn't seem to fix the problem for me.I tried using reconnect=0 in my database connection string. The relevant part of my configuration is below (the DBDParams is all on one line, just in case it wraps).<VirtualHost *:80> DBDriver mysql DBDParams host=localhost,port=3306,user=my_db_user,pass=my_db_pass,dbname=auth_db,reconnect=0,sock=/var/lib/mysql/mysql.sock DBDPersist on <Directory /path/to/protected/files> AllowOverride None Order allow,deny Allow from all AuthName "protected" AuthType Digest AuthDigestProvider dbd Require valid-user AuthDBDUserRealmQuery "SELECT digest FROM users WHERE username = %s AND realm = %s" </Directory></VirtualHost>Is there something I can provide to give more information? Or, does my configuration above need to be adjusted?Thanks in advance for any help you can provide.Greg	13.0	id=47630	9	False	False	dan_j_thompson	1
id=47719	REOPENED	None	Apache httpd-2	Core (	2.2.8	Other Linux	P2 enhancement	Apache HTTPD Bugs Mailing List	2009-08-20 17:37 UTC by	Jeremy Grodberg	2011-09-25 21:18 UTC (	1 user	When a symbolic link is broken (points to a non-existent file), the ErrorDocument specified in the VirtualHost section is returned instead of the ErrorDocument specified in the Directory section.Using out-of-the-box Unbutnu 8.04 server running Apache httpd 2.2.8. Did not touch anything but Virtual Host config. Here is the complete Virtual Host config:NameVirtualHost *<VirtualHost *> ErrorDocument 403 "Vhost 403" DocumentRoot /var/www/ <Directory /> ErrorDocument 403 "root 403" Options FollowSymLinks AllowOverride All </Directory> <Directory /var/www/> ErrorDocument 403 "/var/www 403" Options Indexes FollowSymLinks MultiViews AllowOverride None </Directory></VirtualHost>I then went to /var/www which already had an index.html file in it and ran the following commands:cp index.html noperm.htmlchmod a-r noperm.htmlln -s noperm.html goodlink.htmlln -s broken badlink.htmlThis means:* index.html is a valid, readable file* noperm.html is a valid file but httpd does not have permission to read it* goodlink.html is a valid symbolic link to noperm.html* badlink.html is a symbolic link to a non-existent file in the same directoryExpected result:returns the contents of index.htmlreturns "/var/www 403" and status code 403returns "/var/www 403" and status code 403returns "/var/www 403" and status code 403(I would actually prefer that badlink returns status code 404, but that is a battle I am not going to fight right now.)Actual result:Index, noperm, and goodlink work as expected.returns "Vhost 403" and status code 403Note: setting EnableSendfile off does not change the results.	I forgot to mention this may be related toIIUC The inaccessible link target stops the server from associating any file with the current request, so the per-directory config is not applicable to the error response.Could you perhaps make this a feature request rather than "wontfix"? I think the proper error handling of a broken symbolic link should be to handle it as a missing file in the same directory and with the same name as the symbolic link itself. It should return a 404 as configured for that directory.	3.0	id=47220	6	False	False	horowity	1
id=49827	REOPENED	None	Fop - Now in Jira	svg (	1.0	PC Linux	P1 regression	None	2010-08-26 02:57 UTC by	None	2012-11-26 13:16 UTC (	2 users	Hi,I tried to generate a pdf including a barcode with fop 1.0 and barcode4j 2.0.The following error was generated during the integration of bar code in the pdf : [ERROR] FOUserAgent - Image not available. URI: (instream-object). Reason: org.apache.xmlgraphics.image.loader.ImageException: The file format is not supported. No ImagePreloader found for null (No context info available)This works very well with fop 0.95 and with the same configuration.If I replace the fop.jar by that of fop 0.95 it works.	That setup works for me. Usually, this error means that Barcode4J is not in the classpath. Please recheck your application. When in doubt, make sure that "barcode4j-fop-ext-complete.jar" is in the classpath. If this only happens in your application, please try from the FOP command-line, too. Please re-open if that doesn't help, but I'm not sure how I can help any further since it works for me.CreatedFo fileHi,I have not solved my problem, my classpath is well referenced.I send you the fo file that contains barcode.My classpath is : LOCALCLASSPATH=${FOP_HOME}/build/fop.jar${pathSepChar}${FOP_HOME}/build/fop-sandbox.jar${pathSepChar}${FOP_HOME}/build/fop-hyph.jar${pathSepChar}${FOP_HOME}/barcode4j-2.0/build/barcode4j-fop-ext-complete.jar${pathSepChar}$LOCALCLASSPATHbarcode4j is in the barcode4j-2.0 folder.when I change the fop.jar by that of fop-0.95 it works very well, but with that of his fop-1.0 does not work.when I activate the traces, I see these messages appear : - [TRACE] ImageProviderPipeline - Caching image: org.apache.xmlgraphics.image.loader.impl.ImageXMLDOM: null (image/svg+xml) - [ERROR] FOUserAgent - Image not available. URI: (instream-object). Reason: org.apache.xmlgraphics.image.loader.ImageException: The file format is not supported. No ImagePreloader found for null (No context info available)Can you help me please ?I test only command line.My configuration is a red hat linux 64 bit.I'm sorry, Marc-André. I've made a mistake here: it seems I had a CVS HEAD version of Barcode4J in the classpath when I tried to reproduce your problem. And I didn't notice that. Since the Barcode4J 2.0 release, we added the new intermediate format to FOP which uses a different type of plug-in to handle images (and barcodes). The current development code in CVS HEAD of the Barcode4J CVS repository contains the required plug-ins. You'd have to download and compile the source code yourself for now. I plan to finally do a new release of Barcode4J this week, so if you can wait a few days....A work-around is to set <prefer-renderer>true</prefer-renderer> under the root element in the FOP configuration file which gives you back the behaviour from FOP 0.95. That should work.HTHJeremy,Can you pl. confirm if the latest version of Barcode4j released which is compatible with FOP 1.0?Also can you please confirm if the same is available under "barcode4j-fop-ext-complete.jar"If Yes, can you please provide the links to download the same.ThanksMurthyHi folks, I'm just having the very same issues. Any news about a build that works well with fop 1.0?Cheers,KarstenI get side-tracked constantly lately. I've made some progress preparing Barcode4J for release but there are still a few things I need to finish first. Paying clients first, open source second. Sorry.(In reply to)jeremias, any status change on this bug? should it be moved to resolved?The Barcode4J 2.1 release should have resolved that:The problem still presentFOP 1.1BARCODE 2.1JDK1.6.0_22I don't understand if the problem is barcode4j 2.1, fop 1.1 or xmlgraphic-commons 1.5Here the stacktrace.[LoggingEventListener.java:101] Image not available. URI: (instream-object). Reason: org.apache.xmlgraphics.image.loader.ImageException: The file format is not supported. No ImagePreloader found for null (No context info available)org.apache.xmlgraphics.image.loader.ImageException: The file format is not supported. No ImagePreloader found for null at org.apache.xmlgraphics.image.loader.ImageManager.preloadImage(ImageManager.java:180) at org.apache.fop.render.intermediate.AbstractIFPainter.drawImageUsingDocument(AbstractIFPainter.java:296) at org.apache.fop.render.pdf.PDFPainter.drawImage(PDFPainter.java:203) at org.apache.fop.render.intermediate.IFRenderer.renderForeignObject(IFRenderer.java:1290) at org.apache.fop.render.AbstractRenderer.renderInlineViewport(AbstractRenderer.java:820) at org.apache.fop.render.AbstractPathOrientedRenderer.renderInlineViewport(AbstractPathOrientedRenderer.java:785) at org.apache.fop.render.intermediate.IFRenderer.renderInlineViewport(IFRenderer.java:866) at org.apache.fop.render.AbstractRenderer.renderInlineArea(AbstractRenderer.java:678) at org.apache.fop.render.intermediate.IFRenderer.renderInlineArea(IFRenderer.java:913) at org.apache.fop.render.AbstractRenderer.renderLineArea(AbstractRenderer.java:643) at org.apache.fop.render.AbstractRenderer.renderBlocks(AbstractRenderer.java:561) at org.apache.fop.render.AbstractRenderer.renderBlock(AbstractRenderer.java:598) at org.apache.fop.render.intermediate.IFRenderer.renderBlock(IFRenderer.java:980) at org.apache.fop.render.AbstractRenderer.renderBlocks(AbstractRenderer.java:546) at org.apache.fop.render.AbstractRenderer.renderBlock(AbstractRenderer.java:598) at org.apache.fop.render.intermediate.IFRenderer.renderBlock(IFRenderer.java:980) at org.apache.fop.render.AbstractRenderer.renderBlocks(AbstractRenderer.java:546) at org.apache.fop.render.AbstractRenderer.renderRegion(AbstractRenderer.java:337) at org.apache.fop.render.AbstractRenderer.renderRegionViewport(AbstractRenderer.java:301) at org.apache.fop.render.intermediate.IFRenderer.renderRegionViewport(IFRenderer.java:748) at org.apache.fop.render.AbstractRenderer.renderPageAreas(AbstractRenderer.java:257) at org.apache.fop.render.AbstractRenderer.renderPage(AbstractRenderer.java:238) at org.apache.fop.render.intermediate.IFRenderer.renderPage(IFRenderer.java:597) at org.apache.fop.area.RenderPagesModel.renderPage(RenderPagesModel.java:193) at org.apache.fop.area.RenderPagesModel.checkPreparedPages(RenderPagesModel.java:174) at org.apache.fop.area.RenderPagesModel.endDocument(RenderPagesModel.java:258) at org.apache.fop.area.AreaTreeHandler.endDocument(AreaTreeHandler.java:342) at org.apache.fop.fo.FOTreeBuilder.endDocument(FOTreeBuilder.java:168) at org.apache.xml.serializer.ToXMLSAXHandler.endDocument(ToXMLSAXHandler.java:183) at org.apache.xalan.transformer.TransformerImpl.transformNode(TransformerImpl.java:1367) at org.apache.xalan.transformer.TransformerImpl.transform(TransformerImpl.java:709) at org.apache.xalan.transformer.TransformerImpl.transform(TransformerImpl.java:1273) at org.apache.xalan.transformer.TransformerImpl.transform(TransformerImpl.java:1251) at it.gesi.commons.pdf.impl.Obj2pdf.transform(Obj2pdf.java:102) at it.gesi.commons.pdf.impl.Obj2pdf.generatePFDOutputStream(Obj2pdf.java:63) at it.gesi.wba.gestionali.pdf.CreatePdf.createPDF(CreatePdf.java:69) at it.gesi.wba.gestionali.pdf.CreatePdf.createPDF(CreatePdf.java:62) at it.gesi.wba.gestionali.pdf.CreatePdf.creaPdf(CreatePdf.java:76) at it.gesi.commons.stampa.CreaPdfTest.testCreaPdf(CreaPdfTest.java:86) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:616) at junit.framework.TestCase.runTest(TestCase.java:164) at junit.framework.TestCase.runBare(TestCase.java:130) at junit.framework.TestResult$1.protect(TestResult.java:110) at junit.framework.TestResult.runProtected(TestResult.java:128) at junit.framework.TestResult.run(TestResult.java:113) at junit.framework.TestCase.run(TestCase.java:120) at junit.framework.TestSuite.runTest(TestSuite.java:228) at junit.framework.TestSuite.run(TestSuite.java:223) at org.junit.internal.runners.OldTestClassRunner.run(OldTestClassRunner.java:35) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)	9.0	id=47650	5	False	False	christophe.jaillet	1
id=47580	REOPENED	None	Apache httpd-2	mod_cache (	2.2.21	All All	P2 normal	Apache HTTPD Bugs Mailing List	2009-07-25 05:18 UTC by	Nicholas Sherlock	2013-09-08 15:43 UTC (	1 user	CreatedPHP script to demonstrate the problemIf you make a conditional request for a cached document, but the document is expired in the cache, mod_cache correctly passes on the conditional request to the backend. If the backend responds with a "304 Not Modified" response that indicates that the cached copy is still up to date, mod_cache serves the contents of the cache to the client with a 200 code. This is a standards-conforming valid way of responding.But couldn't it just send a "304 Not Modified" code instead? RFC2616 14.26 says "instead, if the request method was GET or HEAD, the server SHOULD respond with a 304 (Not Modified) response, including the cache- related header fields (particularly ETag) of one of the entities that matched." The current behaviour unnecessarily sends a response body to the client. This ends up wasting bandwidth in the case where you press refresh on an unmodified object (at least in Firefox,) which sends these request headers:If-None-Match="My ETag"Cache-Control=max-age=0I do not want the behaviour given by the "CacheIgnoreCacheControl yes" directive. I still want mod_cache to validate the request against the backend, but I don't want it to waste bandwidth by sending a 200 response code.To test it, I have these cache-related lines in my virtual host definition:CacheRoot C:/tempCacheEnable disk /My index.php is the attached file.My web browser with an empty cache requests index.php, the (trimmed) response is:Status=OK - 200Date=Mon, 20 Jul 2009 07:16:05 GMTExpires=Wed, 19 Aug 2009 07:16:05 GMTEtag="ComputedETag"The log performed by index.php indicates:Mon, 20 Jul 2009 19:16:05 +1200 - Response: 200. Generated document.So far so good. But now I press refresh in my web browser. This makes a conditional request for the document:If-None-Match="ComputedETag"Cache-Control=max-age=0With the max-age of 0, the cache will be bypassed, which is the desired behaviour. The cache passes this conditional request onto the backend, and the backend logs it:Mon, 20 Jul 2009 19:16:12 +1200 - Response: 304 Not ModifiedSo the backend is trying to tell the client that it already has an up-to-date body. But the response sent to the client by mod_cache is:Status=OK - 200Date=Mon, 20 Jul 2009 07:16:12 GMTEtag="ComputedETag"Expires=Wed, 19 Aug 2009 07:16:12 GMTMy Apache configuratiion is:Apache/2.2.11 (Win32) DAV/2 mod_ssl/2.2.11 OpenSSL/0.9.8i SVN/1.6.3 PHP/5.3.0	This is a bug in php or more specific in the httpd module version of php. In sapi_apache2.c::php_apache_request_ctor (line 463 for php 5.3.0) it sets r->no_local_copy to 1 for an unknown reason which causes to return a 200 instead of a 304. If you comment this line everything works as expected by you.httpd itself only set this struct member to 1 forsubrequestserror pagesin both cases it is not desired that even conditional requests return a 304.For subrequests as they only deliver fragments of a page that is processed internally and for error pages this is obvious.In order to fix your problem do one of the following things:1. Use the CGI/FASTCGI version of PHP.2. Comment the line in the PHP code as described above (no idea which further sideeffects this has as I am not a php developer).3. Open a bug report at bugs.php.net to get this fixed.For anyone following this bug, this is now PHP:Confirming that this defect still exists in Apache 2.2.21 with mod_cache enabled.Browser sends requests with If-None-Match headers that correspond with ETag for content.mod_cache is configured to ignore all headers from request and response that could cause a cache miss. mod_cache returns cache hit from the cache, in my test case disk cache, however also reproducible with a mem cache. The cache hit despite matching the If-None-Match in the request returns with a response code of 200 and sends the full body in the response, instead of a terse/concise 304.This has nothing to do with PHP which it looks like Nicholas Sherlock had originally attached to recreate the problem. I can recreate the same problem with Fiddler.Workaround to this bug in mod_cache:Disable ETags and If-None-Match headers for 304 checkingInstead use If-Modified-Since and Last-Modified headers.#To deal with caches already in my users browser I used this config before hitting #mod_cache to ensure they don't get included in the request and confuse mod_cacheRequestHeader unset If-None-Match#This cleans ETag headers from any apps downstream from my Apache incase they generated them. Header unset ETagWith the If-Modified-Since and Last-Modified headers mod_cache appears to be properly writing back files from its cache and also writing back 304 HTTP Status codes when serving files from its cache that already match with the ones on the browser.	4.0	id=47693	5	False	False	poirier	1
id=47778	REOPENED	None	Apache httpd-2	Core (	2.2.13	PC Linux	P2 enhancement	Apache HTTPD Bugs Mailing List	2009-09-03 00:42 UTC by	None	2009-09-06 10:44 UTC (	0 users	I am running httpd on a generated httpd.conf file. If the configuration is changed while httpd is running, and then loaded using if httpd -f /my/httpd.conf -t; then httpd -f /my/httpd.conf -k graceful || exit_with_error fithen the exit value of the graceful restart is 0, even though error_log says[Thu Sep 03 09:12:26 2009] [warn] Init: Session Cache is not configured [hint: SSLSessionCache][Thu Sep 03 09:12:26 2009] [warn] pid file /tomcat/httpd/logs/httpd.pid overwritten -- Unclean shutdown of previous Apache run?[Thu Sep 03 09:12:26 2009] [notice] Apache/2.2.13 (Unix) mod_ssl/2.2.13 OpenSSL/0.9.8k configured -- resuming normal operations[Thu Sep 03 09:12:43 2009] [notice] Graceful restart requested, doing restart(98)Address already in use: make_sock: could not bind to address xx.xx.xx.xx:80no listening sockets available, shutting downUnable to open logsThe suggested workaround to check the semantics ("start httpd as non-root") doesn't work in scripts, because this _always_ fails either due to bind, or due to the attempted access to reserved port 80/tcp, or it succeeds and keeps an unwanted httpd running with the wrong uid.I don't expect any miracles, but the exit value of "httpd -k graceful" _should_ show whether an httpd with the new configuration is running. Reliability is highly important for us. Silently dying Apaches is a _huge_ problem in our environment.	The httpd that's running at this point, to send the graceful restart signal, can't predict/inspect anything about the restarting httpd. A 0 exit code means the restart signal was sent, and checking/polling for the status of your httpd is beyond the scope of -k graceful.I Understand, but the bug is that this procedure is not reliable IRL. Running an httpd I don't want to know whether the signaling mechanism worked, but whether Apache is still alive.I thought you would be interested in improving Apache? Can't you imagine that the restarting httpd and the running httpd perform some kind of IPC other than just a unidirectional signal?Please reconsider.Many thanxHarriMarking as enhancement as the exit code is reasonable, patches welcome.Looks like this was meant to be enhancement, not blocker.	4.0	id=47719	7	False	True	asfbugzilla	1
id=48295	REOPENED	None	Apache httpd-2	mod_proxy (	2.2.14	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2009-11-27 22:26 UTC by	Jie Gao	2009-11-30 16:46 UTC (	0 users	From Apache documentation:"Usage is basically similar to ProxyPassReverse, but instead of rewriting headers that are a URL, this rewrites the path string in Set-Cookie headers."It does not seem to be the case.Example:<Location /sss/>ProxyHTMLEnable OnProxyPassProxyPassReverse /ProxyHTMLURLMap / /sss/ProxyPassReverseCookiePath / /sss/</Location>This will rewrite cookie path from backend server's /cgi-bin to /sss/ for the frontend server.Is this intended behaviour?Jie	Post verbatim headers of the bug you're describing.Script setting the cookie:#!/usr/bin/perl -wuse strict;use CGI;my $q = new CGI;my $cookie_1 = $q->cookie(-name=>'jie_COOKIE_1', -value=>'BEST_COOKIE=chocolatechip', -domain=>'www-dev.xxxxxx.xxx.xx', -path=>'/cgi-bin');print $q->header(-cookie=>$cookie_1);print $q->start_html('My cookie-set.cgi program');print $q->h3('The cookie has been set');print $q->end_html;-----------------------------------------------------Running it directly to the backend serverHTTP/1.0 200 OKDate: Sun, 29 Nov 2009 01:42:56 GMTServer: Apache/2.2.3 (Red Hat)Set-Cookie: jie_COOKIE_1=BEST_COOKIE%3Dchocolatechip; domain=xxx.xxx.xxx; path=/cgi-binContent-Type: text/html; charset=ISO-8859-1X-Cache: MISS from www-cacheE.xxx.xxx.xxX-Cache-Lookup: MISS from www-cacheE.xxx.xxx.xx:8080Via: 1.0 www-cacheE.xxx.xxx.xx:8080 (squid/2.6.STABLE5)Proxy-Connection: closeRunning it on the rp server:HTTP/1.0 200 OKDate: Sun, 29 Nov 2009 01:46:24 GMTServer: Apache/2.2.3 (Red Hat)Content-Type: text/html; charset=utf-8Set-Cookie: jie_COOKIE_1=BEST_COOKIE%3Dchocolatechip; domain=www-dev.xxx.xxx.xx; path=/sss/X-Cache: MISS from www-cacheE.xxxx.xxx.xxX-Cache-Lookup: MISS from www-cacheE.xxx.xxx.xx:8080Via: 1.0 www-cacheE.xxx.xx.xx:8080 (squid/2.6.STABLE5)Proxy-Connection: closePath is defined as the component of the URL up to and including the rightmost slash. So the path of "/cgi-bin" is "/".I changed the script to set the cookie path to "/cgi-bin/', but I got the sameresult from the RP as before.I changed the script to set the cookie path to "/cgi-bin/', but I got the sameresult from the RP as before.From rfc 2965:Path Defaults to the path of the request URL that generated the Set-Cookie2 response, up to and including the right-most /.***has been marked as a duplicate of this bug. ***Note that due to a data loss on 26/27 Nov 2009 the issue that was originally created aswas lost. It has been re-created as.	8.0	id=48295	6	False	False	covener	1
id=48880	REOPENED	None	Apache httpd-2	mod_ssl (	2.2.15	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2010-03-09 17:49 UTC by	None	2011-11-20 07:49 UTC (	0 users	Am attempting to upgrade from 2.2.14 with openssl-0.9.8l to 2.2.15 with openssl-0.9.8m. The os is Red Har Enterprise Linux Server Release 5.4 Tikanga. I get an error running make. Here is the error:make[1]: Entering directory `/root/Desktop/httpd-2.2.15'gcc -pthread -DLINUX=2 -D_REENTRANT -D_GNU_SOURCE -I/root/Desktop/httpd-2.2.15/srclib/pcre -I. -I/root/Desktop/httpd-2.2.15/os/unix -I/root/Desktop/httpd-2.2.15/server/mpm/prefork -I/root/Desktop/httpd-2.2.15/modules/http -I/root/Desktop/httpd-2.2.15/modules/filters -I/root/Desktop/httpd-2.2.15/modules/proxy -I/root/Desktop/httpd-2.2.15/include -I/root/Desktop/httpd-2.2.15/modules/generators -I/root/Desktop/httpd-2.2.15/modules/mappers -I/root/Desktop/httpd-2.2.15/modules/database -I/usr/include/apr-1 -I/root/Desktop/httpd-2.2.15/modules/proxy/../generators -I/usr/kerberos/include -I/root/Desktop/httpd-2.2.15/modules/ssl -I/root/Desktop/httpd-2.2.15/modules/dav/main -c /root/Desktop/httpd-2.2.15/server/buildmark.c/usr/lib64/apr-1/build/libtool --silent --mode=link gcc -pthread -L/usr/kerberos/lib64 -o httpd modules.lo buildmark.o -export-dynamic server/libmain.la modules/aaa/libmod_authn_file.la modules/aaa/libmod_authn_default.la modules/aaa/libmod_authz_host.la modules/aaa/libmod_authz_groupfile.la modules/aaa/libmod_authz_user.la modules/aaa/libmod_authz_default.la modules/aaa/libmod_auth_basic.la modules/aaa/libmod_auth_digest.la modules/filters/libmod_include.la modules/filters/libmod_filter.la modules/loggers/libmod_log_config.la modules/metadata/libmod_env.la modules/metadata/libmod_expires.la modules/metadata/libmod_setenvif.la modules/metadata/libmod_version.la modules/ssl/libmod_ssl.la modules/http/libmod_http.la modules/http/libmod_mime.la modules/dav/main/libmod_dav.la modules/generators/libmod_status.la modules/generators/libmod_autoindex.la modules/generators/libmod_asis.la modules/generators/libmod_cgi.la modules/dav/fs/libmod_dav_fs.la modules/mappers/libmod_negotiation.la modules/mappers/libmod_dir.la modules/mappers/libmod_actions.la modules/mappers/libmod_speling.la modules/mappers/libmod_userdir.la modules/mappers/libmod_alias.la modules/mappers/libmod_rewrite.la modules/mappers/libmod_so.la server/mpm/prefork/libprefork.la os/unix/libos.la -lm /root/Desktop/httpd-2.2.15/srclib/pcre/libpcre.la /usr/lib64/libaprutil-1.la -lldap -llber -ldb-4.3 -lexpat /usr/lib64/libapr-1.la -lpthread -ldlmodules/ssl/.libs/libmod_ssl.a(ssl_engine_kernel.o): In function `ssl_hook_ReadReq':ssl_engine_kernel.c:(.text+0x1eb): undefined reference to `SSL_get_servername'modules/ssl/.libs/libmod_ssl.a(ssl_engine_kernel.o): In function `ssl_hook_Fixup':ssl_engine_kernel.c:(.text+0x1f83): undefined reference to `SSL_get_servername'modules/ssl/.libs/libmod_ssl.a(ssl_engine_kernel.o): In function `ssl_callback_ServerNameIndication':ssl_engine_kernel.c:(.text+0x39c8): undefined reference to `SSL_get_servername'modules/ssl/.libs/libmod_ssl.a(ssl_engine_kernel.o): In function `ssl_find_vhost':ssl_engine_kernel.c:(.text+0x3c42): undefined reference to `SSL_set_SSL_CTX'modules/ssl/.libs/libmod_ssl.a(ssl_engine_vars.o): In function `ssl_var_lookup_ssl':ssl_engine_vars.c:(.text+0x11bf): undefined reference to `SSL_get_servername'collect2: ld returned 1 exit statusmake[1]: *** [httpd] Error 1make[1]: Leaving directory `/root/Desktop/httpd-2.2.15'make: *** [all-recursive] Error 1	is it possible that you're compiling against include files from a newer OpenSSL than you're linking against? Double check that aspect of the build.(In reply to)I am attempting to use the latest openssl-0.9.8m - I have installed that is a new location from 8l. I am able to recompile 2.2.14 using 8m, but get the same error using 2.2.15 - I installed 8m in an attempt to correct the error.Here is my config. ./configure --prefix=/etc/httpd2.2.15 --enable-ssl --with-ssl=/usr/localssl/openssl --with-sslport=443 --enable-rewrite --enable-speling --enable-auth-digest --enable-dav --enable-expiresI also ran red hat update software after encountering the error.The failure is as Jeff suggests.Also, you are linking against the system APR/APR-util libraries here. This may bring in the system OpenSSL library via OpenLDAP, for example.To get this to work I'd recommend doing: # yum remove {httpd,apr,apr-util,openssl}-develfirst, or at minimum passing --with-included-apr to httpd's configure script. I've seen this type of failure because the system pkgconfig file can be picked up in preference to the OpenSSL pkgconfig file in the --with-ssl location.(Alternatively, if you are merely waiting for the TLS secure reneg fix you could wait for the RHEL update to openssl providing this)Thanks. That worked.Removing the openssl-devel did the trick. I attempted --with-included-apr and that did not work by itself. I had to remove the Openssl-devel.I am experimenting the same issue with FreeBSD.with httpd-2.2.11, with the same FreeBSD environment it just links fine.I am experimenting this issue with all versions after httpd-2.2.11, so I am still running 2.2.11.I have openssl-0.9.8m installed.See attachments for config.log's (don't know it helps)All the time the same error message:modules/ssl/.libs/libmod_ssl.a(ssl_engine_kernel.o)(.text+0x17): In function `ssl_callback_ServerNameIndication':/usr/home/walsimou/install/dns.walsimou.com/apache/httpd-2.2.15/modules/ssl/ssl_engine_kernel.c:1980: undefined reference to `SSL_get_servername'modules/ssl/.libs/libmod_ssl.a(ssl_engine_kernel.o)(.text+0x2b7): In function `ssl_find_vhost':/usr/home/walsimou/install/dns.walsimou.com/apache/httpd-2.2.15/modules/ssl/ssl_engine_kernel.c:2063: undefined reference to `SSL_set_SSL_CTX'modules/ssl/.libs/libmod_ssl.a(ssl_engine_kernel.o)(.text+0x1867): In function `ssl_hook_Fixup':/usr/home/walsimou/install/dns.walsimou.com/apache/httpd-2.2.15/modules/ssl/ssl_engine_kernel.c:1139: undefined reference to `SSL_get_servername'modules/ssl/.libs/libmod_ssl.a(ssl_engine_kernel.o)(.text+0x30a9): In function `ssl_hook_ReadReq':/usr/home/walsimou/install/dns.walsimou.com/apache/httpd-2.2.15/modules/ssl/ssl_engine_kernel.c:120: undefined reference to `SSL_get_servername'modules/ssl/.libs/libmod_ssl.a(ssl_engine_vars.o)(.text+0x17be): In function `ssl_var_lookup':/usr/home/walsimou/install/dns.walsimou.com/apache/httpd-2.2.15/modules/ssl/ssl_engine_vars.c:325: undefined reference to `SSL_get_servername'gmake[1]: *** [httpd] Error 1gmake[1]: Leaving directory `/usr/home/walsimou/install/dns.walsimou.com/apache/httpd-2.2.15'gmake: *** [all-recursive] Error 1Createdconfig.log for 2.2.11config.log for 2.2.11Createdconfig.log for 2.2.15config.log for 2.2.15I was getting the same error on Red Hat 5.4.0.3 64-bit, however the problem for me appears to be a problem with configure. I used this option to configure (among others):--with-ssl=/usr/myCo/apps/openssl/openssl-1.0.0This resulted in this flag being sent to apr/libtool:-L/usr/myCo/apps/openssl/openssl-1.0.0/libThat directory does not exist. That flag should have been:-L/usr/myCo/apps/openssl/openssl-1.0.0/lib64I worked around it like this:# cd /usr/myCo/apps/openssl/openssl-1.0.0# ln -s lib64 libThen I went and ran a make clean, configure and make again and it worked.configure should know about the OpenSSL lib64 directory and use it when appropriate.I was running into problems with my server. Turned out I had an old version of openssl is /usr/local/ssl/lib which apache was finding, while I thought I was building against a newer version. The configure script needs to know to look for lib64.	9.0	id=48364	13	False	False	rpluem	1
id=49798	REOPENED	None	Apache httpd-2	mod_log_config (	2.2.16	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2010-08-22 09:11 UTC by	Laurent Declercq	2010-08-23 01:39 UTC (	0 users	Hello ;When I do an 'apache2ctl gracefull', I get following warning in my apache error.log:[Sun Aug 22 14:46:00 2010] [warn] long lost child came home! (pid 10436)After some avestigations, I found that it's the'mod_log_config' module that cause this issue when we use Piped Log. To show, the problem:'ps aux' result before do the gracefull:root 8511 0.1 0.4 6824 4256 ? Ss 14:18 0:03 /usr/sbin/apache2 -k startroot 10502 0.0 0.0 1748 500 ? S 14:46 0:00 /bin/sh -c /var/www/ispcp/engine/ispcp-apache-loggerwww-data 10503 0.0 0.2 6192 2324 ? S 14:46 0:00 /usr/sbin/apache2 -k startroot 10504 0.0 0.2 4532 2984 ? S 14:46 0:00 /usr/bin/perl /var/www/ispcp/engine/ispcp-apache-loggerwww-data 10505 0.0 0.2 6824 2188 ? S 14:46 0:00 /usr/sbin/apache2 -k startwww-data 10506 0.0 0.2 228236 2856 ? Sl 14:46 0:00 /usr/sbin/apache2 -k startwww-data 10507 0.0 0.2 228236 2860 ? Sl 14:46 0:00 /usr/sbin/apache2 -k startHere, you can see my main Piped Log script process (pid 10502) and the second (pid 10504) that is the background process from (pid 10436).When I do the graceful, I get the following:[Sun Aug 22 14:52:50 2010] [notice] SIGUSR1 received. Doing graceful restart[Sun Aug 22 14:52:51 2010] [notice] Stopping ispCP Apache logger (pid 10504)[Sun Aug 22 14:52:51 2010] [notice] Apache/2.2.16 (Debian) mod_fcgid/2.3.5 configured -- resuming normal operations[Sun Aug 22 14:52:51 2010] [warn] long lost child came home! (pid 10502)[Sun Aug 22 14:52:51 2010] [notice] Starting ispCP Apache logger (pid 10579)Note: Log lines for the logger come from my logging script (Added to show the problem).After the gracefull, my 'px aux' result is like this:root 8511 0.1 0.4 6824 4256 ? Ss 14:18 0:03 /usr/sbin/apache2 -k startroot 10577 0.0 0.0 1748 500 ? S 14:52 0:00 /bin/sh -c /var/www/ispcp/engine/ispcp-apache-loggerwww-data 10578 0.0 0.2 6192 2324 ? S 14:52 0:00 /usr/sbin/apache2 -k startroot 10579 0.0 0.2 4532 2984 ? S 14:52 0:00 /usr/bin/perl /var/www/ispcp/engine/ispcp-apache-loggerwww-data 10580 0.0 0.2 6824 2188 ? S 14:52 0:00 /usr/sbin/apache2 -k startwww-data 10581 0.0 0.2 228236 2856 ? Sl 14:52 0:00 /usr/sbin/apache2 -k startwww-data 10582 0.0 0.2 228236 2860 ? Sl 14:52 0:00 /usr/sbin/apache2 -k startOk, it's just a warn but I think that should be solved. I've tried with alternative syntax (Shell not involved at all) and the same problem occurs on the main process.My system:root@ispcp:/etc/apache2/sites-available# uname -aLinux ispcp 2.6.32-5-686 #1 SMP Sat Jul 24 02:27:10 UTC 2010 i686 GNU/LinuxThe involved SHELL:root@ispcp:/etc/apache2/sites-available# ls -la /bin/shlrwxrwxrwx 1 root root 4 21 août 07:11 /bin/sh -> dashThe Apache information:root@ispcp:/etc/apache2/sites-available# apache2ctl -VServer version: Apache/2.2.16 (Debian)Server built: Jul 24 2010 20:24:16Server's Module Magic Number: 20051115:24Server loaded: APR 1.4.2, APR-Util 1.3.9Compiled using: APR 1.4.2, APR-Util 1.3.9Architecture: 32-bitServer MPM: Worker threaded: yes (fixed thread count) forked: yes (variable process count)Server compiled with.... -D APACHE_MPM_DIR="server/mpm/worker" -D APR_HAS_SENDFILE -D APR_HAS_MMAP -D APR_HAVE_IPV6 (IPv4-mapped addresses enabled) -D APR_USE_SYSVSEM_SERIALIZE -D APR_USE_PTHREAD_SERIALIZE -D SINGLE_LISTEN_UNSERIALIZED_ACCEPT -D APR_HAS_OTHER_CHILD -D AP_HAVE_RELIABLE_PIPED_LOGS -D DYNAMIC_MODULE_LIMIT=128 -D HTTPD_ROOT="/etc/apache2" -D SUEXEC_BIN="/usr/lib/apache2/suexec" -D DEFAULT_PIDLOG="/var/run/apache2.pid" -D DEFAULT_SCOREBOARD="logs/apache_runtime_status" -D DEFAULT_ERRORLOG="logs/error_log" -D AP_TYPES_CONFIG_FILE="mime.types" -D SERVER_CONFIG_FILE="apache2.conf"Thank a lot if you can solve this problem.Note: Sorry for my poor English, I'm french.Note2: not 2.2.16 version is selectable here , why ?	I want say graceful, not gracefull (sorry).Forgot the first description and read this:When I do an 'apache2ctl graceful', I get following warning in my apacheerror.log:[Sun Aug 22 14:46:00 2010] [warn] long lost child came home! (pid 10436)After some avestigations, I found that it's the'mod_log_config' module thatcause this issue when we use Piped Log. To show, the problem:'ps aux' result before do the gracefull:root 8511 0.1 0.4 6824 4256 ? Ss 14:18 0:03/usr/sbin/apache2 -k startroot 10502 0.0 0.0 1748 500 ? S 14:46 0:00 /bin/sh -c/var/www/ispcp/engine/ispcp-apache-loggerwww-data 10503 0.0 0.2 6192 2324 ? S 14:46 0:00/usr/sbin/apache2 -k startroot 10504 0.0 0.2 4532 2984 ? S 14:46 0:00 /usr/bin/perl/var/www/ispcp/engine/ispcp-apache-loggerwww-data 10505 0.0 0.2 6824 2188 ? S 14:46 0:00/usr/sbin/apache2 -k startwww-data 10506 0.0 0.2 228236 2856 ? Sl 14:46 0:00/usr/sbin/apache2 -k startwww-data 10507 0.0 0.2 228236 2860 ? Sl 14:46 0:00/usr/sbin/apache2 -k startHere, you can see my main Piped Log script process (pid 10502) and the second(pid 10504) that is the background process from the first (/bin/sh -c ; pid 10502).When I do the graceful, I get the following:[Sun Aug 22 14:52:50 2010] [notice] SIGUSR1 received. Doing graceful restart[Sun Aug 22 14:52:51 2010] [notice] Stopping ispCP Apache logger (pid 10504)[Sun Aug 22 14:52:51 2010] [notice] Apache/2.2.16 (Debian) mod_fcgid/2.3.5configured -- resuming normal operations[Sun Aug 22 14:52:51 2010] [warn] long lost child came home! (pid 10502)[Sun Aug 22 14:52:51 2010] [notice] Starting ispCP Apache logger (pid 10579)Note: Log lines for the logger come from my logging script (Added to show theproblem).After the gracefull, my 'px aux' result is like this:root 8511 0.1 0.4 6824 4256 ? Ss 14:18 0:03/usr/sbin/apache2 -k startroot 10577 0.0 0.0 1748 500 ? S 14:52 0:00 /bin/sh -c/var/www/ispcp/engine/ispcp-apache-loggerwww-data 10578 0.0 0.2 6192 2324 ? S 14:52 0:00/usr/sbin/apache2 -k startroot 10579 0.0 0.2 4532 2984 ? S 14:52 0:00 /usr/bin/perl/var/www/ispcp/engine/ispcp-apache-loggerwww-data 10580 0.0 0.2 6824 2188 ? S 14:52 0:00/usr/sbin/apache2 -k startwww-data 10581 0.0 0.2 228236 2856 ? Sl 14:52 0:00/usr/sbin/apache2 -k startwww-data 10582 0.0 0.2 228236 2860 ? Sl 14:52 0:00/usr/sbin/apache2 -k startOk, it's just a warn but I think that should be solved. I've tried withalternative syntax (Shell not involved at all) and the same problem occurs onthe main process.My system:root@ispcp:/etc/apache2/sites-available# uname -aLinux ispcp 2.6.32-5-686 #1 SMP Sat Jul 24 02:27:10 UTC 2010 i686 GNU/LinuxThe involved SHELL:root@ispcp:/etc/apache2/sites-available# ls -la /bin/shlrwxrwxrwx 1 root root 4 21 août 07:11 /bin/sh -> dashThe Apache information:root@ispcp:/etc/apache2/sites-available# apache2ctl -VServer version: Apache/2.2.16 (Debian)Server built: Jul 24 2010 20:24:16Server's Module Magic Number: 20051115:24Server loaded: APR 1.4.2, APR-Util 1.3.9Compiled using: APR 1.4.2, APR-Util 1.3.9Architecture: 32-bitServer MPM: Worker threaded: yes (fixed thread count) forked: yes (variable process count)Server compiled with.... -D APACHE_MPM_DIR="server/mpm/worker" -D APR_HAS_SENDFILE -D APR_HAS_MMAP -D APR_HAVE_IPV6 (IPv4-mapped addresses enabled) -D APR_USE_SYSVSEM_SERIALIZE -D APR_USE_PTHREAD_SERIALIZE -D SINGLE_LISTEN_UNSERIALIZED_ACCEPT -D APR_HAS_OTHER_CHILD -D AP_HAVE_RELIABLE_PIPED_LOGS -D DYNAMIC_MODULE_LIMIT=128 -D HTTPD_ROOT="/etc/apache2" -D SUEXEC_BIN="/usr/lib/apache2/suexec" -D DEFAULT_PIDLOG="/var/run/apache2.pid" -D DEFAULT_SCOREBOARD="logs/apache_runtime_status" -D DEFAULT_ERRORLOG="logs/error_log" -D AP_TYPES_CONFIG_FILE="mime.types" -D SERVER_CONFIG_FILE="apache2.conf"Thank a lot if you can solve this problem.Note: Sorry for my poor English, I'm french.Note2: not 2.2.16 version is selectable here , why ?Use "||process args" to launch the external logger process without invoking the shell/command interpreter. See the other issue 49768.*** This bug has been marked as a duplicate of***Sorry but it's not the same issue. I've already tried to execute without involving the SHELL. I'll try again and show you the logs result...Apache configuration:CustomLog "|| /var/www/ispcp/engine/ispcp-apache-logger" ispcplogAs you can see, the Shell is not involved here.ps aux result before do the graceful:root 15016 0.0 0.2 4532 2984 ? S 18:39 0:00 /usr/bin/perl /var/www/ispcp/engine/ispcp-apache-loggerwww-data 15017 0.0 0.2 6192 2300 ? S 18:39 0:00 /usr/sbin/apache2 -k startwww-data 15018 0.0 0.2 6824 2164 ? S 18:39 0:00 /usr/sbin/apache2 -k startwww-data 15019 0.0 0.2 228236 2840 ? Sl 18:39 0:00 /usr/sbin/apache2 -k startwww-data 15020 0.0 0.2 228236 2844 ? Sl 18:39 0:00 /usr/sbin/apache2 -k startps aux resulter after the graceful:root 15089 8.0 0.2 4532 2988 ? S 18:42 0:00 /usr/bin/perl /var/www/ispcp/engine/ispcp-apache-loggerwww-data 15090 1.0 0.2 6192 2304 ? S 18:42 0:00 /usr/sbin/apache2 -k startwww-data 15091 0.0 0.2 6824 2132 ? S 18:42 0:00 /usr/sbin/apache2 -k startwww-data 15092 2.0 0.2 228236 2844 ? Sl 18:42 0:00 /usr/sbin/apache2 -k startwww-data 15093 1.0 0.2 228236 2844 ? Sl 18:42 0:00 /usr/sbin/apache2 -k startNow, the corresponding error logs:[Sun Aug 22 18:42:21 2010] [notice] SIGUSR1 received. Doing graceful restart[Sun Aug 22 18:42:23 2010] [notice] Apache/2.2.16 (Debian) mod_fcgid/2.3.5 configured -- resuming normal operations[Sun Aug 22 18:42:23 2010] [warn] long lost child came home! (pid 15016)[Sun Aug 22 18:42:23 2010] [notice] Starting ispCP Apache logger (pid 15089)As you can see, the process 15016 (my logger script) that is launch by apache cause the error with graceful.I hope you understand my sentence now.Updated to 2.2.16Confirmed here:I'm not a crasy man...	7.0	id=37770	61	True	True	apache	1
id=50562	REOPENED	None	Apache httpd-2	Win32 MSI Installer (	2.2.14	PC Windows XP	P2 critical	Apache HTTPD Bugs Mailing List	2011-01-09 21:50 UTC by	lu ye	2015-08-13 16:15 UTC (	0 users	I setup apache on xp. With the following URI:Apache set the PATH_INFO to /???￥?I have checked the log that apache got right utf8 urlencoded uri. here is the log:"GET /moin.cgi/aaa%E5%8C%97%E4%BA%AC%E9%82%AE%E7%94%B5bb HTTP/1.1" 404 4982 "-" "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.10 (KHTML, like Gecko) Chrome/8.0.552.224 Safari/534.10"It seems that apache donnot translate non-ascii character right? Why it will do some conversion here?	sorry, the PATH_INFO is /aaa???￥?bb(In reply to)Wrong; Path info was the unicode value from your URI path.Apache httpd didn't get this wrong, your CGI got it wrong and doesn't correctly handle non-ASCII input from the environment. If your CGI environment's codepage is compatible with the characters transmitted, windows should be translating this itself into the appropriate MBCS.I have replaced that CGI with a C++ .exe to test the the environment vars set by apache, with URI "/aaa你好bbb"， the exe surely get a PATH_INFO with "/aaa\x3f\x3f\x3f\xa3\xa4\x3f". ascii character is 1 byte so the environ vars are not ucs-2. ps: I used both getenv and windows api GetEnvironmentVariable and the result are the same. The .exe is compiled without unicode support with VS. (In reply to)I have replaced that CGI with a C++ .exe to test the the environment vars set by apache, with URI "/aaa你好bbb"， the exe surely get a PATH_INFO with "/aaa\x3f\x3f\x3f\xa3\xa4\x3f". ascii character is 1 byte so the environ vars are not ucs-2. ps: I used both getenv and windows api GetEnvironmentVariable and the result are the same. The .exe is compiled without unicode support with VS. (In reply to)What does CHCP return at the command line?If and only if I use Unicode win api-GetEnvironmentVariableW that I can get the right answer. C lib getenv and GetEnvironmentVariableA all returns wrong answer. Use chcp to change codepage makes the result no difference.I am puzzled. Which coding scheme is used by apache to set environ vars? What happened when passing them to CGI and getting them in CGI? (In reply to)After test, it is quite sure that apache has done wrong utf8 to unicode conversion with non-ascii charcater. It simply extend the bytes of utf-8 with \x00 to get the unicode encoding. Here is the test:1. request a url2. get "PATH_INFO" with windows api GetEnvironmentVariableW, it returns "\x00\x2f \x00\xe4 \x00\xbd \x00\xa0 \x00\xe5 \x00\xa5 \x00\xbd" which should be "\x00\x2f \x60\x4f \x7d\x59"3. my cmd codepage is cp936(GBK), so any programming language which depends on C lib to get this environment will get wrong result as long as a non-ascii uri is requeted.hope to check.(In reply to)Works for me with PHP 5.6 and Apache 2.4.16 on Win 7Maybe this ticket can be closed	8.0	id=47778	6	False	True	covener	1
id=49859	REOPENED	None	Apache httpd-2	mod_mime (	2.2.3	All All	P2 normal	Apache HTTPD Bugs Mailing List	2010-09-01 11:12 UTC by	Ben Griffin	2010-09-02 09:54 UTC (	0 users	Using the following:AddHandler test-file .tstAction test-file /cgi-bin/set.cgiAnd with set.cgi = #!/bin/bashechoechoecho "$@"setwith an existing file "test.tst" and the url: ""everything is fine = we see eg PATH_TRANSLATED showing "...test.tst"BUG is exposed with same environment, but the url ""The handler sees test.tst and calls set.cgi - however, PATH_TRANSLATED includes /ghost.html as a part of the path, even though clearly the logic is wrong. The url should not trigger the handler - (because the file ..../test.tst/ghost.html does not exist )but should instead trigger a 404.	The mapping is due to AcceptPathInfo, and CGI says PATH_INFO should be included in PATH_TRANSLATED. Followups onunless there's some gross misunderstanding, in which casr provide verbatim, complete config and log entries.Eric, thanks. I read up on your comments. I attempted to post a mail as suggested, but was refused."AcceptPathInfo Off" appears to have no effect when using a suffix handler via AddHandler (the handler in the bug report's case is a bash script calling set)The documentation says:"For example, assume the location /test/ points to a directory that contains only the single file here.html. Then requests for/test/here.html/more and /test/nothere.html/more both collect /more as PATH_INFO.""Therefore a request with trailing pathname information after the true filename such as /test/here.html/more in the above example will return a 404 NOT FOUND error."However, this is not what I find.Here is the entire apache config file. (note that this is based on a Mac, but the original bug was found on Debian Linux)apache.conf follows (dso.conf is the default set of modules)#================================================Include /etc/apache2/dso.confServerRoot /Library/WebServerListen 80User _wwwGroup _www <Directory /Library/WebServer/CGI-Executables/> Options +ExecCGI</Directory>AddHandler cgi-script .cgiAddType application/test .tstScriptAlias /cgi/ /Library/WebServer/CGI-Executables/Action application/test /cgi/set.cgi<VirtualHost *:80>AcceptPathInfo OffOptions -Indexes +FollowSymLinksDocumentRoot Documents/public</VirtualHost>#================================================set.cgi follows#================================================#!/bin/bashecho Status: 200 OKecho Content-Type: text/plainechoset#================================================Directory /Library/WebServer/Documents/public contains one file, called "here.tst" which is a text file containing the word "test"Results from the above setting. (taken from the environment as listed by set.cgi above)NOTE THAT AcceptPathInfo IS OFFPATH_INFO=/here.tst/morePATH_TRANSLATED=/Library/WebServer/Documents/public/here.tst/moreWhat I expect is a 404 - after all, AcceptPathInfo is OFF.If this is not a bug, how do I ensure that PATH_TRANSLATED always points to a valid file, especially when using AddHandler	2.0	id=49798	5	False	False	rainer.jung	1
id=48364	REOPENED	None	Apache httpd-2	mod_cache (	2.2.13	Sun Solaris	P2 normal	Apache HTTPD Bugs Mailing List	2009-12-10 03:02 UTC by	HWS	2015-05-20 13:32 UTC (	5 users	Whereas Apache 2.2.x mod_cache (with mod_disk_cache) works with both static files and CGI scripts (including such with #!/path/to/php-cgi), I have never been able to cache output of PHP (5.2.x) scripts handled by the handler+action method. Of course, I added pertinent header entries (Last-Modified, Expires, Cache-Control) by the PHP function header(), which were correct as shown by network sniffing and caused the browser cache to behave as expected. But no cache files appeared in the CacheRoot directory. Also directives CacheIgnoreCacheControl On, CacheIgnoreNoLastMod On, CacheStoreNoStore On have been tried, with no effect.I did not use mod_php but one CGI and two FastCGI methods (alternatively) in the following ways (schematic):# Common <FilesMatch "\.php$"> SetHandler dophp </FilesMatch> Action dophp /phpact CacheRoot /somedir CacheEnable disk /# For CGI: ScriptAlias /phpact /path/to/php-cgi# For FastCGI with mod_fastcgi: ScriptAlias /phpact /path/to/php-fcgi FastCgiIpcDir /otherdir FastCgiServer /path/to/php-fcgi further_arguments# For FastCGI with mod_proxy_fcgi (php-fcgi started separately with -b 54321): ProxyRequests Off ProxyPassMatch ^/phpact/(.+)$ fcgi://localhost:54321/$1Note that PHP as such works perfectly in all three configurations (with cgi.fix_pathinfo=1 and cgi.force_redirect=1).I searched the bug database and the documentation but could not find any hint.	Please set the loglevl to debug, request one of your php pages and provide the log file.For CGI as well as for FastCGI with mod_fastcgi (have not tried mod_proxy_fcgi now), the only debug log entries are (both with identical time stamps):[debug] mod_cache.c(131): Adding CACHE_SAVE filter for /test/tt.php[debug] mod_cache.c(138): Adding CACHE_REMOVE_URL filter for /test/tt.phpThe content of tt.php was (the Cache-Control line does not change anything):<?phpheader('Last-Modified: '.gmdate('D, d M Y H:i:s \G\M\T',filemtime('/docroot/test/tt.php')-10));header('Expires: '.gmdate('D, d M Y H:i:s \G\M\T',time()+60));#header('Cache-Control: public,max-age=60,must-revalidate');ob_start();?><html><head><title>Environment</title></head><body><h2>Environment Variables:</h2><?phpecho("TZ=".$_ENV["TZ"]."<br>");echo("LC_CTYPE=".$_ENV["LC_CTYPE"]."<br>");<!-- some more echo lines -->?></body></html><?php header('Content-Length: '.ob_get_length()); ob_flush(); ?>CONJECTURE: "Action dophp /phpact" causes /test/tt.php to be internally rewritten to /phpact/test/tt.php (otherwise the functioning of "ProxyPassMatch ^/phpact/(.+)$ fcgi://localhost:54321/$1" would be incomprehensible). In fact, mod_action.c contains a call to ap_internal_redirect_handler which seems to do this. Then the cache might not recognize that the action-script output is related to the original URL. (I remember there are several caching issues in the bug database concerning mod_rewrite, which have been fixed. May be mod_action has to be treated in a similar way.)Please provide ALL log entries not only the debug ones.There are practically none. Graceful restart after setting LogLevel debug, with mod_fastcgi:[Thu Dec 10 13:54:33 2009] [notice] suEXEC mechanism enabled (wrapper: /opt/dpiwww/apache2/sbin/suexec)[Thu Dec 10 13:54:34 2009] [notice] Apache/2.2.13 (Unix) mod_fastcgi/2.4.6 configured -- resuming normal operations[Thu Dec 10 13:54:34 2009] [notice] FastCGI: process manager initialized (pid 23046)[Thu Dec 10 13:54:34 2009] [warn] FastCGI: server "/opt/dpiwww/php5/bin/php-fcgi" started (pid 23052)[Thu Dec 10 13:55:43 2009] [debug] mod_cache.c(131): Adding CACHE_SAVE filter for /test/tt.php[Thu Dec 10 13:55:43 2009] [debug] mod_cache.c(138): Adding CACHE_REMOVE_URL filter for /test/tt.phpThe last two lines repeat for each access of /test/tt.php, nothing else is logged.I'm experiencing the same problem. Here's the relevant logfile entries. (LogLevel debug.)First CGI load:[Sun Feb 21 12:10:26 2010] [debug] mod_cache.c(131): Adding CACHE_SAVE filter for /scratch/cachetest.cgi[Sun Feb 21 12:10:26 2010] [debug] mod_cache.c(138): Adding CACHE_REMOVE_URL filter for /scratch/cachetest.cgi[Sun Feb 21 12:10:26 2010] [debug] mod_deflate.c(619): [client 10.0.2.2] Zlib: Compressed 15 to 17 : URL /scratch/cachetest.cgi[Sun Feb 21 12:10:26 2010] [debug] mod_cache.c(633): cache: Caching url: /scratch/cachetest.cgi[Sun Feb 21 12:10:26 2010] [debug] mod_cache.c(639): cache: Removing CACHE_REMOVE_URL filter.[Sun Feb 21 12:10:26 2010] [info] mem_cache: Cached url:?Second CGI load (looks okay to me, and the CGI file itself is not hit):[Sun Feb 21 12:10:34 2010] [debug] mod_cache.c(282): cache: running CACHE_OUT filter[Sun Feb 21 12:10:34 2010] [debug] mod_cache.c(296): cache: serving /scratch/cachetest.cgiFirst PHP load:[Sun Feb 21 12:10:46 2010] [debug] mod_cache.c(131): Adding CACHE_SAVE filter for /scratch/cachetest.php[Sun Feb 21 12:10:46 2010] [debug] mod_cache.c(138): Adding CACHE_REMOVE_URL filter for /scratch/cachetest.phpSecond PHP load (identical to first):[Sun Feb 21 12:10:56 2010] [debug] mod_cache.c(131): Adding CACHE_SAVE filter for /scratch/cachetest.php[Sun Feb 21 12:10:56 2010] [debug] mod_cache.c(138): Adding CACHE_REMOVE_URL filter for /scratch/cachetest.phpContents of cachetest.cgi:#!/usr/bin/env bashecho "Cache-Control: max-age=3600, public"echo "Content-type: text/plain"echoecho "CGI cache test"Full headers returned:HTTP/1.1 200 OKDate: Sun, 21 Feb 2010 12:11:29 GMTServer: Apache/2.2.8 (Ubuntu) mod_fastcgi/2.4.6Cache-Control: max-age=3600, publicVary: Accept-EncodingContent-Length: 15Content-Type: text/plain; charset=utf-8Contents of cachetest.php:<?phpheader("Cache-Control: max-age=3600, public");header("Content-Type: text/plain");echo "PHP cache test";Full headers returned:HTTP/1.1 200 OKDate: Sun, 21 Feb 2010 12:11:18 GMTServer: Apache/2.2.8 (Ubuntu) mod_fastcgi/2.4.6X-Powered-By: PHP/5.3.1Cache-Control: max-age=3600, publicVary: Accept-EncodingTransfer-Encoding: chunkedContent-Type: text/plain;charset=utf-8X-Pad: avoid browser bugIn my case the PHP file is being served via FastCGI, if that makes a difference.(In reply to)No you are not hitting the same problem since you use mod_mem_cache which has a process local cache. So you most likely hit a different httpd process in your second request. Try using mod_disk_cache and come back if the problem persists.Yes you're right, that example was the memory cache. (I was trying both.) I get exactly the same results for the disk cache though:First CGI:[Sun Feb 21 13:23:54 2010] [debug] mod_cache.c(131): Adding CACHE_SAVE filter for /scratch/cachetest.cgi[Sun Feb 21 13:23:54 2010] [debug] mod_cache.c(138): Adding CACHE_REMOVE_URL filter for /scratch/cachetest.cgi[Sun Feb 21 13:23:54 2010] [debug] mod_cache.c(633): cache: Caching url: /scratch/cachetest.cgi[Sun Feb 21 13:23:54 2010] [debug] mod_cache.c(639): cache: Removing CACHE_REMOVE_URL filter.[Sun Feb 21 13:23:54 2010] [debug] mod_disk_cache.c(962): disk_cache: Stored headers for URL?[Sun Feb 21 13:23:54 2010] [debug] mod_disk_cache.c(1051): disk_cache: Body for URL? cached.Second CGI:[Sun Feb 21 13:24:04 2010] [debug] mod_disk_cache.c(476): disk_cache: Recalled cached URL info header?[Sun Feb 21 13:24:04 2010] [debug] mod_disk_cache.c(749): disk_cache: Recalled headers for URL?[Sun Feb 21 13:24:04 2010] [debug] mod_cache.c(282): cache: running CACHE_OUT filter[Sun Feb 21 13:24:04 2010] [debug] mod_cache.c(296): cache: serving /scratch/cachetest.cgiFirst PHP:[Sun Feb 21 13:24:31 2010] [debug] mod_cache.c(131): Adding CACHE_SAVE filter for /scratch/cachetest.php[Sun Feb 21 13:24:31 2010] [debug] mod_cache.c(138): Adding CACHE_REMOVE_URL filter for /scratch/cachetest.phpSecond PHP:[Sun Feb 21 13:24:40 2010] [debug] mod_cache.c(131): Adding CACHE_SAVE filter for /scratch/cachetest.php[Sun Feb 21 13:24:40 2010] [debug] mod_cache.c(138): Adding CACHE_REMOVE_URL filter for /scratch/cachetest.phpIn the case of the CGI request, files are created in /var/cache/apache2/mod_disk_cache, as I'd expect. In the case of the PHP request, they're not.(In reply to)Please provide more debug output in the php case. I vaguely remember a PHP bug that mod_php sets r->no_cache and thus causing all content to be uncachable.Thanks for looking into this. How do I get more debug output? I'm already at LogLevel debug. There's a bug report on bugs.php.net that initially seemed somewhat related[1] but I think that one involves mod_php whereas I and the original reporter are both using mod_fastcgi. Is there some way to turn on mod_fastcgi logging?[1](In reply to)There should be more debug lines from the cache in the log. Please copy and paste them here.I guess this is the one I meant. Does the problem only happen if you execute the PHP script via mod_fastcgi and not if you use mod_cgi(d)?Let me remind that the problem seems not to be due to PHP as such (as initially reported, caching works for simple PHP CGI scripts) but occurs only with the Action mechanism, regardless whether CGI, FastCGI, or even mod_proxy_fcgi are used. Thus I still think (see) that the problem is actually with mod_action and the comments for this bug should also be mailed to the people in charge for mod_action.I suspect the key reason that we're not seeing any debug output is potentially because the php stack is removing the existing filters from the stack, so the cache never gets an opportunity to run.Closing this as worksforme for now, as we'd need clarification that php isn't doing this before we could debug this further. Please reopen if it turns out mod_cache needs to be looked at again.What does this mean? The php processes are independent of the httpd process, since no Apache PHP module was used but external CGI or FastCGI php processes. I do not see how these could influence the Apache filter stack.Please retry with 2.2.17 as there are some fixes regarding the filter stack and internal redirects.There are no improvements with 2.2.17. Moreover, I now proved that the bug is not PHP related at all but due to the redirection by an Action+Handler. For this purpose, I made two simple CGI shell scripts tt.cgi (executable) and tt.msh of equal contents:#!/bin/shLC_TIME=C date -u|awk '{print "Last-Modified: " $1 ", " $3 " " $2 " " $6 " " $4 " " $5}'echo 'Cache-Control: public,max-age=60'echo 'Content-type: text/html; charset=iso-8859-1'echoecho "<p>DOCUMENT_ROOT=$DOCUMENT_ROOT<br>PATH_INFO=$PATH_INFO<br>PATH_TRANSLATED=$PATH_TRANSLATED<br>SCRIPT_NAME=$SCRIPT_NAME<br>SCRIPT_FILENAME=$SCRIPT_FILENAME<br>REQUEST_URI=$REQUEST_URI<br>REDIRECT_URL=$REDIRECT_URL</p>"# End of scripttt.cgi was run directly, whereas tt.msh was assigned a Handler and Action with another CGI shell script mysh taking the role of php-cgi: <FilesMatch "\.msh$"> SetHandler mycgi </FilesMatch> Action mycgi /XyCgi ScriptAlias /XyCgi "/path/to/cgi-bin/mysh"The script mysh contained:#!/bin/shif [ -f "$PATH_TRANSLATED" ]then . "$PATH_TRANSLATED"elseecho 'Content-type: text/html<h2>Not found</h2>'fi# End of scriptThe directly executed tt.cgi correctly entered and used cache entries:[Mon Oct 25 12:17:13 2010] [debug] mod_disk_cache.c(977): disk_cache: Stored headers for URL?[Mon Oct 25 12:17:13 2010] [debug] mod_disk_cache.c(1079): disk_cache: Body for URL? cached.[Mon Oct 25 12:20:06 2010] [debug] mod_disk_cache.c(485): disk_cache: Recalled cached URL info header?[Mon Oct 25 12:20:06 2010] [debug] mod_disk_cache.c(758): disk_cache: Recalled headers for URL?[Mon Oct 25 12:20:06 2010] [debug] cache_util.c(591): Cache lock obtained for stale cached URL, revalidating entry: /test/tt.cgi[Mon Oct 25 12:20:06 2010] [debug] cache_storage.c(272): Cached response for /test/tt.cgi isn't fresh. Adding/replacing conditional request headers.[Mon Oct 25 12:20:06 2010] [debug] mod_cache.c(141): Adding CACHE_SAVE filter for /test/tt.cgi[Mon Oct 25 12:20:06 2010] [debug] mod_cache.c(148): Adding CACHE_REMOVE_URL filter for /test/tt.cgi[Mon Oct 25 12:20:06 2010] [debug] mod_cache.c(705): cache: Caching url: /test/tt.cgi[Mon Oct 25 12:20:06 2010] [debug] mod_cache.c(711): cache: Removing CACHE_REMOVE_URL filter.The script tt.msh called through the Action did not cache; the only log entries were (as described infor PHP):[Mon Oct 25 12:20:33 2010] [debug] mod_cache.c(141): Adding CACHE_SAVE filter for /test/tt.msh[Mon Oct 25 12:20:33 2010] [debug] mod_cache.c(148): Adding CACHE_REMOVE_URL filter for /test/tt.mshThus this issue has nothing to do with PHP, but with Handler+Action, as already conjectured in my. The Issue should be renamed or reopened under a different name.Apologies for resurrecting an oldish bug, but I've hit this in Apache 2.2.27 (and from looking at the relevant bits of code in 2.2.29 I can't see anything that's changed that would fix it).From a bit of investigation it looks like what happens is:1. mod_cache, in its quick handler, adds an output filter CACHE_SAVE to the request.2. action_handler() in mod_actions calls ap_internal_redirect_handler() using the path that was set up in the "Action" directive.3. ap_internal_redirect_handler() in turn calls internal_internal_redirect() to create a new request_rec, which strips off *all* the resource-specific output filters - including CACHE_SAVE.4. ap_internal_redirect_handler() then *doesn't* call ap_run_quick_handler() for the new request_rec, so mod_cache doesn't get a look at the new request at all. N.b. this is different to the behaviour of ap_internal_redirect(), but perhaps deliberately so?So it might be possible to resolve this by allowing ap_internal_redirect_handler() to run the quick handler against the new request_rec (but that might have unintended consequences for quick handlers in other modules than mod_cache?) Or it could perhaps be resolved by backporting CacheQuickHandler to 2.2.x so that mod_cache doesn't have to run "quick".Or I may have misunderstood what's going on entirely(!)	17.0	id=48880	4	False	False	jamesd	1
id=50945	REOPENED	None	Apache httpd-2	mod_proxy_ajp (	2.2-HEAD	PC All	P2 normal	Apache HTTPD Bugs Mailing List	2011-03-18 12:40 UTC by	Peter Pramberger	2012-04-20 18:18 UTC (	4 users	As mod_proxy_ajp currently lacks the ErrorOverride feature as in mod_proxy_http (which is exactly what I'd need now), I took the opportunity to "port" it back from mod_proxy_http.While it seems to work so far without breaking existing functionality (tested by accessing existing and non-existing URLs with ErrorOverride enabled and disabled) it would be great if someone can review it in case I've missed something...Note that it will apply cleanly to 2.3.18 too, however I have no possibilityto test it yet.	CreatedPatchCreatedPatch (review-friendly with WS changes stripped)Will be in 2.3.12***has been marked as a duplicate of this bug. ***Is there a way to build this specific module from source? I'm on Apache 2.2.21 but would like to be able to use the ProxyErrorOverride with connections proxied over AJP.Thanks!CreatedAdopted orignal patch for trunkproposed inI don't suppose Bugzilla has one of them fancy backport items...CreatedUpdated Patch for 2.2.22	9.0	id=49859	6	False	False	covener	1
id=50999	REOPENED	None	Apache httpd-2	All (	2.5-HEAD	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2011-03-30 15:26 UTC by	petrus	2015-02-17 16:16 UTC (	1 user	eems the configure wrong detect my apr-util version,i've already installing apr-util 1.3.10,but the configure files said i need apr-util version 1.3.0 or laterchecking for chosen layout... Apachechecking for working mkdir -p... yeschecking build system type... i686-pc-linux-gnuchecking host system type... i686-pc-linux-gnuchecking target system type... i686-pc-linux-gnuConfiguring Apache Portable Runtime library ...checking for APR... yes setting CC to "gcc" setting CPP to "gcc -E" setting CFLAGS to " -g -O2 -pthread" setting CPPFLAGS to " -DLINUX=2 -D_REENTRANT -D_GNU_SOURCE -D_LARGEFILE64_SOURCE" setting LDFLAGS to " "Configuring Apache Portable Runtime Utility library...checking for APR-util... yeschecking for gcc... gccchecking whether the C compiler works... yeschecking for C compiler default output file name... a.outchecking for suffix of executables... checking whether we are cross compiling... nochecking for suffix of object files... ochecking whether we are using the GNU C compiler... yeschecking whether gcc accepts -g... yeschecking for gcc option to accept ISO C89... none neededchecking how to run the C preprocessor... gcc -Echecking for gcc option to accept ISO C99... -std=gnu99checking for pcre-config... /usr/local/bin/pcre-configconfigure: Using external PCRE library from /usr/local/bin/pcre-config setting PCRE_INCLUDES to "-I/usr/local/include" setting PCRE_LIBS to "-L/usr/local/lib -lpcre" setting HTTPD_LDFLAGS to "$(PCRE_LIBS)"Configuring Apache httpd ... setting INCLUDES to "-I." adding "-I$(top_srcdir)/os/$(OS_DIR)" to INCLUDES adding "-I$(top_srcdir)/include" to INCLUDES adding "-I/usr/local/apr-httpd//include/apr-1" to INCLUDES adding "-I/usr/local/apr-util-httpd//include/apr-1" to INCLUDES adding "-I/usr/local/include" to INCLUDESApplying OS-specific hints for httpd ... forcing SINGLE_LISTEN_UNSERIALIZED_ACCEPT to "1" forcing AP_NONBLOCK_WHEN_MULTI_LISTEN to "1"checking for rm... /bin/rmchecking for pkg-config... /usr/bin/pkg-configchecking for rsync... /usr/bin/rsyncchecking for gawk... gawkchecking whether ln -s works... yeschecking for ranlib... ranlibchecking for lynx... lynxchecking for grep that handles long lines and -e... /bin/grepchecking for egrep... /bin/grep -Echecking for ANSI C header files... yeschecking for sys/types.h... yeschecking for sys/stat.h... yeschecking for stdlib.h... yeschecking for string.h... yeschecking for memory.h... yeschecking for strings.h... yeschecking for inttypes.h... yeschecking for stdint.h... yeschecking for unistd.h... yeschecking minix/config.h usability... nochecking minix/config.h presence... nochecking for minix/config.h... nochecking whether it is safe to define __EXTENSIONS__... yeschecking for library containing strerror... none requiredchecking for APR version 1.3.0 or later... yeschecking for APR-util version 1.3.0 or later... noconfigure: error: APR-util version 1.3.0 or later is required	Insufficient information for a bug report: you haven't told us your configure line or your APR installation path. If it's not detected by default then you have to use --with-apr-util.Please use the user support list for help with build and configuration issues.(In reply to)i'm sorry i forget to include this :"./configure --prefix=/usr/local/apache --with-apr=/usr/local/apr-httpd/ --with-apr-util=/usr/local/apr-util-httpd/"I suggest to the apache team that BEFORE making something public you have to test it. The installation process is full of bugs and this one : configure: error: APR version 1.3.0 or later is requiredIs still not resolved.Still no details. Apache builds on Linux for most people.I see the same issue here on my build. apr-1.5.1 and apr-utils-1.5.4 are installed. But nevertheless the configure script can't find it and abort with checking for APR version 1.3.0 or later... noconfigure: error: APR version 1.3.0 or later is required My assumption is that something is not included the right way or some path is wrongthe dirty workaround for me was the following:cd /usr/includesudo ln -sf apr-1.0/ apr-1after that it found the right apr version	6.0	id=48312	4	False	False	J.Gao	1
id=47392	REOPENED	None	Apache httpd-2	Core (	2.2.11	PC Windows XP	P2 normal	Apache HTTPD Bugs Mailing List	2009-06-18 23:54 UTC by	None	2009-06-26 09:26 UTC (	0 users	The <FilesMatch> directive is using the superior directory if the access through the URL is without a / at the end. Here is an example for better understanding:I created in the htdocs directory a new directory and named it test. In this directory have i created a .htaccess with the content:Allow from all<FilesMatch ^test>Deny from all</FilesMatch>And there are two empty files named test.php and another_file.php. Now the following happens:-> Error 403 forbidden-> Access is allowed and i can see the index-> Error 403 forbidden-> Access is allowedThe problem is, that ^test matches of the superior directory if there is no / at the end of the URL and the access is denied. If i delete the .htaccess the following happens:-> Switches toand the i can see the index-> Access is allowed and i can see the index-> Access is allowed-> Access is allowed	Good detailed problem description, but in the wrong place. Please ask configuration questions on the user's list, don't open bug reports.I have already read the following documentation:There is nothing what explains this behavior. Just try to make a solution with my example. If this is really a bug, nobody can resolve this without a bugfix.If you don't think the server is behaving the way you expect it to, you should always ask about it on the users list. Maybe someone will say "yes, that's a bug, please open a bug report" but it's much more likely that someone will explain to you how to configure the server to behave the way you want.re-opening for some more eyes.Requesting a directory e.g. "/test" without a trailing slash, ap_file_walk applies Files/Filesmatch containers to the "basename" (/test->test) that live in /test/.htaccessIs this as desired or should it be short-circuited (skipped, treated as an empty iflename, ???) if we have the above case? Sounds like a candidate for trunk-only change.FWIW, I took a look this afternoon. This report needs to move to users@ so we can diagnose the OP's real problem, but playing with a .htaccess showed up a more serious bug. I need to poke about a bit more before reporting/fixing it.I guess none of the devs tend to use .htaccess ...(In reply to)I take that back: my issue was a <Location /> overriding the .htaccess (I blame the heat). I can confirm the behaviour reported by the OP.Still thinking about it. The least we should do is document more clearly some of the counter-intuitive things that happen on a merge.	6.0	id=50945	10	False	False	peter	1
id=51020	REOPENED	None	Apache httpd-2	mod_fcgid (	2.2.17	PC All	P2 normal	Apache HTTPD Bugs Mailing List	2011-04-04 22:19 UTC by	Thangaraj AntonyCrouse	2014-02-17 13:52 UTC (	2 users	CreatedPatch for Apache/mod_Fcgid.so startup issue in complex Active Directory Domain environmentProblem:mod_fcgid.so tries to gather CGI process filestat during startup using apr_stat () call and fails to fetch all ACLs from Active Directory Domain environment (having complex groups spanning across multiple domains)Root Cause:apr_stat() using APR_FINFO_NORM ends up calling GetEffectiveRightsFromACL Win32API to proble ACLs for cgi process file object's owner and group trustee accounts, and per MS this GetEffectiveRightsFromACL API is likely to fail in complex AD environment. MS KB:Solution:mod_fcgid.so apr_stat() call is made to use APR_FINFO_IDENT and thereby avoiding the unnecessary ACL lookup during Apache startup, however if there is a real ACL issue, mod_fcgid.so will return error during runtime.	I've experience the same issue with mod_fcgid failing on startup. I'm also in an extremely large Active Directory implementation which explains why I'm suffering.Have tried the patch and it works perfectly.Please attention to this issue, quite some reports on this.A workaround is available, but it's not suitable for all implementations.Change permissions on the fastcgi wrapper (e.g. php-cgi.exe) to remove any Active Directory user or group. Also remove any local groups that contain Active Directory users or groups. I was left with just the local administrator account and system. The Apache service will then start correctly as it doesn't bother accessing Active Directory to check the ACL.I've loggedwith Apache APR about the underlying issue caused by using GetEffectiveRightsFromACL. It'd be ideal if they could provide a fix at that level as it'd fix other modules that suffer from the same problem. Like mod_xsendfileDavid, that's a very interesting workaround, and thanks for filing the APR bug.Thangaraj, looked at your proposed solution and went with that philosophy,vetting all of the cases where we were requesting far more apr_file_stat fieldsthan were used. Fixed for mod_fcgid 2.3.7. Thank you for proposing that fix.I'm currently experiencing the issue on some Windows 2008with AD and as I understand the error that's given me by the Apache Configuration check, it definitely relates to this issue.Therefore I'd say, the bug is still open.Does anybody still experience this issue with Apache 2.2.22 and mod_fcgid 2.3.7.Createdrebuild_for_mod_fcgid-2.3.7-crlfJust replace your mod_fcgid.so Build, what's updated:line :866 comented "/* return missing_file_msg(cmd->pool, "Wrapper", path, rv); */"AD+Apache + mod_fcgid + PHP = (70008)Partial resultsused links :How does commenting out line 866 fix the issue? Is it just prohibiting the AD lookups all together?	8.0	id=50948	4	False	False	jorton	1
id=51103	REOPENED	None	Apache httpd-2	Core (	2.5-HEAD	Sun Solaris	P2 normal	Apache HTTPD Bugs Mailing List	2011-04-21 15:01 UTC by	None	2014-02-17 13:54 UTC (	1 user	Overview--------The mod_reqtimeout module is not dropping connections and returning 408 when dealing with "slow http header" or "slow http body" requests. Instead, it is either truncating the request and handling it, or dropping the request with a 400 status code.Steps to reproduce (A)----------------------1. Launch a slow-post attack using the OWASP HTTP DoS tool ()http_dos_cli --host 1.2.3.4 --port 80 --path /server-status --slow-post --post-field j_username --connections 1000 --rate 1000 --timeout 52. Sniff network traffic using Wireshark, observe requests being truncated and handled, resulting in a 200 return code.Steps to reproduce (B)----------------------1. Launch a slow-headers attack2. Sniff network trafic using Wireshark, observe requests being dropped with a 400 code being returned.Expected behavior-----------------Request is dropped and a 408 status code is returned.Actual behavior---------------Request is dropped and a 400 status code is returned OR request is truncated and handled normally.Platform--------Software: Apache 2.2.17 (MPM-worker)OS: Solaris 5.10 32-bitHardware: Sun SPARCAdditional information----------------------mod_reqtimeout configurationRequestReadTimeout header=10-20,MinRate=500 body=10-20,MinRate=500ModSecurity 2.5.13 is also configured.Apache debug logs show that incoming requests time out:[Tue Apr 19 08:55:09 2011] [info] [client 5.6.7.8] Request header readtimeout[Tue Apr 19 08:55:09 2011] [error] [client 5.6.7.8] request failed: errorreading the headersOR[Tue Apr 19 09:01:20 2011] [info] [client 5.6.7.8] Request body read timeout---Thanks!	(In reply to)I couldn't reproduce this (but I don't have windows to actually try the tool). Can you provide the wireshark dump (maybe filtered to only contain one request)? Do you have mod_status listening for /server-status?This happens in various situations and is fixed in trunk. The fixes should probably be backported to 2.2.x. The relevant commits areCreatedSlow post Wireshark dumpHi Stefan and thanks for taking the time to look at this.(In reply to)I have attached a Wireshark dump to the bug report. Let me know if this is what you expected, I'm actually new to Wireshark.We have mod_status listening on /server-status and it's responding correctly when invoking with a browser.Apache is now returning a 400 code, similar to the slow-headers attack. We did tweak a few settings (disabled ModSecurity, turned off ExtendedStatus) so it might have had that effect. I will investigate further.This is good to know. Do you have any idea when these changes will be backported or when 2.3 will be released?(In reply to)Thanks, that was what I wanted.But as you say, it does not show a 200 result code. I will mark this as invalid because for the other issues, there have already been different PRs. If you can reproduce the code 200 sent despite of a timeout issue, please change the status from INVALID to REOPENED.I have now proposed them for the next 2.2.x release.2.4 is hopefully due in 1-2 months.CreatedSlow post Wireshark dump with 302 code(In reply to)Ok, I have reproduced the problem, although it's slightly different from what I initially wrote.You will see in the new attachment that the request times out but returns a 302 Redirect instead of a 400 Bad Request. A valid request should indeed return a 302 because of a RedirectMatch rule in the httpd-vhosts.conf file, but the request times out and a 302 is returned anyway. Also, you will see that the client keeps sending data after the connection is closed - although this may be an issue with the http_dos_cli tool.(In reply to)And because of this, the http_dos_cli tool can continue sending data slowly and hogging worker threads. The ModSecurity rule that checks for 408 bursts from a single IP cannot know that this is coming from a slow http DoS attack, and so cannot drop further connections from that IP.Thanks for the additional dump. This is definitely a bug in httpd, but I am not sure yet how to fix it.The problem is that ap_finalize_request_protocol() ignores errors returned by ap_discard_request_body(). But simply calling ap_die() inside ap_finalize_request_protocol() causes various test failures.Within trunk, the connection will be closed correctly after a timeout during a request body that is discarded.It still means that the request is logged as successful an not as 408, though. One could argue that this is correct because the request was completed successfully. But I see that other modules may be interested if there was a timeout. I will have to think some more about that.has been backported to 2.2.20 as.	10.0	id=50999	9	False	False	nick	1
id=51104	REOPENED	None	Apache httpd-2	support (	2.4.16	All Linux	P2 major	Apache HTTPD Bugs Mailing List	2011-04-21 18:25 UTC by	Torden Cho	2015-10-14 07:39 UTC (	1 user	Hi All,This is Torden Cho, I doing development little module base on C++.Current I doing 1.3 api migration 2.x api however I have a problem in http_config.hthat's line is below159: typedef const char *(*cmd_func) ();and below is g++ error message.error: invalid conversion from ‘const char* (*)(cmd_parms*, void*, const char*)’ to ‘const char* (*)()’Anyway, I have been self path to http_config.h so therefore I be fine.I thinks so we providing solution to who c++ module developer.--- /usr/local/httpd/include/http_config.h 2011-04-22 05:18:19.000000000 +0900+++ /usr/local/httpd/include/http_config.h.new 2011-04-22 05:17:59.000000000 +0900@@ -155,7 +155,11 @@ #else /* AP_HAVE_DESIGNATED_INITIALIZER */ -typedef const char *(*cmd_func) (cmd_parms *, void *, const char *);+#ifdef __cplusplus+typedef const char *(*cmd_func) (cmd_parms *cmd, void *dummy, const char *path);+#else+typedef const char *(*cmd_func) ();+#endif # define AP_NO_ARGS func # define AP_RAW_ARGS funcAny Idea? Any Suggest?Thanks Buddy.--Torden.	The types of the union members differ. Therefore you cannot find a definition for cmd_func that works for all union members. For example, your patch will only work with AP_RAW_ARGS and AP_TAKE1 but not with AP_TAKE2, ...I think the only valid solution in current C++ is to use a cast.C++11 supports designated initializer for union.So it must be fixed.Following works fine.$ diff /usr/include/apache2/ap_config.h /usr/include/apache2/ap_config.h.old175c175< #if (defined(__GNUC__) && !defined(__cplusplus)) || (defined(__cplusplus) && 201103L <= __cplusplus) || defined(AP_HAVE_C99)---	2.0	id=51103	7	False	False	guillaume.bilodeau	1
id=50948	REOPENED	None	Apache httpd-2	mod_cgi (	2.2.17	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2011-03-19 10:16 UTC by	Zach Lutho	2012-08-12 09:15 UTC (	0 users	When a CGI program dies while it is waiting for the client to send its POST data, Apache does not reap the child process and close the connection but instead waits for whatever timeout is configured globally before closing the connection. This seems senseless as there's nothing to be done once the child has exited.Consider this simple Perl script:<snip>#!/usr/bin/perl$SIG{ALRM} = sub { die };alarm 5;sysread STDIN, $data, 4096;END { print "Content-Type: text/plain\n\n"; print $data;}</snip>and this client request sending all the headers but no body and keeping the connection open:<snip>POST /test.cgi HTTP/1.1Host: localhostContent-Length: 10</snip>After the five seconds timeout the child will hang around as a zombie but the connection is still open.	Ping?mod_cgi does not discover the child has died until it reads or writes from the child's stdout or stdin respectively. If the client is sending a body, mod_cgi will not write to the stdin until it reads a block from the client. So, this is expected behaviour, sorry. (It is a design constraint with httpd filters that it is not possible to wait for I/O across {client connection, pipes to children}.)How about allowing apr_bucket_read() to return when interrupted by a signal, i.e. implementing something in between APR_BLOCK_READ and APR_NONBLOCK_READ?	3.0	id=51020	9	False	False	info	1
id=51258	REOPENED	None	Apache httpd-2	mod_substitute (	2.2.17	All All	P2 enhancement	Apache HTTPD Bugs Mailing List	2011-05-24 13:21 UTC by	horowity@checkpoint.com	2015-10-14 07:21 UTC (	3 users	features included:1. Support for SubstituteCond - ability to apply substitute rule only if the configured conditions are met. The conditions input could be compared to request data, environment variables and internal Apache notes. -- This section is similiar to RewriteCond, and I think it should go to a common section in the code.2. Support for SubstituteCheck – ability to perform quick search on the data and use the results in order to avoid using “expensive” regular expression.3. Supporting evaluation (at run time) of the searched and replaced patterns before application (instead of static evaluation in configuration time). Evaluation result could be taken from request/response headers, environment variables and internal Apache notes.4. Adding some more options: a. Skip the replaced text from being processed by following rules. -- For this, I used a binary tree, which I didn't find in APR, can you consider adding such implementation to APR? b. Stop applying other rules after specific rule is matched. c. Option not to break the response into lines (and apply the substitute rules on the whole response). I know these changes need documentation, which I can provide, just I want to know if they will accepted into the trunk, before writing it.I hope you will find these changes helpful, and I'll be glad to hear your opinion about them.	CreatedPatch + new .h file as gzipped tarball***has been marked as a duplicate of this bug. ***Thank you for your interest, and contribution.Note that it is very unlikely to be adopted at this time, as httpd 2.3-beta(soon to be 2.4) deprecated mod_substitute, and introduced mod_sed.I'd strongly encourage you to review mod_sed and determine if any of theimprovements you propose to mod_substitute remain applicable to mod_sed,and absolutely offer them as a patch to httpd 2.3.All this said, httpd is a collaborative environment, and I will leave yourticket open for others to evaluate. It's entirely possible you would findenough httpd committers to entertain the idea of restoring an enhancedmod_substitute, if mod_sed is not so easily tailored.HiI think that mod_substitute with my new features is doing very good job (depends on Admin configuration).I don't see any reason why to dreprecate the existing mod_substitute (taking into account that new module usually brings new bugs).Since mod_sed is not exist in "released" version, it is hard for me to use it in my environement (and I'm quite sure there are many developers with this problem).Why not give the user option to choose which module she wants to use?Regards,YehezkelI've tried the patch specifically to substitute in environment variables (%{env:var_name}) but it seems to only allow access to local variables created by the use of 'define' (${var_name}).Please could you confirm how your code is to be used in this manner?Thanks Yehezkel,For anyone else who has this problem - it can be fixed by appending 'e' to the substitution command.How to use the patch file? Can't seem to use it with patch tool for Windows.Thanks!(In reply to ambqqpdp from)It was created with Unix diff command.Use Unix patch command to apply it.Hmm, it seems that the attachement is a tar.gz at first.The patch itself may be contained.(In reply to Yann Ylavic from)I tried saving it as tar.gz but uncompressing it returns an error. I will try horowity's suggestion. Thanks!(In reply tofrom)patch unexpectedly ends in middle of linepatch: **** Only garbage was found in the patch input.Comment onPatch + new .h file as gzipped tarballThe attachment is in tar.gz format. I have changed the meta data so that browsers won't try to display the raw binary file.(In reply to Rainer Jung from)Thank you very muchI'm having problems upon compiling the patched source when targeting Apache 2.2, but for 2.4 it's perfectly working. The error seems to be coming from some functions not defined in 2.2 such as ap_regexec_len, etc.(In reply to ambqqpdp from)The ap_regexec_len was submitted in #51231, and is very easy to adopt.So if you need these features in 2.2.x - you can just apply both of patches.CreatedUpdate of original patch.In how, the original patch is not applying to 2.2.x any more due to a code change, so I attach an update patch.Hiseveral years were passed since my original commit and I got several private mails with questions about the patch, so you might want to reconsider adopting the patch to 2.4/trunk (even if mod_sed is considered as replacement of mod_substitute).Anyhow, the original patch is not applying to 2.2.x any more due to a code change, so I attach an update patch (#33182).Regards,Yehezkel Horowitz	17.0	id=51104	6	False	False	ioemen	1
id=51982	REOPENED	None	Apache httpd-2	mod_proxy (	2.2.21	All Linux	P2 normal	Apache HTTPD Bugs Mailing List	2011-10-06 19:56 UTC by	Luke Meyer	2014-03-26 12:49 UTC (	2 users	This report appears similar to some others, like 39203 and particularly 51489. But I believe the following configuration demonstrates a specific problem simply and is in fact a bug - feel free to prove me wrong:<Proxy balancer://cluster >BalancerMember</Proxy>ProxyPass / balancer://cluster/# option 1ProxyPassReverse /# option 2ProxyPassReverse / balancer://cluster/Option 1 and 2 should both correctly rewrite Location: headers from the backend. Option 1 does, but option 2 adds an extra / - so if the request wasand the backend returns a Location:then the rewritten result will be(with two slashes). Aside from looking/being incorrect, any cookies with path /foo (as is common with backend applications) won't match this path and thus the browser won't send them after the redirect.Some things to note:1. The behavior is no different with:ProxyPassReverse / balancer://cluster(i.e. without the trailing /)2. This works fine if you do the following instead:ProxyPass /foo balancer://cluster/fooProxyPassReverse /foo balancer://cluster... however, the goal here is to proxy / (perhaps with exceptions) so there is no way to eliminate the trailing slashes.3. You can try other combinations of adding/removing trailing slashes to your heart's content - I have not found one that both proxies to the correct backend location (without adding/removing a slash) and rewrites Location correctly. The only workaround is to ProxyPassReverse each of the individual BalancerMembers, which is what the balancer:// construct is intended to avoid.	I ran into this problem too and didn't find a workaround yet.My config# Works perfectProxyPass /ProxyPassReverse /# Doesn't work when receiving Redirects / Location from the backend. An extra leading '/' is added.## ProxyPass / balancer://balancer/# ProxyPassReverse / balancer://balancer/<Proxy balancer://balancer/> BalancerMemberretry=1 BalancerMemberretry=1 status=D</Proxy>Any workaround is appreciated.This bug also exists in apache 2.2.12 and 2.2.22(In reply to)Patch available insolved the issue for me.Does somebody have a patch for 2.2.22 for this?The above patch seems to be for an older version and the logic seemed to have changed (I only checked .21 and .22).The workaround to this problem is to put multiple ProxyPassReverse directives, one for each balancer memberProxyPass / balancer://balancer/ProxyPassReverse /ProxyPassReverse /Does anybody know if this is fixed in a later version of Httpd? If so which versions?Createdhttpd-2.2 backport of proxy doubleslash fixes,Folks, please test the attached backport before I submit it to STATUS.William, your changes for the httpd-2.2 proxy double slash tested successfully. Thanks.Added to 2.2 STATUS for review.This is now committed for inclusion in a future 2.2.25I don't think the backport was applied successfully in 2.2.26. That is, I believe this issue remains unresolved in 2.2.26. (I ran into it on 2.2.22, and found identical behavior in 2.2.26.) I ended up migrating to 2.4(.9) where it is in fact resolved.I don't have time to recreate my 2.2.26 conf details here, but I can attest we saw this exact problem: extra slash introduced in Location header of redirect response from proxied apps, when proxying to a balancer (vs directly to a particular member -- which didn't meet our needs). Sorry for not having more rigorous details to reprod, but for anyone reading this experiencing frustration, may I heartily recommend biting the bullet and upgrading to 2.4. In the end our conf is cleaner and more concise, and mod_proxy_balancer + mod_rewrite are working in perfect harmony to route requests to one of multiple load-balanced apps behind a given apache. I'm reopening this to draw attention to it, but sorry in advance for not having time to discuss it much further.	10.0	id=51258	16	False	False	horowity	1
id=52120	REOPENED	None	Apache httpd-2	mod_cache (	2.2.22	All All	P2 normal	Apache HTTPD Bugs Mailing List	2011-11-01 15:34 UTC by	Graham Leggett	2012-08-08 08:53 UTC (	2 users	When an attempt is made to freshen a stale entry in the cache, and the 304 Not Modified response from the backend contains no ETag, no Last-Modified, and no Cache-Control header, the following code path is followed:X-Cache-Detail: "conditional cache hit: 304 was uncacheable though (s-maxage or max-age zero and no Last-Modified or Etag; not cacheable); entity removed" from ...In this case, because the 304 is technically uncacheable, we send the cached copy once and invalidate the entry. What we don't do however is send the original cached headers, just the headers from the 304, and in the process we see headers like Content-Type being lost or replaced.	Fixed on trunk in.As far as I can see, this fix was never merged into the 2.2.x branch and although the circumstances to trigger this bug are very specific, its effects can be quite nasty when it manifests.The failure to merge headers means that entity header fields (which are not included in a 304 response) are not sent to the client. In the case of an object with a Content-Encoding such as gzip, this results in a gzipped body being returned to the client without the header specifying the content encoding (hence resulting in the client receiving data it is unable to interpret).The code that that contains the error was introduced to trunk into fix PR45341, and the current PR (PR52120) was fixed in trunk in, so any release that contains thefix but not thefix will suffer from PR52120.For the case where the request we are handling was in fact an if-modified-since request, the old, pre-r1001884 code would have done the right thing, so this is a regression in 2.2.18. It might therefore be worth considering backportingto the 2.2.x branch.was backported to 2.2.x branch inand 2.2.18 is the first 2.2.x tag that contains the fix (and therefore this regression).2.4.x was branched from trunk, so already contained thefix, andwas merged into 2.4.x asprior to 2.4.1 being tagged, so no 2.4.x release suffers from this problem.roySee also	3.0	id=51982	6	False	False	sven.peters	1
id=52578	REOPENED	None	Apache httpd-2	mod_log_config (	2.2.17	PC Linux	P2 enhancement	Apache HTTPD Bugs Mailing List	2012-02-02 09:41 UTC by	Mark Jenkins	2012-02-02 12:40 UTC (	0 users	The log formats "%B" and "%b" log the full size of the file requested even if the client reads only a few kB and then closes the connection.Using "%O" from mod_logio is not an option because it counts the headers too instead of just the payload.	You're asking for something non-meaningful.At the network or connection level (%O) you can't distinguish between metadata (like the HTTP headers, and chunking information) and payload unless you re-parse the entire response - which would be a huge overhead.At the request level (%[Bb]), if a request is aborted you can know how much data has been encoded for the connection level, but not how much of that has been sent down the wire. The latter may also be affected by such things as SSL encryption.The write() and sendfile() system calls do tell you how much data has been sent down the wire. Even at this level you can distinguish between headers and payload because Apache sends them separately.	2.0	id=52120	9	False	False	minfrin	1
id=48312	REOPENED	None	Apache httpd-2	mod_proxy (	2.2.14	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2009-11-30 16:00 UTC by	Jie Gao	2009-12-03 02:23 UTC (	0 users	This was originallybut was lost in issues.a.o data loss on 2009-11-26/27 and re-created from the archives."Usage is basically similar to ProxyPassReverse, but instead of rewritingheaders that are a URL, this rewrites the path string in Set-Cookie headers."It does not seem to be the case.Example:<Location /sss/>ProxyHTMLEnable OnProxyPassProxyPassReverse /ProxyHTMLURLMap / /sss/ProxyPassReverseCookiePath / /sss/</Location>This will rewrite cookie path from backend server's /cgi-bin to /sss/ for thefrontend server.Is this intended behaviour?Jie	The OP already re-created this issue.*** This bug has been marked as a duplicate of***	1.0	id=52578	5	False	True	nick	1
id=53388	REOPENED	None	Apache httpd-2	mod_proxy_balancer (	2.2.22	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2012-06-08 12:55 UTC by	Peter	2012-11-05 09:41 UTC (	0 users	Hi,Camed across a strange thing today. A customer said his reverse proxied website was not working over https.After some troubleshooting I found this:[Fri Jun 08 08:18:40 2012] [error] proxy: BALANCER: (balancer://idm). All workers are in error state[Fri Jun 08 08:19:38 2012] [error] proxy: BALANCER: (balancer://idm). All workers are in error state<Proxy balancer://iam> Order deny,allow Allow from all BalancerMemberroute=worker0 BalancerMemberroute=worker1 </Proxy>ProxyPass /iam balancer://idm stickysession=JSESSIONIDProxyPassReverse /iam balancer://idmShould be:ProxyPass /iam balancer://iam stickysession=JSESSIONIDProxyPassReverse /iam balancer://iamSo it seems like when you write a name of a balancer member which isn't defined, it's not checked when verifying the config. It starts up with no errors. But when accessing it, it gererates 503s.Best regards,Peter	Hiuse <Proxy balancer://idm> and it should work better ;-)from my personal experience, my advice is to use the <Proxy> object as a pure substitution for {protocol://host:port}, and nothing more.<Proxy balancer://foo> # do not add a trailing '/' after IP:PORT (do not add a path, too) BalancerMemberBalancerMember</Proxy>this view allows to clarify the substitution which is made by mod_proxy_balancer, by keeping equivalent things together: - level: substitute protocol://host:port (balancer's job) * balancer://foo --->- level: substitute request path (ProxyPass' job) * balancer://foo/path1 -->I found this logic to be a safe method to avoid the famous "double /" side-effect that you get when you want to "ProxyPass /"...ericI don't understand your answer I am afraid.I still find this to be an issue. Their should be an inbuilt mechanism checking that the balancermember exits./Peter(In reply to)HiMy answer was only about the path appended to the 'scheme:/host:port/' declaration of your BalancerMembers (my experience says a BalancerMember should be seen as a "pure" host definition, and that the optional backend path should be declared in the ProxyPass/ProxyPassReverse lines)Anyway, I understand your point. Indeed, using a non-existent Proxy or using a Proxy which has zero BalancerMember has the same '503' effect, because this information is a pure run-time evaluation made by lbmethod providers (it is not their role to detect a wrong setup)This happens because the object is transparently created when the 'ProxyPass' directive is evaluated (it is probably made in order to allow a forward declaration of the balancer itself):(mod_proxy.c)((( proxy_balancer *balancer = ap_proxy_get_balancer(cmd->pool, conf, r); if (!balancer) { const char *err = ap_proxy_add_balancer(&balancer, cmd->pool, conf, r);)))Maybe a coherence check in the post_config hook could validate that there is no 'balancer://' object with zero elements ? (I can /try/ to make it...)ericWhat would be great Eric!/Peter	4.0	id=52795	5	False	False	wrowe	1
id=52616	REOPENED	None	Apache httpd-2	mod_ssl (	2.2.16	PC Linux	P2 enhancement	Apache HTTPD Bugs Mailing List	2012-02-07 06:14 UTC by	None	2013-03-17 16:07 UTC (	0 users	I've trouble using SSL_CLIENT_S_DN_CN in SSLUserName with FakeBasicAuth.My apache config:<Location /repos> SSLOptions +FakeBasicAuth +StdEnvVars SSLUserName SSL_CLIENT_S_DN_CN AuthName "Restricted area" AuthType Basic AuthUserFile /etc/apache2/fakeauth.passwd require valid-user</Location>In this case SSL_CLIENT_S_DN_CN equals SSL_CLIENT_S_DN. I've used patchwhich correct this behaviour to the right way.	At best this is a missing feature as the documentation () clearly states that SSLUserName and FakeBasicAuth do not work in conjunction.Fixed in httpd-trunk, proposed for backport to v2.4.Alternative fix in trunk as follows:	3.0	id=52616	5	False	True	rpluem	1
id=52795	REOPENED	None	Apache httpd-2	mod_fcgid (	2.2.17	PC All	P2 normal	Apache HTTPD Bugs Mailing List	2012-02-29 12:50 UTC by	Florian Winter	2014-06-18 12:07 UTC (	0 users	If ScriptAlias contains spaces (such as "C:\Program Files\..."), then mod_fcgid fails to spawn the process.In error.log, I see something similar to:"Can't run C:\Program"The problem seems to be in fcgid_proc_win.c:proc_spawn_process, which seems to tokenize the command-line without distinguishing between spaces in the executable path and spaces between command-line arguments. The following code is responsible: /* Build wrapper args */ argc = 0; tmp = cmdline; while (1) { word = ap_getword_white(procnode->proc_pool, &tmp); if (word == NULL || *word == '\0') break; if (argc >= APACHE_ARG_MAX) break; wargv[argc++] = word; } wargv[argc] = NULL;The following workaround solves the problem for me. However, it makes it impossible to pass any additional arguments to the FastCGI process (which is probably not an issue for most people): // HACK(flw): Strip all command-line arguments and make the CGI path the first. // This works around a problem of the code above which splits the command name at every whitespace. argc = 1; wargv[0] = procinfo->cgipath;Please note: I am aware of. However, I think that is a different problem, because it occurs on Ubuntu, where fcgid_proc_win.c is not used.Perhaps, a proper solution is to make it possible to escape spaces or use quotes in ScriptAlias. The documentation does not state that this is possible, and the implementation in fcgid_proc_win.c clearly does not support that.	This is either duplicating 52436 or shares the same root issue as 51194*** This bug has been marked as a duplicate of***Verified that this was the same root issue as 51194, see the patch attached to that ticket. Verified that php script paths and names containing spaces work correctly with that patch applied.This has not been resolved. After upgrading to Apache 2.4.9 and the latest modules without our own patch applied, the same problem still occurs. Snippet from httpd.conf: ScriptAlias /fcgid-bin/ "C:/Program Files (x86)/FotoWare/My Application/" <Location /fcgid-bin/> SetHandler fcgid-script </Location>Is it necessary to use any special syntax so that ScriptAlias respects the spaces?The problem is still the same. Tokenization of the command-line causes the executable path to be split. This happens in proc_spawn_process in fcgid_proc_win.c, except a different tokenization function is used now.A workaround (which only works if the FCGI script does not take any arguments) is as follows:/* Build wrapper args */ // HACK(flw): Whitespace handling is still broken when using ScriptAlias, so skip tokenization of the command-line //apr_tokenize_to_argv(cmdline, (char ***)&wargv, procnode->proc_pool); wargv = apr_palloc(procnode->proc_pool, 2 * sizeof(char*)); wargv[0] = cmdline; wargv[1] = NULL;I tried escaping spaces as "\ " in the ScriptAlias and Directory directives in httpd.conf. This only causes requests to fail with 403 Forbidden. Can it be that the paths in Directory and ScriptAlias are parsed differently?There is no official documentation anywhere about how paths with spaces must be escaped. I tried to learn about it in the code of apr_tokenize_to_argv, but it is C code cluttered with preprocessor macros and memory allocations, so I gave up (working with strings in C is hard. I guess that's why it's broken.../rant)	4.0	id=53388	4	False	False	jim	1
id=53693	REOPENED	None	Apache httpd-2	mod_fcgid (	2.2-HEAD	PC All	P2 normal	Apache HTTPD Bugs Mailing List	2012-08-10 23:47 UTC by	Mike	2014-12-26 20:53 UTC (	0 users	I wrote a simple PHP script to simulate activity (sleep for 500ms). I set up a vhost to handle PHP requests via mod_fcgid. I - used PHP5.2, PHP5.3, PHP5.4 -- doesn't seem to matter. - used the latest trunk snapshot of mod_fcgid (2.3.8), and that didn't change behavior - used Apache 2.2.22 (and 2.2.17) with both worker and prefork mpms- httpd is configured with 512 StartServers and at least 1024 MaxClientsTrying to figure out how to prevent serialized blocking of requests in mod_fcgid.What seems to happen is mod_fcgid on initial process spin-up won't tune the number of available of child processes to handle the concurrent request load.. there is a serialization that occurs. I have tried turning a number of the spawn parameters to try to spin up more at once.. and I can't seem to make it happen.So here's an extreme case of 100 concurrent clients with 1 requests to be made, each:Concurrency Level: 100Time taken for tests: 5.672 secondsComplete requests: 100Failed requests: 0Write errors: 0Total transferred: 15600 bytesHTML transferred: 0 bytesRequests per second: 17.63 [#/sec] (mean)Time per request: 5671.918 [ms] (mean)Time per request: 56.719 [ms] (mean, across all concurrent requests)Transfer rate: 2.69 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 4 8 2.1 8 10Processing: 614 2956 1604.2 2665 5661Waiting: 614 2956 1604.2 2664 5661Total: 618 2963 1604.1 2674 5671Percentage of the requests served within a certain time (ms) 50% 2674 66% 3612 75% 4540 80% 4627 90% 5633 95% 5634 98% 5668 99% 5671 100% 5671 (longest request)The process spin up for this over time looks like:Fri Aug 10 16:37:19 MST 2012USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDapache 3571 0.0 0.3 57004 8040 ? R 16:37 0:00 /usr/bin/php-cgiapache 3572 0.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3573 0.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3574 0.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3575 0.0 0.3 56872 7928 ? R 16:37 0:00 /usr/bin/php-cgiapache 3576 0.0 0.3 56872 7832 ? R 16:37 0:00 /usr/bin/php-cgiapache 3577 0.0 0.3 56872 7868 ? R 16:37 0:00 /usr/bin/php-cgiapache 3578 0.0 0.3 56744 7808 ? R 16:37 0:00 /usr/bin/php-cgiapache 3580 0.0 0.0 8112 384 ? R 16:37 0:00 /usr/bin/php-cgi10Fri Aug 10 16:37:19 MST 2012Fri Aug 10 16:37:19 MST 2012USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDapache 3571 6.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3572 5.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3573 6.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3574 5.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3575 6.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3576 6.0 0.4 57132 8416 ? S 16:37 0:00 /usr/bin/php-cgiapache 3577 6.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3578 6.0 0.4 57132 8416 ? S 16:37 0:00 /usr/bin/php-cgiapache 3580 6.0 0.4 57132 8416 ? S 16:37 0:00 /usr/bin/php-cgiapache 3582 5.0 0.4 57132 8408 ? S 16:37 0:00 /usr/bin/php-cgi10Fri Aug 10 16:37:19 MST 2012Fri Aug 10 16:37:19 MST 2012USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDapache 3571 6.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3572 5.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3573 6.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3574 5.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3575 6.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3576 6.0 0.4 57132 8416 ? S 16:37 0:00 /usr/bin/php-cgiapache 3577 6.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3578 6.0 0.4 57132 8416 ? S 16:37 0:00 /usr/bin/php-cgiapache 3580 6.0 0.4 57132 8416 ? S 16:37 0:00 /usr/bin/php-cgiapache 3582 5.0 0.4 57132 8408 ? S 16:37 0:00 /usr/bin/php-cgi10Fri Aug 10 16:37:19 MST 2012Fri Aug 10 16:37:19 MST 2012USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDapache 3571 6.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3572 5.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3573 6.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3574 5.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3575 6.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3576 6.0 0.4 57132 8416 ? S 16:37 0:00 /usr/bin/php-cgiapache 3577 6.0 0.4 57132 8412 ? S 16:37 0:00 /usr/bin/php-cgiapache 3578 6.0 0.4 57132 8416 ? S 16:37 0:00 /usr/bin/php-cgiapache 3580 6.0 0.4 57132 8416 ? S 16:37 0:00 /usr/bin/php-cgiapache 3582 5.0 0.4 57132 8408 ? S 16:37 0:00 /usr/bin/php-cgi10Fri Aug 10 16:37:19 MST 2012...Fri Aug 10 16:37:23 MST 2012USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDapache 3571 1.2 0.4 57132 8480 ? S 16:37 0:00 /usr/bin/php-cgiapache 3572 1.0 0.4 57132 8480 ? S 16:37 0:00 /usr/bin/php-cgiapache 3573 1.2 0.4 57132 8480 ? S 16:37 0:00 /usr/bin/php-cgiapache 3574 1.2 0.4 57132 8480 ? S 16:37 0:00 /usr/bin/php-cgiapache 3575 1.2 0.4 57132 8480 ? S 16:37 0:00 /usr/bin/php-cgiapache 3576 1.2 0.4 57132 8484 ? S 16:37 0:00 /usr/bin/php-cgiapache 3577 1.2 0.4 57132 8480 ? S 16:37 0:00 /usr/bin/php-cgiapache 3578 1.2 0.4 57132 8484 ? S 16:37 0:00 /usr/bin/php-cgiapache 3580 1.2 0.4 57132 8484 ? S 16:37 0:00 /usr/bin/php-cgiapache 3582 1.0 0.4 57132 8476 ? S 16:37 0:00 /usr/bin/php-cgiapache 3657 1.5 0.4 57132 8484 ? S 16:37 0:00 /usr/bin/php-cgiapache 3706 2.0 0.4 57132 8480 ? S 16:37 0:00 /usr/bin/php-cgiapache 3760 3.5 0.4 57132 8480 ? S 16:37 0:00 /usr/bin/php-cgiapache 3816 6.0 0.4 57132 8408 ? S 16:37 0:00 /usr/bin/php-cgi14It takes at least 4 seconds to spin up to 14 processes (well under our concurrent connection limit). On a very small scale, I ran some tests to simulate this:test.php script:usleep(500000);echo $_SERVER['SCRIPT_FILENAME']." :: ".$_SERVER['SERVER_NAME'];With this script, each run should take ~500ms plus some very minor overhead.o Loaded Apache with the worker MPM.. Used ab concurrency level of 2, feeding 10 requests, we see a bottleneck due to this serialization:Concurrency Level: 2Time taken for tests: 3.116 secondsComplete requests: 10Failed requests: 0Write errors: 0Total transferred: 1560 bytesHTML transferred: 0 bytesRequests per second: 3.21 [#/sec] (mean)Time per request: 623.257 [ms] (mean)Time per request: 311.628 [ms] (mean, across all concurrent requests)Transfer rate: 0.49 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 3 3 0.2 3 3Processing: 504 619 336.6 505 1574Waiting: 504 619 336.6 505 1574Total: 507 622 336.6 508 1577Percentage of the requests served within a certain time (ms) 50% 508 66% 508 75% 508 80% 577 90% 1577 95% 1577 98% 1577 99% 1577 100% 1577 (longest request)o Once the processes are spun up however, we see 500ms (+~10ms overhead) for all requests as long as our concurrency does not exceed the number of child processes availableUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDapache 30209 0.0 0.4 57132 8488 ? S 15:30 0:00 /usr/bin/php-cgiapache 30214 0.0 0.4 57132 8480 ? S 15:30 0:00 /usr/bin/php-cgiConcurrency Level: 2Time taken for tests: 2.546 secondsComplete requests: 10Failed requests: 0Write errors: 0Total transferred: 1560 bytesHTML transferred: 0 bytesRequests per second: 3.93 [#/sec] (mean)Time per request: 509.200 [ms] (mean)Time per request: 254.600 [ms] (mean, across all concurrent requests)Transfer rate: 0.60 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 3 3 0.1 3 3Processing: 504 506 1.7 505 509Waiting: 504 506 1.7 505 509Total: 507 508 1.7 508 512Percentage of the requests served within a certain time (ms) 50% 508 66% 509 75% 509 80% 511 90% 512 95% 512 98% 512 99% 512 100% 512 (longest request)o Adding +1 concurrency level (taking concurrency to 3), we produce a serializing bottleneck again:USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDapache 30209 0.0 0.4 57132 8488 ? S 15:30 0:00 /usr/bin/php-cgiapache 30214 0.0 0.4 57132 8484 ? S 15:30 0:00 /usr/bin/php-cgiapache 31866 1.2 0.4 57132 8488 ? S 15:34 0:00 /usr/bin/php-cgiConcurrency Level: 3Time taken for tests: 2.083 secondsComplete requests: 10Failed requests: 0Write errors: 0Total transferred: 1560 bytesHTML transferred: 0 bytesRequests per second: 4.80 [#/sec] (mean)Time per request: 625.025 [ms] (mean)Time per request: 208.342 [ms] (mean, across all concurrent requests)Transfer rate: 0.73 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 3 3 0.2 3 3Processing: 504 612 337.9 505 1573Waiting: 504 612 337.9 505 1573Total: 507 615 337.9 508 1576Percentage of the requests served within a certain time (ms) 50% 508 66% 508 75% 508 80% 509 90% 1576 95% 1576 98% 1576 99% 1576 100% 1576 (longest request)o And again, at 3 concurrency since 3 already spun up processes, no bottleneck:USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDapache 30209 0.0 0.4 57132 8488 ? S 15:30 0:00 /usr/bin/php-cgiapache 30214 0.0 0.4 57132 8484 ? S 15:30 0:00 /usr/bin/php-cgiapache 31866 1.2 0.4 57132 8488 ? S 15:34 0:00 /usr/bin/php-cgiConcurrency Level: 3Time taken for tests: 2.032 secondsComplete requests: 10Failed requests: 0Write errors: 0Total transferred: 1716 bytesHTML transferred: 0 bytesRequests per second: 4.92 [#/sec] (mean)Time per request: 609.480 [ms] (mean)Time per request: 203.160 [ms] (mean, across all concurrent requests)Transfer rate: 0.82 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 3 3 0.2 3 3Processing: 504 505 0.5 505 506Waiting: 504 505 0.5 505 506Total: 507 508 0.5 508 508Percentage of the requests served within a certain time (ms) 50% 508 66% 508 75% 508 80% 508 90% 508 95% 508 98% 508 99% 508 100% 508 (longest request)	This might be related to enhancement submission:I tried changing in modules/fcgid/fcgid_bridge.c, /* Avoid sleeping the very first time through if there are no busy processes; the problem is just that we haven't spawned anything yet, so waiting is pointless */ if (i > 0 || j > 0 || count_busy_processes(r, &fcgi_request)) { apr_sleep(apr_time_from_sec(1));to if (i > 0 || j > 0 || count_busy_processes(r, &fcgi_request)) { apr_sleep(apr_time_from_sec(0));and the serialization block seems to have stopped.Is this meant to just be an artificial anti-thrashing mechanism? If so, is there a better way I can prevent too many processes for trying to spin up concurrently, than adding in this 1s time delay? The 1s delay totally kills concurrent requests.CreatedSpawn faster under low process countThe principle of not rushing into trying to spawn new processes is good: indeed in the case that we're running at max processes per class, it is the only thing that gives time to try to handle the actual requests (rather than spinning on the server). However, 1s is a very long time, and we're doing it if *any* process is currently busy. As a minimum, FcgidMinProcessesPerClass should be considered - under that level, we should be perfectly happy to launch a new process to handle a request. Also, if launching another process is a plausible thing to do (i.e. we're below the limit), we shouldn't wait 1s before re-checking: requests should be being handled a lot more quickly than that. So instead, wait 250ms by default.This patch is based in principle around high/low water marks for process count; at this point, these are just min and max processes per class. Picking figures between these would be better, but these should probably be configurable.Adjusted title slightly to reflect what the OP correctly noted: that the problem is that requests that can't immediately be handled be an existing process are delayed by 1s (excepting when there are no busy processes at all).See also the comments in my patch: ultimately, additional tunables are called for, but adding options isn't something to be done lightly (given the doc impact), and certainly warrants some thought. Possibly explicit high/low, or a target process count, and possibly a configurable delay could be used - or some/all of these values could be generated at startup based on the bounds specified by existing options.a fix is commited to trunkNew protocol to avoid too much sleep, improve performance while stress testing1. procmgr_send_spawn_cmd() now return a status code from PM, so process handler now know the spawn request is denyed or not.2. if a new process is created, no sleep is needed.3. if no process is created, sleep a whileRyan Pan's fix for this bug (and a couple of follow-on fixes) was reverted withdue to issues encountered with testing the proposal for mod_fcgid 2.3.8, which could not be released.Related discussion is in this mailing list thread:Note that at the same time a separate Windows-specific bug could result in more processes than necessary. That didn't affect other platforms and didn't explain all the bad symptoms encountered in Steffen's Windows setup.Any news on this bug?Why is lowering the sleep from 1s to f.e. 50ms not a solution?	7.0	id=53693	7	False	False	xyntrix	1
id=54221	REOPENED	None	Apache httpd-2	mod_log_config (	2.4.3	Sun Solaris	P2 normal	Apache HTTPD Bugs Mailing List	2012-11-28 22:40 UTC by	Mark Tischler	2013-03-07 22:21 UTC (	0 users	Createderror log snippet from before the upgradeWas running Apache 2.2.15 up until mid-October. Then upgraded to Apache 2.4.3. There are statements that are coming out in the error log rather routinely, so they were coming out before the upgrade, and they are coming out since the upgrade. These are generated by some web application running a shell script on our web server. The problem is that these lines lack full information in the new Apache version, which makes debugging hard. I have been searching the internet on and off for the past several weeks trying to figure out what might be wrong, but have had no success. So, unfortunately, I'm having to resort to this method of solving my problem. I have attached a file snippet from our error log *prior* to the upgrade, a file snippet from our error log *after* to the upgrade, as well as our httpd.conf. In the case of the two file snippets, you will see that the information associated with the middle 4 lines is severely lacking in the one from *after* our upgrade. Can you identify if there's a problem in our configuration, or if there is something wrong with Apache?	Createderror log snippet from after the upgradeCreatedconfig fileNote that I'm just giving one example of this. There are many more lines that come out in the error log without any additional information. I haven't been able to establish a pattern of what gets printed with additional information and what does not.LoadModule info_module modules/mod_info.so # mdt - 2012-09-14 - comment out because this module is not built by default as of Apache 2.4.2#LoadModule cgi_module modules/mod_cgi.so # mdt - 2012-09-14 - add this module to take the place of mod_cgi as of Apache 2.4.2LoadModule cgid_module modules/mod_cgid.soDid your logging difference happen during the change from mod_cgi to mod_cgid?Did you change your MPM as well?I made the change from mod_cgi to mod_cgid simultaneous with the move from 2.2.15 to 2.4.3, so I don't know what caused it. As I noted in the httpd.conf, I had to make that change to get things to work. Should I have done something different?I do not know what MPM is, so I guess I don't know if changed it. I will attach my old httpd.conf -- the one from 2.2.15. Perhaps that answers this question.Createdold config fileEric,You were so good about responding quickly the first time. Any thoughts on what the problem might be?The prefix is a difference between mod_cgi and mod_cgid.So, are you saying that I need to load mod_cgi instead of mod_cgid? If so, as I recall (it's been several months, though), when I tried loading mod_cgi, it told me that this module was obsolete (you actually captured this implication in your Nov. 29th response). So, how do you suggest I get mod_cgi to work? At the time, my only recourse seemed to be to *not* load mod_cgi, but to instead load mod_cgid.I'm only saying it's the explanation of the different behavior when you changed from one CGI module to the other, which had been misinterpreted as something that changed just due to the update.So, are you confirming that I can no longer use the mod_cgi module with Apache 2.4.3? And, if so, are you saying that there is no way to get the prefixes back?There's no new restrictions on mod_cgi in 2.4. Just like in 2.2, you can only use it with the prefork MPM, not worker or event.Then I'm confused. What would I have inadvertently changed so that I'm not using prefork MPM in Apache 2.4 vs. (apparently I was) in Apache 2.2? Is there something in the httpd.conf files for 2.2 and 2.4 that leads to that answer?(In reply to)It's a build time decision in 2.2. In 2.4, you can build multiple MPM's and choose whichever you like at runtime by LoadModule on exactly one.OK. So, I'm looking for some advice on how to configure the httpd.conf for 2.4.3, so that I get the correct module(s). As I said, when I tried to load mog_cgi, it complained. It sounds (from what you're saying) that I simply need to adjust the httpd.conf to load some new or different modules.OK. So, I rebuilt the web server, this time with the '--enable-cgi' flag in the config for both building Apache as well as PHP. I loaded the mod_cgi module instead of the mod_cgid module, ran the following script, and still got the problem:<?phpprint "Content-type: text/html\n\nhi\n";$STDERR = fopen('php://stderr', 'w+');fwrite($STDERR, "Error: cannot execute query\n");?>Running this as a URL, yields simply:Error: cannot execute queryinstead of something like:[Tue Jan 29 12:51:44.330891 2013] [:note] [pid 19732:tid 5] [client 135.185.197.151:3798] Error: cannot execute queryYou had said that it was because I was now using mod_cgid, instead of mod_cgi. Given that I am now using mod_cgi and I still have this problem, what's the next step?Eric, I need advice from you. This is not something I can go to the forums with, because I need insider knowledge of what's causing this issue. You had one theory, but, either there's something additional at play, or there is something completely different causing this. Please respond with your insight.I have no idea, you'll have to wait for a volunteer to look further. Are you sure your PHP runs as CGI?The .php script I included above is run as a web page under that newly-built web server, so it should be running as a built-in, not CGI, because I build PHP with Apache. At least, that's the way I was understanding it. Besides the .conf file, which I attached to the ticket, I could include the instructions I used to build the product, if you think that would help.When you say that I would have to wait for another volunteer to see and grab this ticket, it's been 2 weeks since I posted that even using mod_cgi didn't help. So, what time period would I expect to have to wait until someone else jumps in? How does someone know that you are "detaching" from this ticket? Perhaps every other volunteer figures that you are still handling it, and, thus, they do not need to jump in.Bugzilla isn't for support, It's for reporting bugs. It's still unclear what component writes to your error log differently after your upgrade and reconfguration, but no matter what it is originators are expected to do their homework in the report.Anyone who takes interest is always free to comment. Again, I think you might get some guidance on the users list which has more eyes.The problem is that this appears to me to be a bug. I think I have followed the instructions to build a web server properly, but, as yet, there's been no explanation as to why it doesn't work the way it used to work. Once there's a plausible explanation (and I thought it was that I was supposed to use mod_cgi instead of mod_cgid) as to why it's not working, then, to me, it would make sense to go to the user list.No one has responded to this ticket in a very long time. If, indeed, Eric, you were waiting for someone else to jump in and take it (as you indicated on February 11), it appears that nothing is triggering anyone else to look at this. Is there a way to trigger someone else to look at this in a timely fashion?(In reply to)I'm not waiting for anything. We're all volunteers motivated by different things without any implied level of service. As an originator, the best you can do is provide your own detail and analysis to not reduce the overhead of whoever looks at the bug report.On 2.4, I can switch between mod_cgid and mod_cgi and run a CGI that does:echo "XXX CGI TEST" >&2And with mod_cgi, it shows up with the erorrlog prefix, and with mod_cgid it shows up without it.I confirmed on 2.2, mod_cgid behaves the same as it does in 2.4. I didn't bother testing mod_cgi in 2.2.I didn't go back to 2.2.15, just 2.2.latest. You'll need a more concrete reproducible report to minimize the effort of anyone trying to find the bug here.I'm not sure what else someone would need. I provided the httpd.conf files from 2.2.15 and 2.4.3, and I provided an actual example on my posting of 2013-01-29 19:01:26 UTC.	25.0	id=53999	6	False	False	zerg2000-apachebug	1
id=54310	REOPENED	None	Apache httpd-2	mod_proxy_html (	2.4-HEAD	All All	P2 normal	Apache HTTPD Bugs Mailing List	2012-12-15 23:19 UTC by	Christoph Anton Mitterer	2012-12-16 01:15 UTC (	0 users	Hi.According to the original upstream documentation ():"However, it does not strip them from the HTML (except for Content-Type, which is removed in case it contains conflicting charset information)."It seems though that at least meta elements with http-equiv="Content-Type" (but possibly all, I haven't checked) are dropped, even when the Content-Type fits, because e.g. the original document already had UTF-8 or e.g. because ProxyHTMLCharsetOut was set to *.Cheers,Chris.	See also.You seem to be describing the behaviour that is both long-established and documented.Content-Type is removed in case it contains conflicting charset information - as might happen if the charset has changed.Since Content-Type is set as a real HTTP header, this doesn't risk losing information.Hi Nick.But as I've just wrote in my bug report,... it seems to remove it _even_ if it does fit.I have no strong opinion whether this is good or bad... I think is just need to be decided what should be done, checked whether this is actually the case (which it seems not right now)... and then all this should be documented (which it is currently only in the upstream, but not the apache documentation)Cheers,Chris.	3.0	id=54221	11	False	False	covener	1
id=54699	REOPENED	None	Apache httpd-2	All (	2.5-HEAD	PC Linux	P2 enhancement	Apache HTTPD Bugs Mailing List	2013-03-15 02:16 UTC by	None	2013-03-16 11:32 UTC (	0 users	[feature request] cli webaccess testerIt would be very usefull if cli (shell) tool would exist that would tell if a directory/file is accessible on the webserver (by specified IP for example). I'm developing anti-mallware software and it is critical for it to tell if a directory (and file in it) is web-accessible or not - if it is blocked by either something in httpd.conf or in htaccess (like deny from all) (htaccess are recursive). In other words if there is url (configured at webserver) that allows for direct access to a file or directory (its index.(php|htm)).	Sounds like your just describing a command line HTTP client, which there are plenty of. I don't see room for much in this area in our webserver project.It is not possible to accuratly determine via http if file is actualy blocked by apache configuration because:file (like php script) can return 404, 401 for "normal request" while returning other data for specified GET argument (and it is actualy ussed by mallware all the time those days - to mask its presence)also it is not optimal to do any such check via httpSorry, but such a feature (a command line tools that knows not just httpd configuration but somehow knows when PHP will decide to return a 401 or 404) is simply not what our project is about. If you find some novel way to implement this, reopen.only apache httpd can efeciently say if file/directory is blocked by its configuration or not and http response is not a way to deremine it. Duplicating apache configuration parser into other program is pointless.example scenario: You scanner your partition with antivirus software and found that /home/user/public_html/abc/public/images/cache/0/a/2/5/49586.php was malicious codeYou don't know domain or url to it. How how do you suppose you could tell if that file is actualy accessible via any domain served by your apache? Your httpd.conf is very fancy and has a lot of data in it. Worse, user has multiple .htaccess files with rewrite rules, deny rules and authorization rules. Finaly actual php file returns 401 code to apache if run by webserver to mask its presence (unless passed aprorpiate $_GET argument). Answer is YOU CANT know if file is world accessible! It is completly impossible to analize automaticly those configuration, htaccess, rewrites!!Doesn't sound useful in practice to me, but since you insist on reverting my state changes, I'm not going to bother with any more feedback.	6.0	id=54310	4	False	False	nick	1
id=53999	REOPENED	None	Apache httpd-2	mod_fcgid (	2.4.4	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2012-10-13 01:30 UTC by	Bartosz Kwitniewski	2016-12-06 16:57 UTC (	1 user	Hello!I would like to merge servers from Apache 2.2 to 2.4 but a deadlock that was quite rare in 2.2 makes 2.4 unusable. It is between listener_thread() in worker_mpm and at least these mod_fcgid functions: procmgr_send_spawn_cmd()/proctable_lock()/proctable_pm_lock(). In Apache 2.2 it occured only under heavy load while in 2.4 it occurs within few minutes, up to half hour after server start, even under low server load.Deadlock results in httpd process terminating itself and leaving many fcgid processes in busy state that never changes to idle. Busy processes turn into zombies after some time (possibly after process_kill_gracefully()) so process_kill_gracefully()/process_kill_force() in scan_busylist() is unable to kill them and since they are on busy list they are never collected by scan_idlelist_zombie(). The final result is full saturation of FcgidMaxProcessesPerClass limit by zombies for some VirtualHosts which in turn results in permanent 503 error until whole serwer is restarted (can be graceful).I am able to reproduce problem on machine with 2 processors by creating 5 VirtualHosts pointing to a separate Joomla installations and then executing the following test on all VirtualHosts simultaneously:ab -n 10000 -c 10 vhostX/Example MPM/mod_fcgid configuration triggering deadlock:<IfModule worker.c> ServerLimit 3 ThreadLimit 100 StartServers 1 MaxClients 300 MinSpareThreads 50 MaxSpareThreads 175 ThreadsPerChild 100 MaxConnectionsPerChild 100000</IfModule><IfModule mod_fcgid.c> FcgidMaxProcesses 400 FcgidMinProcessesPerClass 0 FcgidMaxProcessesPerClass 5 FcgidMaxRequestsPerProcess 500 FcgidInitialEnv PHP_FCGI_MAX_REQUESTS 500 FcgidProcessLifeTime 135 FcgidSpawnScore 1 FcgidTerminationScore 1 FcgidTimeScore 100 FcgidSpawnScoreUpLimit 100 FcgidIOTimeout 65 FcgidIdleTimeout 60 FcgidIdleScanInterval 30 FcgidBusyTimeout 65 FcgidBusyScanInterval 60</IfModule>System: Debian 6.0.x 64 bitApache: 2.4.3mod_fcgid: SVN rev. 1397462 (bug also present in 2.3.7 and 2.3.6)Compilation flags: -O0 -ggdbDeadlock is usually triggered within 5 minutes after starting ab.----------[ Debug info from deadlock on the worker_mpm side ]----------Error in logfile:[mpm_worker:emerg] [pid 13326:tid 139649122420480] (35)Resource deadlock avoided: AH00273: apr_proc_mutex_lock failed. Attempting to shutdown process gracefully.Deadlock in listener_thread() at worker.c:762: /* We've already decremented the idle worker count inside * ap_queue_info_wait_for_idler. */====> if ((rv = SAFE_ACCEPT(apr_proc_mutex_lock(accept_mutex))) != APR_SUCCESS) { if (!listener_may_exit) { accept_mutex_error("lock", rv, process_slot); } break; /* skip the lock release */ }Backtrace:#0 0x00007f5003d9f45c in __pthread_kill (threadid=<value optimized out>, signo=<value optimized out>) at ../nptl/sysdeps/unix/sysv/linux/pthread_kill.c:63#1 0x000000000047bf64 in wakeup_listener () at worker.c:287#2 0x000000000047bf97 in signal_threads (mode=1) at worker.c:310#3 0x000000000047cd7c in accept_mutex_error (func=0x4929b8 "lock", rv=35, process_slot=0) at worker.c:678#4 0x000000000047d0b8 in listener_thread (thd=0x21ccb18, dummy=0x209b290) at worker.c:766#5 0x00007f5003fdbc03 in ?? () from /usr/lib/libapr-1.so.0#6 0x00007f5003d998ca in start_thread (arg=<value optimized out>) at pthread_create.c:300#7 0x00007f5003b0092d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:112#8 0x0000000000000000 in ?? ()Local variables:ti = 0x209b290process_slot = 0tpool = 0x21edb18csd = 0x7f4fc4086d30ptrans = 0x7f4fc4018a68pollset = 0x21edb90rv = 35 <=== EDEADLKlr = 0x1e97178have_idle_worker = 1last_poll_idx = 1----------[ Debug info from deadlock on the mod_fcgid side ]----------Error in logfile:[fcgid:emerg] [pid 11624:tid 139810533033728] (35)Resource deadlock avoided: [client x.x.x.x:49337] mod_fcgid: can't get pipe mutexThere is also another one in proctable_lock()/proctable_pm_lock() but it is quite rare and I didn't get backtrace from it:[fcgid:emerg] [pid 27202:tid 140334891595520] (35)Resource deadlock avoided: [client x.x.x.x:33273] mod_fcgid: can't lock process table in pid 27202Deadlock in procmgr_send_spawn_cmd() at fcgid_pm_unix.c:467: /* Get the global mutex before posting the request */====> if ((rv = apr_global_mutex_lock(g_pipelock)) != APR_SUCCESS) { ap_log_rerror(APLOG_MARK, APLOG_EMERG, rv, r, "mod_fcgid: can't get pipe mutex"); exit(0); }Backtrace:#0 procmgr_send_spawn_cmd (command=0x7f5311e0b520, r=0x1403738) at fcgid_pm_unix.c:468#1 0x00007f532808082b in handle_request (r=0x1403738, role=1, cmd_conf=0x13fa380, output_brigade=0x1404f70) at fcgid_bridge.c:450#2 0x00007f5328081769 in bridge_request (r=0x1403738, role=1, cmd_conf=0x13fa380) at fcgid_bridge.c:765#3 0x00007f532807df92 in fcgid_handler (r=0x1403738) at mod_fcgid.c:290#4 0x000000000045662b in ap_run_handler (r=0x1403738) at config.c:169#5 0x0000000000456f0b in ap_invoke_handler (r=0x1403738) at config.c:432#6 0x0000000000473963 in ap_internal_redirect_handler (new_uri=0x1403718 "/cgi-bin/php-fcgi/index.php", r=0x1576880) at http_request.c:669#7 0x00007f53288ad394 in action_handler (r=0x1576880) at mod_actions.c:205#8 0x000000000045662b in ap_run_handler (r=0x1576880) at config.c:169#9 0x0000000000456f0b in ap_invoke_handler (r=0x1576880) at config.c:432#10 0x00000000004727b3 in ap_process_async_request (r=0x1576880) at http_request.c:317#11 0x0000000000472899 in ap_process_request (r=0x1576880) at http_request.c:363#12 0x000000000046ed81 in ap_process_http_sync_connection (c=0x13ef3b8) at http_core.c:190#13 0x000000000046ee97 in ap_process_http_connection (c=0x13ef3b8) at http_core.c:231#14 0x0000000000463bf2 in ap_run_process_connection (c=0x13ef3b8) at connection.c:41#15 0x00000000004640bf in ap_process_connection (c=0x13ef3b8, csd=0x13ef1a0) at connection.c:202#16 0x000000000047ca2c in process_socket (thd=0x1099478, p=0x13ef118, sock=0x13ef1a0, my_child_num=1, my_thread_num=42, bucket_alloc=0x13f1128) at worker.c:620#17 0x000000000047d8a9 in worker_thread (thd=0x1099478, dummy=0x12056b0) at worker.c:979#18 0x00007f532dfc4c03 in ?? () from /usr/lib/libapr-1.so.0#19 0x00007f532dd828ca in start_thread (arg=<value optimized out>) at pthread_create.c:300#20 0x00007f532dae992d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:112#21 0x0000000000000000 in ?? ()Local variables:rv = 35 <=== EDEADLKresult = 0notifybyte = 0 '\000'nbytes = 13160procmgr_send_spawn_cmd() executes exit(0) on error when trying to lock mutex - shouldn't it return status code that indicates error? There is also lack of information that httpd process is actually terminated - "mod_fcgid: can't get pipe mutex" in logs does not look too harmful except for the emerg level.Should I fill separate bug report for busy zombies not being collected?	I was unable to reproduce this bug with event MPM using unencrypted connections but it is still present when using SSL, probably due to this: "For SSL connections, this MPM will fall back to the behaviour of the worker MPM and reserve one worker thread per connection."I'm unable to reproduce error on Linux 3.16.6 and Apache 2.4.10 anymore. I have received report that kernel upgrade in itself can fix this bug. Closing.Unfortunately I am able to reproduce the problem with a Debian Jessie installation.Using mpm_worker and mod_fcgid the following messages begin to appear in the error log under heavy load (also triggerable via ab -c15):[Mon Aug 17 15:21:13.509763 2015] [fcgid:emerg] [pid 21497:tid 139901035079424] (35)Resource deadlock avoided: [client XXX.XXX.XXX.XXX:38964] mod_fcgid: can't get pipe mutex[...][Mon Aug 17 15:22:49.387129 2015] [fcgid:emerg] [pid 22016:tid 139900959545088] (35)Resource deadlock avoided: [client XXX.XXX.XXX.XXX:39590] mod_fcgid: can't lock process table in pid 22016[Mon Aug 17 15:22:53.479254 2015] [fcgid:emerg] [pid 22047:tid 139901035079424] (35)Resource deadlock avoided: [client XXX.XXX.XXX.XXX:39617] mod_fcgid: can't lock process table in pid 22047[...]At the same time, gracefully terminated FCGI processes don't get wait()/waitpid() ed and are left in a zombie state by Apache until FcgidMaxProcessesPerClass are used up (then 503 errors).Using mpm_event I was also able to reproduce the problem, however, the frequency was greatly reduced.The only solution for me up to this point is using mpm_prefork, with which the error is not reproductible at all.Apache version: 2.4.10mod_fcgid version: 2.3.9Linux kernel version: 3.16.7Unfortunately I can confirm this problem with another Debian Jessie installation.When using mpm_worker OR mpm_event and mod_fcgid i observed the same things as Thomas and could also resolve them by switching to mpm_prefork.Apache version: 2.4.10-10+deb8u1mod_fcgid version: 2.3.9-1+b1Linux kernel version: 3.16.7-ckt11-1+deb8u3PHP versions: 5.6.12+dfsg-0+deb8u1, self-compiled 5.5.28, self-compiled 5.6.12Another Debian Jessie user chiming in. I'm seeing this error too, but I'm not using mod_fcgi, but mod_fastcgi (which communicates via sockets with php-fpm):[Fri Oct 02 06:40:08.558059 2015] [mpm_worker:notice] [pid 29795:tid 140436668389248] AH00292: Apache/2.4.10 (Debian) mod_fastcgi/mod_fastcgi-SNAP-0910052141 mod_jk/1.2.37 OpenSSL/1.0.1k configured -- resuming normal operations[Fri Oct 02 06:40:08.558084 2015] [core:notice] [pid 29795:tid 140436668389248] AH00094: Command line: '/usr/sbin/apache2'[Fri Oct 02 08:06:43.819080 2015] [mpm_worker:emerg] [pid 10105:tid 140436228171520] (35)Resource deadlock avoided: AH00273: apr_proc_mutex_lock failed. Attempting to shutdown process gracefully.[Fri Oct 02 08:26:34.332824 2015] [mpm_worker:emerg] [pid 12883:tid 140436228171520] (35)Resource deadlock avoided: AH00273: apr_proc_mutex_lock failed. Attempting to shutdown process gracefully.[Fri Oct 02 10:06:00.666482 2015] [mpm_worker:emerg] [pid 25550:tid 140436228171520] (35)Resource deadlock avoided: AH00273: apr_proc_mutex_lock failed. Attempting to shutdown process gracefully.we just switched this machine from prefork to worker to avoid an issue where mod_dbd would starve the database of connections.apache2 2.4.10-10+deb8u3libapache2-mod-fastcgi 2.4.7~0910052141-1.1libapache2-mod-jk 1:1.2.37-4+deb8u1php5-fpm 5.6.13+dfsg-0+deb8u1 linux-image-3.16.0-4-amd64 3.16.7-ckt11-1+deb8u4did debian change to using fcntl by default? See this old thread, it seems unusable with multiple independent mutexes.well, another debian v8.4 in this case.recently switched for performance reasons to mpm_worker, now back to mpm_prefork for the same reason (and yet as we're on dbd too this fills up db-connections):apache2 2.4.10-10+deb8u4 amd64apache2-mpm-worker 2.4.10-10+deb8u4 amd64php5-fpm 5.6.19+dfsg-0+deb8u1 amd64libapache2-mod-fastcgi 2.4.7~0910052141-1.1+deb8u1 amd64Kernel is a bit different: 2.6.32-44-pveany direction for debian build team to fix this would be highly appreciated.thxhk@Andreas,Peter,Harald: Can you reproduce the bug with 'Mutex sem' (or a flock/pthread mutex)?Not using the default (file -> fcntl) Mutex seems to fix the issue for me, so Eric might be correct (even if the liked report seems to only mention Solaris).If this is true, the Debian maintainer should be informed about this (and the default Mutex type should be changed to sem, pthread or flock) and this bug report may be closed.Thomas, we were getting the same problem, and your comment helped a lot."Mutex sem" seems the solution.Did two tests in siege using this command:siege -f /tmp/urls.sort1.awk.txt -i -d0 -c20 -v | grep -vE "200 [01]"I did only a change between first and second test: /etc/apache2/apache2.conf# Mutex file:${APACHE_LOCK_DIR} default # Original config (1st Test)Mutex sem # (Config 2nd test)In the first siege test:Transactions: 32958 hitsAvailability: 99.43 %Elapsed time: 115.76 secsData transferred: 14.80 MBResponse time: 0.07 secsTransaction rate: 284.71 trans/secThroughput: 0.13 MB/secConcurrency: 19.42Successful transactions: 32960Failed transactions: 189Longest transaction: 8.96Shortest transaction: 0.00In 115 seconds I had to stop because I got 115 PHP orphaned processes.In the second siege test:Transactions: 99533 hitsAvailability: 99.98 %Elapsed time: 292.05 secsData transferred: 44.50 MBResponse time: 0.05 secsTransaction rate: 340.81 trans/secThroughput: 0.15 MB/secConcurrency: 17.80Successful transactions: 99533Failed transactions: 21Longest transaction: 29.04Shortest transaction: 0.00After 5 minutes and 0 zombies and 0 orphaned processes.As you can see it is slightly faster than the prior configuration. And 0.02% failure, which is 10x times less than before.I'm using Debian Jessie, kernel 4.4.4-xxxx-grs-ipv6-64 #7 SMP x86_64 GNU/LinuxJust a note: if You have strange crashes/deadlocks while using "Mutex sem", then it might be related to semaphore arrays limit in the kernel, which is 128 by default. You can check semaphores currently used by Apache with "ipcs -s". Limit can be changed from 128 to e.g. 1024 using sysctl (last number is the limit):sysctl -w kernel.sem="250 32000 32 1024"	10.0	id=54367	9	False	False	wiml	1
id=55985	REOPENED	None	Apache httpd-2	mod_log_config (	2.2.26	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2014-01-10 15:39 UTC by	Rainer Canavan	2014-01-10 19:29 UTC (	0 users	Since we want to use different "postrotate" scripts for different CustomLogs, we configure lgrotate with 3 different sections, one for each "type" of log file. Since none of the sections is guaranteed to run every time, each has its own call to apachectl graceful. Sometimes, this causes multiple calls to apachectl graceful within the same second, which in turn sometimes results in httpd not reopening the access logs. This is usually fixed by manually performing an apachectl graceful at a later time, but httpd may just as well continue to write to the same (deleted) access_log.1 for multiple days. This results in log data that never gets analyzed or can result in the /var partition filling up.I was able to reproduce this behaviour by running ab, a loop that just does graceful restarts every 5 seconds, and a second loop that uses lsof to find deleted files opened by httpd processes and forces logrotate to run for the single configured transfer log. I only have a single virtualhost configured, and ab is requesting a small static file.bin/ab -n 999999 -c 4 -t 10000for i in {1..1000}; do echo $i; lsof |grep httpd |grep deleted | egrep -v 'ssl_mutex'; echo; logrotate -f logrotate.conf ; j=0; while lsof |grep httpd |grep deleted | egrep -v 'ssl_mutex'; do echo ====== $j ======; sleep 1; j=$[ $j + 1 ]; done; donewhile true; do apachectl graceful; sleep 5; doneI've tried httpd 2.2.22, 2.2.24 and 2.2.26 on debian Wheezy (i686), RHEL 6.5 x86_64 and SLES 11 SP2 x86_64, all of which show the described behavior. The deleted access_log.1 is used even by children forked after the previous ones have exceeded their MaxRequestsPerChild. httpd -t -D DUMP_MODULESLoaded Modules: core_module (static) authn_file_module (static) authn_default_module (static) authz_host_module (static) authz_groupfile_module (static) authz_user_module (static) authz_default_module (static) auth_basic_module (static) cache_module (static) disk_cache_module (static) mem_cache_module (static) include_module (static) filter_module (static) deflate_module (static) log_config_module (static) log_forensic_module (static) logio_module (static) env_module (static) expires_module (static) headers_module (static) unique_id_module (static) setenvif_module (static) version_module (static) proxy_module (static) proxy_connect_module (static) proxy_ftp_module (static) proxy_http_module (static) proxy_scgi_module (static) proxy_ajp_module (static) proxy_balancer_module (static) ssl_module (static) mpm_prefork_module (static) http_module (static) mime_module (static) dav_module (static) status_module (static) autoindex_module (static) asis_module (static) cgi_module (static) dav_fs_module (static) dav_lock_module (static) vhost_alias_module (static) negotiation_module (static) dir_module (static) actions_module (static) userdir_module (static) alias_module (static) rewrite_module (static) so_module (static)Syntax OKthe logrotate.conf used:logs/access_log { su rainer rainer compress nodelaycompress compresscmd /bin/bzip2 compressext .bz2 create 644 rainer rainer sharedscripts postrotate bin/apachectl graceful endscript}	Logs are opened in the parent, not in the children. New children cannot be expected to reopen logs that have been deleted out from under them, this requires at least a graceful restart.Based on these comments, tt sounds like in some way you may know that a graceful does not always follow your unlink, so I am marking as invalid.Can you explain why the loop while true; do apachectl graceful; sleep 5; doneis required to trigger this behaviour if there's only one apachectl graceful in the logrotate config, or, alternatively the logrotate must have multiple sections with one apachectl graceful in each section to causes the described behaviour?I assume there are windows where nearly simulataneous gracefuls happen don't "stack", and that somehow leads to you unlinking without a graceful being processed after. But i don't think there is a remedy in httpd.I think the fact that just running logrotate -f with a single section over and over again does not cause the problem to happen, as opposed to doing the same and running a separate loop with just apachectl graceful and sleep, is a clear indicator that logrotate is not at fault.	4.0	id=54699	8	False	False	covener	1
id=55981	REOPENED	None	Apache httpd-2	mod_mbox (	2.5-HEAD	PC Windows XP	P2 normal	Apache HTTPD Bugs Mailing List	2014-01-09 01:08 UTC by	Sebb	2014-01-12 16:30 UTC (	0 users	Seefor full details.A Message-ID is allowed to contain the character "&".For example, MS Outlook 12.0 can generate ids of the form:Message-ID: <!&!A...>The ajax processor stores the id in the response without encoding the ampersand.For example:<?xml version="1.0" encoding="UTF-8"?><mail id="%3c!&!A...%3e"> The XML processor then chokes on the "&!A" because it is not a valid entity reference.It should be sufficient to encode "&" as "&amp;"	This is the code which fails when the XML is malformed:And here I think is the code that needs to escape ampersands:I could not find any docs on the APR method ap_escape_uri - which is what the macro URI_ESCAPE_OR_BLANK uses - so I don't know if that is supposed to escape ampersands or not.Sebb: there must be something more than just the missing "&" encoding. I added encoding for "&" in message IDs to the version running on aurora (mail-archives.eu.apache.org) but the request to retrieve the mail hangs just as when accessing the unchanged version on mail-archives.us. Unfortunately Firebug doesn't show the hanging request. On the server side I can see a new connection being opened, but then not http request being send.Can you try again with your client against eu and see whether you find anything else the inhibits the processing?Thanks!RainerAmpersand encoding in message IDs committed as.Problem persists.The thread starts at [1].Now when I click Thread >>, I getNot FoundThe requested URL /mod_mbox/openoffice-users/201401.mbox/<!&;!> was not found on this server.However on the US box I get the message OK at [2]Looks like there is an extra ";" in the URL.Fixing that might fix the other problem.[1][2]NOte, when I use JMeter to download the ajax link, I see:<mail id="%3c!&amp%3b!AAAAAAAAThat does not look correct.AFAIK, "&" is valid in quoted HTML attribute values, so it should only be necessary to replace in with "&amp;" in XML output.(In reply to Sebb from)Further, does it need to be done in XML CDATA sections?(In reply to Sebb from)The updated code seems to replace "&" with "&amp;" and then subject the result to uri encoding (whatever that is).However, not all of the locations where the msgId is used are URIs.The ajax response which was causing the problem is a quoted value in an XML document. Maybe it would be sufficient to just fix the value so it does not contain a bare ampersand? Are there any other values that need to be encoded?Alternatively, fix the & after the URI encoding.Also, I have just thought: URIs are allowed to contain &, so encoding those first may cause problems.I think the encoding probably needs to be context-specific.May I suggest starting with just fixing the ajax output in the msgID value?(In reply to Sebb from)I changed the ampersand encoding to happen after uri encoding. But for me the links look different than for you. Using curl:Retrieving the URLthe broken message shows up as <message linked="1" depth="1" id="&lt;!&amp;!&gt;"> <from><![CDATA[Think]]></from> <date><![CDATA[Mon, 06 Jan, 04:47]]></date> <subject><![CDATA[RE: Folks seeking help]]></subject> </message>and the one working directly before is <message linked="1" depth="0" id="&lt;&gt;"> <from><![CDATA[Timothy Wulf]]></from> <date><![CDATA[Mon, 06 Jan, 03:43]]></date> <subject><![CDATA[Folks seeking help]]></subject> </message>I switched to first uri encoding, then ampersand.I could try, but the only other encoded chars in the example above are "<" and ">" which are also encoded in the working message ids.Done.What do you mean by this "ajax output in the msgID value"?Regards,RainerThe ajax/xxx URL generates XML containing<mail id="%3c!&!A...%3e">on the original sofware. That is the only bit that does not seem to work, so I suggest just fixing that piece of code, i.e. as originally quoted:The currently updated code has got lots of additional changes; it's not clear to me that they are necessary - and some may be incorrect. Even if they do turn out to be required, it makes sense to try and fix one problem at a time.The EU server now works OK for me using Chrome (limited testing), but still hangs in Firefox.I get output starting<mail id="%3c!%26!AAAAAAAAAAwhen I access the URLThis is the one generated from Thread >> as explained previouslyFor the thread?0 URL you used, I get the same result on both EU and US, so clearly the patch has not changed that. But AFAICT that is not the problem area.The original problem output is as stated inNot sure if you have made a further change to the EU server, but it seems to be working OK for me now in both Firefox and Chrome. Or perhaps there was a caching problem previously with my browser?Hi Sebb, thanks for your feedback and tests.The last change I made to the eu mail-archives were on January 9th, 10:51 p.m. UTC. I commited that final change right now asto svn.I tried to debug the root cause as well, but also got problems with browser the cache. The javascript engine complained about non-well formed ajax reponses (the same URL that you provided in) because of an ampersand in the id attribute. But when I retrieved the same content with a simple commandline client it passed the tests for well-formedness and there was the %26 instead of the ampersand.I copied the latest variant of mod_mbox to the us mail-archives now as well (and restarted it).I will close this now, feel free to reopen if you stumble over the same problem again.One thought did occur to me last night - the existing encoding uses hex (%xx), so it would have been better to use the same for the "&", rather than using "&amp;". My bad there.I'm still unsure that all the changes were necessary, but I guess that will be discovered in due course.Unfortunately just discovered another hang in the same mailbox.Start with:Find the first email from Carmen Putrino, and click on that.Then click Next - works OKClick Next again - hangs.It looks as though the e-mail being requested cannot be found [1]However in the thread list one can click on the message from "Think" [2]It looks like the failing URL has been encoded too much (or is not being decoded correctly).I don't have time to look at this further just now.[Perhaps we could discuss on infra-dev how to set up a local test server? There are probably other encoding issues.][1][2]Yes there are probably more ones.This one is due to the client sending a %2F for a slash. By default %2F is disabled for security reasons. To make it work, we would either need to allow %2F in the mail-archives vhost on the proxy in front of mail-archives - which I have just done for eu - or change the JavaScript file distributed to not switch from "/" to %2F in message id encoding. There's two calls to encodeURIComponent() in archives.js, one of which likely is responsible. At least I didn't see the %2F in the message ids contained in responses the server itself sends.	15.0	id=55981	4	False	False	rainer.jung	1
id=55989	REOPENED	None	Apache httpd-2	mod_rewrite (	2.2.22	All Linux	P2 enhancement	Apache HTTPD Bugs Mailing List	2014-01-11 19:21 UTC by	Viktor Szépe	2014-10-31 22:33 UTC (	0 users	Is it possible to write to error log - like mod_authz_host - upon HTTP/403 with RewriteRule?Thank you!	Bugzilla is for reporting bugs, not support or Q&A. If you're suggesting an enhancement for 2.2, please reopen and reclassify.Sorry! I know it.I've just notived recently that fail2ban does not ban RewriteRule [F] requests.And yes, this is a feature request.I'd like to have "client denied by server configuration:" - like in mod_authz_host - written to the error log.I think that this is a one-liner in the source code.Could you please add this error message.Thank you!Using Apache 2.4 you can add custom logging using mod_log_debug.E.g. set your own env var in the RewriteRule as a side effect and then use mod_log_debug to log your chosen message when the env var is set.This feature doesn't exist for 2.2 though.I'm not judging on whether a builtin log statement makes sense w.r.t. consistency with AAA modules.Regards,RainerThank you for the mod_log_debug suggestion!I think enabling another module is the last resort.Is it function call needed to write in the error log?I think is should be in "case 'F':" in mod_rewrite.c.ap_log_rerror(APLOG_MARK, APLOG_ERR, APR_SUCCESS, r, APLOGNO(01630) "client denied by server configuration: %s%s", r->filename ? "" : "uri ", r->filename ? r->filename : r->uri);	6.0	id=55070	10	False	True	naox	1
id=56500	REOPENED	None	Apache httpd-2	Core (	2.2.22	PC Linux	P2 enhancement	Apache HTTPD Bugs Mailing List	2014-05-07 23:03 UTC by	Ninos	2014-05-16 23:12 UTC (	1 user	OS: Debian Wheezy x64Server version: Apache/2.2.22 (Debian)Hey there, I wanted to create a server default page and found following error.At first, I have configuration files like 00_master.conf, domain1.conf.In the 00_master.conf you can find the defined vhost for the admin panel (I'm using an iscp [i-mscp.net]). Here's an example:<VirtualHost SERVERIP:80> ServerName admin.srv01.example.tld (url to the panel) DocumentRoot /var/www/.../panel/ ...</VirtualHost>For the domains I'm using following:<VirtualHost SERVERIP:80> ServerName domain1.tld ServerAlias www.domain1.tld DocumentRoot /var/www/.../domain1.tld/ ...</VirtualHost>No to my problem. As already mentioned I want to add a server default page. So if someone is visiting an url like "doesnotexists.domain1.tld" or "asd.admin.srv01.example.tld" I want to show a default page with an static html output like "Site not found" or something like that.For that I added in the 00_master.conf over the first vhost another vhost, which should be used if no ServerName is defined. I tried with a hardcoded ip and it works fine. Here's an example:<VirtualHost SERVERIP:80> DocumentRoot /var/www/.../default </Directory></VirtualHost>But if I use an wildcard like *:80 or _default_:80 the vhost will not be used. I think it's because of a wrong priority of the vhosts.Best regards,Ninos	This is working as desined -- If there's a wildcard and an exact match IP in the <virtualhost>, the exact match wins and the name-based vhost will be picked from the best IP-based match.I think the design is a little bit wrong or wrong implemented. The vhosts with the wildcard should have a higher priority on a not defined vhost. Here're some examples:EXAMPLE 1Visit:Domain: example1.comIP: IP1Defined vhost:Domain: example2.comIP: IP1Priority:Current: IP1:example2.com -> wildcardShould be: wildcard -> IP1:example2.comEXAMPLE 2Visit:Domain: example1.comIP: IP1Defined vhost:Domain: example2.comIP: IP2Priority:Current: wildcard -> IP2:example2.comShould be: wildcard -> IP2:example2.comEXAMPLE 3Visit:Domain: example1.comIP: IP1Defined vhost:Domain: *IP: IP1Priority:Current: IP1:* -> wildcardShould be: IP1:* -> wildcardWhat I mean is, if all vhosts have a defined ServerName (expect whildcard), the vhosts shouldn't have a higher priority than the wildcard IF the domain can't be matched with a vhost. If someone want's to match all undefined ip-requests to a vhost, he can also use a wildcard for the ServerName. (Servername *,example1.com, *.example1.com).This design would be cleaner and more logical.Not a bug, this is by design. IP/interface based mapping first, name-based mapping second. If you just want name-based, you have to limit yourself to all *:port. Otherwise, ip-based discrimination happens first.I understand the current mapping and also know now, that's not a bug. In my second comment I asked for a little modified design, because for me my example seems more logical. :)(In reply to Ninos from)Reclassifying as an enhancement. I don't think it's well defined here thouh.	5.0	id=55985	8	False	False	covener	1
id=56342	REOPENED	None	APR	APR (	1.5.1	PC All	P2 normal	Apache Portable Runtime bugs mailinglist	2014-04-03 03:43 UTC by	lhh	2015-12-29 20:00 UTC (	0 users	Createdlog fileHi,I'm building apr with minGW-w64 (x86_64-4.8.2-release-win32-seh-rt_v3-rev3) on windows 7 but failed. Attached file is the log, can someone help me? Thanks very much.	Can confirm this with MSYS2/MinGW-w64. I'm getting the exact same build errors as lhh.Error snippet:In file included from C:/msys64/mingw64/x86_64-w64-mingw32/include/objbase.h:66:0, from C:/msys64/mingw64/x86_64-w64-mingw32/include/ole2.h:17, from C:/msys64/mingw64/x86_64-w64-mingw32/include/wtypes.h:12, from C:/msys64/mingw64/x86_64-w64-mingw32/include/accctrl.h:10, from C:/Users/Shane/Downloads/apr-1.5.1/include/arch/win32/apr_private.h:52, from C:/Users/Shane/Downloads/apr-1.5.1/include/arch/win32/apr_arch_dso.h:20, from C:/Users/Shane/Downloads/apr-1.5.1/dso/win32/dso.c:17:C:/msys64/mingw64/x86_64-w64-mingw32/include/objidl.h:10935:9: error: unknown type name 'CLIPFORMAT' typedef CLIPFORMAT *LPCLIPFORMAT; ^From what I can tell, the problem lies in file dso.c when the header accctrl.h (windows API header) is imported by include/arch/win32/apr_private.h . I've tested accctrl.h out in a small test C program and it compiles fine. I'm inclined to believe this is an issue with the way accctrl.h is being included, as the first error reported indicates that the type CLIPFORMAT is undefined, yet this type is defined by wtypes.h (or rather, objidl.h which is included indirectly by wtypes.h), which is included by accctrl.h. I'll do some more investigating tomorrow and come up with a patch if I can find the root cause. My guess is that somewhere, one of the include guards for some header that includes objidl.h is being tripped up by a falsely defined macro.Hi, I just comment out the lines of the following two files, FYI1. mingw64/x86_64-w64-mingw32/include/wtypes.hline #12- #include <ole2.h>+ //#include <ole2.h>2. mingw64/x86_64-w64-mingw32/include/wtypesbase.hline #12- #include <ole2.h>+ //#include <ole2.h>Do source files other than dso.c that include apr_private.h compile successfully?Createdapr-0.patchI've seen this bug as well when building apr_getpass.c. I found what appears to be an error in include/arch/win32/apr_private.h. __wtypes_h__ is defined in there rather than including it. Here is the patch that works for me using mingw 4.9.0.Createdapr-0.patchI posted too soon. The previous patch fixes the error about CLIPFORMAT, but now I've got an error about LPMSG. I'm building apr-1.4.6 with mingw 4.8.2.This patch fixes the LPMSG errors as well. It looks like more things are being put into wtypes.h now, so it needs to actually be included rather than just faked. There may actually be a bug in mingw around the definition and use of LPMSG as including wtypes.h *may* include ole2.h which eventually references LPMSG before it's defined in wtypes.hHi, I can reproduce this, and applying Jon's patch fixes this cleanly.@Jeff passwd/apr_getpass.c does not compile either, because it includes the offending header.I am changing the bug status back to NEW as I don't see any information still is to be provided, and a working patch is here.One should rather define COM_NO_WINDOWS_H in the project to fix instead of adding patches to mingw headers.APR compiles just fine now.Reopening. Nobody needs to look in bugzilla for secret sauce... If COM_NO_WINDOWS_H is the answer that should land in mingw configure.in hints.	8.0	id=55989	5	False	True	covener	1
id=57212	REOPENED	None	Apache httpd-2	Core (	2.2.22	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2014-11-14 05:31 UTC by	Roman Vasilev	2014-11-15 12:29 UTC (	0 users	Apache Internal error with send headers > 8Kb, for example:function randomPassword($len) { $alphabet = "abcdefghijklmnopqrstuwxyzABCDEFGHIJKLMNOPQRSTUWXYZ0123456789"; for ($i = 0; $i < $len; $i++) { $n = rand(0, count($alphabet)-1); $pass .= $alphabet[$n]; } return $pass;}setcookie("test", randomPassword(8173));And this code OK:function randomPassword($len) { $alphabet = "abcdefghijklmnopqrstuwxyzABCDEFGHIJKLMNOPQRSTUWXYZ0123456789"; for ($i = 0; $i < $len; $i++) { $n = rand(0, count($alphabet)-1); $pass .= $alphabet[$n]; } return $pass;}setcookie("test", randomPassword(8172));Difference in one byte.Please, check.	This is a restriction imposed by the core APIs which modules like mod_fcgid use to read the response header (ap_scan_script_header_err_core() and friends).(In reply to Jeff Trawick from)You can fix this? How?It may be possible to recompile everything with a bigger MAX_STRING_LEN, but I can't confirm that.The right solution seems to be new APIs in httpd core, and changes to modules like mod_fcgid to use the new APIs. The new APIs would respect user configuration of higher limits for script header size, just as the LimitRequestFieldSize directive controls something similar for client request headers.#define MAX_STRING_LEN 256(In reply to Jeff Trawick from)How much increase?LimitRequestFieldSize present in Apache 2.2, but does not help :(That's the definition in a couple of utility programs.It would need to be edited in httpd.h to be larger than 8192. It is probably less risky to keep MAX_STRING_LEN and HUGE_STRING_LEN equivalent, so/* old value: #define HUGE_STRING_LEN 8192 */#define HUGE_STRING_LEN 1000010000 assumes that your cookies aren't so big.You'd have to recompile all of httpd, mod_fcgid, and any other third-party modules that use HUGE_STRING_LEN. And I'm not sure if it would all work.Perhaps you can reduce the size of your cookie.Yes, this solved problem. Thanks!(In reply to Jeff Trawick from)Let's leave this open to track a better solution in the future. Many users, such as those that obtain httpd from the OS, aren't able to recompile everything.	7.0	id=56342	9	False	False	lookatyouhacker	1
id=57119	REOPENED	None	Apache httpd-2	mod_proxy (	2.4.7	PC Linux	P2 enhancement	Apache HTTPD Bugs Mailing List	2014-10-20 09:01 UTC by	Broisy	2014-10-23 07:38 UTC (	0 users	Hello everybody,I am using Apache as a reverse proxy to access an application on a server. The application is replicate on two servers. There is a dns-resolver which supplies the ip address of only one of the server depending on which one is running at the moment of DNS name resolving. So when one server is shutting down, the reverse proxy must automatically switch to the second server.When I shut down one server, Apache doesn't switch to the second server.But when I restart Apache, it switches to the second server and it works perfectly.I think the reverse proxy doesn't update its IP addresses cache.Has anyone of you ever had a similiar problem of chached DNS-resolved IPaddresses? Have you found any satisfactory solution? Any additional cluesfor me? Any help would be greatly welcome.Regards Broisy	This reads like an email/question rather than a bug report. Instead of closing it, I've changed it to an enhancement to either lookup DNS for each new backend connection or to periodically do the same.It does cache it for performance reasons. Have you thought about using mod_proxy_balancer instead of changing the DNS entry? Otherwise you can use the disablereuse option () with ProxyPass.Because of my network architecture, I am obliged to use a DNS Server, so I can't use mod_proxy_balancer.I will try to use the disablereuse option.I have seen another solution that consist in changing TTL and SMAX values.Thanks for your answers	3.0	id=57119	6	False	True	covener	1
id=57526	REOPENED	None	Apache httpd-2	All (	2.2.29	PC All	P2 major	Apache HTTPD Bugs Mailing List	2015-02-03 07:32 UTC by	Atul Sharma	2015-04-08 11:13 UTC (	1 user	Hello,Environment Details:Apache: apache-2.2.29_openssl-1.0.1k_x86_64Linux: Linux 2.6.18-400.1.1.el5 #1 SMP Sun Dec 14 06:01:17 EST 2014 x86_64 x86_64 x86_64 GNU/LinuxProxy servers: 34 instances (httpd)Virtual Proxy: 68 instancesProblem:We have recently updated apache from apache-2.2.27_openssl-1.0.1g_x86_64 to apache-2.2.29_openssl-1.0.1k_x86_64 in PROD server (details above). We have stated receiving the semaphore issues.[26/Jan/2015:18:53:28] [Info] [CA WebAgent IPC] [3033] [CSmSem::getSem] Attempted to attach to non-existent semaphore with key 0xc8b9bf3c[26/Jan/2015:18:53:28] [Error] [CA WebAgent IPC] [3033] [CSmSem::getSem] Error creating semaphore using key 0xc8b9bf3c - No space left on device (28)The same was done to QA servers also, but no errors found(varies because of load)Current Configuration:#kernel.sem=<SEMMSL> <SEMMNS> <SEMOPM> <SEMMNI>kernel.sem=512 524288 100 1024 # Controls the maximum shared segment size, in byteskernel.shmmax = 68719476736# Controls the maximum number of shared memory segments, in pageskernel.shmall = 4294967296Question:1. Does apache-2.2.29 have more memory requirement than apache-2.2.27?2. Could you please help us in checking the thread-safety for below modules used in our environement:libmod_sm22.somod_actions.somod_alias.somod_asis.somod_authz_host.somod_autoindex.somod_deflate.somod_dir.somod_filter.somod_headers.somod_include.somod_negotiation.somod_proxy_balancer.somod_rewrite.somod_setenvif.somod_status.somod_logio.so3. Do you consider updating the kernel semaphore on sysctl.conf based on the values shown above?Thanks in advance.Best Regards,Atul Sharma	(In reply to Atul Sharma from)This is a SiteMinder's module error.No it does not.We can't talk about this module which is not maintained by the ASF nor supported in this bugzilla.This ones are thread-safe (when used with a threaded MPM).The values look good for your httpd's use, but it also depends on other software using SysV IPC on the system.The issue is probably more about semaphores which are not cleaned when httpd stops (or is it killed? crashes? libmod_sm22 cleanup?), and hence previous ones stay on the system until no more is available.I'm rejecting this report since it's not an httpd issue, please contact SiteMinder's support.To resolve the same issue increased the amount of semaphores avalible by editing /etc/sysctl.conf and adding the below line, also comment the old setting/etc/sysctl.conf:#kernel.sem=512 524288 100 1024 <--Old valuekernel.sem = 512 1048576 100 2048---- Even this limit is getting crossed now and I suspect some issue of memory leak or somehow apache is leaving many stale connections.Could you please help us in determining the problem.Best Regards,Atul Sharmalsof on 2 different servers varies a lot:4901 vs 43818Even when the lsof values are same the semaphores doesn't gets cleared:lsof values:4781 vs 4529------ Semaphore Status --------used arrays = 498allocated semaphores = 500vs ------ Semaphore Status --------used arrays = 1169allocated semaphores = 1171vice-versa values.	4.0	id=57212	6	False	False	trawick	1
id=57313	REOPENED	None	Apache httpd-2	Core (	2.5-HEAD	All All	P2 normal	Apache HTTPD Bugs Mailing List	2014-12-05 13:25 UTC by	Jon Ribbens	2016-05-16 15:00 UTC (	0 users	The DefaultType directive has been removed. This means that you can no longer set the default MIME type. The DefaultType directive should be added back again. The ForceType directive is not a substitute as it does not set the default MIME type, it sets an override MIME type.	See the discussion at PR#13986OK I read through that 7-year comment thread; it has absolutely nothing to do with this PR except that it also mentions DefaultType. No one in that PR was asking for DefaultType to be removed, merely for it to be optional. Removing it completely is bizarre and removes important functionality, i.e. being able to explicitly set a default MIME type when you know that it is correct.	2.0	id=56918	6	False	False	nephaste91	1
id=57529	REOPENED	None	Apache httpd-2	support (	2.4.10	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2015-02-03 13:29 UTC by	Kamal Das Gupta	2015-02-03 14:32 UTC (	1 user	If we run the htcacheclean utility with '-a' or '-A' option, it doesn't show the cached url.	CreatedPatch for this bug.We use state=fixed to mean it shipped in a release.Sorry...my bad !!	3.0	id=57313	4	False	False	covener	1
id=55070	REOPENED	None	Apache httpd-2	mod_fcgid (	2.4.4	PC Linux	P2 enhancement	Apache HTTPD Bugs Mailing List	2013-06-06 21:09 UTC by	jan	2013-09-01 22:19 UTC (	1 user	mod_fcgid does not listen to per vhosts errorlog directivemod_fcgid loggs to apache error_log using his own format or more exacly complete lack of it:Cannot load the ionCube PHP Loader - it was built with configuration 2.2.0, whereas running engine is API220090626,NTSZend Optimizer requires Zend Engine API version 220060519.The Zend Engine API version 220090626 which is installed, is newer.Contact Zend Technologies atfor a later version of Zend Optimizer.No date, no identification of process, virtualhost, directives. Full wtf.mod_fcgid logs without any regard for LogLevel.mod_fcgid spams apache error_log and should not log to it as it is.	ioncube PHP loader is writing to stderr. stderr is sent to the main error log.Then I guess stderr should not be send to main apache error_log by mod_fcgidThen I guess stderr should not be send to main apache error_log by mod_fcgid - especialy without any formating - date, time, identificationMarking as an enhancement, processes kicked off from fcgid could have their stderr redirected or wrapped by some pipe and related back to some config directive, but not IMO a bug.CreatedA patch to redirect stderr of processes spawned by mod_fcgid to the vhost's error logThe attached patch was tested with mod_fcgid 2.3.7 and apache 2.2.25. It should be simple enough to adapt to newer versions, if necessary at all. It does not wrap the output in any way. Also note that it depends on the order of vhosts at initialization. If any module changes that order, this patch will not write to the correct error log.Comment onA patch to redirect stderr of processes spawned by mod_fcgid to the vhost's error logCan a FCGI app handle requests only on the VH that started it?(In reply to Eric Covener from)As far as I can tell, fcgid_bridge.c:62 should prevent that from happening. No running process with vhost_id different from the request's vhost_id is ever taken into consideration.Createdfixed a bug in the patchThis is a new version of the patch, which fixes a off-by-one error in the previous version. Sadly, I also learned that the patch only works when all virtual hosts have at least one Fcgid* directive in their configuration. Otherwise, the vhost_id of main_server will be used for requests and, thus, errors will be written to the main error log.	8.0	id=57529	5	False	False	kdg.cts	1
id=56918	REOPENED	None	Apache httpd-2	Core (	2.2.29	PC Linux	P2 blocker	Apache HTTPD Bugs Mailing List	2014-09-06 11:26 UTC by	stephane	2014-09-16 14:53 UTC (	1 user	Createdconfig.logusing last 2.2.29 (not 2.2.27) compiling on big Endian errorerror: 'ap_copy_scoreboard_worker' undeclared heregawk -f /raid/SRC/httpd-2.2.29/build/make_exports.awk `cat export_files` > exports.c/raid/data/module/apache/sys/share/build-1/libtool --silent --mode=compile gcc -pthread -O2 -I/raid/data/module/apache/sys/include -DBIG_SECURITY_HOLE -DLINUX -D_REENTRANT -D_GNU_SOURCE -D_LARGEFILE64_SOURCE -O2 -I/raid/data/module/apache/sys/include -lldap -I. -I/raid/SRC/httpd-2.2.29/os/unix -I/raid/SRC/httpd-2.2.29/server/mpm/prefork -I/raid/SRC/httpd-2.2.29/modules/http -I/raid/SRC/httpd-2.2.29/modules/filters -I/raid/SRC/httpd-2.2.29/modules/proxy -I/raid/SRC/httpd-2.2.29/include -I/raid/SRC/httpd-2.2.29/modules/generators -I/raid/SRC/httpd-2.2.29/modules/mappers -I/raid/SRC/httpd-2.2.29/modules/database -I/raid/data/module/apache/sys/include -I/raid/SRC/httpd-2.2.29/server -I/raid/SRC/httpd-2.2.29/modules/proxy/../generators -I/raid/SRC/httpd-2.2.29/modules/ssl -I/raid/SRC/httpd-2.2.29/modules/dav/main -prefer-non-pic -static -c exports.c && touch exports.loexports.c:1682:63: error: 'ap_copy_scoreboard_worker' undeclared here (not in a function)make[2]: *** [exports.lo] Error 1make[2]: Leaving directory `/raid/SRC/httpd-2.2.29/server'make[1]: *** [all-recursive] Error 1make[1]: Leaving directory `/raid/SRC/httpd-2.2.29/server'make: *** [all-recursive] Error 1	I suspect you have an older scoreboard.h in your include path that shadows the one that came with 2.2.29 and contains 'ap_copy_scoreboard_worker' (which is new).Especially suspect is -I/raid/data/module/apache/sys/include which comes before any -I/raid/SRC/httpd-2.2.29/... in your provided libtool snippet. The .../SRC/... seems to be the build path of your new 2.2.29 and /.../data/... might be from some older instalation.indeed you are right...it goes pick up the old header from the Apache 2.2.27 still present in my prefixI tried to rename scoreboard.h and error is gone..but a new old header make another breack in compilation i think i have reinstall apache 2.2.27 and a make uninstall to clean old headersthanks a lot for your help,EDIT : no make uninstall:(((i have remove header manually i thinkhttpd needs to look in its source tree for its includes before looking elsewhere in order to avoid build failures when an incompatible (usually older) level of httpd is in the search path for some library.	4.0	id=57553	4	False	False	trawick	1
id=57978	REOPENED	None	Apache httpd-2	mod_proxy_fcgi (	2.4.12	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2015-06-01 09:21 UTC by	Rainer Canavan	2016-06-28 14:58 UTC (	0 users	ProxyPassMatch or RewriteRules that reference Unix Domain Sockets do not work if unix: is followed by exactly two slashes. Any other number (including zero, resulting in a path relative to the ServerRoot) appears to work. If two slashes are used (as in "unix://"), the first path component is removed. For example, the following two methods to specify an FCGI proxy with unix domain socketsRewriteRule ^\/\/?test.fit$ unix://home/canavan/foo/run/php-fpm.sock|fcgi://localhost//home/canavan/foo/htdocs/engine/test.fit [P,L]ProxyPassMatch ^/(.*)$ "unix://home/canavan/foo/run/php-fpm.sock|fcgi://localhost//home/canavan/foo/testhost/staticwww"result in 503 errors and a message in the error log as follows:[Mon Jun 01 11:12:45.124529 2015] [proxy:debug] [pid 13534:tid 139897419593472] proxy_util.c(2226): [client 127.0.0.1:47978] AH02545: fcgi: has determined UDS as /canavan/foo/run/php-fpm.sock	This is not a bug.General qualified URI scheme is:<scheme>:[ "//" [ <hostname> ] ] <path>Thus, protocol:something - local relative path (implementation-dependent, do NOT rely on this functionality);protocol:/something - local absolute path;protocol://something - unknown path on remote host "something" (commonly considered equivalent to "protocol://something/", i.e. root path on "something" host);protocol:///something - absolute path on unnamed(=local) host (also often seen the more explicit form "protocol://./something"; since FQDN name ends in a dot, a lone dot represents an empty name).I would argue that the (optional) authority part of the URI is obviously not useful for the unix: scheme, and thus unix:/foo and unix://foo should be equivalent.Alternatively, apachectl configtest should reject any configuration that references unix: URIs with an authority.	2.0	id=57785	11	False	False	apache	1
id=59311	REOPENED	None	Apache httpd-2	Core (	2.4.18	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2016-04-12 14:49 UTC by	Michael Kaufmann	2017-01-10 14:40 UTC (	1 user	Apache httpd sends these response headers to HTTP/1.1 clients if the HTTP/2 protocol is enabled:Upgrade: h2Connection: UpgradeThese headers should NOT be sent when SSL is used, because support for HTTP/2 is advertised using ALPN. If the client does not advertise that it supports HTTP/2 with ALPN, then Apache httpd knows that the client does not support HTTP/2, so it should not send these headers.This is a compatibility issue: Some clients behave strangely when they receive these headers, e.g. old versions of NodeJS ()I have tested some popular sites, and none of them sends these headers to HTTP/1.1 clients:-(nghttpx nghttp2/1.10.0-DEV)-(gws)-(tsa_b)-(nginx)-(h2o/2.0.0-beta2)-Note: This has previously been reported here:	CreatedProposed patch: Do not send "Upgrade: h2"From RFC 7540 (HTTP/2), section 3.2:This means that clients must not upgrade from HTTP/1.1 to HTTP/2 when using TLS. It makes no sense that Apache sends an "Upgrade: h2" response header.This has been fixed in trunk in. Proposal for backport to 2.4.x:Thinking about this again, I tested a simple solution using mod_headers. Just add the following to your host config: Header unset UpgradeThis will suppress any 'Upgrade' header in responses from the given host (or all hosts if placed in the base server). Further conditional application is possible using expressions. See mod_headers documentation for details.Please let me know if this works for you.I don't need a workaround, I can compile Apache from the sources and apply+;-)I think a good solution would be that the setting "H2Upgrade on/off" also controls whether the "Upgrade: h2" or "Upgrade: h2c" is sent to the client (depending on the type of the virtual host). I think wrowe would agree on this (see)If you disagree, then please go ahead and close this bug as "won't fix".It's not really HTTP/2 specific, although that is where you currently observer it. So, a better directive would be placed in core, such as ProtocolsHttpUpgradeAnnounce on|offto suppress a response header and ProtocolsHttpUpgrade on|offto disable the whole mechanism (including announce).One other thing this breaks: nginx cached responses. This caused us to rollback our Apache HTTP/2 upgrade because iOS Safari clients were unable to load files from our nginx caches which had been served from our Apache origin.We worked around this at the time with proxy_hide_header Upgrade;The Headers fix is better because it then doesn't even save to the cache.Unfortunately, the commit mentioned inhas been reverted in, so this bug is still present :-(See this thread:Michael, it is nice that you reference my mail. It would be even more helpful if you could answer the question I asked you:As per dev discussion on the list, we'd like to fix this (by default without further config) for the UAs that have problems with it. I asked you for the UA strings (or pattern) where the fix is needed. Unless I overlooked something, you did not give an answer to that.As stated in that mail discussion, Apache httpd will make workarounds for UAs with faults in implementing the HTTP specs. It will however make use of all features of the protocol and expect others to cope with that as well.If you prefer your installations to behave in a non-default way, there are plenty of directives that allow you to do so. In this case, an easy example to suppress the header was provided.Either you can make a case for UA strings where httpd should make a work-around or I will close this ticket as wont-fix.There was a consensus on the mailing list that more discussion is needed about how to address this issue. There was no consensus about what to implement.I have no such list of affected user-agents. NodeJS is affected, but I don't know the affected versions.It is disputed whether sending the "Upgrade: h2" header is a "default". Most other HTTP/2 servers don't send it.	10.0	id=59311	8	False	False	apache-bugzilla	1
id=45210	REOPENED	None	XmlCommons - Now in JIRA	Resolver (	1.x	All All	P2 normal	Commons Developers Mailing List	2008-06-15 11:39 UTC by	Earl Hood	2008-07-18 02:44 UTC (	1 user	CreatedPatch to specify systemId for catalog intput sourceThe systemId of the XML input source of a catalog fileis not set when calling SAX parser. This causes anyrelative external references in the file to resolveagainst the incorrect base (current working dir) versus thecorrect base (location of catalog file).This problem normally leads to a fatal exception bythe SAX parser since the external entity cannot be found.Attached is a patch that supports setting the systemId ofthe catalog input source so relative external entitieswill be resolved against the proper base.NOTE: The classes DOMCatalogReader and TextCatalogReadercurrently have stubs for systemId support: they just callbase readCatalog() method w/o systemId support. Unsureif these classes are even used in practice.	Have you considered the effect of the "relative-catalogs" property?(Woops, accidently closed this when Bz skipped to the next issue.)Are you stating the patch breaks some existing behavior?This fixes a problem with the actual parsing of catalogfiles via the SAX parser.In the project I'm working on, the resolver errors outon XML catalog files that have doctype declarations. For example: <!DOCTYPE catalog SYSTEM "../../../dtd/catalog.dtd> <catalog>...The problem is the resolver does not set the initial baseURI when calling SAX. Therefore, the *SAX parser* willresolve all (XML) entities with a base URI of the current workingdirectory, and NOT the location of the catalog file.Therefore, in the above, the URI "../../../dtd/catalog.dtd"will be looked up by the SAX parser based on the currentworking directory. But, the URI should be resolved basedon the location of the catalog file.The patch basically sets the systemId setting when SAX isinvoked when parsing the file so any relative URI *entity*references in the catalog itself will be resolved againstthe correct base.None of this affects how catalog paths are resolved viathings like <nextCatalog> entries since that is anapplication-specific processing task and not something theSAX parser resolves itself.I have not tried the patch yet, and now away for another week.When you talked about relative references, i thought that you were referring to entries within the catalog. See one of our examples at:[1]The "uri" attributes of the "public" elements do get resolved relative to the catalog.forrest.xcat file.Anyway, now i see that you mean the DTD for the catalog itself.By the way, i think that i am still a bit confused by that "relative-catalogs" property and will attempt again to understand the docs.Note that we use a PublicIdentifier for our catalogs (e.g. [1] above). IIRC the Resolver handles this internally when we do it that way. We don't need the catalog.dtd to be present, and it doesn't bang on the net to try to get it from that SystemIdentifier URL.I will still investigate your patch and do some testing.	4.0	id=60458	8	False	False	covener	1
id=60458	REOPENED	None	Apache httpd-2	mod_proxy (	2.4.23	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2016-12-09 00:37 UTC by	Alexandre Schaff	2017-01-30 12:56 UTC (	0 users	Hello,(Finally :o) Upgrading from 2.2, I have either a regression or the same "behavior change" as commented in.Please update based on following details.This conf is added to fresh 2.4.23 build with minimal set of modules ( mpm worker )# =======BEGINErrorDocument 400 /error/unavailableErrorDocument 401 /error/emptyErrorDocument 403 /error/forbiddenErrorDocument 404 /error/unavailableProxyErrorOverride OnAlias /error "/tmp/error"ProxyPass /error !<Directory "/tmp/error"> AllowOverride None Require all granted</Directory>ProxyPass /ProxyPassReverse /# ========= END"Server" on 127.0.0.1:1234 responds with status 403, no body.The configuration above works as expected.If ProxyPass(Reverse) directives are in a Location block : (rest of configuration unchanged)....# =======BEGIN <Location /> Require all granted ProxyPassProxyPassReverse</Location># ========= END... then an internal requests loop starts until LimitInternalRecursion (10) is reached. Client receives latest 403 response, a status 500 is recorded in access_log.note : the same configuration (using Order+Allow instead of Require ) on a fresh minimal build of 2.2.31 works as expected.++N'Alex.	Createdpotential fixpotential fix, untested. Iterates over server conf to check for ! entries.Are you able to test a patch?(In reply to Eric Covener from)Fix applied and tested 2.4.23.On that single test, the behavior is consistent with 2.2 : no more internal loop.Thanks Eric.Note : component changed from core to mod_proxy.If this affects trunk, we should fold in there and then propose a back port to 2.4Fixed in 2.4.25FYI this caused a regression, it may be reverted in the next 2.4 release. After re-reviewing the PR, a better answer may have been "don't put it in <Location> if you want to have an exception before it.Bug in the fix aside, one of the points of ProxyPass in <Location> is that it's a single proxy rule for a location. So the server-scoped rules, even exceptions, do not "come first".	6.0	id=33411	5	False	False	gregor	1
id=54367	REOPENED	None	Apache httpd-2	mod_dav (	2.4.3	Macintosh All	P2 major	Apache HTTPD Bugs Mailing List	2013-01-02 23:18 UTC by	Wim Lewis	2015-03-10 02:38 UTC (	1 user	The DAV PUT method returns the location of the newly created resource in the Location: header of the response (RFC2616 14.30; RFC4918 13.1). However, Apache does not quote the contents of the newly created path segments when constructing the new Location: URL (RFC3986 2.4). This means that the response can be incorrect if the underlying resource name contains octet-sequences that are significant in URLs (such as "#" or "?") or in the HTTP protocol (such as CRLF) or both (whitespace, etc).Here's an example of a request and response using httpd 2.4.3 on an OS X machine (httpd 2.2.x has the same behavior):Request:PUT /pdav/test%23file HTTP/1.1Content-Length: 8Host: localhostHello!Response:HTTP/1.1 201 CreatedDate: Wed, 02 Jan 2013 22:40:00 GMTServer: Apache/2.4.3 (Unix) OpenSSL/1.0.0jLocation:Content-Length: 71Content-Type: text/html; charset=ISO-8859-1<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN"><html><head><title>Connection closed by foreign host.The file is correctly created in the filesystem as "test#file", but the Location: header in the response is incorrect.PUTting to a URL such as ".../davcoll/foo%0A%0DBar:%20blahblah" may be a way to perform a response splitting attack (a la).	See also <>, which seems like a more general case of Location headers not doing proper URI encoding.In trunk/2.5, Apache simply doesn't return a Location: header for PUTs any more (sigh), but the problem still happens for eg MKCOL. % curl -ik -X MKCOL ''HTTP/1.1 201 CreatedDate: Tue, 23 Apr 2013 00:48:10 GMTServer: Apache/2.5.0-dev (Unix) OpenSSL/1.0.1cLocation:ooContent-Length: 71Content-Type: text/html; charset=ISO-8859-1Can you verify whether the patch applied from 54611 fixes this?Yes 54611 resolved this.The problem still exists under Debian 8 (Apache 2.4.10-8).So I think 54611 fix was not fixing this one(In reply to Juergen Mertens from)I just ran the 'curl' test above against a bunch of recent httpd releases and it appears to be fixed in 2.4.6 and later (working: 2.4.6, 2.4.7, 2.4.9, 2.4.10, 2.4.12, 2.4.13-dev (), and 2.5.0-dev (); not working: 2.4.2, 2.4.4); methods MKCOL and PUT; on OSX 10.9.5.(I did notice that the bug you can see in my earlier report where the response is truncated after the "<title>" is also fixed as of 2.4.12!)Can you give more information on what request you're sending and what you're receiving back from the server?	6.0	id=56500	11	False	True	me	1
id=35959	REOPENED	None	Tomcat Connectors	mod_jk (	unspecified	Other other	P2 enhancement	Tomcat Developers Mailing List	2005-08-01 07:56 UTC by	Sven	2011-10-25 19:03 UTC (	0 users	Hi,people must enable apache's UseCanonicalName-option to make sure, that mod_jksends the servername (and not the http-hosh-header value) to tomcat.It would be nice to leave UseCanonicalName off and instead set a mod_jk specificoption for it.	The servlet spec (@see section 14.2.16) is very specific that request.getServerName() must be the value of the Host header.I feel a bit misunderstood.I'm sure, the Servlet Spec is clear about HTTP-Headers. The HTTP-Headers shouldbe transmitted unchanged to Tomcat, no dought!The thing i talk about is the AJP13 layer. Beside the headers, mod_jk sendsanother field. You talked about it in. You called itrequest.getLocalName()I'd like mod_jk to alway send the servername of apache's vhost no matter whatUseCanonicalName is set to.Addition:I will refer toAJP13_FORWARD_REQUEST contains a field called "server_name". I expect this tothe what you referred to as request.getLocalName().I'd like an option for mod_jk, so that field "server_name" always matches theServerName of apache's vhost - which is not the case AFAIK.This field is meant to implement the new Servlet API call, nothing more, nothingless.Section 14.2.16 doesn't say anything, how this data is obtained - for examplewhether the result of getLocalName() is obtained by a DNS-reverselookup or byusing the host-header or the name of the host like it is stored in tomcat's config.Well, there's no secure source for that _except_ a reverselookup. Thehost-header isn't reliable, so why should the data send by mod_jk be? Well, itisn't, since mod_jk sends the host-header if UseCanonicalName iss off and sendsthe ServerName of apache's vhost if UseCanonicalName is enabled.So where's you're point?Could you please explain your use case? I think it's good, that you can switchthe behaviour with UseCanonicalName, because it makes the behaviour of thereverse proxy consistent with a single directive. Why do you need to haveUseCanonicalName set to off and still want mod_jk to send the data extractedfrom ServerName to Tomcat?UseCanonialName was simply unwanted by me. I didn't want users to be redirectedbetween domains just because they hit one of Apache's redirection.That also causes losing of the session, doesn't it?At the same time, i wouldn't want to clone a rather complicated VirtualHostconfig in Tomcat. Whatever VirtualHost Apache chooses, i want to be sure, whichHost will be used on the Tomcat side. I'd like a clear association, instead oftwo configs which i have to keep in sync.Actually i ask myself, if any Apache-config can be represented by a Tomcat-config.The easiest (and only) way for a clean association between Apache and TomcatHosts was (and still is?), to "UseCanonialName on" on Apache side anduseIPVHosts="true" on Tomcat side.I don't really understand your answer, maybe there are to many aspects in it.Could we start simple: please give a basic setup where your initial statementapplies:"It would be nice to leave UseCanonicalName off and instead set a mod_jk specificoption for it."Please give a concrete example.No response in nearly 4 years.User didn't provide a concrete use case for the feature.(In reply to)UseCanonicalName influences things like redirects on the apache side. As far as I can recall, a redirect will use the canonical name if UseCanonicalName=on. That behaviour of apache can be totally undesired, since obviously the http client would be redirected to another domain and therefor cookies will be invalid and whatnot.On the other hand, UseCanonicalName=on was the only way to make sure that a http-request that matched a certain vhost on the apache site goes to a specific tomcat vhost. With UseCanonicalName=off you would basically have to adjust the tomcat config so that resembles the apache config with its servernames and serveraliases as close as possible. I'm not sure, if the latter is even possible in any case. The apache configuration language might be more powerful than the tomcat's.So with respect to mod_jk, UseCanonicalName=on can be desired while at the same time UseCanonicalName=off can be desired for redirect on the apache side.	10.0	id=57978	4	False	False	anrdaemon	1
id=38471	REOPENED	None	Lenya	Miscellaneous (	Trunk	Other other	P2 normal	Lenya Developers	2006-02-01 10:25 UTC by	Simon Litwan	2007-07-16 03:05 UTC (	0 users	in a lot of forms the form element and the submit/button elements doesn't have aname attribute.	Createdadds name attribut to form elementsadds name attribute to form, submit and button elements.I applied the patch to 1.2.5-dev. Thanks a lot!I'm not closing the bug, since we should apply the changes to 1.4-dev as well.Createdadds id attribut to form elements and name attributes to buttonsadds id attributes to the form elements and a name attribute to input elementstype=submit/buttonI applied the patch. Thanks a lot!I think this patch broke the AC Live and AC Auth usecases in 1.4.Seems there is two sets of name attributes on some of the inputs:<input i18n:attr="value" type="submit" name="addCredential_user" value="Add"name="input-add"/>What is the bug? Does the form and submit elements need the same name attributes?rolled back change on ac.jxplease advise if I have reintroduced bug	6.0	id=57526	8	False	True	atsharma9	1
id=39872	REOPENED	None	Lenya	Build System (	Trunk	Other other	P2 minor	Lenya Developers	2006-06-23 02:39 UTC by	renaud richardet	2007-07-16 02:31 UTC (	0 users		Createdunified diff against headI applied the patch, thanks a lot!Createdadded annotations and regex for package namei tried to guess the meaning of the fields and added annotations. could youreview and extend them according to your intentions?imho this needs to be more elaborate. the rng should enable module writers tofully understand all fields in module.xml.can somebody clarify what the @lenya.version@ macro means? it's not expanded asfar as i can tell.btw, the "package" field seems redundant to me, as the information is alsocontained in the id. is there a scenario where the information in "package"would be important? if not, let's get rid of it, or re-define id to only containthe module name, not the full "path".(In reply to)the regex does not work with the current naming scheme, as it does not allowhyphens. for domain components, hyphens must be allowed, and for the modulename, they are legal but should be discouraged. modules should probably be namedlike classes, i.e. AcImpl rather than ac-impl...(In reply to)there is also the question of how modules are referenced by publications.currently, they use the old "shortname", and it even seems to work, although idon't know why. my gut feeling says we should always use the full module name.(In reply to)We don't have any hyphenated module names anymore. Maybe we should stick tosmall letters.(In reply to)Actually I don't think that placeholders make much sense here. What's thepurpose of a version tag if it is expanded by the build?	8.0	id=35959	16	False	False	william.barker	1
id=57553	REOPENED	None	Apache httpd-2	mod_ssl (	2.5-HEAD	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2015-02-09 15:17 UTC by	Tom Ritter	2016-07-05 11:43 UTC (	0 users	I'm running mod_ssl_ct on 2.4 backport, as described here:I set up a benign configuration, that is, as far as I can understand that shouldn't do anything. Specifically:CTAuditStorage /run/ct-auditCTSCTStorage /run/ct-sctsCTStaticSCTs /etc/apache2/ssl/rittervg-leaf.cer /etc/apache2/ssl/sctsAll directories start off empty, I have two SSL sites enabled, one is ritter.vg the other crypto.is. After running a little bit: - ct-audit gets a couple audit_XXXX.tmp files - ct-scts gets two directories, one with servercerts.pem the other with servercerts.pem and collated.tmp - /etc/apache2/ssl/scts remains emptyI get the following Apache error that causes connection failures in Chrome:(2)No such file or directory: AH02779: couldn't read /run/ct-scts/4c3fbfbac7589ee68d753f806acc822cbd5082c735fdc3fce3924dc32959288f/collated(I get back a Close notify alert, attempt to fallback to TLS 1.1 which fails because I'm running an OpenSSL that doesn't permit that.)	I was able to trace this down a little further, but I'm not terribly familiar with Apache modules. I have two VHOSTs, defined as listening on separate IP addresses. And I confirmed that refresh_all_scts() is only iterating over the first VHOST, although I'm not sure why.Okay, I tracked it down and figured it out. look_for_server_certs() is called multiple times for multiple VHOSTs, but is not set up for that. Specifically, sconf->server_cert_info = apr_array_make(p, 2, sizeof(ct_server_cert_info)); overwrites the initial allocation. (Leaking memory in the process.)I don't know what the 'correct' fix for this, you'd probably allocate one slot and then grow the array on subsequent calls, but I don't know how to do that. I did a simple fix by just putting a if(!sconf->server_cert_info) in front of it and making it allocate 4 slots instead of 2.Thanks for tracking that down. I'll try (again) to catch up with you today or tomorrow.This should be fixed now by trunk revision.The issue I found was that each vhost would not be using its own module configuration (i.e., "sconf" in the previous discussion) if the vhost didn't contain mod_ssl_ct directives. That's an expected core httpd "feature" which makes sense for almost all modules, but it is a problem here because mod_ssl_ct's module config needs to also represent the vhost's certificates, which are not reflected in the mod_ssl_ct configuration. The fix was to create a vhost-specific sconf when reuse of the global configuration is detected.The submitter's suggested fix would also accommodate the current requirement, but I think it is better for each vhost to have its on config in support of future changes.I think this has happened again, but this time on a mixed configuration where some vhosts have SCTs and others don't . My server config is the same, except I have more CTStaticSCTs lines for more vhosts. They point to empty directories.The collated.tmp file is written, but it is never copied to 'collated'. When I manually copy it, server works again.	5.0	id=39872	9	False	True	andreas	1
id=57785	REOPENED	None	Apache httpd-2	Core (	2.5-HEAD	All All	P2 normal	Apache HTTPD Bugs Mailing List	2015-04-01 15:11 UTC by	Nick Kew	2015-11-27 06:22 UTC (	7 users	The non-standard REDIRECT_URL environment variable is introduced in UTIL_SCRIPT in. Although the commit message is not clear, it appears to be intended to serve as a "return to" URL when the server executes an internal redirect.To serve correctly as such, it must return a full URL. Instead, it simply returns r->prev->uri, which is likely to be a relative URL, and may resolve incorrectly in an application.The example we recently encountered was using mod_auth_form, where we needed to patch util_script to generate a variable we could use with SSI in<input type="hidden" name="httpd_location_old" value="<!--#echo var="REDIRECT_URL"-->">.I propose to tidy my patch and apply in trunk. I'll also check whether pr53772 might be low-hanging fruit for this patch.	Backported to v2.4.17.This patch appears to break the prexisting PHP behaviour (all versions, verified as far back as PHP 5.2). When cgi.fix_pathinfo=1, PHP will under some conditions set SCRIPT_NAME directly from REDIRECT_URL (noticed running under FastCGI).I'm not going to argue correct behaviour, only point out that the existing behaviour that was relied by another project has been broken by this change.Dave, thanks for the heads-up. I had hoped in vain that a spell in trunk would pick up any such issues. REDIRECT_URL is a total misnomer for the PHP usage you describe, but I guess there's a lot of history in HTTPD and PHP patching API features.What do you think is the best fix now? 1) Leave it to PHP? 2) Introduce yet another env var? 3) Introduce a flag to switch between the two? 4) Other (please specify?)Heh - don't ask me, I'm not affiliated with either project! From my perspective we worked around the problem by backing out this patch on our 2.4.17 build, but we've been experiencing other issues with 2.4.17 that means we've reverted to 2.4.16 for now while I try to track them down.Obviously the correct behaviour is always preferred, but someone probably needs to give PHP the heads up to sort out their SCRIPT_NAME handling.As a web hosting provider with customers affected by this change, my vote would be to offer some type of option to allow switching between the two, preferably something in .htaccess that is easily managed by the end-customer on a per-directory basis.- ScottI second Scott Neader. Please review what changes you have introduced in Apache 2.4.17 because 2.4.16 worked without problems.Since 2.4.17 we are seeing all kind of issues in websites using mod_rewrite. Mainly, CMS systems in general trying to use pretty permalinks, and specifically WordPress sites using cache plugins are currently unable to correctly rebuild their cache dirs.And since we use cPanel, we are stuck with 2.4.17 with no possibility to roll back to 2.4.16.Ideas are welcome. Also a bugfix (ASAP please).Here at cPanel we are reverting our 2.4.17 release and are going to re-release 2.4.16. Too many issues with mod_rewrite/REDIRECT_URL causing a *lot* of applications to stop working.I can confirm that after downgrading to 2.4.16, WordPress and other CMS went back to normal functioning.I work with WordPress, Joomla and Magento across different server/PHP configurations, and the fixing factor was the downgrade to Apache 2.4.16.Hopefully, this bug may not happen again as it difficulted operations to hundreds of sites.Thanks!I can confirm that this change broke several sites that I manage without me knowing due to automatic updates. Please fix this ASAP!The next 2.4 release will revert back to the old behavior by default.I believe this patch in 2.4.x branch restores previous behavior	11.0	id=40503	7	False	False	andreas	1
id=46650	REOPENED	None	Apache httpd-2	mod_log_config (	2.2.11	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2009-02-02 13:38 UTC by	Gabor M.	2009-07-13 10:57 UTC (	1 user	the following condition is not coming true when the url is rewritted:CustomLog /var/log/apache2/access.log extra env=SCRIPT_FILENAMEso the request is logged only when i call the index.php directly.i'm using these rewrite rules: RewriteCond %{REQUEST_FILENAME} !-f RewriteCond %{REQUEST_FILENAME} !-d RewriteCond %{REQUEST_URI} !=/favicon.ico RewriteRule ^(.*)$ index.php?q=$1 [L,QSA]	but this request is not will be logged:This is not a user support forum. If you're having trouble configuring Apache, you should use the user email list to ask for help. Explain there what you're trying to do, what you've tried, what you think it should do, why you think that, and what it does instead.i see the SCRIPT_FILENAME has a value.LogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\" \"%{Cookie}i\" \"%{Content-Type}o\" \"%{SCRIPT_FILENAME}e\"" extrabut the condition is not coming true:CustomLog /var/log/apache2/access.log extra env=SCRIPT_FILENAMEi think it's a bug.sorry. RewriteEngine On RewriteCond %{REQUEST_URI} !=/favicon.ico RewriteRule ^(.*)$ /index.php?q=$1 [L,QSA]sorry but i reopen this bug again.The logging is don't work if the RewriteRule is _in the .htaccess_ !!!.htaccess: RewriteEngine on RewriteCond %{REQUEST_FILENAME} !-f RewriteCond %{REQUEST_FILENAME} !-d RewriteCond %{REQUEST_URI} !=/favicon.ico RewriteRule ^(.*)$ index.php?q=$1 [L,QSA]apache.conf: CustomLog /var/log/apache2/test.log extra env=SCRIPT_FILENAMEIf you will try access any url except index.php you will be redirected to index.php but the requst will not be logged!	5.0	id=41007	12	False	True	markt	1
id=33411	REOPENED	None	Lenya	Miscellaneous (	1.2.1	All All	P2 normal	Lenya Developers	2005-02-05 20:56 UTC by	Jon Linczak	2007-04-23 08:41 UTC (	0 users	When creating a new document, the create usecase specifies that you will beredirected back to the document ID in the same area from which you started.However, when adding a new document in the info-authoring (or Site) area, youare redirected to the document ID created, but in the authoring area instead. Upon further inspection using DEBUG logs, PageEnvelopeModule is not resolvingthe info-authoring area correctly:DEBUG (2005-02-05) 08:41.40:077 [sitemap.modules.input.page-envelope](/lenya/default/info-authoring/index.html)http-8080-Processor25/PageEnvelopeModule: Resolving page envelope for URL[/lenya/default/info-authoring/index.html]DEBUG (2005-02-05) 08:41.40:091 [sitemap.modules.input.page-envelope](/lenya/default/info-authoring/index.html)http-8080-Processor25/PageEnvelopeModule: Returning [area] = [authoring]	this is not a bug, as the site area is not a real area. you can useinfo-{page-envelope:area}Actually info-* is a valid area also in the Java API.	2.0	id=43746	5	False	False	andreas	1
id=40503	REOPENED	None	Lenya	Miscellaneous (	2.0	Other other	P2 normal	Lenya Developers	2006-09-13 20:19 UTC by	Simon Litwan	2007-07-19 07:14 UTC (	0 users	saving a document with neutron does not seem to work.	I can confirm this. There's no error message, but the changes aren't applied tothe document.I'm using Yulup 0.1.11.Yulup sends a PUT request:PUT /default/authoring/index.xml?lenya.module=neutron&lenya.step=checkin HTTP/1.1i think it's a bug in the neutron module which appeared after the change to uuid.see also the mailthread SourceWritingTransformer and lenya:// protocol from2006-10-11 18:12I'm trying to fix it and migrate it to the usecase framework.Saving should work now.There's a problem, though - when a validation error occurs, it just says"Document could not be saved". This will hopefully not occur anymore whenclient-side validation is available.Renaming Lenya 1.4 to 2.0The problem occurs again.	8.0	id=43050	5	False	False	ivan.izaguirre	1
id=59618	REOPENED	None	Apache httpd-2	mod_proxy_fcgi (	2.4.20	PC Linux	P2 normal	Apache HTTPD Bugs Mailing List	2016-05-23 16:21 UTC by	Jacob Champion	2017-01-08 02:40 UTC (	0 users	Partial duplicate of 50851, but that bug is an omnibus with quite a bit of history behind it and this issue is more easily patched.When using mod_proxy_fcgi with the recommended SetHandler/Proxy architecture: Alias /cgi-bin/ "/usr/local/apache2/cgi-bin/" <Location /cgi-bin/> SetHandler "proxy:fcgi://localhost:4000" </Location> <Proxy "fcgi://localhost:4000"> </Proxy>...the SCRIPT_FILENAME passed to the backend is prefixed with proxy:fcgi: [Fri May 20 12:08:40.299263 2016] [proxy_fcgi:trace8] [pid 21189:tid 139785465222912] mod_proxy_fcgi.c(294): [client 127.0.0.1:51060] AH01062: sending env var 'SCRIPT_FILENAME' value 'proxy:fcgi://localhost:4000/usr/local/apache2/cgi-bin/test-cgi'This makes it difficult to use with general-purpose FCGI backends, since they cannot understand the file path. PHP-FPM appears to have been hard-coded to ignore the prefix as a workaround; see.mod_proxy_fcgi already strips the proxy:balancer prefix; I propose that we add the proxy:fcgi prefix to that logic as well (or refactor the logic so that the internal prefixes aren't considered by the environment variable generation, but that would be significantly more work).	CreatedStrip proxy:fcgi as well as proxy:balancerOne possible approach. Patch was made against 2.4.20.CreatedAn Apache::Test caseHere's a test case for the SCRIPT_FILENAME variable.Regarding the XXX marker in the patch: I couldn't find a way to get the value of a @NextAvailablePort@ from inside the test client. Maybe a new Apache::Test feature is needed? Or we could just hardcode the port number.Thanks Jacob, fixed inand proposed for 2.4.x.Backported to v2.4.21.Need go to back to the drawing board on this one, php-fpm was using this prefix as a hint that this was a non mod_fastcgi / non Action kind of invocation. When its gone, they're using PATH_TRANSLATED instead of SCRIPT_FILENAME because I guess that's how some Action-based mod_fastcgi configs worked.I am tempted to flip the default and just guard it under a directive.	5.0	id=43961	7	False	False	rfrovarp	1
id=43746	REOPENED	None	Lenya	Miscellaneous (	unspecified	Other other	P2 normal	Lenya Developers	2007-10-31 05:03 UTC by	Andreas Hartmann	2007-10-31 07:05 UTC (	0 users		The save request still contains the attribute, so it is removed on the server.It is removed in modules/xhtml/xslt/clean-xhtml.xsl: <!-- Unsupported by the schema --> <xsl:template match="@shape|@target|xhtml:u"> <xsl:apply-templates /> </xsl:template> Why does the XSLT make assumptions about the schema?So the problem lies in the XHTML module.target isn't a valid attribute of, well anything in XHTML 1.0 strict. And uisn't an element in XHTML 1.0 strict either. @shape is valid for area and a, sothat one could be removed.(In reply to)The assumption is to support XHTML 1.0 Strict, which would make sense to me.target isn't a valid attribute of, well anything in XHTML 1.0 strict. And uisn't an element in XHTML 1.0 strict either. @shape is valid for area and a, sothat one could be removed.The Uniklinik Freiburg needs the attribute.Maybe we could add an option which XHTML schema to use(strict/transitional/basic) and choose the schema and clean-xhtml.xsl accordingly?(In reply to)Well, this is a larger issue. They should go through and edit the file, due tothe fact they have a custom non-standard need and should be able to put theirversion of the file in their publication. I need center align and have had tohack my rng files to include it. Anyone who uses FCKeditor or TinyMCE has to hack thesrc/modules/xhtml/resources/schemas/xhtml.rng file to include a larger set ofXHTML. Doing this break BXE unfortunately. Adding an option to easily choose the level of support makes sense. However, thedefault should be a subset of XHTML Strict.	6.0	id=43990	10	False	True	andreas	1
id=43961	REOPENED	None	Lenya	Miscellaneous (	1.2.7	Other OSF/1	P2 enhancement	Lenya Developers	2007-11-26 06:58 UTC by	Thomas Comiotto	2007-12-20 06:52 UTC (	0 users	* Store image dimensions in asset metadata* Clean up after file upload	Createdneutron.js PatchCreatedmeta.jx Patchwe should have a look at this patch and consider it for inclusion in 2.0.1.CreatedStore file extent in .metaMinor fix for meta data handlingAdded the new patch. The new patch did also include changes from the first twopatches! Applied the third patch by hand!	5.0	id=45210	5	False	False	crossley	1
id=43050	REOPENED	None	Lenya	Site Management (	1.2.5	Other Linux	P2 major	Lenya Developers	2007-08-06 12:18 UTC by	Ivan Izaguirre	2007-08-26 05:10 UTC (	1 user	This page is not Valid XHTML 1.0 Strict because missing xmlns attribute forelement html. The value should be:.Ej: <html xmlns="" xml:lang="en" lang="en">I put this attribute in lenya/{pub}/xslt/page2xhtml.xsl but no append this.I try modify the file lenya/xslt/util/strip_namespaces.xsl but this attribute noappend in page code.	New file lenya/lenya/xslt/util/strip_namespaces.xsl of 2007-07-24<!--The XHTML namespace is declared as the default namespaceto cause the stylesheet to output XHTML using the empty stringas namespace prefix (<html> instead of <xhtml:html>).--><!-- $Id: strip_namespaces.xsl 559100 2007-07-24 16:55:19Z nettings $ --><xsl:stylesheet version="1.0" xmlns:xsl="" xmlns=""> <xsl:output method="xml" version="1.0" encoding="UTF-8" indent="yes"/> <xsl:template match="*"> <xsl:element name="{local-name()}" namespace="{namespace-uri()}"> <xsl:copy-of select="@*"/> <xsl:apply-templates/> </xsl:element> </xsl:template> <!-- fixme: this might be generalized to also pass on processing instructionsetc...--> <xsl:template match="comment()"> <xsl:copy/> </xsl:template> <!-- Workaround to prevent the serializer from collapsing these elements, since browsers currently can not handle things like <textarea/> The XHTML serializer currently used by Lenya can not be configured to avoid this collapsing; as long as that is the case this workaround is needed. --> <xsl:template match="textarea|script|style"> <xsl:element name="{local-name()}"> <xsl:copy-of select="@*"/> <xsl:apply-templates/> <xsl:if test="string-length(.) = 0"><xsl:text> </xsl:text></xsl:if> </xsl:element> </xsl:template></xsl:stylesheet>you should not mark a bug "resolved fixed" unless the fix is in the repository.can you create a patch with your fix and attach it here?(In reply to)The fix is en the repository lenya 2.0.x. It's a new filelenya/lenya/xslt/util/strip_namespaces.xsli don't understand. the bug was reported for 1.2.5...what i meant is it should only be marked fixed when it's fixed in the 1.2.5 branch.(In reply to)branch.OK, I'm sorry.Created[PATCH] This page is Valid XHTML 1.0 StrictThis file is present on lenya 2.0.x	6.0	id=44067	5	False	False	rfrovarp	1
id=44067	REOPENED	None	Lenya	Miscellaneous (	Trunk	Other other	P2 major	Lenya Developers	2007-12-12 07:58 UTC by	Richard Frovarp	2009-06-04 08:31 UTC (	0 users	The problem is OneForm and FCK don't translate the URL to UUID for content thatwas uploaded after the browser was started. Or at least content that the useruploaded.To replicate:Open up either editorInsert image dialogUpload new imageInsert that imageSaveIf the image is already there, or if the page is saved a second time, there isno problem. Kupu doesn't have an image upload (that I can find), BXE works fine,and TinyMCE mucks with the URL preventing the URL to UUID translation fromworking to begin with.	I just confirmed:If user B uploads and image while user A is editing a page. User A then insert B's uploaded imagePage is saved without URL to UUID translation.I need to confirm the exact issue, but UrlToUuidRewriter doesn't always have thelatest sitetree data, causing the line factory.isDocument to return false someof the time. I believe this is caused due to the information being stored in thesession. If the sitetree changes after it has been loaded into the session, therewriter can't properly act on the updates.This could be fixed in. It would be great if you could test it, TALIA!Seems to be fixed, I can't reproduce the problem anymore. Please reopen if it persists.I'm still able to reproduce.CreatedPatch to kupu config to filter non-basic XHTML tags.(In reply to)OOPS sorry, attached to wrong bug number. Not sure how this happened.Please ignore.	7.0	id=44312	5	False	True	markt	1
id=44312	REOPENED	None	Tomcat 6	Catalina (	6.0.14	PC Linux	P2 enhancement	Tomcat Developers Mailing List	2008-01-28 08:31 UTC by	Jan Bielik	2014-06-23 18:39 UTC (	1 user	On a system running tomcat 6.0.14 we encountered the problem, that the specifieddocBase of the default Host of an Engine is being overwritten, when an Aliaswith the same name (as the default Host) is defined on a different Host of thesame Engine.The user should be warned if the docBase of the default Host is being changed tothe one of an Alias.When using tomcat 6.0.13 this problem didn't occur; means that the docBase ofthe default Host remained the same for an equal configuration.Are there any future plans on providing such a warning?Best regards, Jan	There are so many ways to do bad things with the configuration files I am notsure we could reasonably catch them all or should even try.That said, if you want to provide a patch it will be considered. Keep in mindthat the more invasive the patch, the less likely it is to be considered.I guess this only thing needed is to utter a "SEVERE" warning whenorg.apache.tomcat.util.http.mapper.Mapper.Context.extensionWrappers are createdthat overwrite a pre-existing one...No patch has been provided for consideration.Reopening.From looking into this, I think messing up host and alias names can really be a surprise for a sloppy admin.In the Mapper class the hosts and aliases are in the same list. Duplicates are silently prevented by Mapper.insertMap() with the "first come, first served" behaviour. The results may be surprising.I have a patch for this.A test case:Configure the following hosts in server.xml:[[[ <Host name="localhost" appBase="webapps"> <!-- Alias equal to the name of its own Host is OK. --> <Alias>localhost</Alias> </Host> <Host name="second" appBase="secondwebapps"> <Alias>localhost</Alias> </Host> <Host name="third" appBase="thirdwebapps"> <Alias>fourth</Alias> </Host> <Host name="fourth" appBase="fourthwebapps"> <Alias>localhost</Alias> <Alias>second</Alias> </Host>]]]There is also a bug in Mapper.removeHost(name):If host with such name is not registered in the mapper, it results in NPE at "if (newHosts[i].object == host)" line.(The find() method returns the closest match. To check that the correct item was found, one has to compare names).Fixed in Tomcat 8 byand will be in 8.0.10 onwards.(In reply to Konstantin Kolinko from)For a record:The bug in removeHost(), removeHostAlias() that was fixed here is more serious than just an NPE.The bug is that those methods may remove a wrong host.They remove whatever host is located at hosts[pos], but the find() method does not return the exact name match, but closest name.Fixed in Tomcat 7 byand will be in 7.0.55 onwards.Created2014-06-23_tc6_44312.patchPatch that will be proposed for Tomcat 6.TestMapper includes test case for(testContextListConcurrencyBug56653)	8.0	id=45879	9	False	True	markt	1
id=45879	REOPENED	None	Tomcat 6	Native:Packaging (	6.0.18	PC Windows XP	P2 enhancement	Tomcat Developers Mailing List	2008-09-24 08:29 UTC by	Sebb	2008-09-25 05:33 UTC (	0 users	The Windows service installer fails to install the NOTICE and RELEASE-NOTES files in the target directory.	Release notes are included in the ROOT webapp and the docs webapp.The NOTICE file is only in the docs. It should probably be moved to the install dir to sit alongside the LICENSE fileIt would be useful to have RELEASE-NOTES in the top-level directory as well.This what is done in the binary archive.The webapp RELEASE-NOTES.txt files are not as easy to find when trying to establish what release a directory structure contains.The NOTICE file has been moved in trunk and the fix proposed for 6.0.xTest of e-mail address change - please ignore.The fix has been applied to 6.0.x and will be in 6.0.19 onwards.What about the RELEASE-NOTES file (not RELEASE-NOTES.txt)?This should be in the same directory as NOTICE and LICENSE - as is done in the binary zip file.The release notes are displayed as part of the install. They don't have to be in the install dir.Nevertheless, please could the RELEASE-NOTES be included in the top level directory?It makes it much easier to determine which version of Tomcat is installed in a particular directory.This is not available from the default installation directory name - e.g. Tomcat 6.0 - which does not include the full version number.	8.0	id=45931	7	False	True	markt	1
id=46411	REOPENED	None	POI	HSSF (	3.5-dev	PC Windows XP	P2 minor	POI Developers List	2008-12-17 04:04 UTC by	Steven Butler	2016-10-11 14:32 UTC (	0 users	In Excel 2003, when a TIME(h,m,s) value is formatted as a date, or includes a date component. For example, when TIME(12,0,0) is formatted with the format string "yyyy-mm-dd hh:mm:ss", the date is shown as1900-01-00 12:00:00 in Excel.This is obviously not a correct date but it is what Excel does with the value.POI formats the value as1899-12-31 12:00:00which is a valid date, but not the same as the value produced by Excel.	The same issue for Excel 2010. Moreover if you write this date back to Excel - it won't set correct date (because Excel doesn't handle dates before 1900).Thus when you just read a value and write it back without changing - you get broken data. Can it be considered as a bug? Shouldn't POI handle such cases?My comment isn't strictly about the initial issue but is very close. Decided to put it here.	1.0	id=46938	8	False	False	yegor	1
id=46938	REOPENED	None	POI	HSSF (	3.5-FINAL	PC Windows XP	P2 normal	POI Developers List	2009-03-30 07:11 UTC by	Maksym Symonov	2009-11-16 05:36 UTC (	1 user	After opening existing workbook with some created styles in Excel and applying them to new cells, and then creating any new style by calling HSSFWorkbook.createCellStyle() all existing styles lose their foreground colors. Seems like they are painted with default palette colors.After code exploring of createCellStyle() it appears that one new ExtendedFormatRecord is created and added to HssWorkbook.workbook.records, where palette record is stored. So somehow existing palette is overrided or hided by default.	Createdtest xls file for a test caseInputStream in = null; OutputStream out = null; try { in = new FileInputStream( new File("D://foo.xls") ); HSSFWorkbook workbook = new HSSFWorkbook( in ); HSSFSheet existingSheet = workbook.getSheet( "existing" ); HSSFCellStyle style = existingSheet.getRow( 0 ).getCell( 0 ).getCellStyle(); HSSFSheet sheet = workbook.createSheet( "test" ); workbook.createCellStyle(); sheet.createRow( 0 ).createCell( 0 ).setCellStyle( style ); workbook.setActiveSheet( 1 ); out = new BufferedOutputStream( new FileOutputStream( new File("D://new.xls") ) ); workbook.write( out ); } catch( Exception e ) { e.printStackTrace(); } finally { IOUtils.closeQuietly( in ); IOUtils.closeQuietly( out ); }Createda file generated by the posted code that doesn't exhibit the problemI can't reproduce the problem. What version of POI and Excel are you using? I'm looking at new.xls created by your sample code and all styles are there. A1 cells both on the existing and new sheets have the same style with white foreground. I attached the generated file. Please confirm that the foreground is lost. Tested with trunk and Excel 2003.Regards,YegorYegor, problem is in row:workbook.createCellStyle();as you see created style isn't set to any of cells. Please, try commenting this line in example code and look into results generated. Both cells A1 on on both sheets should have light blue foreground as in existing.xls file which is some kind of template for generating new.xls. If this row is commented all works fine, style is fully copied to A1 on "test" sheet, if it is not both cells loose their color and it becomes white.After some more code exploring it was found that palette from original existing.xls file after it is loaded by POI is stored at HSSFWorkbook.workbook.records in a record of class UnknownRecord which has toString() interpretation[XFEXT] (0x87D) rawData=[7D, 08, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 3E, 00, 00, 00, 03, 00, 0D, 00, 14, 00, 03, 00, 00, 00, 01, 00, 00, 00, 04, 5F, 00, 2E, 00, 5F, 00, 2D, 0E, 00, 05, 00, 02, 04, 00, 14, 00, 02, 00, 00, 00, E7, EC, F4, FF, 23, 00, 30, 00, 2E, 00, 30, 00][/XFEXT]needed light blue color is in this record in a rawdata field E7, EC, F4Createdfoo.xls opened in Excel 2003Excel 2003 does not recognize custom palette saved in the compatibility mode in Excel 2007. The foreground in A1 should be light blue.Createdfoo.xls opened in Excel 2007Maksym,The problem seems to be specific to Excel 2007. Did you create the template in Office 2007 and saved in the compatibility mode? Were there any warnings? If I open foo.xls in Excel 2007 then the foreground is light blue. If I open it in Excel 2003 the foreground is white. It's not a bug of POI, rather a "feature" of Excel. When saving custom colors in the .xls format, Excel 2007 does not translate them into the standard palette. That's why the foreground is not recognized by Excel 2003.The [XFEXT] (0x87D) record is not a part of the BIFF8 specification. It's a new stuff introduced by Excel 2007. If you are fancy to decode it - patches are welcome. Otherwise, I would recommend you to create your templates in Excel 2003. This way it should always work.Regards,YegorNot related to HSSF, but the color palette itself can be changed in Excel 2007 to work with earlier versions of Excel (and the current version of HSSF).This way you don't need Excel 2003 to create your color templates. The process is explained in Microsoft support:Hi, I ran into the same problem and must agree with the previous speakers, this "bug" is an unsupported behaviour of Excel 2007, not an error of POI.However, the situation is extremely disappointing for me, since I'll have to support Excel 2007 for editing templates - and colors are very important for some customers.After a weekend of tracing the serialization of records and trial+error I tried the following hack:using POI 3.5-FINALWorkbook.class, line 812public ExtendedFormatRecord createCellXF() { int insertPos = records.getXfpos() + 1; if (insertPos < records.size()) { while (records.get(insertPos) instanceof UnknownRecord) { insertPos++; if (insertPos == records.size()) { break; } } } records.add(insertPos, xf); records.setXfpos( insertPos ); numxfs++; return xf;}Obviously this is rather a hack than a solution. It seems(!) to help.And if so, this workaround could be useful for many people using POI.I'd be pleased if some of the gurus could check if this "happy reordering" is in conflict with any known rule of the excel format and thus may be discouraged.Especially testing with different excel versions (I only have 2007) would be important.Thanks!Regards.KarlUh-oh! I overlooked a method. public ExtendedFormatRecord getExFormatAt(int index) { int xfptr = records.getXfpos() - (numxfs - 1); ... }With my patch from the last post I created a "hole" in the table which breaks the index. I'll have to think about that, maybe I can fix that, too.CreatedKeep Excel2007-colors patchKeep color palette for following situation:- create Excel workbook (.xls) with Excel 2007- modify this workbook with POI- reopen the workbook with Excel 2007Finally, my results regarding this issue are a little disappointing.The main problem cannot be fixed because the color models of Excel 2007 is simply incompatible to that of earlier versions.There are several discussions in the web, for example:Especially the last link seems to be interesting for template creators to completely avoid the problem in advance.However for my situation the attached modified Workbook-class helped.We use Excel 2007 but old Excel .xls-format (NOT .xlsx). This is far from being perfect, but I did not have the time yet to migrate to XSSF. The attached patch is experimental but I think it works.The idea is to save the positions of UnknownRecords containing the new color information, so that Excel 2007 will show the original colors when reopening a POI-modified workbook instead of replacing them with "similar colors". Unfortunately earlier versions of Excel (I tested with Excel 2000) will still show replacement colors, if a workbook contains "new color-model colors".Regards.KarlRemark:Today I tested (successfully) with a more complex excel sheet and found out that the color-palette corruption problem may return if you modify cell styles that came with the original file. Strange ...To avoid this do not modify original cell styles but copy them (only once(!), you do not have to duplicate them for single usage).Problematic:HSSFCellStyle myStyle = workbook.getCellStyleAt(idx);myStyle.setFillPattern(HSSFCellStyle.BIG_SPOTS);myStyle.setFillForegroundColor(HSSFColor.WHITE.index);myStyle.setBorderLeft(HSSFCellStyle.BORDER_THIN);//...//use myStyle n-timesBetter:HSSFCellStyle myStyle = workbook.createCellStyle();myStyle.cloneStyleFrom(workbook.getCellStyleAt(idx));//...//use myStyle n-timesRegards.Karl	13.0	id=47570	9	False	False	yegor	1
id=43990	REOPENED	None	Lenya	TinyMCE Integration (	Trunk	Other other	P2 critical	Lenya Developers	2007-11-29 02:49 UTC by	Andreas Hartmann	2009-07-15 13:56 UTC (	0 users	The OneForm editor inserts images with absolute URLs instead oflenya-document:{uuid} URLs:<img src="/default/authoring/doctypes/logo.png" title="Logo" alt="Logo"width="199" height="63"/>Non-proxy environment, standard Jetty.	hmm, yes. i figured that when we're in edit mode, we need real URLs so thateditors can display images (does not strictly apply to oneform, but i'm stilldreaming of the Grand Unified Editor Usecase In The Sky). i thought thaturl2uuid conversion was handled in the post-processing, but it seems it isn't...(In reply to)Yes, that's OK from my POV.I guess the OneForm editor doesn't have such a post-processing yet. Do you havethe time to take a look at it?Same problem in tinyMCE.The bad aspect of this bug is, that if it's fixed later (after 2.0), a content migration from <img src... links to uuid-links would be necessary....I really hate to say this, but isn't this a blocker? Insert an image using oneform, publish, view under live. You'll end up with this as the URL in the live page:If you aren't proxying the authoring URLs through from live, the image won't bevisible from live. I've seen FCK do this. For some reason with FCK, just goingback to the page and hitting save fixes the problem. Not sure why it doesn'talways do the translation from path to uuid.(In reply to)migration from <img src... links to uuid-links would be necessary....Saving the page would do the content migration. That's how it works with BXE,and usually FCK. That's how I fix the issue when FCK doesn't cooperate.Should be fixed for the OneForm editor, at least for non-proxy, root-contextenvironments. Please test. TIA!(In reply to)Does the image insertion code generate proxy-based URLs? If yes, we have to addanother processing step when saving the file (IncomingLinkRewriter).(In reply to)Should now also work for proxy environments.TinyMCE isn't updated yet. Any volunteers? :)Should be fixed for TinyMCE too, needs to be tested.(In reply to)In TinyMCE the image url I see is still the non-uuid one (after a switch to the trunk and a build clean-all)(In reply to)confirmed here(In reply to)IIUC TinyMCE should indeed show the non-UUID URL, but after saving it should bereplaced with the UUID-based one. Can you confirm this, or is it saved withnon-UUID URLs? BTW, you can just use the .xml extension in the browser URL boxto get the document source.No, I can't confirm this - in the page source, the non-uuid reference is still used.Unfortunately I can't test it at the moment, I always get "onclick attribute notsupported" errors when I save.I just tested and see the problem. TinyMCE keeps rewriting the image url tosomething like this: index/test.jpg?lenya.module=svg&height=120&width=160When what is needed for the matcher is this:/default/authoring/index/test.jpg?lenya.module=svg&height=120&width=160So the transform may be running, but it doesn't match and you can't forceTinyMCE to have the proper URL as it keeps changing it to the first one.ui - think I found the option that is needed for this purpose:TinyMCE can be configured to use absolute image urls:I just gave it a try by inserting relative_urls : false, into tiny_config.js;It seems to work (link gets rewritten @ save) - but I'll have to do more testing before committing this.Mmmmh - setting relative_urls to false seems to work for image links but breaks internal links to other lenya pages (the end up having links in the page source like: /default/authoring/lenya-document:5d353320-a8ca-11dc-96fc-db94b5998549,lang=en).There is also:This looks to be cleared up with the possible exception of TinyMCE. Could a user of that editor comment?(In reply to)TinyMCE seems to be currently broken - I tried to get it running by removing the (deprecated) menu.xsp and create a menu entry in xhtml/config/menu.xml like this:<item uc:usecase="tinymce.edit"><i18n:text>With Tinymce</i18n:text></item>results in a (usecase) error.Reading the svn-history of the tinyMCE module, I think, that the current state still is: links have lenya-document:{UUID} syntax, image src attributes don't.(In reply to)... got tinymce running againImage references are still non-uuid:<img src="name.jpg" />What about the resulting saved source? When they're inserted they go in as non-uuid, but they should be converted to uuids when saved. If you change the extension of a file from html in your browser to xml you'll see the saved source. Or you can always look on the filesystem as well.(In reply to)the source is non-uuidWe could add a transformation step to transform the relative to absolute URIs:Fixed in. It would be great if someone could find the time to verify the fix. Thanks!I just tested the uuid image link behaviour inserting an image as child of features. Here are the result in the xml of the edited page. IIUC the behaviour is still broken.The first time a freshly uploaded (from within timymce) image is inserted, non uuid style:<img alt="test-image" height="133" src="/default/authoring/features/test-image.jpg" width="250"/>The second time the same image is inserted, things look correct:<img alt="test-image" height="133" src="lenya-document:a86b2740-3d7b-11de-a5c8-868a4fd960e2" width="250"/> When a page is reopened in tinymce svn parameters are added:<img alt="test-image" height="133" src="lenya-document:a86b2740-3d7b-11de-a5c8-868a4fd960e2?lenya.module=svg&amp;height=133&amp;width=250" width="250"/>Does any of the TinyMCE users have the time to take a look at this issue?(In reply to)The problem is that when the textarea for TinyMCE is filled, not the document source but the actual rendered page from the publication is used. This has several downsides:* <object> is converted to <img>* the svg module params are added* …IMO the whole integration is a hack, but I don't see an immediate remedy :(I just did some prototyping: Include the source instead of the assembled page. Seems to work fine if the source contains <img> as opposed to <object> elements.Index: sitemap.xmap===================================================================--- sitemap.xmap ()+++ sitemap.xmap (working copy)@@ -36,6 +36,13 @@ <map:pipelines> <map:pipeline internal-only="yes">+ + <map:match pattern="content.xml">+ <map:generate src="lenya-document:"/>+ <map:transform type="uuid2url"/>+ <map:transform src="xslt/extractContent.xsl"/>+ <map:serialize type="xml"/>+ </map:match> <!-- when editing, the page should look exactly like the original, and since we cannot know anything about the pipelines used for rendering, we must@@ -104,6 +111,7 @@ <map:call resource="style-cms-page"/> </map:otherwise> </map:select>+ <map:transform type="include"/> <map:transform type="i18n"> <map:parameter name="locale" value="{request:locale}"/> </map:transform>Index: xslt/page2edit.xsl===================================================================--- xslt/page2edit.xsl ()+++ xslt/page2edit.xsl (working copy)@@ -3,6 +3,7 @@ xmlns="" xmlns:xhtml="" xmlns:i18n=""+ xmlns:i="" exclude-result-prefixes="#default i18n" > @@ -212,7 +213,10 @@ <xsl:choose> <!-- firefox bug workaround: prevent <textarea/> from collapsing if empty --> <xsl:when test=".//*">+ <i:include src="cocoon:/content.xml"/>+ <!-- <xsl:apply-templates/>+ --> </xsl:when> <xsl:otherwise> <xsl:text>&#160;</xsl:text>Rainer Schoepf: you could use the cleanup function in tiny_config.js to convert between <object> and <img>	29.0	id=46411	5	False	False	josh	1
id=47570	REOPENED	None	POI	XSSF (	3.10-dev	Macintosh All	P2 normal	POI Developers List	2009-07-23 18:06 UTC by	David Agnew	2014-06-18 20:06 UTC (	0 users	Use Mac Excel 2008 to create a workbook with a formula.Run a POI-based program that reads in the workbook, deletes the sheet containing the formula, creates a new sheet, and writes the result out as a new workbook.Mac Excel 2008 opens the new workbook fine, as does NeoOffice and the Finder's built-in file viewer. However, Excel 2007 on Windows complains of unreadable content and offers to remove it. It then reports removing the following:<?xml version="1.0" encoding="UTF-8" standalone="yes" ?> - <recoveryLog xmlns=""> <logFileName>error005320_01.xml</logFileName> <summary>Errors were detected in file 'D:\DynSQL\FRx\IO_Data\workbook.xlsx'</summary> - <removedRecords summary="Following is a list of removed records:"> <removedRecord>Removed Records: Formula from /xl/calcChain.xml part (Calculation properties)</removedRecord> </removedRecords> </recoveryLog>after which it successfully opens the file.If you modify the POI-based program to delete all the content in the sheet to be deleted before deleting the sheet, the problem still occurs. However, if you also set all the cell formulas to null, then the problem disappears.	I can't reproduce it with files created in Excel 2007.Could you please attach the following data: - source .xlsx file created by Mac Excel 2008 - Java code that modifies it - output file that is unreadable by Excel 2007Yegorno response in a long time => resolving for now, please reopen with more information if this is still an issue(In reply to Dominik Stadler from)I can confirm this issue is still happening and I'm using 3.10-beta2.I've attached a very simple test to verify this issue using Groovy.Basically, test.xlsx is the original spreadsheet with no problems and contains two very simple formulas in cells A1 and A2. testOut.xlsx is the output after running sheet.createRow(0) on test.xlsx.What's expected in testOut.xlsx is the first becomes blank whilst second row still has the formula from cell A2.What actually happens is that an error from Excel saying:"Excel could not open testOut.xlsx because some content is unreadable. Do you want to open and repair this workbook?Selecting open and repair fixes the problem and the "Review Log File" from Excel contains this:<?xml version="1.0" encoding="UTF-8" standalone="yes"?><recoveryLog xmlns=""><logFileName>Repair Result to testOut 03366.xml</logFileName><summary>Errors were detected in file 'Root:Development:tmp:testOut.xlsx'</summary><removedRecords summary="Following is a list of removed records:"><removedRecord>Removed Records: Formula from /xl/calcChain.xml (Calculation properties)</removedRecord></removedRecords></recoveryLog>I also found related posts on previous POI mailing list:A reply to this suggested "workbook.onDeleteFormulaCell wasn't being called"... not sure if it's related.I also verified that instead of sheet.createRow(0), I do, sheet.removeRow() works and does not produce the error.Createdtest script in groovyCreatedtest input to scriptCreatedtest output to scriptHello,I'm having the same problem with all the latest version I tested.Do you have any feedback on this issue?Thanks in advance	7.0	id=50562	11	False	False	wrowe	1
id=48358	REOPENED	None	Tomcat 6	Jasper (	unspecified	All All	P2 enhancement	Tomcat Developers Mailing List	2009-12-09 06:58 UTC by	Isabel Drost	2014-02-17 13:54 UTC (	0 users	CreatedPatch including tests that fixed the problem for us.Currently Tomcat does not support unloading JSPs. When constantly changing and reloading JSP files (especially during development time) this causes the JVM to run out of memory.The patch fixes this problem by tracking the last time a JSP page was requested. Objects are destroyed if a configurable number of JSPs is live, starting with the "oldest" ones. This behaviour is deactivated by default. It must be configured explicitly.The patch comes with tests that check the added functionality. To make testing easier I added easymock as dependency to the classpath.I'd appreciate any feedback on the code changes - suggestions for improvement, potential problems with the code etc.This work was done in collaboration with some of my colleagues at work. I will forward a link to this bug entry to those involved so they can provide more information on the background of the patch if needed.On a side note: Just in case you might like to include the patch - I couldn't find a "patch intended for inclusion" check box (like the one in jira) in bugzilla - thus stating explicitly: License to ASF granted for inclusion in ASF works (as per the Apache License §5).	Hi Isabel,feature looks fine for me :-) Many Thanks.But some comments...- Patch missing message jsp.warning.maxLoadedJsps at java/org/apache/jasper/resources/LocalStrings.properties- The oldest JSP search seams not cheap. Some CMS-Sites have more 30000 active JSP's!- Have you tested your JSP unloading with heavy load?PeterAs a point of clarification, if a JSP is changed then the old implementation should be unloaded before the new implementation is loaded. Therefore, lots of changes to a small number of pages shouldn't cause an issue. If such a scenario does cause an issue then that is a bug and I would ask that you please open a separate issue.This enhancement appears to be addressing the use case where an application consists of many thousands of pages that are rarely used so, in an effort to conserve resources, pages that have not been accessed for a while are unloaded. I can see how this could be useful in development, particularly if memory is tight on an individual developer's machine. In production, I think it would be easier (and give better performance) to spend few hundred dollars on some extra memory for the server.I also share Peter's concerns about the cost of the oldest JSP search and think that this part of the patch needs to be revisited.I agree - will look into that ASAP - Thanks pointing out the problem.CreatedPatch including tests that adds support for selectively unloading jsps.The patch includes changes to the way of identifying the jsp(s) to destroy: If unloading is activated, the JspRuntimeContext tracks the relative "age" - in terms of last execution time - of each jsp.Age is tracked in a queue that supports additions, updates and removal of its nodes in constant time.If unloading is activated this gets rid of the costly loop for identifying candidate jsps. It should incur only slight overhead at jsp execution time.We ran initial smoke-tests against artificial setups as well as load tests against a "real-world" web application with user requests extracted from log files on our test systems. So far everything looks good. Further tests are currently being run. Will report the results as soon as they are available.You are right: 1) My explanation was wrong in this respect, 2) re-loading existing jsps this doesn't cause an issue.The refined patch was tested with some 400 requests per second (up to 700 max) - all looked fine.Mark, do you have any comments on the refined version of the patch? Anything in particular I should clarify?(In reply to)We're using the patched version of tomcat for all *.lecker.de and *.wunderweib.de domains, i.e..We're talking about appr. 20Mio PI/month over all portals. Might not be heavy load, but it's pretty high load.The patch made operation of the portals much easier and reduced the number of forced restarts of tomcat to one scheduled restart every night.Prior the patch we were heading towards 10-20 restarts of each of the two load balanced tomcats each day, resulting in temporarily unavailability of the whole site from time to time, because both tomcats have been automaticaly restarted in parallel ...Thanks for the patch. It has been applied to 7.0.x and will be included in 7.0.0.I had to make a few changes:- correct indentation- add documentation- add AL2 headersAnother Thanks goes to Christian Lorenz for helping me understand the Tomcat sources and integrate the changes.Any chances that this patch might make it into the 6.0.x line?Thanks for the reminder.The patch has substantially changed since it has originally been applied to Tomcat 7. The updated patch for TC 6 is available atThe patch itself is low risk because the feature is off by default. Committers will have to decide whether we want to keep the feature as a new feature in TC 7, or whether the problem addressed is important enough to warrant applying it to TC 6. I proposed it for backport.Excellent, Rainer!Excuse my ignorance, if this is somehow obvious from the patch itself (this'll be my first patch): Against which version can this be applied? 6.0.29? 6.x trunk from SVN?Thanks,ChrisAs far as I remember it should apply cleanly to 6.0.29 and 6.0.x trunk (as of now). Try it and shout if it doesn't work.Tha patch applies cleanly to 6.0.29 from the src archive.However it does not work for me the way I expected it and I am still facing PermGen Out Of Memory errors.- The counters in the JspMonitor MBean for jspCount, jspReloadCount, jspUnloadCount, jspQueueLength were updated appropriately- When running with -XX:+TraceClassUnloading I did not see any org.apache.jsp.* classes being unloaded until I shut down the Tomcat process- Looking at a Heap Dump written at the time of the crash, I saw 337 instances of org.apache.jsp.xxx classes being held while I configured the maxLoadedJSPs to be 200Are there any additional settings that have to be adapted? My current settings are:Java 1.6 32bit on Windows 2003 Server with the following settings (memory parameters are set to small values to shorten the time until tomcat crashes)-Xms256m -Xmx256m-XX:MaxPermSize=48M-XX:+UseConcMarkSweepGC-XX:+CMSIncrementalMode-XX:+CMSClassUnloadingEnabled-XX:+CMSPermGenSweepingEnabled-XX:+PrintGCDetails-XX:+PrintGCTimeStamps-XX:+HeapDumpOnOutOfMemoryError-XX:HeapDumpPath=E:\logs\wcmInternet\ContentProxy1\tomcat-Dorg.apache.jasper.compiler.Parser.STRICT_QUOTE_ESCAPING=false-Dorg.apache.jasper.runtime.BodyContentImpl.LIMIT_BUFFER=true-Dorg.apache.jasper.runtime.BodyContentImpl.USE_POOL=false-XX:+TraceClassUnloadingGlobal web.xml: <servlet> <servlet-name>jsp</servlet-name> <servlet-class>org.apache.jasper.servlet.JspServlet</servlet-class> <init-param> <param-name>fork</param-name> <param-value>false</param-value> </init-param> <init-param> <param-name>development</param-name> <param-value>true</param-value> </init-param> <init-param> <param-name>enablePooling</param-name> <param-value>false</param-value> </init-param> <init-param> <param-name>modificationTestInterval</param-name> <param-value>0</param-value> </init-param> <init-param> <param-name>genStringAsCharArray</param-name> <param-value>true</param-value> </init-param> <init-param> <param-name>xpoweredBy</param-name> <param-value>false</param-value> </init-param> <init-param> <param-name>maxLoadedJsps</param-name> <param-value>200</param-value> </init-param> <load-on-startup>3</load-on-startup> </servlet>Side note: If this is not the correct place to discuss this, I'm happy to continue it elsewhere.Update:I ran a similar test against the examples webapp with maxLoadedJsps set to 10. Right before and after a manually triggered GC there were 43 instances of org.apache.jsp. classes (jsps and some tag instances) in the heap dump.So it looks to me as if somehow the classes are not unloaded.(In reply to)CreatedTrivial fix for NPE problem for problem clarificationAfter checking the changes made to the initial patch version I think I have found the main reason why unloading currently does not work as expected:In org.apache.jasper.servlet.JspServlet in line 385 you call rctxt.addWrapper(...) which adds the jsp to the list of known jsps in the runtime context. However, for each wrapper the pointer to the unloadHandle is not updated before wrapper.service is called (same file, line 391).In the org.apache.jasper.compilerJspRuntimeContext in "checkUnload" (which is triggered periodically) you iterate over the list of jsps, calling jsw.getUnloadHandle() in line 609. This method will return null, until wrapper.service has been called. As a result the unload loop crashes with an NPE in that case:org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor processChildrenSCHWERWIEGEND: Exception invoking periodic operation:java.lang.NullPointerException at org.apache.jasper.util.FastRemovalDequeue$Entry.access$700(FastRemovalDequeue.java:250) at org.apache.jasper.util.FastRemovalDequeue.remove(FastRemovalDequeue.java:173) at org.apache.jasper.compiler.JspRuntimeContext.checkUnload(JspRuntimeContext.java:609) at org.apache.jasper.servlet.JspServlet.periodicEvent(JspServlet.java:360) at org.apache.catalina.core.StandardWrapper.backgroundProcess(StandardWrapper.java:660) at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.processChildren(ContainerBase.java:1393) at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.processChildren(ContainerBase.java:1403) at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.processChildren(ContainerBase.java:1403) at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.processChildren(ContainerBase.java:1403) at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.run(ContainerBase.java:1382) at java.lang.Thread.run(Thread.java:619)The attached patch fixed the problem for us - however neither does it solve the root cause of the issue, nor am I certain that it works in general (e.g. there is another call to addWrapper in TagFileProcessor). Patch is merely for clarification than for fixing the issue.See comment above.One thought about limitations of this feature:as far as I remember, and as mentioned in the Javadoc comment for java.lang.String#intern(), all String constants are placed by JVM into the same global cache.Unloading the JSPs will not free those strings. Though if new versions of JSPs do not differ from the old ones in their text, there will be no noticeable consequences.See also.(In reply to)There is a typo above. The correct name of the above init-param name is genStrAsCharArray. See comments in conf/web.xml or the code of EmbeddedServletOptions class.Konstantin:there is no typo.At least for Tomcat Version 7.0.12 it's not getStrAsCharArray but genStringAsCharArray. The only occurrence of the string "getStrAsCharArray" I can find is in the localization files (LocalStrings.properties etc.).The class org.apache.jasper.EmbeddedServletOptions contains: String genCharArray = config.getInitParameter("genStringAsCharArray");Comments in web.xml also refer to genStringAsCharArray.Moving to 6.0.x as this feature has been present in 7.0.x for some time.I'm not so sure this problem is restricted to Tomcat 6.I encountered the same issues as Isabel Drost inon a fresh Tomcat 7 install with jspIdleTimeout (and maxLoadedJsps) set.	22.0	id=48358	17	True	True	markt	1
id=45931	REOPENED	None	Tomcat 6	Jasper (	6.0.16	All All	P2 enhancement	Tomcat Developers Mailing List	2008-10-01 12:43 UTC by	Meetesh Karia	2011-11-22 16:34 UTC (	1 user	Createdreplace whitespace with single spaceEnabling trimSpaces collapses all spaces to nothing which can end up removing desired whitespace from the generated html.A possible solution to this is to reduce whitespace to a single space rather than removing it altogether. The attached patch has this implementation.	trimSpaces is equivalent to trimDirectiveWhitespacesSection JSP.3.3.8 is clear that the whitespace must be removed completely.trimSpaces can currently be true or false. I would not be against an enhancement that added the value "single" and in that case using the code in your patch.Let us know if you need some pointers on writing a new patch.Hi Mark,Thanks for the comment. The main question I have is whether you'd want to leave the -trimspaces option for JspC backwards-compatible (ie, if there's no "true", "false", or "single" after it, treat it as "true"), or whether you'd be ok with changing the behavior to requiring the value after the flag.Also, if there's anything else that's important to keep in mind when writing a patch for tomcat, please share.Thanks!(In reply to)Keep the current behaviour (ie true) if nothing is specified for trimspaces since that is backwards compatible.Things to keep in mind: - patches should be in diff -u format - avoid changing stuff that isn't relevant (ie don't mix a functional patch with code clean-up) - keep the patch as simple as you canNote I have re-opened this as an enhancement so it doesn't get lost.CreatedAdds a third state for the trimSpaces flagThis patch adds the "single" value for trimSpaces. I wasn't able to find any automated tests for jasper so I did lots of manual testing. If there is a test suite and someone can point me to it, I can add in an automated test or two.Also, when trying to manually test this, I noticed that precompiling tags that worked against 6.0.18 didn't work against the trunk. So, I was able to test the embeddable servlet option functionality against the trunk but only able to test the jspc functionality against 6.0.18. Also, none of the places of code affected by the patch appear to have been changed between 6.0.18 and the current trunk.Hi all, I was just wondering if there was a timeline for review/inclusion of this patch. Thanks!(In reply to)I'm wondering this as well. Any ETA for a release?I think it's worth mentioning that adding this feature potentially makes Tomcat incompatible with other app servers: if people use this "single" feature, and then move to another app server that doesn't support this kind of configuration, they'll have to change all their JSPs to suit.If this feature is added, it should be very clear in the documentation that its use should be restricted to dire circumstances, and that JSP authors should be using the more portable workaround to forcing spaces to appear between directives, etc.:${thingOne}${' '}${thingTwo}(In reply to)>In other web servers the space will not be removed at all. They will render several spaces instead of that single one. Nothing is lost, but just wasting some bandwidth.	8.0	id=49176	5	False	True	markt	1
id=41007	REOPENED	None	Tomcat 7	Catalina (	unspecified	All All	P2 enhancement	Tomcat Developers Mailing List	2006-11-20 12:24 UTC by	Veit Guna	2011-12-20 20:35 UTC (	0 users	Hi.I didn't find any way to customize a global 503 error page. Sure, that can't beon webapps level because when it's stopped it can't be reached. So it shouldgive a way to customize that default tomcat error page at least for 503 errors.I've searched the web and mailing lists on this issue and no one got it solved.Someone said it's hardcoded inorg.apache.catalina.valves.ErrorReportValve.report(). If that is the case, thatwould be really bad.	Bugzilla is not the forum to ask questions. Please post your question to theTomcat users mailing list.I didn't find any question mark in my sentences, did you?On the mailing list they say, it's hardcoded. It's surely no bug - but where canI put feature requests then?I assumed there was an implied question "How do I set a global 503 error page?".The reply to your post on the users list points you towards how to customisethis to use your own page.Yes, errorReportValveClass should do the trick. Why is it so complicated forjust an easy task? Ok, where can I put feature requests? Then you can close thisone.To register this an enhancement request, I have re-opened this issue and set theseverity to enhancement. I also set a few other attributes.Why is it like this? The current approach works and no one has felt the need tochange it.Obviously, enhancement requests with patches that implement them are more likelyto make it into the codebase.This Tomcat 5 enhancement request has been moved to Tomcat 7 (the latest version) since Tomcat 5 development is limited and focussed on bugs and security issues whereas Tomcat 7 is still seeing new feature development.	6.0	id=50677	8	False	False	knst.kolinko	1
id=51834	REOPENED	None	POI	HWPF (	3.8-dev	PC Windows XP	P2 major	POI Developers List	2011-09-16 11:47 UTC by	Gilbert	2014-03-11 11:54 UTC (	2 users	CreatedOpening and re-writing this file corrupts the outputThis code run against the attached document results in a corrupt word document that crashes MSWord 2003 and 2007 refuses to open. private void start() throws FileNotFoundException, IOException { POIFSFileSystem fsfilesystem = null; HWPFDocument hwpfdoc = null; InputStream resourceAsStream = getClass().getResourceAsStream("/com/blackbox/admin/templates/rma.doc"); try { fsfilesystem = new POIFSFileSystem(resourceAsStream ); hwpfdoc = new HWPFDocument(fsfilesystem); FileOutputStream fos = new FileOutputStream(new File("C:\\temp\\newTemplate.doc")); hwpfdoc.write(fos); fos.flush(); fos.close(); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } System.out.println("Opened");}	Please, check latest code from trunk and attachment with saved document. It is passed Microsoft BFFValidator.Several bugs were fixed: - summary properties handling - extended FIB handling - lists handlingCreatedResult doc (correct one)CreatedValidation resultCreatedOpening and re-writing this file corrupts the outputTable cells seems to be problematic.Tested : Merging any cells (using WORD 2007) from the input document before re-writing it makes the output clean.Removing the table produces a clean output tooReopening for 3.8-beta5 : See previous commentThis bug still exists in Version 3.10 final.The following Situation occured:My Word Document contains a table and I want to replace some text in a cell.This works fine and I can open the file with Word 2010, but not with Word 2003 (It is a doc file).There are three cases after replacing the text:1. same length of the text: no problem, it is possible to open the file in Word 20032. old one is longer than replacement: open and repair is possible with Word 20033. old one is shorter than replacement: Word 2003 crashesIt is possible to open all documents with Word 2010.Another test was to replace a text that is contained in an enumeration, but not in a table and it has got the same behavior.	6.0	id=51604	16	False	True	sanmoy	1
id=51604	REOPENED	None	POI	HWPF (	3.8-dev	PC All	P2 normal	POI Developers List	2011-08-03 02:43 UTC by	sanmoy	2017-01-21 19:07 UTC (	0 users	I had written this simple piece of code FileInputStream fileInputStream = new FileInputStream(new File("C:\\in.doc")); FileOutputStream fileOutputStream = new FileOutputStream(new File("C:\\out.doc")); HWPFDocument hwpfDocument = new HWPFDocument(fileInputStream); Range range = hwpfDocument.getRange(); int numParagraph = range.numParagraphs(); for (int i = 0; i < numParagraph; i++) { Paragraph paragraph = range.getParagraph(i); int numCharRuns = paragraph.numCharacterRuns(); for (int j = 0; j < numCharRuns; j++) { CharacterRun charRun = paragraph.getCharacterRun(j); String text = charRun.text(); charRun.replaceText(text, "added"); } } hwpfDocument.write(fileOutputStream); After the execution, the output file becomes corrupted ( tried to open with office 2007) , input file was properly opening. input file is a very basic file, with 4-5 simple linesNo exception thrown. had written similar code using XWPFDocument, and it works fine	One addition, fileOutputStream.close() is present in the original code, by mistake, which i didn't copy while raising the defect. Please remember, the following code is working perfectly XWPFDocument xwpfDocument = new XWPFDocument(inputStream); List<XWPFParagraph> paragraphs = xwpfDocument.getParagraphs();for(XWPFParagraph xParagraph:paragraphs){for(XWPFRun xwpfRun : xParagraph.getRuns()){xwpfRun.setText("replace", 0);}}xwpfDocument.write(outputStream);outputStream.close();Which version of POI? Please try with the latest build from trunk, there have been quite a lot of updates recently.If the problem is still there, please attach the problematic file. Without the input .doc file we can't do much to help you.YegorCreatedtest input file for hwpfI have tested with 6-Jul-2011 nightly build. poi-bin-3.8-beta3-20110606.tarNow I have tested with yesterday's nightly build poi-3.8-beta4-20110803 fromVery interested in this bug. Having the same issue. Can't use XWPF since contract requires us to maintain original version.Fixed in(3.8-beta4-20110810 or later). Please, test.Createdtest output file for hwpfI have tested with 3.8-beta4-20110810, but the defect still exists.Now, I have added a check, when the line contains the string "Header" ( please see the input doc ) then it will replace the text. if(text.contains("Header"))charRun.replaceText(text, "added");And I have compared the output file with "beyond compare" ( a file comparison tool ), it shows the text has been replaced properly, but the document format is corrupted. I don't know much about the doc format so cannot comment. I have attached the output file, hope it will help.Createdcomparison resultcomparison resultSanmoy,I don't see the difference between in.doc and out.doc, except "Header" -> "added" change. What exaclty is broken?SergeyCreatederror-snap-shotplease try to open the output file with MS office 2003 or 2007, it will display file corruptedSanmoy,I did fix a couple of issues (FIB and stylesheets processing) that may be reason of why file is not opening by Microsoft Office. Please try with next night build or trunk version.Result file is still not passed binary file validation tool thought :(SergeySanmoy,Please, check the latest trunk version or 3.8-beta4 or later.the defect has been fixed .. thanks/*** Replace (all instances of) a piece of text with another...** @param pPlaceHolder* The text to be replaced (e.g., "${organization}")* @param pValue* The replacement text (e.g., "Apache Software Foundation")*/public void replaceText(String pPlaceHolder, String pValue) The replaceText API will not work if the String pValue contains the String pPlaceHolderFor example if pPlaceHolder="abcd" and pValue="abcd" or "abcdef" or "12abcdef" this code will go to a infinite loopModify the original testcode charRun.replaceText(text, text); that is, try to replace the original value with itself, it will not work, it will fall into a infinite loop. For your convenience, I am copying the original code again. Please test it with the attached filesFileInputStream fileInputStream = new FileInputStream(new File("C:\\in.doc")); FileOutputStream fileOutputStream = new FileOutputStream(newFile("C:\\out.doc")); HWPFDocument hwpfDocument = new HWPFDocument(fileInputStream); Range range = hwpfDocument.getRange(); int numParagraph = range.numParagraphs(); for (int i = 0; i < numParagraph; i++) { Paragraph paragraph = range.getParagraph(i); int numCharRuns = paragraph.numCharacterRuns(); for (int j = 0; j < numCharRuns; j++) { CharacterRun charRun = paragraph.getCharacterRun(j); String text = charRun.text(); charRun.replaceText(text, text); } } hwpfDocument.write(fileOutputStream); fileOutputStream.close();I have tested with the latest nightly buid 3.8-beta5-20110904I have debugged the poi code and found the problem in this following logic String text = text();int offset = text.indexOf(pPlaceHolder);text is returning the replaced value and if the replaced value contains the original String, offset will always be >=0 and it will keep on increasingpublic void replaceText(String pPlaceHolder, String pValue) { boolean keepLooking = true; while (keepLooking){ String text = text(); int offset = text.indexOf(pPlaceHolder); if (offset >= 0) replaceText(pPlaceHolder, pValue, offset); else keepLooking = false; } }Hi, I have tested poi-bin-3.8-beta5-20111217.tar.gz with a MS office 2003 (input file) using "charRun.replaceText("XYZ", "ABC");"and the output file is corrupted. without replacement the output is ok, so I guess that problem come from replacement logic.Also it worked out with poi-bin-3.8-beta4 if length of pValue was equal to the length of PlaceHolder.. otherwise the output file was also corrupted ! Any idea !Thanks	15.0	id=51834	6	False	False	vlsergey	1
id=53282	REOPENED	None	POI	XSSF (	3.8-dev	PC All	P2 normal	POI Developers List	2012-05-23 21:13 UTC by	lakshmi	2016-07-28 06:23 UTC (	5 users	I sent this query to the POI mailing list couple of days back and was advised to open a bug here.I tested hyperlinks with Apache POI (3.8 version) and it throws java.lang.IllegalStateException: The hyperlink for cell A2 references relation rId2, but that didn't exist!for hyperlinks with non-breaking spaces. The non-breaking space in an hyperlink relation is not a valid java URI. These invalid characters should also be encoded for valid URI. Looks like the issue with white space in target URI was fixed in. When trying to convert such hyperlink to URI, a URISyntaxException is thrown.Please find the attachment of the excel file including the hyperlink with a non-breaking space. Below is the sample java code to demonstrate the issue.import java.io.FileInputStream;import java.io.IOException;import org.apache.poi.ss.usermodel.Workbook;import org.apache.poi.ss.usermodel.WorkbookFactory;public class DemonstrateHyperlinkIssue { public static Workbook getCell(String fileName) throws Exception{ FileInputStream input = null; Workbook excelDoc = null; try { input = new FileInputStream(fileName); excelDoc = WorkbookFactory.create(input); input.close(); } catch(IOException e) { throw e; } finally { if (input != null) { input.close(); } } return excelDoc; } public static void main(String [] args) { try { Workbook wb = getCell("C:\\pathToFile\\test.xlsx"); System.out.println(wb); } catch (Exception e) { // TODO Auto-generated catch block e.printStackTrace(); } }}	CreatedExcel file containing the hyperlink with a non-breaking spaceComment onExcel file containing the hyperlink with a non-breaking spaceplease ignore this attachment.Unable to attach the excel file.To reproduce the error, copy/paste the below in an excel fileI can't reproduce it. Please try again to attach a sample file with a nbsp . Yegor(In reply to)CreatedExcel file with a non-breaking space hyperlinkFixed in. I included your sample in our collection of test files.YegorCreatedExcel sample with hyperlinksExcel sample with hyperlinksWe have tried with version 3.9 and 3.10 beta. Seems still issue is there. Attached a sample file the japanese hyperlink with a space.Are you able to produce a unit test, similar to the one Yegor did for, which shows how the problem still remains? That will help us track down what's still incorrect.CreatedExcel file containing invalid hyperlinkComment onExcel file containing invalid hyperlinkThis file contain a invalid email address hyperlink which can reproduce the probelm.Fixed by svn rev.Although the last sample file containing the invalid hyperlink seems to be a bit hypothetical, I think it's better to receive/return an invalid-url, in case such a malformed uri is really generated, opposed to throwing an IllegalStateExceptionIt seems that the bug still exists in 3.10 final.I got a file from a customer where value like "; TEL: +001-123-456789" is given as email address hyperlink in it. Try to import this file a IllegalStateException is thrown at the beginning and there is no way to correct the value afterwards in program.I have been facing the same issue as well and I am using POI 3.10.1.The excel that I have has autogenerated hyperlinks and some of them have spaces in the hyperlink.Createdhyperlink errorI am getting a similar error on workbook.cloneSheetThe hyperlink for cell O2 references relation rId1, but that didn't exist!	15.0	id=53282	17	False	False	lakshmi.nchandana	1
id=49176	REOPENED	None	Tomcat 6	Jasper (	6.0.24	All All	P2 enhancement	Tomcat Developers Mailing List	2010-04-23 09:31 UTC by	Scott Hamilton	2010-04-23 19:37 UTC (	0 users	org.apache.jasper.compiler.Compiler.compile(), ~line 357 has this code: // Only get rid of the pageNodes if in production. // In development mode, they are used for detailed // error messages. //if (!this.options.getDevelopment()) { pageNodes = null; }Turns out this has a pretty substantial impact on the heap usage for JSPs. Our application has ~2300 JSPs, which if all are compiled and loaded when Jasper is in production mode yields roughly an 80m overhead. However, if all are compiled and loaded while Jasper is in development mode, the overhead is ~1.8g - yes, that's gigabytes.For most development environments this won't be an issue b/c all of the JSPs won't need to be compiled and loaded, and one would assume that in production you turn off development mode.However, for reasons outside the scope of this bugzilla issue, we can't turn this off in production.However, it seems like a very small change might result in big big savings for those who might also be like us, plus also just raw savings in development mode.If my analysis is correct (and I'm all ears to hear that it is not), the pageNodes data is really only used after compilation if a JSP compile error is enountered. The pageNodes data will be picked up and used in generating a more detailed error page.So... can the Compiler.compile() method ALWAYS clear this out if there is no exception in the compilation? This would give the best of both worlds - huge memory savings plus detailed error pages.Final note: to prove this out, we wrapped the JspServlet in our own servlet so which basically just proxies the Jasper JspServlet EXCEPT that when the JspServlet.service() method completes, we check to see if the JSP was just compiled, and if it was, we reflectively set the pageNodes property of the associated Compiler to null.	The pageNodes are required to generate detailed error messages for runtime (eg NPE) errors as well as compile time errors.That's a good point - indeed I missed that in looking thru the code.I wonder, though, if there's any optimization that could be done here to reduce the memory footprint of this pageNodes data structure, however. As I indicated, it does seem to be quite expensive.Another alternative might be to perhaps temporarily reconstruct the pageNodes when an exception occurs and this information is to be displayed. Granted, you'd incur a performance hit, but not much of one (Jasper is pretty fast these days) and we're talking about this only in development mode anyway...I don't mean to belabor the point, but I also noticed that the usage of this data for runtime exceptions is not quite consistent. It will only be used and show up in error messages on a JSP that is compiled, but if the server has started up and the already compiled .class file is not out of date, Jasper will load the class without reconstructing this pageNodes data, and your subsequent error messages will be devoid of this info.Maybe you could consider that a different bug if indeed the intent here is for when Jasper is in dev mode that errors both compile-time and runtime should show this extra pageNodes data.Not having the page nodes after a restart is probably a separate issue (although one worth keeping in mind whilst thinking about this issue).Keep in mind that pageNode generation is expensive, relative to serving a response. Not caching the pageNodes is likely to open up a DOS attack vector.A theoretical (as in I haven't looked at the code to see if it is feasible or how much work it would be) is to ignore the pageNodes and use the SMAP. The SMAP could either be cached (should be much smaller than pageNodes) or read when required. Since it is saved in the .class file it would also survive a restart. I'd be tempted to cache it for the life of the JVM on compilation and read it from disk as required after a restart.I'm re-opening this and marking it as a (performance) enhancement.It will get looked at eventually, but enhancements tend to be at the bottom of people's todo list. If you wanted to provide a patch...	4.0	id=53320	7	False	False	j.brauge	1
id=53320	REOPENED	None	POI	XSSF (	3.8-FINAL	PC All	P2 normal	POI Developers List	2012-05-29 14:43 UTC by	None	2016-06-15 10:51 UTC (	1 user	CreatedClone formulaShift a formula that contains absolute cells references with method adjustFormula change abosulte rows references.	CreatedTemplate excelTemplate excel.Cell A1 contains formula :=SI($C1=O$1;SI(STXT($F1;1;1)="6";$H1-$I1;$I1-$H1);0)After copy and shitfing formula from cell A1 to cell A2, we have:=SI($C2=O$2;SI(STXT($F2;1;1)="6";$H2-$I2;$I2-$H2);0)instead of=SI($C2=O$1;SI(STXT($F2;1;1)="6";$H2-$I2;$I2-$H2);0)Adding test on isRowRelative() in rowMoveRefPtg (in FormulaShifter) seems toworks.private Ptg rowMoveRefPtg(RefPtgBase rptg) { int refRow = rptg.getRow(); if (_firstMovedIndex <= refRow && refRow <= _lastMovedIndex) { // Rows being moved completely enclose the ref. // - move the area ref along with the rows regardless of destination if (rptg.isRowRelative()) rptg.setRow(refRow + _amountToMove); return rptg; }....I think this is fixed in POI 3.13, if not earlier.I'm using XSSFSheet.shiftRows and formulas with absolute row references are getting adjusted, if my memory serves correctly.This was fixed for row copy but not row move (Sheet.shiftrows)This behavior may be by design. Shift rows isn't the same as insert rows. I'm not sure if it's the same as cut-paste, either. I'd be happy to discuss this on the dev mailing list.	5.0	id=53411	10	False	True	markt	1
id=50677	REOPENED	None	Tomcat 6	Catalina (	unspecified	All All	P2 enhancement	Tomcat Developers Mailing List	2011-01-27 20:54 UTC by	Jim Riggs	2014-01-09 15:33 UTC (	0 users	We currently have two hardcoded "variables" that we substitute in catalina.properties, ${catalina.base} and ${catalina.home}. Is there value in expanding this functionality? Here is my scenario:I have several apps, app1, app2, etc. I have a shared lib directory of jars that the apps share. In addition, the apps may have several instances running in different "environments" (prod, QA, dev, etc.), and each environment has a corresponding version of the lib directory. The Tomcat configs for the apps (catalina.base) are version-controlled.Now, I want each instance of these apps (catalina.base) to be as portable as possible without having to make a bunch of changes for each environment. So, I want to be able to copy the app1-prod catalina.base to app1-qa and not have to make a lot of local modifications to catalina.properties and friends to make it work. My init script knows that app1-qa is a QA instance and needs to point to the lib-qa shared directory. So, I set a system property in the init script via CATALINA_OPTS: -Dshared.lib.dir=/path/to/lib-qa for QA and -Dshared.lib.dir=/path/to/lib-prod for production. What I would like to do is use this system property via ${property.name} in my common.loader in catalina.properties. The result is that the catalina.base files are the exact same from a configuration perspective without a bunch of local modifications.I have created two proposed patches for this functionality. Both work well, but each one has the potential for some slightly different behavior, so I would like to hear the thoughts of the developers on each.--Proposal A (currently in production use for my environment):The variable substitution takes place upon retrieval in CatalinaProperties.getProperty(), pulling in the current value of the system property. This allows other properties defined in catalina.properties to be substituted. The (potential) downside or risk is that the value of a catalina.property value may change over time if the system properties referenced in its value are changed by the code during the JVM's lifecycle.--Proposal B:The variable substitution takes place in the class initializer, loadProperties(). This means that every call to CatalinaProperties.getProperty() will return the same result, with system property variables replaced with their value at the time the class was loaded. The downside of this is that properties set in catalina.properties cannot be used in other properties defined in that file, because they may not yet be set based on the order they are returned by the Enumeration. This could be worked around, but it would probably take another iteration over the properties (not really a big deal).	CreatedProposal ASubstitute on retrieval.CreatedProposal BSubstitute on initialization.I'd prefer B and I agree some care is needed with the substitution algorithm.One more discussion at users@:"tomcat 7: common.loader property in catalina.properties does not take ANT style variable"I implemented the substitution inand it will be in 7.0.17.I used a different approach to the one proposed above:I implement substitution for the *.loader properties only. This is the use case that was discussed here and on users@.Can we have this in 6.0.x too?I have a similar usecase. I need to add some JARs from the Oracle client installation. I have defined -Doracle.home=.. in my setenv.sh and then common.loader=${oracle.home}/jlib/oraclepki.jar,...My usecase is the same as Yongqin Xu's from the mailing list.Re-opening for consideration for back-port to Tomcat 6.	8.0	id=53737	6	False	False	markt	1
id=53737	REOPENED	None	Tomcat 8	Jasper (	8.0.x-trunk	PC Windows XP	P2 enhancement	Tomcat Developers Mailing List	2012-08-18 00:54 UTC by	Konstantin Kolinko	2014-04-11 15:49 UTC (	0 users	In Tomcat 7.0.29 in ContextConfig#webConfig() a merged web.xml file is serialized into String and is put as an attribute into ServletContext.[[[ String mergedWebXml = webXml.toXml(); sContext.setAttribute( org.apache.tomcat.util.scan.Constants.MERGED_WEB_XML, mergedWebXml);]]]Then in JspConfig#processWebDotXml() of Jasper it is parsed again from XML into objects.It would be better to access the necessary configuration through Servlet 3.0 API methods such as ServletContext.getJspConfigDescriptor().	Fixed in trunk and 7.0.x and will be included in 7.0.30 onwards.The fix wasn't as simple as it first appeared due to having to support JspC. The original fix has been reverted pending a re-think.CreatedAdd support for JspConfig to JspC's ServletContextRelated to(reverted), this patch adds support for identifying the effective version of an application and for setting up a JspConfigDescriptor for JspC that would match the one returned by a container at runtime.This should mean that Mark's original change would also work in JspC, allowing the two implementations to be consolidated.This patch does not yet attempt to merge in web-fragment.xml files as that functionality is not supported by the current JspC implementation either. I plan to add that later if this looks like a good way to go.There is also some duplication with implementations in o.a.c.core. Jasper does not have any dependencies on catalina so I did not reuse those. However, these could potentially be moved to o.a.tomcat.util and shared.CreatedAdd changes fromUpdate previous patch to merge in changes fromthat were reverted due to issues with JspC. This removes need for Jasper's compiler to parse a web.xml, instead relying on a ServletContext provided by the container or now by the JspC shell. Jasper's WebXml class was deprecated in, it is not longer referenced in Jasper and could be removed.Ran test-bio with no failures, and ran JspC separately on a simple webapp to verify <jsp-config> is being handled (checked default-content-type is picked up from web.xml).Patch still has todo's to add web-fragment support when JspC builds its ServletContext.CreatedIncremental, adds merging of fragments and scanning using ServletContextIncrement on prior patch to add merging of jsp-config found in web-fragments.Also change to JspC to scan for JSPs based on resources found in the ServletContext. This currently works just for resources in the main webapp but not for those in META-INF/resources in jars - work is still needed in JspCServletContext to map those.This has been partially fixed using a combination of the original fix, the patches here and patches from violetagg.web-fragment and annotations are still TODO so the code is effectively in the same place funcitonaly as before the patch.Support for web fragments has been added.Support for annotations is not required as they do not affect JspC.Support for JSPs in resource JARs is still to do.	7.0	id=54399	6	False	False	paulvancingel	1
id=53411	REOPENED	None	Tomcat 7	Catalina (	7.0.27	Other Linux	P2 enhancement	Tomcat Developers Mailing List	2012-06-13 16:22 UTC by	kubak	2014-10-22 19:46 UTC (	1 user	I can see in my log file a regular exception throwed by AbstractHttp11Processor, with this stack trace:org.apache.coyote.http11.AbstractHttp11Processor processError processing requestjava.lang.NullPointerException at org.apache.tomcat.util.buf.CharChunk.append(CharChunk.java:355) at org.apache.tomcat.util.http.mapper.Mapper.map(Mapper.java:667) at org.apache.catalina.connector.CoyoteAdapter.postParseRequest(CoyoteAdapter.java:646) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:402) at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:999) at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:565) at org.apache.tomcat.util.net.AprEndpoint$SocketWithOptionsProcessor.run(AprEndpoint.java:1770) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:722)I cannot find the request which makes this exception, it's rather rare, but still, I think it's worth to look at.	The NPE is triggered by the default host name being null. Such a configuration is invalid. Please follow up on the users list for assistance.Well, I think you've changed the status too early.My Apache Tomcat 7 server is running, all web apps are working and defaultHost parameter in server.xml is set to "localhost": <Engine name="Catalina" defaultHost="localhost">So, this exception doesn't stop tomcat or any context, I don't even noticed if this exception is visible for any user. From the user point of view everything works fine.I've just noticed this strange stack trace in the log file and this is the only one exception which I can find there - that's why it caught my attention. Even if the problem is in the configuration - Tomcat should not start or should give a clear message during the startup, in my opinion.There is insufficient information provided to reproduce the issue. The only information provided - the stack trace - points to a configuration problem. Please do not re-open this issue without providing either:- sufficient information for it to be produced- analysis that identifies a code path that code trigger this issueThe users list is the place to seek help with obtaining either of the above.A warning is already logged if the default host cannot be identified.Just for reference, confirming what Mark already wrote,in tc7.0.x/tags/TOMCAT_7_0_27CharChunk.java line 355[[[354 public void append(String s) throws IOException {355 append(s, 0, s.length());356 }]]]So "s" is null.Mapper.java line 667[[[666 if (host.isNull()) {667 host.getCharChunk().append(defaultHostName);668 }]]]So "defaultHostName" is null.The "if(host.isNull())" branch is executed when it is an HTTP/1.0 request that does not have a "Host" header. Such requests are rare nowadays.(In reply to)There must exist <Host name="localhost" > for that. If it does not, the defaultHost value will be ignored.MapperListener.java[[[ if(found) { mapper.setDefaultHostName(defaultHost); } else { log.warn(sm.getString("mapperListener.unknownDefaultHost", defaultHost, connector)); }]]]Reopening this as enhancement.Looking at AbstractHttp11Processor#process() this error is correctly written to access log, etc, so I do not see much concern.Still I think there is a room for improvement, and I noted several minor issues from my code review.1) in MapperListener#findDefaultHost()First, maybe s/log.warn(sm.getString("mapperListener.unknownDefaultHost",/log.error/ Second, maybe mention something like " Tomcat will not be able to process HTTP/1.0 requests that do not specify a Host header" in the message.Third, I am a bit wondering why not to call mapper.setDefaultHost() unconditionally. What is wrong with passing a name there?Hosts can be added and removed through JMX calls on StandardEngine#addChild()/removeChild() and it seems that MapperListener fails to update defaultHost setting on the Mapper when it happens.So why not to pass the defaultHost name to the Mapper as is and let it handle missing matches (like it already does)?2) in Mapper#map(MB,MB,S,MD)Fourth,Maybe just return without mapping here, as if the Host is not found. We already do if(defaultHostName==null){ return; } in its #internalMap(CC,CC,S,MD) method.It will need some update to its caller though, which isCoyoteAdapter#postParseRequest()3) In CoyoteAdapter#postParseRequest()This request could be rejected with error 404, instead of 400 that exception handling in AbstractHttp11Processor#process() does. The postParseRequest() already has code for handling it as 404, butFifth,access logging needs to be changed a bit. The current code:[[[ // Make sure there is a host (might not be during shutdown) if (host != null) { host.logAccess(request, response, 0, true); }]]]I think that it should fallback to CoyoteAdapter#log(..) when host is null.(In reply to)In my opinion if Tomcat cannot process some requests because of wrong configuration - Tomcat should not start and give an error with clear message at start up. Current message doesn't say anything about after-effects, is easy to ignore.CreatedPatch for the enhancements mentioned aboveI have incorporated the changes mentioned above, with this patch.P.S. This is my first patch submission. Let me know if I could do anything to help better.Thanks for the patch. Don't be concerned about how much of the patch I am suggesting you change (most of it). The first patch I proposed was torn apart beyond recognition before it got anywhere near the Tomcat code base.The view expressed by Kubak that Tomcat should not start if the default host is not valid is not the consensus opinion of the Tomcat developers. It is safe to assume that Konstatin's comments (as a committer) does represent the consensus opinion unless another committer comments otherwise (in which case expect a discussion on the dev list to reach a consensus).I suggest you go through Konstantin's comments inone by one and address each of them in your patch.	8.0	id=54509	5	False	False	dominik.stadler	1
id=54399	REOPENED	None	POI	XSSF (	3.9-FINAL	PC All	P2 normal	POI Developers List	2013-01-10 05:04 UTC by	Paul van Cingel	2016-03-17 12:06 UTC (	1 user	CreatedXLSX file that fails sheet rename.For one particular spreadsheet, renaming a sheet returns java.lang.IllegalArgumentException: Sheet index (-1) is out of range (0..1) at org.apache.poi.xssf.usermodel.XSSFWorkbook.validateSheetIndex(XSSFWorkbook.java:1043) at org.apache.poi.xssf.usermodel.XSSFWorkbook.getSheetName(XSSFWorkbook.java:876) at org.apache.poi.xssf.usermodel.XSSFEvaluationWorkbook.getSheetNameByExternSheet(XSSFEvaluationWorkbook.java:135) at org.apache.poi.xssf.usermodel.helpers.XSSFFormulaUtils$1.getSheetNameByExternSheet(XSSFFormulaUtils.java:81) at org.apache.poi.ss.formula.ptg.ExternSheetNameResolver.prependSheetName(ExternSheetNameResolver.java:42) at org.apache.poi.ss.formula.ptg.Area3DPtg.toFormulaString(Area3DPtg.java:100) at org.apache.poi.ss.formula.FormulaRenderer.toFormulaString(FormulaRenderer.java:92) at org.apache.poi.xssf.usermodel.helpers.XSSFFormulaUtils.updateName(XSSFFormulaUtils.java:143) at org.apache.poi.xssf.usermodel.helpers.XSSFFormulaUtils.updateSheetName(XSSFFormulaUtils.java:97) at org.apache.poi.xssf.usermodel.XSSFWorkbook.setSheetName(XSSFWorkbook.java:1230)Code is:import org.apache.poi.ss.usermodel.*;import java.io.*; FileInputStream fileInputStream = new FileInputStream(filePath); Workbook workbook = WorkbookFactory.create(fileInputStream); System.out.println("sheets:" + workbook.getNumberOfSheets());for (int i = 0; i < workbook.getNumberOfSheets(); i++) { System.out.println("i:" + i); workbook.setSheetName(i, "Sheet" + (i + 1));} fileInputStream.close(); FileOutputStream fileOutputStream = new FileOutputStream(filePath); workbook.write(fileOutputStream);fileOutputStream.close(); This outputs sheets:1i:0Tested on POI versions 3.8 and 3.9.	The error happens at a point where the external references are resolved in order to adjust all formulas/references for the new sheet-name. FormularParser.createAreaRefParseNode() tries to read the external sheet index, but does not find the referenced external sheets and thus ends up using -1. I still lack details of how it is supposed to work, though, to say if this is somehow expected here or an invalid .xlsx or really a bug in POI...This seems to work with the current trunk version of POI, some other fix seems to have fixed this too.I downloaded the latest version 3.14 from 2016-03-07 and checked.It was fixed in XSSFWorkbook class, but the problem remains on HSSFWorkbook.	3.0	id=54618	14	False	False	jens.borgland	1
id=54618	REOPENED	None	Tomcat 8	Catalina (	8.5.x-trunk	All All	P2 enhancement	Tomcat Developers Mailing List	2013-02-28 12:24 UTC by	Jens Borgland	2017-02-16 11:12 UTC (	4 users	CreatedThe actual filterThe specification for HTTP Strict Transport Security (HSTS) has now been published (RFC 6797). Tomcat should include a filter implementing the specification to make it easy to add to web applications.I have attached an implementation suggestion.	CreatedA testCreatedNew error message propertyCreatedProposed implementation as unified diffHSTS has a fairly major hole: the bootstrap MITM problem. There are suggested solutions but the current pre-loaded lists contain a very small number of sites. Further, the practicalities of trying to build a reasonable pre-loaded list mean that a pre-loaded list is very unlikely to resolve the bootstrap MITM problem.Personally, I not convinced of the usefulness of this feature at this point in time.I agree that the bootstrapping is a problem (and it's recognized in the RFC as well) but I still think that HSTS helps reduce the attack window quite drastically and therefore has significant benefits.Tomcat is also often used for more or less internal applications within organizations where a pre-loaded list (perhaps configured using a group policy) may very well be a viable solution.Is there a reason to implement this at the container level? Since it's a Filter, it can be installed directly into the web application. I think this protection is better done at the application level and not at the container. Perhaps your code could be put into the wiki for anyone who wants to use it.My thought was to make it easy for both developers and, more importantly, admins to enable HSTS without having to write or compile code. The actual implementation itself is quite trivial but I think that having the functionality available out of the box adds significant value (just like the CSRF Prevention and Expires filter for example).HSTS is also done per host so using the filter for all applications in a container makes sense to me.Another option could be to create a more general filter or similar that simply adds one or more configured headers (possibly based on some condition) - like mod_headers for Apache.I think this is an important feature for Tomcat to support out of the box. Furthermore though, headers like this should be insanely easy to just add to all the headers of a domain hosted on a machine. Apache solves this very easily with a single configuration line:Header add Strict-Transport-Security "max-age=15768000"So this is incredibly trivial to do in Apache since adding headers is very, very easy. It's far harder to do this on Tomcat since it requires code modifications. Why can't Tomcat have a similar feature?IMO the solution should be broader than just this one header, and should be a simple config option that an admin can add or subtract rather than having to implement this on every web application.I think it's vitally important that the admin should be able to control this, since the security feature it implements crosses multiple applications on a server, not just one. That's something a good administrator can implement quickly, and would be far harder and more error prone to add at the application level.(In reply to Steve Sether from)Then vote for it: there are currently 5 votes from a single person for this issue. More votes = more attention.Adding a Filter (assuming it's already been written/compiled) only requires configuration, just like Apache httpd. If you don't have mod_headers, it "requires a code change" just as this does.The Filter is attached to this issue. Feel free to download it and use it. It just hasn't made it into Tomcat's distribution yet.The Filter can be added to conf/web.xml and will apply to all web applications hosted by the container. I'm not sure in what order it will be applied, though. My wild guess without trying is that everything in conf/web.xml will be applied first, then all the filters defined in the application's WEB-INF/web.xml.Admins have the ability to modify conf/web.xml.I think there is a bug in the Filter implementation provided by Jens:The filter calls chain.doFilter() and then adds the header afterward. This isn't going to work for any request that generates output that exceeds the buffer size, or that explicitly calls getOutputStream().flush (or getWriter().flush).Is anyone using this Filter implementation anywhere, or was it a proof-of-concept?(In reply to Christopher Schultz from)To clarify: the order is the opposite in Tomcat 7 onwards. The WEB-IND/web.xml filters are first, the ones in conf/web.xml are second. This is documented in Migration Guide.(In reply to Steve Sether from)If it is that simple, maybe just use the well-known UrlRewriteFilter, as linked here:A <set type="response-header" /> rule will set a response header.The filter I wrote was intended as a suggestion, I haven't used this actual implementation anywhere. I agree that the header should be set before calling chain.doFilter() so if the filter is to be included in Tomcat (or used anywhere else) the order needs to be reversed.Something like the UrlRewriteHeader or a filter providing somehting similar to mod_headers could of course be used as well. One benefit of having a more specific filter like the one I provided would be that it makes it easier for an administrator to avoid spelling or syntactic errors.I agree that this is something that should be able to be implemented without having to edit application code, particularly if it prevents the issues regarding ordering that Konstantin Kolinko mentionedI've just added an HTTP header security filter that adds the HSTS header by default for 9.0.x. I plan to expand the set of headers this filter adds/Added to 8.0.x for 8.0.23 onwards and 7.0.x for 7.0.63 onwards.A web application using Spring hijacked the global filter configuration (did not add the header).Interesting client side discussions inadd a "preload" init-param as perand	19.0	id=55324	6	False	False	apache	1
id=55604	REOPENED	None	POI	HWPF (	3.8-FINAL	PC All	P2 major	POI Developers List	2013-09-27 17:35 UTC by	Willy Solaligue	2016-10-18 16:18 UTC (	0 users	CreatedPatch with fix about SprmBuffer problemWe had a issue when we tried to get the body content of a MS Word 97 file. We got this exception:..Caused by: java.lang.ArrayIndexOutOfBoundsExceptionat java.lang.System.arraycopy(Native Method)at org.apache.poi.hwpf.sprm.SprmBuffer.append(SprmBuffer.java:128)at org.apache.poi.hwpf.model.PAPBinTable.rebuild(PAPBinTable.java:293)at org.apache.poi.hwpf.model.PAPBinTable.rebuild(PAPBinTable.java:116)at org.apache.poi.hwpf.HWPFOldDocument.<init>(HWPFOldDocument.java:136)at org.apache.tika.parser.microsoft.WordExtractor.parseWord6(WordExtractor.java:437)at org.apache.tika.parser.microsoft.WordExtractor.parse(WordExtractor.java:79)at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:186)at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:161)at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)... 15 more so We did a fix to avoid that exception, I am attaching a patch file with the fix.Regards	Additionally, we tried again using POI 3.9, but it couldn't solve the issue.Closing this as there was no update for a very long time and the attached patch just catches and completely ignores any Exception thrown at this code-location. This is surely not a fix, not even a workaround, but a rough hack that might hide any type of problem at that point. Please reopen this bug if this is still a problem for you and you can provide a fix that can be applied to the library.Same problem here, surfaced via Tika. The following file reproduces the behavior:Tika issue here:	3.0	id=55081	12	False	True	me	1
id=54509	REOPENED	None	POI	HSSF (	3.9-FINAL	PC Windows XP	P2 normal	POI Developers List	2013-01-31 10:33 UTC by	Sylvain Delafoy	2016-01-31 17:53 UTC (	1 user	Createdmaven projet with an input file and a sample main method.When shifting rows in an HSSFSheet, formulas such as "=sum(C[1])" do not use new cells' values.You need to open the resulting file in excel and edit (F2) the cell in order for the formula to use the cell's value.Sample file and code in the attachment.	I tried this with latest POI and it did work fine, seems some of the related fixes since POI 3.9 has fixed this as well.Looks like the difference comes from the software used to open the file. I checked my example this morning with Excel 2010 and the latest libreoffice and both showed 10. Unfortunately, I don't have access to the computer where I noticed this problem to check excel's version.However, if I addSystem.out.println(new HSSFWorkbook(new FileInputStream("out.xls")).getSheetAt(0).getRow(0).getCell(0).getNumericCellValue());and use POI version 3.10.1 or 3.11-beta2 at the end of the test, the output is 7.However, I wonder if it's a POI bug or not.PS: My apologies to clean code, there is obviously a lot of stream closing to be done. If this bug is reopened, I volunteer for writing a clean unit test.reopen to investigate some moreI'm sorry I can't help more, I left the company at which I encountered this issue so no access to the original excel version anymore.Can I help in any other way?	4.0	id=56490	7	False	False	apache	1
id=55324	REOPENED	None	POI	HSSF (	3.10-dev	PC All	P2 normal	POI Developers List	2013-07-29 09:23 UTC by	None	2015-08-09 20:08 UTC (	0 users	When you create an excel file and use the following forumula, if condition is true, we see a #VALUE in Excel. However if you click on the cell (or press F2 key) and press enter, the formula gets populated. =IF(LEN(A1)>4,MID(A1,4,2),A1)	As detailed in the documentation, and covered in a lot of detail at <>, after making changes to formulas or the cells they depend on you need to re-evaluate the formulas to update the cached value that gets stored with them in the file.I have read the documentation. I added this line before saving and closing the workbook but nothing change.FormulaEvaluator evaluator = wb.getCreationHelper().createFormulaEvaluator();evaluator.clearAllCachedResultValues and evaluator.evaluateAll(); wb.setForceFormulaRecalculation(true);After few tests, * LEN(A1) works perfectly* MID(A1,4,2) works perfectly* IF (LEN(A1)>0, A1, A2) works perfectly* IF (1==1, MID(A1,4,2), A1) works bad* IF (1==1, A1, MID(A1,4,2)) works badall formulas are re-evaluate before closing workbook.All works perfectly with XSSF but not with HSSF.are you sure there is no bug when combinating IF instruction and MID ?After few tests, * LEN(A1) works perfectly* MID(A1,4,2) works perfectly* IF (LEN(A1)>0, A1, A2) works perfectly* IF (1=1, MID(A1,4,2), A1) works bad* IF (1=1, A1, MID(A1,4,2)) works bad* IF (MID(A1,4,2)="ab", A1, "bad") works fineall formulas are re-evaluate before closing workbook.All works perfectly with XSSF but not with HSSF.are you sure there is no bug when combinating IF instruction and MID ?excuse me there is a bug in my comment, * LEN(A1) works perfectly* MID(A1,4,2) works perfectly* IF (LEN(A1)>0, A1, A2) works perfectly* IF (1=1, MID(A1,4,2), A1) works bad* IF (MID(A1,4,2)="ab", A1, "bad") works fine* IF (1=1, A1, MID(A1,4,2)) works fine* IF (1>2, A1, MID(A1,4,2)) works bad(In reply to Nick Burch from)are you sure there is no bug ?=IF(1=1,MID(A1,4,2),A1) works bad=IF(1=1,MID("abcdefgh",4,2),A1) works perfectly.thanks for your answerThere might be a bug, it's always possible...I'd suggest you try running BiffViewer on the file saved by POI, and then on a file loaded by excel + explicitly evaluated + saved. Find the formula record for the cell, and compare the POI one to the Excel one. Are the ptgs the same? Cached value the same? etc(In reply to Nick Burch from)it seems that cached value are the same but ptgs are not (one difference on ptg[6])here are ptgs generate by POI[FORMULA] .row = 0x0004 .col = 0x0004 .xfindex= 0x000F .value = <string> [00, 00, 00, 00, 00, 00] .options = 0x0002 .alwaysCalc= false .calcOnLoad= true .shared = false .zero = 0x00000000 Ptg[0]=org.apache.poi.ss.formula.ptg.IntPtg [1]. Ptg[1]=org.apache.poi.ss.formula.ptg.IntPtg [2]. Ptg[2]=class org.apache.poi.ss.formula.ptg.GreaterThanPtg. Ptg[3]=org.apache.poi.ss.formula.ptg.AttrPtg [if dist=9]. Ptg[4]=org.apache.poi.ss.formula.ptg.RefPtg [A5]R Ptg[5]=org.apache.poi.ss.formula.ptg.AttrPtg [skip dist=21]. Ptg[6]=org.apache.poi.ss.formula.ptg.RefPtg [A5]R Ptg[7]=org.apache.poi.ss.formula.ptg.IntPtg [4]. Ptg[8]=org.apache.poi.ss.formula.ptg.IntPtg [2]. Ptg[9]=org.apache.poi.ss.formula.ptg.FuncPtg [MID nArgs=3]V Ptg[10]=org.apache.poi.ss.formula.ptg.AttrPtg [skip dist=3]. Ptg[11]=org.apache.poi.ss.formula.ptg.FuncVarPtg [IF nArgs=3]V[/FORMULA]Offset=0x00003801(14337) recno=174 sid=0x0207 size=0x0005(5)[STRING] .string = $$[/STRING]ptgs generate by Excel[FORMULA] .row = 0x0004 .col = 0x0004 .xfindex= 0x000F .value = <string> [00, 00, 84, 2C, 4F, 06] .options = 0x0000 .alwaysCalc= false .calcOnLoad= false .shared = false .zero = 0xFE040005 Ptg[0]=org.apache.poi.ss.formula.ptg.IntPtg [1]. Ptg[1]=org.apache.poi.ss.formula.ptg.IntPtg [2]. Ptg[2]=class org.apache.poi.ss.formula.ptg.GreaterThanPtg. Ptg[3]=org.apache.poi.ss.formula.ptg.AttrPtg [if dist=9]. Ptg[4]=org.apache.poi.ss.formula.ptg.RefPtg [A5]R Ptg[5]=org.apache.poi.ss.formula.ptg.AttrPtg [skip dist=21]. Ptg[6]=org.apache.poi.ss.formula.ptg.RefPtg [A5]V Ptg[7]=org.apache.poi.ss.formula.ptg.IntPtg [4]. Ptg[8]=org.apache.poi.ss.formula.ptg.IntPtg [2]. Ptg[9]=org.apache.poi.ss.formula.ptg.FuncPtg [MID nArgs=3]V Ptg[10]=org.apache.poi.ss.formula.ptg.AttrPtg [skip dist=3]. Ptg[11]=org.apache.poi.ss.formula.ptg.FuncVarPtg [IF nArgs=3]V[/FORMULA]Offset=0x00001022(4130) recno=169 sid=0x0207 size=0x0005(5)[STRING] .string = $$[/STRING]$$ was the formula result valueIs this difference on ptg is important ? are there any solution ?all Formula where re-evaluate before closing workbook.if (_evaluator == null) _evaluator = _workbook.getCreationHelper().createFormulaEvaluator(); _evaluator.clearAllCachedResultValues(); for(int sheetNum = 0; sheetNum < _workbook.getNumberOfSheets(); sheetNum++) { Sheet sheet = _workbook.getSheetAt(sheetNum); for(Row r : sheet) { for(Cell c : r) { if(c.getCellType() == Cell.CELL_TYPE_FORMULA) { _evaluator.setDebugEvaluationOutputForNextEval(true); _evaluator.evaluateFormulaCell(c); } } } }_workbook.setForceFormulaRecalculation(true);it seems that the ptg[6] was a reference in POI but in Excel it was a value.It think that the bug is in OperandClassTransformer but i don't know how to resolve it.Can you help me ?why POI puts in some case a reference or a value to the cell in the MID function ?is there a workaround ?have you a solution for this problem ?You'll need to step through the formula evaluator in a debugger, and try to work out what area of the code is responsible for outputting the Ptg different to Excel. Once that's narrowed down, the fix will hopefully be easier to identify!If I change the second line in the resource file functionMetadata.txt by "1 IF 2 3 R V V V" all works perfectly.by default, MID Function return Value class. Or in IF function, the second and third parameter is a reference class. Could it be the problem ?	13.0	id=56522	6	False	True	hampidu	1
id=56490	REOPENED	None	POI	XSSF (	3.10-FINAL	PC All	P2 normal	POI Developers List	2014-05-05 16:56 UTC by	sam	2014-05-16 06:20 UTC (	0 users	Createdexcel sheetWe have an xlsx file exported for a regular tool.The file when we try open it using poi, throws null pointer exception.But when the xlsx file is opened saved and closed then poi works fine.	Any chance you could post the stacktrace you get?Exception in thread "main" java.lang.NullPointerException at org.apache.poi.xssf.usermodel.XSSFWorkbook.onDocumentRead(XSSFWorkbook.java:284) at org.apache.poi.POIXMLDocument.load(POIXMLDocument.java:159) at org.apache.poi.xssf.usermodel.XSSFWorkbook.<init>(XSSFWorkbook.java:221) at CompareData.Compare_Data.read_file(Compare_Data.java:68) at CompareData.Compare_Data.main(Compare_Data.java:50)Line 67-69: FileInputStream fileIn2 = new FileInputStream(fn); XSSFWorkbook filename2 = new XSSFWorkbook(fileIn2); XSSFSheet sheet1 = filename2.getSheetAt(0);I stumbled across the same in files produced by the Suunto Sports Watch export on movescount.com, when taking a closer look I saw that the latest trunk-version already has a fix via, so this one is actually a duplicate.*** This bug has been marked as a duplicate of***public static void main(String[] args) throws IOException { FileInputStream fileIn = new FileInputStream("C:/Lenovo/Lenovo_A.xlsx"); XSSFWorkbook filename = new XSSFWorkbook(fileIn); XSSFSheet sheet = filename.getSheetAt(0); String columnWanted = "Link"; Integer columnNo = null; List <Cell> cells = new ArrayList<Cell>(); Row firstRow = sheet.getRow(0); for (Cell cell:firstRow){ if(cell.getStringCellValue().equals(columnWanted)){ columnNo = cell.getColumnIndex(); } } if(columnNo != null){ for (Row row: sheet){ Cell c = row.getCell(columnNo); if(c == null || c.getCellType() == Cell.CELL_TYPE_BLANK){ } else { cells.add(c); } } } else { System.out.println("could not find column" + columnWanted + "in first row of" + fileIn.toString()); } }I have this code. Now I get Exception in thread "main" at line for (Cell cell:firstRow) with same excel fileI even tried to print all rows, it only prints the last row.This file seems to be built in a way we do not expect at all, when I run the following: for(Row row : sheet) { for(Cell cell : row) { System.out.println("Cell in Row: " + row.getRowNum() + ": " + cell.toString()); } }It prints out the following, i.e. only one row with num "-1" is found where the last value in the column is returned, irrespective of the row in which it actually is, looks like all cells are added to the same "-1" row:Cell in Row: -1: ThinkPad X240Cell in Row: -1: Intel® Core™ i5-4200U processor (2 cores, 1.60GHz, 3MB cache)Cell in Row: -1: Windows 8 Standard 64 - EnglishCell in Row: -1: 12.5" HD WXGA (1366 X 768) LED Backlight w/ 720p HD CameraCell in Row: -1: Intel HD Graphics 4400Cell in Row: -1: 4 GB PC3-12800 DDR3 SDRAM 1600MHz SODIMM MemoryCell in Row: -1: 500GB, 7200RPM Serial ATA 2.5" Hard DriveCell in Row: -1: No Optical IncludedCell in Row: -1: $761.60	6.0	id=58489	6	False	False	havelj1	1
id=58240	REOPENED	None	POI	HSSF (	3.12-FINAL	PC Linux	P2 major	POI Developers List	2015-08-12 15:15 UTC by	None	2016-04-10 11:16 UTC (	0 users	I have XLS file generated with POI (either 3.11 or 3.12). File is OK if I open it with Open Office or Google Drive but fails in MS Office. Found that it fails with the MS Validator() with :<BFFValidation path="NAME.xlsx" datetime="08/12/15 07:44:35" result="FAILED"> <ParseStack> <Type builtinType="Docfile" docName="MS-XLS" sectionTitle="Compound File" msdnLink=""> <Info>Built-in type "Docfile": The root storage object of an OLE compound file. For more information, see.</Info> </Type> <Type builtinType="Stream" docName="MS-XLS" sectionTitle="Stream" msdnLink="" streamName="Workbook" streamOffset="0" hexStreamOffset="0x0"> <Info>Built-in type "Stream": Any stream object for OLE compound files. The entire file contents for other files.</Info> </Type> <Type docName="MS-XLS" sectionTitle="Workbook Stream (Workbook)" msdnLink="" streamName="Workbook" streamOffset="2747" hexStreamOffset="0xabb"/> <Type docName="MS-XLS" sectionTitle="Workbook Stream (Workbook)" msdnLink="" streamName="Workbook" streamOffset="2747" hexStreamOffset="0xabb"/> <Type docName="MS-XLS" sectionTitle="Substream" msdnLink="" streamName="Workbook" streamOffset="2767" hexStreamOffset="0xacf"/> <Type docName="MS-XLS" sectionTitle="Record" msdnLink="" streamName="Workbook" streamOffset="2767" hexStreamOffset="0xacf"/> <Type docName="MS-XLS" sectionTitle="Index" sectionNumber="2.4.144" msdnLink="" streamName="Workbook" streamOffset="2771" hexStreamOffset="0xad3"/> <Type docName="MS-OSHARED" sectionTitle="FilePointer" msdnLink="" streamName="Workbook" streamOffset="2783" hexStreamOffset="0xadf" childId="4" hexChildId="0x4"/> </ParseStack> <LastData><![CDATA[00 00 00 00 -- -- -- -- -- -- -- -- -- -- -- -- ....]]></LastData></BFFValidation>So it seem POI has generated incompatible file. The content is quite simple - single sheet with two rows (first one bold) and several cells.Any suggestions?ThanksAleksandar	Can you try again with a recent nightly build / wait a week or so then try again with 3.13 beta 2?I tried with the latest available maven dependency 3.13-beta1 - same result.As requested:OK, same with the nightly build from today -Best,AleksandarHey,I played a bit to see if I can find some better reason - nope.I cut the code to just this : Workbook workbook = new HSSFWorkbook(); Sheet sheet = workbook.createSheet(); workbook.write(stream); workbook.close();The validation still fails. I wouldn't care if MS Office didn't refuses to load the file.It runs under JBoss wildfly-8.1.0.Final with jdk1.7.0_75, same with OpenSuse(13.1) and Ubuntu (both x64).The situation is really strange ...Best,AleksandarI tried the steps you list, the resulting document opens just fine in Excel for me.I also tried the validator, it reported FAILED similar to your output, however I tried a few other documents and many fail, even some that I saved using Excel itself, not sure how good the validation matches to what Excel actually allows...Any chance that an older version of poi is pulled in here for some reason? It seems like it is somehow only happening for you here.No update for some time therefor I am resolving this for now until we get more information that allows us to reproduce this. We primarily depend on Excel itself to verify files.Hello,what other information do you need to have a look at the issue. As described the file generated fails the MS validation.Best,Aleksandar	8.0	id=58935	5	False	True	markt	1
id=55081	REOPENED	None	POI	SS Common (	3.10-dev	All All	P2 enhancement	POI Developers List	2013-06-08 16:26 UTC by	Cédric Walter	2017-02-24 05:49 UTC (	0 users	Createdtestcase WeekNumFunctionTestCaseData.xlsReturns a number that indicates where the week falls numerically within a year.patch for missing function WEEKNUMWEEKNUM function is part of Analysis ToolPak add-in.Testcases in WeekNumFunctionTestCaseData.xlsWritten for Java 1.5	Createdpatch for missing function WEEKNUMCreatedAdd additional sanity check/integration test FormulaEvalTestData.xlsNote that WEEKNUM is a built-in function (not part of the analysis tookpak) for Excel 2010/2011 and Excel 2013 (and possibly Excel 2007/2008)(In reply to Niggler from)That could be fun. Any chance someone could create a simple spreadsheet with Excel 2003, and another with 2013 (both need to be fresh files), both of which make a single use of WEEKNUM? That'll let us confirm if they both write it the same even with the analysis tookpak changePatch committedNow assert validity of weeknum implemntation with 2 simple spreadsheet (Excel 2003, and another with 2013 - both are fresh files), both of which make a single use of WEEKNUMPlease note that implementation is incomplete as it does not handle the default return value for the function. The function works fine if it has 2 arguments (the last being the return type) but fails if the user just use WEEKNUM(cell) and omits the second argument for the return type. RegardsDmitry VasilenkoUpdate summary to reflect current state of this bugDocumentation:CreatedPatch adding the 1-argument version of WeekNumThis patch is untested. The unit test coverage for WeekNum is currently in Excel files.test-data/spreadsheet/WeekNumFunctionTestCaseData*.xls	9.0	id=38471	10	False	False	andreas	1
id=58489	REOPENED	None	Tomcat Modules	jdbc-pool (	unspecified	All All	P2 major	Tomcat Developers Mailing List	2015-10-08 14:08 UTC by	Jan Havel	2016-07-24 10:31 UTC (	2 users	CreatedJUnit test to show the exceptionOverview: QueryStatsComparator may throw java.lang.IllegalArgumentException: Comparison method violates its general contract! This happens in specific situations depending on the data in the reported queries. In our environment this occurs every time we keep the server running until the maxCapacity (1000 items) is reached and the removeOldest method is called. I have managed to narrow down the test data from more than 1000 records to around 40 items and created a JUnit test to showcase the error. I needed to copy some parts out of the SlowQueryReport class and made some simplifications as the QueryStatsComparator is not reachable from outside.Steps to Reproduce: Please use the attached JUnit source code to reproduce this error.Additional Builds and Platforms: This bug seems to happen on all platforms.We are using Tomcat 8, tomcat-dbcp 8.0.24, JDK 1.80.0_60.Additional Information: It seems to be a problem of the first two lines of the QueryStatsComparator. Note that a single change in the data of the unit test may cause the Exception not to occur.Stacktrace:java.lang.IllegalArgumentException: Comparison method violates its general contract! at java.util.TimSort.mergeLo(TimSort.java:773) at java.util.TimSort.mergeAt(TimSort.java:510) at java.util.TimSort.mergeCollapse(TimSort.java:435) at java.util.TimSort.sort(TimSort.java:241) at java.util.Arrays.sort(Arrays.java:1512) at java.util.ArrayList.sort(ArrayList.java:1454) at java.util.Collections.sort(Collections.java:175) at org.apache.tomcat.jdbc.pool.interceptor.SlowQueryReport.removeOldest(SlowQueryReport.java:218) at org.apache.tomcat.jdbc.pool.interceptor.SlowQueryReport.getQueryStats(SlowQueryReport.java:205) at org.apache.tomcat.jdbc.pool.interceptor.SlowQueryReport.reportQuery(SlowQueryReport.java:119) at org.apache.tomcat.jdbc.pool.interceptor.AbstractQueryReport$StatementProxy.invoke(AbstractQueryReport.java:254)	SlowQueryReport.java line 478 is the reason for this crash. compare(x,x) should not return 1.if (stats1.lastInvocation == 0 && stats2.lastInvocation != 0) return 1;Thanks for the report.Will be fixed with 8.0.29. Maybe you could give the trunk of tomcat 8 a try?I forgot to mention that it affects the Tomcat 7 as well. (Thats where I have this issue right now). I think the bug was introduced with 7.0.63 release.I have ported the fix back to tomcat 7 as well. It will be in tomcat 7.0.66.And again it would be nice, if you could test the current trunk of tomcat 7.CreatedTomcat-7.0.68 - Caused by: java.lang.IllegalArgumentException: Comparison method violates its general contract!Comment onTomcat-7.0.68 - Caused by: java.lang.IllegalArgumentException: Comparison method violates its general contract!We are facing same exception in our env’s when we have upgraded our setup from: From : Java : jdk1.6.0_45 Java Apache Tomcat Version 7.0.37 To Java : jdk1.8.0_92 Java Apache Tomcat Version 7.0.68Sample SQL which during which this exception occurred :SELECT DISTINCT c.comp_id AS dbkeyFROM equipment e, component cWHERE e.equip_id = c.equip_id AND((e.ptnii_equip_name = 'cl2oh406me3' AND c.beg_slot = 7) AND trunc(c.port/1000)-1 = '2') AND mod(c.port, 1000) = 0JFYI : Result I see in TOAD by directly executing above SQL is only 1 record with value as : 1166506Caused by: java.lang.IllegalArgumentException: Comparison method violates its general contract! at java.util.TimSort.mergeLo(TimSort.java:777) at java.util.TimSort.mergeAt(TimSort.java:514) at java.util.TimSort.mergeCollapse(TimSort.java:439) at java.util.TimSort.sort(TimSort.java:245) at java.util.Arrays.sort(Arrays.java:1512) at java.util.ArrayList.sort(ArrayList.java:1454) at java.util.Collections.sort(Collections.java:175) at org.apache.tomcat.jdbc.pool.interceptor.SlowQueryReport.removeOldest(SlowQueryReport.java:213) at org.apache.tomcat.jdbc.pool.interceptor.SlowQueryReport.getQueryStats(SlowQueryReport.java:200) at org.apache.tomcat.jdbc.pool.interceptor.SlowQueryReport.reportQuery(SlowQueryReport.java:110) at org.apache.tomcat.jdbc.pool.interceptor.AbstractQueryReport$StatementProxy.invoke(AbstractQueryReport.java:256) at com.sun.proxy.$Proxy666.executeQuery(Unknown Source) at sun.reflect.GeneratedMethodAccessor4273.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.tomcat.jdbc.pool.interceptor.AbstractQueryReport$StatementProxy.invoke(AbstractQueryReport.java:235) at com.sun.proxy.$Proxy666.executeQuery(Unknown Source) at com.att.lpp.pa.core.asset.dao.AssetSearchDaoImpl.findAssetDbkeys(AssetSearchDaoImpl.java:116)I believe this can happen, when the QueryStats change while the list of stats get sorted.I see three ways to go: 1. add synchronization, so that the stats don't change while the list gets sorted 2. extract the information to sort on from the QueryStats and sort on that information 3. make QueryStats immutable and change the QueryStats instances, when the stats gets updatedThe first path probably will slow down the execution of jdbc calls.The second one will be more memory intensive, but relatively simple to implement. The third seems to be the one where we would have to do the most work.I tend to go for option two. What do you think?***has been marked as a duplicate of this bug. ***	8.0	id=58966	14	False	True	jmclej	1
id=56522	REOPENED	None	Tomcat 8	EL (	8.0.5	PC All	P2 enhancement	Tomcat Developers Mailing List	2014-05-14 08:11 UTC by	Hanspeter Dünnenberger	2016-03-11 08:44 UTC (	0 users	 	According to EL 3.0 Spec 1.23.1 and 1.23.2 the Rule for null String is to coerce to "". EL RI 3.0 behaves that way, but jasper-el 8 doesn't.The null-String to "" coercion works well in ValueExpression.getValue(elContext), but in case of ValueExpression.setValue(elContext, null) jasper-el 8 does not coerce null-String to "", instead null ends up on the VE target.One more thing to mention: I think the EL Spec is wrong to define null String must be converted to "". If you also think the spec'ed behavior for null String is wrong, support the EL spec issueThis has been fixed in 8.0.x for 8.0.7 onwards.Please remove this aligment to spec on setValue method.Because javax.faces.INTERPRET_EMPTY_STRING_SUBMITTED_VALUES_AS_NULL do not WORK with jasper-el 8EL RI javax.el-3.0.1-b08.jar - already did this.We can look at adding an option to make this configurable but, by default, Tomcat is going to remain specification compliant.	5.0	id=60102	11	False	True	kenneth_lau	1
id=58966	REOPENED	None	POI	SS Common (	3.10-FINAL	PC All	P2 critical	POI Developers List	2016-02-03 15:52 UTC by	None	2016-02-09 16:15 UTC (	1 user	Hello,I am having a problem with retrieving the right data from excel:I have the cell D5 that contains the formula "=3000000/D10" and D10 cell contains the formula "=1.41973/1.2939".When I read this cell within VBA macro like this : Worksheets("My sheet").Cells(5, 4), I get 2734111.41555084 but when I read it via POI in Java:XSSFSheet sheet = workbook.getSheetAt(1);final Iterator<Row> rowIterator = sheet.iterator();Cell cell;while (rowIterator.hasNext()) { final Row row = rowIterator.next();... cell = row.getCell(col, Row.RETURN_BLANK_AS_NULL);...}cell contains here 2734111.4155508447 (see the extra 47), which is wrong.Indeed, when I type in a calculator 3000000/(1.41973/1.2939), the result is 2734111.4155508441745965782226198 ()I expect to get the same result as VBA gives.Regards,	The number stored in the file is rarely what Excel shows. To get the value that Excel shows, as a String, you must use something like DataFormatter to apply the formatting rules to the raw cell valueIndependently of what Excel shows, there is definitely a problem with POI as 2734111.4155508441745965782226198 cannot be rounded to 2734111.4155508447 if you look closely at the first number (...844174 cannot give ...8447)(In reply to jmclej from)Sure it can. You're thinking of base 10 rounding, while the computer does base 2 rounding. The binary representation of ...844174 and ...8447 might be adjacent, depending on how the floats are stored, what arithmetic is used to calculate the values, the rounding errors that are accumulated along the way, etc. Even simple 3*(1.0/3) could give an answer like 1.000000000007 or 0.9999999999994, which doesn't round to the correct answer(In reply to Nick Burch from)In VBA, I have :Dim MyVal As DoubleMyVal = Worksheets("My sheet").Cells(5, 4)And I am talking about the number I see in the file into which I print MyVal without transformation or that I look at MyVal in the VBA spy window.So I am expecting to get the same number via POI without the need to apply any DataFormatter in Java, which I couldn't know which it is anyway as I am using the default behavior of VBA. POI should reproduce the default behavior of Excel as well.(In reply to jmclej from)Re-read Nick's comment. POI does have rhe samw behavior as Excel when you use the DataFormatter class.Ok, Indeed, I didn't get that DataFormatter was provided by POI.But when I am doing this now :XSSFSheet sheet = workbook.getSheetAt(1);final Iterator<Row> rowIterator = sheet.iterator();Cell cell;while (rowIterator.hasNext()) { final Row row = rowIterator.next();... cell = row.getCell(col, Row.RETURN_BLANK_AS_NULL); DataFormatter df = new DataFormatter();//same with DataFormatter(true) org.apache.poi.ss.usermodel.CellValue cv = evaluator.evaluate(cell); String val = df.formatCellValue(cell, evaluator);...}I still get val that is equal to 2734111.4155508447In, it says that the default format for decimal numbers is "#.##########", but it I can't reproduce that. Or should I use a different code than the one above to use DataFormatter ?I also tried (new java.text.DecimalFormat("#.##########")).format(cell.getNumericCellValue()) but it gives also 2734111.4155508447 instead of 2734111.41555084 on Excel.I forgot to declare and define evaluator :final FormulaEvaluator evaluator = workbook.getCreationHelper().createFormulaEvaluator();Also, workbook is of type XSSFWorkbook. Can it have an effect on my issue ?Sorry about the different small comments, I am trying to give as much relevant information as possible...So I must say that I read a xlsm file.(In reply to jmclej from)I have tried using WorkBook instead of XSSFWorkbook with WorkbookFactory.create method but it still gives me the same result.Additional info : I am using Excel 2013Version of poi : 3.10.1Looks like the DataFormatter is working as expected. The number of digits is conaistenr with the format string.We could add another 2 # characters to the format string, but someone would need to prove that every version of Excel on all platforms use the longer format string for any number or formula. I'm guessing that the default format string was chosen to be the length it is for a good reason, and wasn't selected at random.Until then, if you need your code to return exactly the same value as Excel, you'll need to specify a custom format string.If you have usage questions, please use the POI Users mailing list [1].[1]When you read this for instance :We understand that Excel never gives more than 15 digits, so neither should POI, whatever the format put with more than 15 '#'. It should only add non-significative zeros.But in my case POI gives 17 digits (2734111.4155508447) instead of the 15 that I am talking about (2734111.41555084).More authoritatively, [1] which specifies[1]Hello,Now that the bug has been prooved, how does it work? On which version can we expect to have the correction and do we have an idea when it will be delivered ?Thanks for your work,Refer toPlease always remember: nobody is paid to work on POI, the team is a bunch of volunteers who look at things in their free time. Because of that developers might choose to work on things based on a different priority than yours! Especially the quality and maturity of bug reports will affect if somebody decides to look at it. So the best way to help a bug report see progress is to provide more information if available or supply patches together with unit-tests.	17.0	id=60750	5	False	False	knst.kolinko	1
id=60102	REOPENED	None	POI	XWPF (	3.15-dev	PC All	P2 enhancement	POI Developers List	2016-09-09 23:57 UTC by	None	2016-09-27 15:57 UTC (	1 user	CreatedWord docx file failed to writedocx XWPFDocument opened from FileInputStream failed to write out to a new FileOutputStreamHere's the stacktrace --org.apache.poi.openxml4j.exceptions.OpenXML4JRuntimeException: Rule M2.4 exception : this error should NEVER happen! Please raise a bug atand attach a file that triggers it, thanks! at org.apache.poi.openxml4j.opc.internal.ContentTypeManager.getContentType(ContentTypeManager.java:343) at org.apache.poi.openxml4j.opc.internal.ContentTypeManager.removeContentType(ContentTypeManager.java:256) at org.apache.poi.openxml4j.opc.OPCPackage.removePart(OPCPackage.java:958) at org.apache.poi.openxml4j.opc.PackagePart.getOutputStream(PackagePart.java:522) at org.apache.poi.xwpf.usermodel.XWPFDocument.commit(XWPFDocument.java:716) at org.apache.poi.POIXMLDocumentPart.onSave(POIXMLDocumentPart.java:464) at org.apache.poi.POIXMLDocument.write(POIXMLDocument.java:211)	I am unable to reproduce this in the latest nightly build of POI ().Could you either check this against 3.15-beta2 or later a nightly, or produce a unit test that produces the error seen in?Apache commons collections was recently added as a dependency and will need to be added to your class path.Here's my test case:@Testpublic void test60102() throws Exception { // read in from a java.io.File //XWPFDocument doc = XWPFTestDataSamples.openSampleDocument("60102.docx"); // or read in from a java.io.FileInputStream File infile = POIDataSamples.getDocumentInstance().getFile("60102.docx"); InputStream fis = new FileInputStream(infile); XWPFDocument doc = new XWPFDocument(fis); // Write out to a file File outfile = TempFile.createTempFile("60102-resaved", "docx"); OutputStream fos = new FileOutputStream(outfile); doc.write(fos); fos.close(); // Write out to a byte array output stream XWPFDocument doc2 = XWPFTestDataSamples.writeOutAndReadBack(doc); doc2.close(); doc.close();}I've debugged into the issue further. The problem is reproduced after the OutputStream is closed by mistake after the first write.Change test case test60102() to following --// Write out to a file File outfile = TempFile.createTempFile("60102-resaved", "docx"); OutputStream fos = new FileOutputStream(outfile); doc.write(fos); fos.close(); doc.write(fos);The second doc.write(fos) failed because OutputStream was closed by mistake.The exception message was misleading. However, I've consider this is a usage error.Throw IOException when writing a closed document implemented inOutputStream out;doc.close();doc.write(out);I have not yet implemented throwing an exception when writing an open document to a closed output stream.OutputStream out;out.close();doc.write(out);CreatedReplace OpenXML4J return boolean with throws OpenXML4JException(In reply to Javen O'Neal from)To get this one closed, we're going to have to make quite a few changes to methods in org.apache.poi.openxml4j.opc.internal.These methods currently catch exceptions, occasionally log the error to the POILogger, and then return a boolean success value.These will need to be replaced with void-returning functions that raise an exception. This will eliminate the need to check a return code, enable the POI logger, and check the logs.A quick glance at the source code history reveals that the OpenXML4j classes have returned boolean success rather than throwing exceptions. Perhaps this is because this library originated from a C project rewritten in Java without replacing the C idioms with Java idioms.***has been marked as a duplicate of this bug. ***	5.0	id=55604	11	False	False	wsolaligue	1
id=60469	REOPENED	None	Tomcat 8	Catalina (	8.5.x-trunk	All All	P2 enhancement	Tomcat Developers Mailing List	2016-12-12 13:20 UTC by	Michael Osipov	2017-02-11 11:01 UTC (	0 users	If your custom realm extends RealmBase but uses a custom principal, just like mine, you need to duplicate the entire #hasRole(Wrapper, Princpal, String) method.The only interesting part for an implementor is!(principal instanceof GenericPrincipal)andGenericPrincipal gp = (GenericPrincipal) principal;boolean result = gp.hasRole(role);A verbatim copy of the method is need just to adapt three lines. Which basically means to double-check every Tomcat release for changes in this method.I am proposing to introduce a new sub-method: hasRoleInternal(Wrapper, Principal, String) (or alike) which performs just the code above. The entire boilerplate code remains in #hasRole()My custom realm would simply do:public boolean hasRoleInternal(Wrappr wrapper, Principal principal, String role) { if(!(principal instanceof CustomPrincipal)) return false; CustomPrincipal cp = (CustomPrincipal) principal; return cp.hasRole(role);}I can provide a patch for that if you agree with.	I disagree, it's not worth changing the API for it and it's worse than "checking for changes" which actually never happens.BTW, this is an "enhancement", not a "major" bug.(In reply to Remy Maucherat from)Why do you close this issue without having a discussion first?No one is changing the API. You can still continue to override #hasRole(), it will have the same effect. The change can happen and are not a part of an API at at all. The code of RealmBase#hasRole() is implementation specific, not to be known to the user at best.Anyone else care to share an opinion on that?Feel free to post a patch. I think it's a reasonable change.(In reply to Christopher Schultz from)Thanks, I will prepare one next week.	5.0	id=60469	5	False	True	remm	1
id=60750	REOPENED	None	Tomcat 7	Catalina (	7.0.70	PC Linux	P2 major	Tomcat Developers Mailing List	2017-02-20 13:39 UTC by	None	2017-02-23 17:04 UTC (	0 users	Createdchunked with content-lengthPOSTHTTP/1.1Host: 192.168.1.153:8080Connection: keep-aliveContent-Length: 15Accept: application/json, text/javascript, */*; q=0.01Origin:X-Requested-With: XMLHttpRequestUser-Agent: Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36Content-Type: application/x-www-form-urlencoded; charset=UTF-8DNT: 1Referer:Accept-Encoding: gzip, deflateAccept-Language: zh-CN,zh;q=0.8,en;q=0.6,ja;q=0.4,zh-TW;q=0.2Cookie: JSESSIONID=58A5A067D9686D43146177A27F7C3ADA; __lnkrntdmcvrd=-1; currMenu=%E9%A6%96%E9%A1%B5; PLAY_SESSION="2c9de2e98e99047a6f56d82c3558b38ba77efb9e-accountName=&stationCode=CHS201604060327&appId=wxee37461196767726&flag=&accountId=586&wxAppName=F6%E9%AB%98%E5%AE%9D%E6%B1%BD%E8%BD%A6%E6%9C%8D%E5%8A%A1%E5%8F%B7&activeDate=null&stationId=537&account=gbxq&parentAccountId=null&password=96e79218965eb72c92a549dd5a330112&isSystem=1"; F6ONL_SESSION="d4b1bcc15c082a61a1e12db295e3c51f3635de20-pwd=96e79218965eb72c92a549dd5a330112&username=gbxq&creditValue=0.0&orgCode=CHS201604060327&prjRights=bsx2016%2Cpts2016&userid=243515852076984501487578195330"; totalReminder=%5Bobject%20Object%5DHTTP/1.1 200 OKServer: Apache-Coyote/1.1Content-Type: application/json;charset=UTF-8Transfer-Encoding: chunkedDate: Mon, 20 Feb 2017 08:12:05 GMTe2{"btnStatus":"","idBill":null,"idCar":null,"idCustomer":null,"idPay":null,"idStock":null,"list":null,"noBill":null,"noPay":null,"noStock":null,"options":null,"pageResult":null,"param":"","status":"success","str":"","token":""}	I do not see any bug here.The request has Content-Length.The response has "Transfer-Encoding: chunked" and does not have a Content-Length. This is all as expected.Read the HTTP protocol specification and ask you question on the users mailing list. Bugzilla is not a support forum.Creatednormal requestwith 0-sized chunkCreatederror requestwithout 0-sized chunkWhat code do you have that results in a non-zero chunk followed by *no* final zero-byte chunk?Very simple code! OutputStream out = outputMessage.getBody(); String text = JSON.toJSONString(obj, features); byte[] bytes = text.getBytes(charset); out.write(bytes);The same code runs in product or development env very normal .Just become abnormal in test env (with ip).信息: Server version: Apache Tomcat/7.0.70信息: Server built: Jun 15 2016 16:27:45 UTC信息: Server number: 7.0.70.0信息: OS Name: Linux信息: OS Version: 2.6.32-504.el6.x86_64信息: Architecture: amd64信息: Java Home: /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.65.x86_64/jre信息: JVM Version: 1.7.0_65-mockbuild_2014_07_16_06_06-b00信息: JVM Vendor: Oracle Corporation信息: CATALINA_BASE: /home/opt/apache-tomcat-7.0.70-erp信息: CATALINA_HOME: /home/opt/apache-tomcat-7.0.70-erp信息: Command line argument: -Djava.util.logging.config.file=/home/opt/apache-tomcat-7.0.70-erp/conf/logging.properties信息: Command line argument: -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager信息: Command line argument: -Djdk.tls.ephemeralDHKeySize=2048信息: Command line argument: -Dcom.sun.management.jmxremote信息: Command line argument: -XX:PermSize=64M信息: Command line argument: -XX:MaxNewSize=128m信息: Command line argument: -XX:MaxPermSize=128m信息: Command line argument: -Djava.rmi.server.hostname=192.168.1.153信息: Command line argument: -Dcom.sun.management.jmxremote.port=9999信息: Command line argument: -Dcom.sun.management.jmxremote.ssl=false信息: Command line argument: -Dcom.sun.management.jmxremote.authenticate=false信息: Command line argument: -XX:+PrintGCTimeStamps信息: Command line argument: -XX:+PrintGCDetails信息: Command line argument: -Xloggc:logs/gc.log信息: Command line argument: -Xdebug信息: Command line argument: -Xnoagent信息: Command line argument: -Djava.compiler=NONE信息: Command line argument: -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5888信息: Command line argument: -Djava.endorsed.dirs=/home/opt/apache-tomcat-7.0.70-erp/endorsed信息: Command line argument: -Dcatalina.base=/home/opt/apache-tomcat-7.0.70-erp信息: Command line argument: -Dcatalina.home=/home/opt/apache-tomcat-7.0.70-erp信息: Command line argument: -Djava.io.tmpdir=/home/opt/apache-tomcat-7.0.70-erp/temp信息: The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: /usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/libcase CLOSE_NOW: { // Block further output getOutputBuffer().finished = true; setErrorState(ErrorState.CLOSE_NOW, null); break; } public void endRequest() throws IOException { if (!committed) { // Send the connector a request for commit. The connector should // then validate the headers, send them (using sendHeader) and // set the filters accordingly. response.action(ActionCode.COMMIT, null); } if (finished) return; if (lastActiveFilter != -1) activeFilters[lastActiveFilter].end(); finished = true; }"http-bio-8080-exec-15@10822" daemon prio=5 tid=0x1dd nid=NA runnable java.lang.Thread.State: RUNNABLE at org.apache.coyote.Response.setCommitted(Response.java:217) at org.apache.coyote.http11.InternalOutputBuffer.commit(InternalOutputBuffer.java:194) at org.apache.coyote.http11.AbstractHttp11Processor.action(AbstractHttp11Processor.java:776) at org.apache.coyote.Response.action(Response.java:174) at org.apache.coyote.Response.sendHeaders(Response.java:348) at org.apache.catalina.connector.OutputBuffer.doFlush(OutputBuffer.java:352) at org.apache.catalina.connector.OutputBuffer.flush(OutputBuffer.java:334) at org.apache.catalina.connector.CoyoteOutputStream.flush(CoyoteOutputStream.java:101) at org.springframework.http.converter.AbstractHttpMessageConverter.write(AbstractHttpMessageConverter.java:180) at org.springframework.web.servlet.mvc.method.annotation.AbstractMessageConverterMethodProcessor.writeWithMessageConverters(AbstractMessageConverterMethodProcessor.java:148) at org.springframework.web.servlet.mvc.method.annotation.AbstractMessageConverterMethodProcessor.writeWithMessageConverters(AbstractMessageConverterMethodProcessor.java:90) at org.springframework.web.servlet.mvc.method.annotation.RequestResponseBodyMethodProcessor.handleReturnValue(RequestResponseBodyMethodProcessor.java:189) at org.springframework.web.method.support.HandlerMethodReturnValueHandlerComposite.handleReturnValue(HandlerMethodReturnValueHandlerComposite.java:69) at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:122) at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:745) at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:686) at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:80) at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:925) at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:856) at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:953) at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:855) at javax.servlet.http.HttpServlet.service(HttpServlet.java:650) at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:829) at javax.servlet.http.HttpServlet.service(HttpServlet.java:731) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61) at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108) at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137) at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66) at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449) at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365) at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90) at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83) at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:387) at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362) at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:343) at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:260) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:218) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122) at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:505) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103) at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:956) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:442) at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1082) at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:623) at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:318) - locked <0x2bf4> (a org.apache.tomcat.util.net.SocketWrapper) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:745)Then "http-bio-8080-exec-15@10822" daemon prio=5 tid=0x1dd nid=NA runnable java.lang.Thread.State: RUNNABLE at org.apache.coyote.http11.AbstractHttp11Processor.action(AbstractHttp11Processor.java:905) at org.apache.coyote.Response.action(Response.java:172) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:117) at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:956) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:442) at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1082) at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:623) at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:318)public void endRequest() throws IOException { if (!committed) { // Send the connector a request for commit. The connector should // then validate the headers, send them (using sendHeader) and // set the filters accordingly. response.action(ActionCode.COMMIT, null); } if (finished) return; if (lastActiveFilter != -1) activeFilters[lastActiveFilter].end(); finished = true; }At this point (finished==true),tomcat will not send /** * End the current request. It is acceptable to write extra bytes using * buffer.doWrite during the execution of this method. */ @Override public long end() throws IOException { // Write end chunk buffer.doWrite(END_CHUNK, null); return 0; }So the error occours.Updated the SUMMARY field.You are using Tomcat 7.0.70. The current version of Tomcat 7 is 7.0.75.I wonder whether the behaviour is reproducible with Tomcat 7.0.75.The above class in stacktrace means that you are using BIO connector implementation.Can this be simplified further to be reproduced without 3-rd party classes?A simple sample web application?It seems that your hypothesis inandis the following:1. The zero-length chunk is not send because org.apache.coyote.http11.filters.ChunkedOutputFilter.end() method is not called.2. The end() method is not called (in org.apache.coyote.http11.AbstractOutputBuffer.endRequest())because 'AbstractOutputBuffer.finished' flag is 'true'.3. The 'AbstractOutputBuffer.finished' flag is set to 'true' inorg.apache.coyote.http11.AbstractHttp11Processor.action()when processing 'case CLOSE_NOW'.This behaviour is by design. The "CLOSE_NOW" action code means that connection must be terminated immediately, without any further I/O.E.g. some fatal error has occurred.E.g. the client went away (closed its side of connection) without waiting for your end chunk.See the javadoc:The CLOSE_NOW code was introduced by(Tomcat 7.0.55 onwards), 2,5 years ago.	9.0	id=47311	11	True	True	vhennebert	1
id=47311	REOPENED	None	Fop - Now in Jira	general (	all	All All	P2 enhancement	None	2009-06-03 10:53 UTC by	Peter Coppens	2012-04-11 03:22 UTC (	0 users	CreatedPatch adding support for bleed, trim and crop box and scalingThe attached patch adds support for 4 new simple-page-master fop extension attributes, namely- fox:crop-box- fox:trim-box- fox:bleed-box- fox:scaleThe box attributes can consist out of up to 4 numeric values (top,left, width, height) and can have units as suffix to each of the numbers.The scale attribute can consist out of two numbers each between 0 and 1It is implemented for PDF and Java2D renderers.	Thanks for this patch, Peter!I know this is going to be very useful, as questions regarding that functionality pop up on the user list from time to time. We'll look into it, and will keep you informed. I currently have other priorities, but as soon as I see an available slot of time, if no one beats me to it, I'll go into it in more detail.(In reply to)I was wondering whether there is anything I can do to make this move forward. Thanks,PeterHi Peter,(In reply to)Not much until we have reviewed the patch and provided comments. Then, if modifications are needed you may want to do them yourself and provide an updated patch, which would speed up its integration in the code base.I've only had a quick look so far and can't give much feedback yet. I'll try to have a deeper look in the next days. A few things I noticed, though:- the IllegalArgumentException in ExtensionElementMapping will have to be replaced by a call to FOP's event notification mechanism (so that the message can be localized, among others things). Have a look in, e.g., org.apache.fop.fo.flow.Table.java to see how it is used (TableEventProducer in that case)- the new features will have to be documented on the website. The corresponding source files may be found in the src/documentation/content/xdocs/trunk directory. The ideal place probably is extensions.xml, with a link in output.xml.- there are Checkstyle warnings in the new code. You can set up Checkstyle using the checkstyle-4.0.xml at the root of the project.Those are things that we will have to do before applying the patch anyway, so if you want to speed up the process you can have a go at them.Back later, hopefully, for comments on the functionalities themselves.Thanks!VincentI've had a more detailed look at the patch and it looks good to me. A few other comments:- the definition and parsing of the extension properties should not be put in ExtensionElementMapping, but in a class (and a sub-package) of its own. Where exactly it should be put is not entirely clear to me yet. Probably in a new o.a.f.render.extensions package. How to name the new sub-package also is an open question (scalingcropbleedtrim? rather ugly... pageboundaries (taken from the PDF spec)? prepress?).- amendment to what I said about the event notification mechanism: the extension should not use it, rather throw appropriate exceptions, which would be passed over by the PDF library (the classes in o.a.f.pdf) to the PDFDocumentHandler. The former two should remain independent of the notification mechanism to allow later extraction from the FOP codebase and modularization. Only the PDFDocumentHandler must be aware of the notification mechanism.- the regexp parsing properties should be made slightly more robust and unit-tested.- I have a concern about when the properties are actually parsed. Like it is now they will be parsed only at rendering stage, so if there is a mistake in them the error will be thrown rather 'late' in the process (only after layout has been performed). May that be a problem? Do we want a 'fail-fast' behaviour instead? Open question.I'm happy to do all the mentioned changes myself, but this will be in one week at the earliest as I'm away next week. Peter, if meanwhile you want to submit an updated patch feel free to do so :-)Thanks,Vincent(In reply to)Well, since the extension attributes are specific to the PDF renderer, I think it is not a blocker to treat them as plain 'foreign attributes' at parse-time. One case comes to mind where we would need to treat them as genuine 'properties': suppose we want to be able to specify them as expressions, based on other properties.(In reply to)Sorry, ignore this. I just noticed that in the patch, the extension is also implemented for the Java2D-based renderers...Just noting this for general interest (has little or nothing to do with this issue per se):The previous comments suddenly reminded me that I have always considered the current way that extension /properties/ are handled, as lacking in robustness. We practically force potential implementors of extension properties to modify FOP's codebase. For extension elements or attributes, the pattern is much more open and generic. They can be implemented without necessarily having to modify FOP and recompile. The same should become the case for extension properties, eventually. We probably will want to take a look at offering something like an 'ExtensionPropertyMapping' (currently non-existent), so that implementors can re-use existing PropertyMakers, define their own initial values/enums/keywords, mark properties as inherited, and so on...There are already a few 'native' extension properties, for which we define symbolic literals in fo.Constants and which are also registered in fo.FOPropertyMapping. I have never really been too happy with that practice. Contributors are invited to follow that pattern, which will only lead to more clutter.Tackling that issue, however, is not for the faint of heart. It would likely also imply revisiting the way properties are attached to the FONodes and how they are exposed to the outside (layoutengine & renderers), so would lead to changes in a Lot of classes (capital 'L', if you catch the drift...)Createdupdated patch in an attempt to address fop-dev suggestionsI think I might have missed something on the mailing lists explaining these features, but Peter, can you explain what these new properties do? What they offer that it's in the spec?(In reply to)See beginning of bugzilla entry which says<quote>Patch adding support for bleed, trim and crop box and scalingThe attached patch adds support for 4 new simple-page-master fop extensionattributes, namely- fox:crop-box- fox:trim-box- fox:bleed-box- fox:scaleThe box attributes can consist out of up to 4 numeric values (top,left, width,height) and can have units as suffix to each of the numbers.The scale attribute can consist out of two numbers each between 0 and 1It is implemented for PDF and Java2D renderers.</quote>Yeah, I saw that, but that still doesn't tell me what I'm looking for. What's the use case for using these properties? What functionality do they offer that's not in other properties in the specification? Is this for background images or something? Watermarks?Based on Andreas's comments in this bug this sounds like a common feature request, I've just never heard of it.I'm only curious because I use FOP pretty extensively and wonder if these extensions add something I would find useful.(In reply to)Ah..icThe box'es set the pdf page boxes with the same name. I founduseful (not necessarily finding the pdf spec an easy read ;)). These boxes are often used to drive what is actually printed as compared to what e.g. acrobat displaysScale is something we needed to use fop for creating adverts. The size of the output depends on e.g. the magazine you want your advert to be published in. As we want to use same stylesheet for different magazines a scale factor makes that possible(In reply to)Thanks for the updated patch. I tried to run the AWT renderer and it appears to have been broken by the changes. The main window is opened but no document is displayed, and I get the following exception:Exception in thread "AWT-EventQueue-0" java.awt.image.RasterFormatException: (x + width) is outside raster at sun.awt.image.IntegerInterleavedRaster.createWritableChild(IntegerInterleavedRaster.java:450) at java.awt.image.BufferedImage.getSubimage(BufferedImage.java:1156) at org.apache.fop.render.java2d.Java2DRenderer.getPageImage(Java2DRenderer.java:379) at org.apache.fop.render.java2d.Java2DRenderer.getPageImage(Java2DRenderer.java:425) at org.apache.fop.render.awt.viewer.ImageProxyPanel.paintComponent(ImageProxyPanel.java:124) at javax.swing.JComponent.paint(JComponent.java:1027) at javax.swing.JComponent.paintChildren(JComponent.java:864) at javax.swing.JComponent.paint(JComponent.java:1036) at javax.swing.JComponent.paintToOffscreen(JComponent.java:5122) at javax.swing.BufferStrategyPaintManager.paint(BufferStrategyPaintManager.java:277) at javax.swing.RepaintManager.paint(RepaintManager.java:1217) at javax.swing.JComponent._paintImmediately(JComponent.java:5070) at javax.swing.JComponent.paintImmediately(JComponent.java:4880) at javax.swing.RepaintManager.paintDirtyRegions(RepaintManager.java:803) at javax.swing.RepaintManager.paintDirtyRegions(RepaintManager.java:714) at javax.swing.RepaintManager.seqPaintDirtyRegions(RepaintManager.java:694) at javax.swing.SystemEventQueueUtilities$ComponentWorkRequest.run(SystemEventQueueUtilities.java:128) at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:209) at java.awt.EventQueue.dispatchEvent(EventQueue.java:597) at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:269) at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:184) at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:174) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:169) at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:161) at java.awt.EventDispatchThread.run(EventDispatchThread.java:122)I'll investigate (but: do you really need this feature also for the AWT renderer? ;-) )Thanks,VincentOops...that is not so goodActually 'only' for png output but both awt and png output probably originate from the Java2DRenderer ?CreatedUpdated patch avoid awt issueAnother attempt (hopefully) not breaking awt rendererThere seems to be another problem: in the Swing preview the zoom no longer works. I fixed that by multiplying the scales by scaleFactor instead of overwriting that latter: if (scales != null) { scaleX *= scales.getX(); scaleY *= scales.getY(); }but then scrolling bars appear the same way as if fox:scale had not been specified. When resizing the window they appear whereas there is obviously still room for the whole document to fit in.I have doubts about that scale extension, I must say. It seems very ad-hoc to me. Can't that be left to some post-processing mechanism? For PDF output this usually is a job that is handled by the printer. For PNG output I'm sure that there are plenty of programs that can do that very well (actually I had a better quality result when re-scaling the PNG output with an external program than by using the new extension —might be a problem with the Java2D renderer though).Also, is there a use case for a non-proportional scale (x scale != y scale)? Not that having different x and y factors makes the whole thing a lot more complicated, but...Thanks,Vincent(In reply to)CreatedAWTRenderer change to take into account fop:scale settingYeah..that seems correct.>Obviously scaling can be handled through a post processing step, just like adding the pdf boxes can be handled using e.g. PDFBox after fop has rendered the stylesheet to pdf. This is what we currently use. But it is very inelegant as we now need to also store 'template/stylesheet' information outside the stylesheet, dispatch postprocessing based on output type, and it also adds extra processing overhead where, with the integrated approach, almost no extra overhead is needed. Once confronted with things like 'adverts' where page size options are very restricted by publishers it does seem to make sense to integrate it all together, at least from a 'users' perspective. Whether it makes sense for fo(p), I feel not very well placed to comment (at lease the box requirement has been requested before)Publishers do restrict aspect ratio's. It does not make sense, layout wise, to do 'big' non-proportional scalings, but small factors allow to reuse the same stylesheet page content, for different 'publishers' and that does make the amount of maintenance a lot more manageable.ThanksPeterHi,(In reply to)In the meantime, I found out why the Swing renderer doesn't take the scale extension into account: the AWTRenderer class (which should have been named SwingRenderer really) defines its own getPageImageSize method instead of re-using the stuff from Java2DRenderer.getPageImage. I'll see if I find the energy to fix that.Note that I don't question the box extensions, which are indeed useful and have already been requested in the past. Only the scale extension was looking very specific to me.Ok. Makes sense.There is an inconsistency between PDF and Java2D regarding the coordinates of the boxes: in PDF the x and y coordinates are relative to the left and bottom sides, in Java2D they are relative to the left and /top/ sides. One of the two possibilities will have to be chosen, probably the PDF way.Thanks,VincentCreatedBox implementation for Java2d now uses "pdf box settings"The attached patch fixes the previous comment"There is an inconsistency between PDF and Java2D regarding the coordinates ofthe boxes: in PDF the x and y coordinates are relative to the left and bottomsides, in Java2D they are relative to the left and /top/ sides. One of the twopossibilities will have to be chosen, probably the PDF way."The boxes for the Java2D renderer now also follows the "PDF way"Hi Peter,incidentally, I need this functionality myself in a project I'm currently working on. I've locally applied your patch to play with it. I apologize for the late feedback.I notice you chose the page size of the simple-page-master as the baseline for the MediaBox. I would have expected the SPM's size to define the TrimBox instead. Bleed and cut marks would then lie outside the actual logical page. I've checked what other FO implementations do and they seem to follow that pattern rather than your approach.I'm also finding the specification of the areas a bit counter-intuitive. A simple value only sets the left side, rather than the value for all four sides as with other FO properties. I guess that also points to Andreas' comment about reusing property infrastructure where this is already handled. Granted, it is not easy to use. I'm not even sure myself if this can easily be reused with some serious refactoring. At any rate, a print shop will usually just give you the information that you should use 2 or 3 mm for the bleed area. Just specifying one simple value is quite handy.Rather than just criticizing, I'm willing to invest some time to help with this. I would like to make a counter-proposal for the extensions:The simple-page-master's width and height properties shall define the TrimBox. If there is no bleed and crop mark area, the MediaBox will be equal to the TrimBox, or rather just the MediaBox is generated in this case, like it happens today.fox:bleed: <length>{1,4}Default: 0ptIf there is only one value, it applies to all sides. If there are two values, the top and bottom bleed widths are set to the first value and the right and left bleed widths are set to the second. If there are three values, the top is set to the first value, the left and right are set to the second, and the bottom is set to the third. If there are four values, they apply to the top, right, bottom, and left, respectively. (Corresponds to)The BleedBox is calculated by expanding the TrimBox by the bleed widths.(I'd prefer to call the property fox:bleed rather than fox:bleed-box as we don't set the BleedBox values directly. We specify the bleed amount.)fox:crop-offset: <length>{1,4}Default: 0ptSame behaviour as with fox:bleed.The MediaBox is calculated by expanding the BleedBox by the crop offsets.BTW, the naming above pretty much matches other FO implementations, so should we ever have a standard for these properties (like by reviving exslfo.sf.net), it's likely we probably don't have to change much besides the namespace prefix.fox:crop-box: (trim-box|bleed-box|media-box)Default: media-boxThe crop box controls how Acrobat display the page or how the Java2DRenderer sizes the output media. The PDF spec defines that the CropBox defaults to the MediaBox, so it makes sense to do the same here. We could define a fox:crop-box extension which could take three "magic" values: "trim-box", "bleed-box" and "media-box" to set the CropBox to one of those three other boxes. That should cover 95% of all use cases. If anyone needs more control, that could easily added later.I don't have much feedback on fox:scale. I guess it can be useful but I don't see a big use case for an extension. I'd rather want to control that from application code. But it shouldn't get in the way of anything so I have not problem with it.WDYT?Hi Jeremias,I can't afford to spend much time on this patch so I'm happy to hand over the job to you. I'm attaching a new patch with my own modifications as I had started to prepare it for commit. Not sure it's going to be any useful given the different approach you'd like to take, but who knows.A note about the patch: it seemed more sensible to me to deal with integer values rather than double. Indeed every length value is internally converted into a whole number expressed in mpt (AFAICT). That means that I'm using Rectangle instead of Rectangle2D, and I changed some parameters into some classes accordingly (mainly, o.a.f.layoutmgr.Page.java, o.a.f.area.PageViewport.java). But then I noticed that the AreaTreeParser reads the value of the "bounds" attribute as double instead of integer. AFAIK, the XML area tree is produced with whole numbers for the page boundaries; I don't see why a user would change that into decimal numbers (which would mean a precision below the mpt!). So I also modified the AreaTreeParser. I don't think this may create any problem?Also, a few questions:(In reply to)<snip/>Just to be sure: values will be allowed to be negative, right?It would make more sense to me to set the default value of crop-offset to the value of bleed.I'm really not sure about that one. Calling it 'crop-box' is likely to create confusion with the 'crop-offset' property above. Apparently the CropBox is not used in prepress at all, so I would suggest to forget about that for the moment. That is, make the CropBox match with the MediaBox.VincentCreatedUpdated patch with some clean-up and modificationsWe were just about to start to rework the patch according to the suggestions made by Jeremias which seem to make a lot of sense, but I am not so sure anymore that is still useful given Vincent's alternative implementation/efforts which we were unaware of.It's a bit unclear right now how we can continue to contribute given the different implementation effortsWe still have a bit of bandwith to spare on this (it is important for us) but we'd rather make sure those efforts are part of a consolidated actionCan someone advice?ThanksPeterPeter, I'll respond to Vincent's comments shortly. I think his changes shouldn't affect your changes too much. I think Vincent's comments make sense and I'll see to it that they can be committed separately (they don't really have much to do with the issue at hand). Then it will ideally just be a "svn up" on your side to resolve this. And a minor SVN conflict in the worst case. I suggest you just continue as intended.Hi Vincent,(In reply to)I'm glad to take over.I don't think so. I'll extract your changes from your patch and apply that separately. That should make it easier for Peter, too.<snip/>Negative values don't make much sense, if I didn't miss anything.Right.Yes, I thought about that naming closeness, too. Not sure if that's so easy to resolve. If Peter is OK with this, I guess the fox:crop-box could be left out for the moment. I personally don't need it although at some point, such a property might be useful to some.<snip/>Partially applied Vincent's patch (from today). I've also applied the undisputed parts of Peter's patch. So the next patch by Peter will only need to be about the extensions and their integration into the renderers. I hope the result will merge as painlessly as possible.Hmmm...not entirely happy with this unfortunately. We have a webapp that using a flex wysiwyg editor allows creation of print materials but that uses adobe's pdf browser plugin for end-user proofing and approval. In some cases we'd rather display the trimbox iso the mediabox so my guess is we should be able to control the CropBox setting, no?Perhaps a name change for the attribute is sufficient? E.g. something like crop-box-selector, or crop-box-source or something similar?(In reply to)<snip/>I've just committed a change to also replace Rectangle2D with Rectangle inPDFFactory and PDFPage. Forgot to do that in my patch.Actually not, because the simple-page-master's width and height would definethe TrimBox. What I had in mind is to make the BleedBox coincide with thesimple-page-master's boundaries, and define a TrimBox inside it. I was thinkingthat sometimes the document may be easier to design this way (especially whenyou want areas with dark backgrounds to bleed off the boundary of the finalpage). I guess it's not actually the case.<snip/>VincentI've started a Wiki page for the prepress feature:I'll follow up with some additional thoughts on the fop-dev list as discussion in Bugzilla is a bit awkward.CreatedPatch including Jeremias'es proposed changesThanks for the new patch, Peter. I've taken a look and found a few issues. I've already started fixing them. Among them:- In the Wiki I've switched the meaning for crop-offset to align with what AntennaHouse did (see their illustration): crop-offset expands from the TrimBox, not the BleedBox. That is currently not reflected in the code, so I'll change that, too, if there's no opposition.- In the AWT preview, the positioning wasn't ok, yet, when there's a bleed or crop-offset.- The Java2DRenderer generates an ugly page border which I've disabled locally. Not sure why we even had that in place. It doesn't make much sense.- Also, I've changed the background painting in the Java2DRenderer to use the BleedBox instead of the page size.Otherwise, the patch makes a good impression. The whole thing is now very intuitive to use, just like I imagined it should be. I'll just allow some time for additional feedback from others. In the meantime, I'll finish the changes to the patch I've started and finally commit the whole thing. BTW, I've also written a little demo FO which demonstrates the features and how I would go about doing crop marks with SVG. I'll commit that after the patch is processed.Peter, why can fox:scale not have any number greater than 1? Why only support shrinking but not upsizing?(In reply to)Hmmm...probably just an oversight on our end. For the sake of generality it should be possible to also use >1 values. You want us to send a new patch?(In reply to)Ok for me.(In reply to)Not necessary, thanks. I'll just change that myself then.Applied the patch with the changes already mentioned. Thanks a lot for your patch Peter, and thanks also for your patience with my late involvement.I hope we will have a chance some day to improve the property subsystem so we can also handle extension like this more elegantly.(In reply to)This is not quite finished I'm afraid:- I thought we agreed to set the default value of crop-offset to bleed- errors in the specification of extension properties are not redirected through the event mechanism- there are typos in the PageBoundariesAttributes and PageScaleAttributes classes- there is an encapsulation problem: the fall back boxes used when one of the properties has not been defined are controlled by the client code, instead of being handled inside the extension class itself (PageBoundariesAttributes). This can easily lead to inconsistencies (one default value used in the PDF renderer, another one in the Java2D renderer).Re-opening the bug as a reminder, as I don't have the time to handle that right now.3 of the 4 latest comments have been hopefully satisfyingly taken care of:(In reply to)This is looking much better now, thanks. There are still a few remaining issues that I'll handle in the next days.CreatedFix for /trunkCreatedFull patch for FOP-0.95 versionHi All,We have found one issue during testing this new feature.The issue lies in PageBoundaries.java in calculating crop/bleed boxes.The offsets order is: [top, right, bottom, left], so to calculate Y size of the final box we should use the 'bottom' instead of 'top' offset : return new Rectangle(originalRect.x - coords[3],- originalRect.y - coords[0],+ originalRect.y - coords[2], originalRect.width + coords[3] + coords[1], originalRect.height + coords[0] + coords[2]); Please find in the attachments the fix patch. ()Also I have attached the full patch for FOP-0.95 version () if somebody will have a need to use this feature with previous version.(In reply to)I've taken a look at that. Thanks for spotting the problem, Boris, but your solution was not the right one. But you brought me on the right track. I've just found out what our mistake is: PDF specifies the boxes as "rectangles" which are defined as "llx lly urx ury" (i.e. lower left to upper right). But our/FOP's Rectangle2D objects are actually "upper left to lower right". In PageBoundaries we're still in FOP's coordinate system which starts at the upper left. So we have to calculate the right values for the default PDF coordinate system. Boris' change would have broken a test case and created a bug on the bitmap production side. So the right change is to do a transformation from FOP's internal coordinate system to PDF's default one in PDFDocumentHandler:Boris, can you please verify that this fix also work for you? Thanks!increase priority for bugs with a patch	45.0	id=51962	9	False	False	peter.hancock	1
id=51465	REOPENED	None	XMLGraphicsCommons - Now in Jira	image writer (	1.4	PC All	P2 regression	XML Graphics Project Mailing List	2011-07-02 12:05 UTC by	None	2012-08-15 21:48 UTC (	1 user	CreatedProfilerHello, I’m using fop 1.0 (xmlgraphics 1.4) to generate PDF, I noticed that PDF generation with PNG or TIF image is 7 times longer. The problem doesn't appear in the version 1.3 of xmlgraphics.The problem seems to come from color model associated with the image.* if is_sRGB_stdStale = true then getRBG method is very fast* if is_sRGB_stdStale = false then getRBG is slow because the treatment isdelegated to ICC_ColorSpace.toRGBThe color model is correct, but it is modified by the classorg.apache.xmlgraphics.image.loader.impl.imageio.ImageLoaderImageIOby the following code : if (providerIgnoresICC && cm instanceof ComponentColorModel) { // Apply ICC Profile to Image by creating a new image with a new color model. ICC_Profile iccProf = tryToExctractICCProfile(iiometa); if (iccProf != null) { ColorModel cm2 = new ComponentColorModel( new ICC_ColorSpace(iccProf), cm.hasAlpha(), cm .isAlphaPremultiplied(), cm .getTransparency(), cm.getTransferType()); WritableRaster wr = Raster.createWritableRaster(imageData .getSampleModel(), null); imageData.copyData(wr); BufferedImage bi = new BufferedImage(cm2, wr, cm2 .isAlphaPremultiplied(), null); imageData = bi; cm = cm2;I removed this code and it works fine now, but it's dirty. What's the impact ?Until a fix, best regards.	patch (from) applied at:Hi, We applied your patch but it dont fix my issue.What information do you need about the PNG Image ?Regards,this bug is linked tobut the issue may be different... the slowdown addressed by the patch appeared due to a change in the jdk, not from a change in xmlgraphics (from 1.3 to 1.4).please provide the image so that the issue can be investigated. does it happen with all images or just some?CreatedPNG imageI attached the concerned PNG image.The image is used inside a PDF document generated by FOP.This image used by FOP 0.95+XmlGraphics 1.3.1 had no performance issue during the PDF generation.Changing JDK has no effect in my case.Createdan example to test icc profilesthe slowdown that happens between xmlgraphics-1.3 and xmlgraphics-1.4 is due to the fact that the color profiles were being ignored in 1.3 while in 1.4 they are taken into account, as the PNG specification suggests. so even though 1.3 was faster, it was not correct. the impact of removing the check for color profile is that the output may be wrong, although in most cases that does not happen because the color pixel values in the PNG image already match what the embedded color profile expects.attached is an example with two images that look the same when viewed in an application that does not apply color profiles (like Chrome), but that look different when viewed with an application that applies the color profile (like Safari). the example, when processed with FOP-0.95 (xmlgraphics-1.3) generates two images that look the same, while with FOP-1.0 (xmlgraphics-1.4) the images look different.a workaround, as was suggested somewhere else, is to remove the color profile from the image (this can be done with imagemagick). obviously this only works in the cases where the color profile is not adding anything new to the image. another option is to use the new raw png image loader, which for now, still ignores the color profile (of course, this will not help in the cases where the color profile is important).as a follow-up to previous, see, with(a patch that adds support for color profiles)	7.0	id=53817	7	False	False	gadams	1
id=53817	REOPENED	None	Fop - Now in Jira	pdf (	1.0	PC Linux	P2 normal	None	2012-09-02 15:36 UTC by	Mathieu Malaterre	2012-10-03 07:08 UTC (	1 user	Createdgenerated fo fileHi,I cannot process the following dobbook slides:<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE slides PUBLIC "-//Norman Walsh//DTD Slides XML V3.4.0//EN" ""><slides> <foil> <title>My Title<footnote><para><ulink url=""/></para></footnote></title> <para/> </foil></slides>it keeps on failing with:$ xsltproc -o t.fo /usr/share/xml/docbook/custom/slides/3.4.0/xsl/fo/plain.xsl s.xml$ fop t.fo t.pdf Image not found. URI: images/draft.png. (See position 2:9711)Image not found. URI: images/draft.png. (See position 2:10376)Image not found. URI: images/draft.png. (See position 2:11039)Image not found. URI: images/draft.png. (See position 2:11699)Image not found. URI: images/draft.png. (See position 2:12356)Image not found. URI: images/draft.png. (See position 2:13013)Image not found. URI: images/draft.png. (See position 2:13667)Image not found. URI: images/draft.png. (See position 2:14326)Image not found. URI: images/draft.png. (See position 2:14985)Image not found. URI: images/draft.png. (See position 2:15641)Image not found. URI: images/draft.png. (See position 2:16299)Image not found. URI: images/draft.png. (See position 2:16957)Image not found. URI: images/draft.png. (See position 2:17612)Image not found. URI: images/draft.png. (See position 2:18270)Image not found. URI: images/draft.png. (See position 2:18928)Image not found. URI: images/draft.png. (See position 2:19583)Image not found. URI: images/draft.png. (See position 2:20242)Image not found. URI: images/draft.png. (See position 2:20901)Image not found. URI: images/draft.png. (See position 2:21557)Exceptionjava.lang.NullPointerException	just verified this on dev trunk, the full backtrace isjava.lang.NullPointerException at org.apache.fop.layoutmgr.PageBreakingAlgorithm.handleFootnotes(PageBreakingAlgorithm.java:370) at org.apache.fop.layoutmgr.PageBreakingAlgorithm.handleBox(PageBreakingAlgorithm.java:319) at org.apache.fop.layoutmgr.BreakingAlgorithm.handleElementAt(BreakingAlgorithm.java:751) at org.apache.fop.layoutmgr.BreakingAlgorithm.findBreakingPoints(BreakingAlgorithm.java:554) at org.apache.fop.layoutmgr.BreakingAlgorithm.findBreakingPoints(BreakingAlgorithm.java:503) at org.apache.fop.layoutmgr.AbstractBreaker.doLayout(AbstractBreaker.java:421) at org.apache.fop.layoutmgr.StaticContentLayoutManager.doLayout(StaticContentLayoutManager.java:144) at org.apache.fop.layoutmgr.PageSequenceLayoutManager.layoutSideRegion(PageSequenceLayoutManager.java:180) at org.apache.fop.layoutmgr.PageSequenceLayoutManager.finishPage(PageSequenceLayoutManager.java:186) at org.apache.fop.layoutmgr.PageSequenceLayoutManager.activateLayout(PageSequenceLayoutManager.java:115) at org.apache.fop.area.AreaTreeHandler.endPageSequence(AreaTreeHandler.java:267) at org.apache.fop.fo.pagination.PageSequence.endOfNode(PageSequence.java:128) at org.apache.fop.fo.FOTreeBuilder$MainFOHandler.endElement(FOTreeBuilder.java:347) at org.apache.fop.fo.FOTreeBuilder.endElement(FOTreeBuilder.java:181) at org.apache.xalan.transformer.TransformerIdentityImpl.endElement(TransformerIdentityImpl.java:1101) at org.apache.xerces.parsers.AbstractSAXParser.endElement(Unknown Source) at org.apache.xerces.xinclude.XIncludeHandler.endElement(Unknown Source) at org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanEndElement(Unknown Source) at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source) at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source) at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source) at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source) at org.apache.xerces.parsers.XMLParser.parse(Unknown Source) at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source) at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source) at org.apache.xalan.transformer.TransformerIdentityImpl.transform(TransformerIdentityImpl.java:484) at org.apache.fop.cli.InputHandler.transformTo(InputHandler.java:285) at org.apache.fop.cli.InputHandler.renderTo(InputHandler.java:115) at org.apache.fop.cli.Main.startFOP(Main.java:177) at org.apache.fop.cli.Main.main(Main.java:208)looks like this code was last modified by Andreas inAndreas would you have time to take a look at this?CreatedMinimal test caseThe NPE is thrown when foot-note is declared within the static-before region.If I move the foot-note to the body, the NPE disappears.Note that FOP behaves in the same way whatever the version is (tried against v0.95, v1.0, latest trunk).That said, IMHO I don't think that a static region is the right place for a footnote declaration, so author should avoid to use that at this place, as a good practice/workaround.As per comment on:This is a bug in the original fo generator (docbook). Closing issue as fixed.FOP should throw/warn a more explicit Exception/message than throwing NPE in this case.So, I reopen it as a reminder.	5.0	id=33549	5	False	False	scheglov_ke	1
id=33549	REOPENED	None	BCEL - Now in Jira	Main (	5.1	PC Windows 2000	P2 major	issues@commons.apache.org	2005-02-14 03:00 UTC by	Marco Petris	2009-02-16 01:19 UTC (	1 user	Scenario:I insert into a method which has a parameterized argument some instructions andtwo local variables.The methods signature looks like this:public void doSomething( double d, ArrayList<Integer> list )The method is situated in the class MyClassFile.Result:Loading the class MyClassFile which contains the new method 'doSomething'results in a ClassFormatError:LVTT entry for 'list' in class file MyClassFile does not match any LVT entryThis error is not produced if the method declaration is done without generics:public void doSomething( double d, ArrayList list )The difference is the LocalVariableTypeTable. It is not present if I do not usegenerics. Due to the insertion of the new instructions the start_pc and the length of thelocal variable have to be updated in the LocalVariableTable, which is done. But the corresponding entry in the LocalVariableTypeTable gets not updated itsstart_pc and length entries.LocalVariableTable ( BEFORE INSERTION ):start_pc, length, index, name0, 26, 0, this0, 26, 1, d0, 26, 3, listLocalVariableTypeTable( BEFORE INSERTION ):0, 26, 3, listLocalVariableTable ( AFTER INSERTION ):start_pc, length, index, name44, 33, 0, this44, 33, 1, d44, 33, 3, list1, 85, 4, wrapper_argumentlist26, 60, 5, wrapperLocalVariableTypeTable( AFTER INSERTION ):0, 26, 3, listThe class works fine before the insertion.I've done bcel CVS checkout, but it did not help.The original instructions of the methods are not modified.I simply install a wrapper around the original body. I add an exception handler, therefore there is also an additional entry inthe exceptiontable.I can toggle the error behaviour by using or not using the generic argument.generic -> error no generic -> no errorI tried JDK 5.0 build 1.5.0-b64 and eclipse 3.1.0 build 200412162000 as a compiler.I use the JVR of the above mentioned JDK.	One possible workaround before BCEL supports Local Variable type Tablesis to simply remove the table. It is optional and everything seems towork fine if you remove it. Simply loop through the attributes andremove the one named "LocalVariableTypeTable". I think the onlydownside is that the information won't be available to a debugger.The proposed workaround works even with the Instrumentation.redefineClasses( ClassDefinition[] definitions ) of JDK1.5So we this should be fixed for proper jdk 1.5 supportIt looks like a dup ofi have loop through the attributes andremove the one named "LocalVariableTypeTabe". The problem still occurs	5.0	id=54005	5	False	False	p.mouawad	1
id=42248	REOPENED	None	JMeter	Main (	2.5.1	All All	P2 enhancement	JMeter issues mailing list	2007-04-25 08:22 UTC by	Michel Nolard	2015-12-14 00:04 UTC (	10 users	Hi !I am very happy with JMeter, which is very useful and powerful ! So, I decidedto contribute in my way by pointing those little annoying things which the usershad to put up with but didn't notice or didn't take the time to report.My thought are that a Undo/Redo feature is required while editing a test plan asit is well known that user _do_ mistakes and don't want to be punished simplyfor being human.At first, I would argue that adding such a framework to JMeter would imply somework, refactoring some things and implementing Command Pattern to ensure properefficiency and power. When I think a little bit more about it, I feel it to belike an "internal XML patching system", as the problem is to apply modificationsto the plan test tree (which can be represented as an XML tree) and add moremodifications or revert some others.I hope this won't be too hard to implement in JMeter. I simply don't know asI've only _thought_ about it but not designed code to do it (which I am not ableto write at this time).	The classic UNDO shortcut 'CTRL-Z' has been assigned to [Launch > Remote startall] which is not very consistent in a GUI point of view.If the undo-redo support is not implemented/finalized/integrated to the nextrelease, the shortcut should be removed/changed even though : I can't count thenumber of times JMeter lagged for 2 seconds before throwing me an ironical "Badcall to remote host". By the way, that error message should be improved by, forexample, showing what's the actual "remote host" and maybe what's the actual"bad call" (i.e command name or something else).I agree that Control-Z is confusing.Since Control-R is used for a normal run, I've changed Run Remote to Control-Shift-R.This is in SVN.Also fixed the confusing "Bad call" messages in.It has been more than two years since this enhancement request was opened. Lacking undo/redo is bad and frustrating nuisance. Just one bad edit, forgetting the previous contents of some obscure request parameter... what then?Yes, it would be a useful enhancement, but as far as I can tell, implementing undo/redo would involve a lot of code changes.If anyone can provide a patch, then of course it will be evaluated and applied if appropriate.***has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ***Hi!Just wanted to express my support for addressing this issue. I would love to see undo/redo support for two independent areas:1) text areas (it would be more-than-sufficient to just have that information while working in the textarea, no need to get sophisticated with having this work in multiple textareas at the same time or shit like that), it is just really annoying to accidentally delete something and then having to do it all again.2) the element tree on the left side (probably more complicated?)Many thanks for listening :)ImmCreatedproposed patchHere's my current progress patch.Notes: 1) I failed to make it select recorded tree selection path properly, tree shows no nodes selected. Tree selection is commented out and after each undo/redo Workbench becomes selected 2) I didn't use undo action hints like for action labeling like "Undo (Add Thread Group)" but it would be nice to have it. There is "comment" field in history items to have it.Thanks very much. Looks good but not tried the code yet.Just a couple of minor points:We release code under the Apache License and each file has to have the standard AL header. These are missing from the new files you have provided.Also, we don't use @author tags in code. This is mainly because the ASF is about community rather than individuals; also the tags are impossible to maintain accurately as code is further developed. Instead, we credit contributors in the changes list and elsewhere (e.g. Wiki)So please could you either confirm that it is OK for us to add AL headers and remove @author tags, or provide an updated patch? Thanks!Hi, I understand that ASF has its rules for code formatting and meta-info (and JMeter has its rules for unit tests, too). So feel free to reformat code. I believe you can make Undo working much better with your experience, starting with my patch.Of course it would be nice to be mentioned in changelog or somewhere else, it is left for your choice. I just glad to be reason for closing 5-year old issue :)Hello Andrey,I made some tests and I see the following issues:- Limitation : Using CTRL+Z in text fields has some really disturbing behaviour- I have tried to load a consequent Test Plan ( around 6.6 Mo on disk), it ends with OOM and analysing dump, it shows UndoHistory occupates 336 MO, it contains 301 item where each one has a size > 2.3Mo,So I think patch need more work.Thanks for it anyway and we will try to find some solutions.RegardsPhilippePhilippe,If it cause problems, Ctrl+Z can be replaced with any other shortcut, or even left without shortcut.To lower memory usage you can limit history size, it is trivial task, I suppose.CreatedUpdated PatchHello,Here is a modified version with the following changes:- Added a limit on the history (25 it should be configurable), to avoid OutOfMemoryError when Test Plan is big- Added Pause/Resume methods that are used during loading , closing and merging of Tree, cause if we don't add it (at least for close and loading):1) It does not seem useful to be able to undo elements that were added during load phase2) Performance are really bad , a big test plan (6 Mo) takes more than 30 seconds to load- Added javadocsRegardsPhilippe- There is one drawback to previous change, it's that we won't be able to undo merged part (right click>merge) - Added javadocsAlso note that patch is onPhilippe, you made everything much better with your changes!One question: have youn noticed my notes on selecting historical path in tree? Is it possible in JMeter?I think the problem is due to the following:- You record a TreePath pointing to the node after modification- When you undo, a full JTree with new nodes JMeterTreeNodes is rebuilt- If you try to call tree.setSelectionPath(path), then it uses the content before undo.Philippe, dou you see any other (working) solution to have historical tree selection paths?I managed to fix the issue with non-undoable merge. Now will work on historical selection path.CreatedPatch for next attempt of undoFixed most of the earlier problems.Creatednew files packHello Andrei,I only reviewed patch. no pack file as I only have smartphone currently.I didn't see any change to undo/redo of inout fields.Do you confirm that patch only adresses undo-redo on test tree ?ThanksYes, I carefully checked that there is no side changes. I cleared any formatting changes that were produced by IDE.Thanks Andrey.As I understand you made on patch file for files that differ but already exist and 1 zip that contains new files.Is it possible to only create 1 patch file containing everything ?ThanksPhilippe, I don't know how to do that using 'svn diff' command. If I could just send a pull request into GitHub repository, then it could be very easy.If you use Eclipse:Right click on jmeter project> Team > create patch and select all files to include.If you want you can also provide a pull request It will allow me to review it and give you feedback very soon, otherwise you will have to wait for 21th august.As you're working on it now, I would br sorry to give you feedback once you don't have time to work on it snymore :)RegardsOtherwise have a look at this:-Createdpatch with added filesThanks! I completely forgot how to use svn. Added new patch, icon files will follow.Createdundo toolbar iconCreatedredo toolbar iconThanks for update Andrei.First review looks good, some notes:- you should remove import x.y.* from code, only import the classes needed- I suggest we add an option to enable/disable history (undo/redo), we will decide later if we enable/disable it by default- undo/redo looks a bit confusing, as user may think he can also undo changes in text fields or other properties of test element. We need to find a better name although for now I am not inspired- implementing also undo/redo on test element properties would make this feature complete but it is more impactingCreatedproposed patch with more changesPhilippe, thanks for reviewing the patch. I made some changes:- removed wildcard import- added some comments and JavaDocs- added undo.size property to manipulate history length, default is 25The undo for TestElement property change already works. History is recorded once changes applied to TestPlan.I think disabling the undo is unnecessary, since it is vital feature for JMeter users. The option to limit history size helps protecting from too much memory consumption. The choice seems to be to provide people undo without in-fields undo, or not provide ability to revert the actions at all. I believe the value for this feature is too high to block it for more years waiting to implement in-fields undo...Hope this makes sense.Is there a build already with this feature?I'd like to give that a try...@Andrei, thanks, I get your point.@Shmullik, see @jmeter_plugins twitter account for info on this. Note that once we integrate this in nightly build, we will require test help from users community, so your tests will be welcome.Here's the link for build to try the feature:Hello Andrei,Could you generate the patch in Unified format ?I am not able to integrate it with the current format.To do this use Eclipse, select jmeter project and then Team > Create Patch.ThanksSorry, I don't use Eclipse. I generated the patch using "svn diff" command, it usually applies OK using "patch" command-line utility.(In reply to Andrey Pohilko from)Looks like this is a Bugzilla issue.Eclipse expects headers for each patch, but these are dropped when usingDiff / Raw UnifiedYou need to click on the attachment name, or use Details.Date: Sat Sep 6 21:14:49 2014New Revision: 1622936URL:Log:- Undo-redo support on Test Plan tree modificationBugzilla Id: 42248Added: jmeter/trunk/src/core/org/apache/jmeter/gui/UndoHistory.java (with props) jmeter/trunk/src/core/org/apache/jmeter/gui/UndoHistoryItem.java (with props) jmeter/trunk/src/core/org/apache/jmeter/gui/action/UndoCommand.java (with props) jmeter/trunk/src/core/org/apache/jmeter/images/toolbar/redo.png (with props) jmeter/trunk/src/core/org/apache/jmeter/images/toolbar/undo.png (with props)Modified: jmeter/trunk/bin/jmeter.properties jmeter/trunk/src/core/org/apache/jmeter/gui/GuiPackage.java jmeter/trunk/src/core/org/apache/jmeter/gui/action/ActionNames.java jmeter/trunk/src/core/org/apache/jmeter/gui/util/MenuFactory.java jmeter/trunk/src/core/org/apache/jmeter/images/toolbar/icons-toolbar.properties jmeter/trunk/src/core/org/apache/jmeter/resources/messages.properties jmeter/trunk/src/core/org/apache/jmeter/resources/messages_fr.properties jmeter/trunk/xdocs/changes.xmlDate: Sat Sep 6 21:21:41 2014New Revision: 1622938URL:Log:- Undo-redo support on Test Plan tree modificationsvn:eolBugzilla Id: 42248Modified: jmeter/trunk/src/core/org/apache/jmeter/gui/UndoHistory.java (props changed) jmeter/trunk/src/core/org/apache/jmeter/gui/UndoHistoryItem.java (props changed) jmeter/trunk/src/core/org/apache/jmeter/gui/action/UndoCommand.java (props changed)Date: Sat Sep 6 21:23:48 2014New Revision: 1622939URL:Log:- Undo-redo support on Test Plan tree modificationsvn:mime-typeBugzilla Id: 42248Modified: jmeter/trunk/src/core/org/apache/jmeter/images/toolbar/redo.png (props changed) jmeter/trunk/src/core/org/apache/jmeter/images/toolbar/undo.png (props changed)Date: Sat Sep 6 21:34:45 2014New Revision: 1622941URL:Log:- Undo-redo support on Test Plan tree modificationChanged icons to use open_icon_library-CC and have a different color for undo and redoBugzilla Id: 42248Modified: jmeter/trunk/src/core/org/apache/jmeter/images/toolbar/redo.png jmeter/trunk/src/core/org/apache/jmeter/images/toolbar/undo.png@Andrei, few questions:- Testing feature, I see icon is not disabled when undo or redo cannot be done- Where do the icon come from , we need to know the license of icons. For now I replaced them by open_icon_library used in JMeterMany thanks for this contribution !Date: Sat Sep 6 21:59:18 2014New Revision: 1622945URL:Log:- Undo-redo support on Test Plan tree modificationUpdate icons state on redo/undoBugzilla Id: 42248Modified: jmeter/trunk/src/core/org/apache/jmeter/gui/action/UndoCommand.java jmeter/trunk/src/core/org/apache/jmeter/gui/util/JMeterToolBar.java@Andrei, icon update issue fixed now.So only icons license question remains.Date: Sat Sep 6 22:04:42 2014New Revision: 1622947URL:Log:- Undo-redo support on Test Plan tree modificationUpdate icons state on redo/undoOups missed 1 classBugzilla Id: 42248Modified: jmeter/trunk/src/core/org/apache/jmeter/gui/MainFrame.javaRegarding icons license you did everything right. Using the same icons collection for whole toolbar is better than using different. Initial icons were taken from the Internet, I did not know their license.Regarding toolbar buttons enabling-disabling there is a problem, since JMeter code does not offer interface for enabling/disabling icons easily, at leas I did not find one. So I left it enabled and put a TODO comment for this. I have dowloaded latest code and now toolbar buttons are always disabled. I suppose this was not your intention. From what I've seen in the code, implementing the interface for proper buttons enabling/disabling would require some work, that's why I did not try to solve it together with Undo, just don't want to link two big problems.Hello Andrei, I made a mistake in the place where I update toolbar, so they are only updated in undo/redo calls instead of being so every time history changes.I will fix this this afternook hopefully, I disabled it temporarily in trunk.Thanks for noticing and reporting it.Date: Sun Sep 7 08:31:46 2014New Revision: 1622984URL:Log:- Undo-redo support on Test Plan tree modificationTemporary disable as I missed the right place to call itBugzilla Id: 42248Modified: jmeter/trunk/src/core/org/apache/jmeter/gui/action/UndoCommand.javaDate: Sun Sep 7 14:04:34 2014New Revision: 1623019URL:Log:- Undo-redo support on Test Plan tree modificationCorrect toolbar undo/redo buttons disable/enableBugzilla Id: 42248Modified: jmeter/trunk/src/core/org/apache/jmeter/gui/GuiPackage.java jmeter/trunk/src/core/org/apache/jmeter/gui/MainFrame.java jmeter/trunk/src/core/org/apache/jmeter/gui/UndoHistory.java jmeter/trunk/src/core/org/apache/jmeter/gui/action/UndoCommand.javaHello JMeter Team,We started testing the nightly build recently and we wanted to report some issues we noticed related to this great feature:- When you have a "realistic " Test Plan of 2.4 mb, feature makes JMeter unusable. Whenever you change something, it takes 60 seconds to do it and UI is completely blocked (during clone), this happens with -Xms2g -Xmx under 2.7 Ghz Intel Core I7 Mac Book with 16 Gb of RAM with Java 8u20 Mac OSX Mavericks- Undo/Redo is called "abusively" when for example you:1) Search for a node by name2) In Module Controller GUI, when you expand a nodeAnalysing it, is it due to JMeterTreeNode#setMarkedBySearch calling treeModel.nodeChanged(this).RegardsUbik Load Pack TeamCreatedPatch that disables Undo/Redo feature if undo.history.size is set to 0Hello,Find attached a patch that disables effectively feature if "undo.history.size" property is set to 0.Currently (before patch) if it is set to 0, all the work related to it is done but no history is stored, so for a medium to big Test Plan you get big slowdowns without any benefit.RegardsUbik Load PackHi Andrey / Philippe,I've reported on couple of issues that I've found with the most recent nightly build:I think that this undo-redo is a major milestone and it will require some more effort to make it public due to the current quality, which is not enough.It might make sense to close this ticket/bug - and have all follow-ups in separate bugs.This will allow to at least make this feature part of the next release, but disabled with the default configuration, so it will not affect the reputation of JMeter from one hand, but on the other - will allow more users to further test it and report on bugs.URL:Log:- Undo-redo support on Test Plan tree modificationDisable feature by default as it is still in ALPHA MODEMention this in changes.xmlBugzilla Id: 42248Modified: jmeter/trunk/bin/jmeter.properties jmeter/trunk/src/core/org/apache/jmeter/gui/UndoHistory.java jmeter/trunk/xdocs/changes.xml(In reply to UbikLoadPack support from)It does not look like JMeterTreeNode#setMarkedBySearch gets called when the project is loaded. So the 60 second delay in loading a 2MB project does not appear to be caused by setMarkedBySearch function. I made this determination by putting a breakpoint on the function: public void setMarkedBySearch(boolean tagged) { this.markedBySearch = tagged; treeModel.nodeChanged(this); }I am working on finding the root cause for the delay during the project loading phase.(In reply to Philippe Mouawad from)Philippe,It looks like Save#convertSubTree is the culprit for slowing down the clone operation. Any idea why this blow code is slow? void convertSubTree(HashTree tree) { Iterator<Object> iter = new LinkedList<>(tree.list()).iterator(); while (iter.hasNext()) { JMeterTreeNode item = (JMeterTreeNode) iter.next(); convertSubTree(tree.getTree(item)); TestElement testElement = item.getTestElement(); // requires JMeterTreeNode tree.replaceKey(item, testElement); } }	52.0	id=54176	7	False	True	milamber	1
id=53976	REOPENED	None	JMeter	HTTP (	2.13	All All	P2 enhancement	JMeter issues mailing list	2012-10-06 21:49 UTC by	Philippe Mouawad	2015-07-01 16:06 UTC (	2 users	The aim is to clean a Cookie with more flexibility than today.See:--	Feature is in fact available through:- Result Status action Handler- If Controller + Test ActionSo adding a function for this is not necessaryOups wrong Bugzilla(In reply to)Concerns another bugzilla	2.0	id=42248	35	False	False	michel.nolard	1
id=54005	REOPENED	None	JMeter	HTTP (	2.8	All All	P2 enhancement	JMeter issues mailing list	2012-10-14 13:43 UTC by	Philippe Mouawad	2013-08-22 11:39 UTC (	1 user	It can be useful to have more control on response when using Mirror Server.I propose to add:- X-ResponseStatus to control status of response- X-ResponseLength to control size of response	Date: Sun Oct 14 13:52:00 2012New Revision: 1398084URL:Log:- HTTP Mirror Server : Add special headers "X-" to control Response status and response contentBugzilla Id: 54005Modified: jmeter/trunk/src/protocol/http/org/apache/jmeter/protocol/http/control/HttpMirrorThread.java jmeter/trunk/xdocs/changes.xml jmeter/trunk/xdocs/usermanual/component_reference.xmlDocumentation says that adding a Header Manager controls the responses such as X-ResponseStatus. However, this is not strictly true.At present the mirror server only looks for settings such as X-ResponseStatus in the headers it receives. It does not check the contents of any Header Manager that may be added to the workbench.The only way to tell the server return the headers is to add the Header Manager to the client request.Adding a Header Manager to the Mirror Server does not work, contrary to how the docs can be read.This is not particularly flexible - for example it cannot easily be used with browsers, only with other JMeter test elements.I suspect it will be quite difficult to add, as Header Managers need special handling in the test plan. Also again it is not particularly flexible.It would probably be a lot easier to just add a table of settings to the Mirror Server GUI.There should be a table for arbitrary http headers to be added verbatim.The other features (length/response code etc) would need separate fields.For compatibility, the X-headers should still be processed if found in the request.Alternatively, the docs need to be clarified.	2.0	id=59618	6	False	False	champion.p	1
id=54176	REOPENED	None	JMeter	HTTP (	2.8	All All	P2 enhancement	JMeter issues mailing list	2012-11-20 14:49 UTC by	Marek	2012-11-21 22:16 UTC (	1 user	CreatedAssertion doesnt work with \Q and \EMain use-case: escape spatial (for regular expressions) characters from content of variables (see stack overflow topic for details:."\Q" and "\E" marks doesn't work for "Response Assertion" and "Regular Expression Extractor".I'm inducing minimal test case showing the problem.Run test plan included.Go to "View Results Tree"Select assertion (it is failed).copy regular expression "google.time\Q()\E"go to request select responsenow test this regular expression and it will find match so it works in this response!"\Q" and "\E" can surround anything (not only regular expression special characters) and problems always appears.	Bugzilla isn't a support forum.Please use the JMeter user mailing.Tips: JMeter uses Jakarta ORO for his RegExp engine (not Java RegExp)Ok. I missed this in documentation, but I've found there: 20.6 Testing Regular Expressions Since JMeter 2.4, the listener View Results Tree include a RegExp Tester to test regular expressions directly on sampler response data.As I wrote: this regular expression (containing \Q\E) works in "View Results Tree", so there is definitely some incoherency how regular expressions works in JMeter and that is why I threated this as a bug.The Regexp Tester uses the same Regex engine, i.e. Jakarta ORO.I just tried using JMeter 2.8, and \Q and \E do *not* work.CreatedScreenshot showing that \Q \E works in "View Results Tree"Some magic happens or we are talking about different things, it works in my "View Results Tree", see screenshot.I was using the Regexp Tester.You were using the Search faciliry, which uses the Java regex engine.I find this a little disturbing for users.Shouldn't we either:- Replace java regexp engine from search feature by oro- Or replace oro by java engine regexp but this is an issue for existing Test Plans	6.0	id=51465	8	False	False	patches	1
id=54434	REOPENED	None	JMeter	Main (	unspecified	All All	P2 enhancement	JMeter issues mailing list	2013-01-16 14:07 UTC by	None	2016-08-05 15:27 UTC (	2 users	Since, JMeter is able to handle ByteMessage for JMS sample. But when I read a message in this format, the content is not save, I can just read this message : "XX bytes received in BytesMessage"So, could you please add in the sample result object, the real content of the ByteMessage like this :// copy data into a byte[] arrayint dataSize = (int) msg.getBodyLength();byte[] array = new byte[dataSize];msg.readBytes(array , dataSize);[...]result.setResponseData( array );The goal of this, is to send later the message (or save into a file )Thanks !	Fix in 2.11(In reply to maxime.chassagneux from)Hi maxime,Was this issue fixed? I can't seem to see the commit for this ticket in GIT. I am currently using Jmeter to subscribe to Websphere MQ and getting that "xx bytes received in BytesMessage" response data.Thanks in advance for the reply mate.Cheers!DanHave you enabled the "Read Response" checkbox?This is badly named and not well documented, but it controls whether or not the response data ia saved in the sample result.(In reply to Sebb from)Hi Sebb,Yes it is enabled, if i disable it , response data shows blank.OK, I see now.A BytesMessage is a stream of uninterpreted bytes; it is up to the sender and receiver to agree on what the interpretation is. There's no way that JMeter can know this.JMeter currently does not attempt to store the response.It could just store the raw bytes as a binary response, but the data would not be stored in any log file (only text responses are stored).You would need to provide a Post-Processor or Listener to handle the data.Note that the SubscriberSampler can aggregate multiple messages.These are currently appended to the response.It would not make sense to mix binary responses, so storing a binary response would only be possible if the aggregate count is set to 1.	6.0	id=54434	6	False	False	maxime.chassagneux	1
id=54481	REOPENED	None	JMeter	Main (	2.9	PC All	P2 normal	JMeter issues mailing list	2013-01-24 21:02 UTC by	None	2016-02-27 09:13 UTC (	2 users	I have a response assertion on the root of my test plan.I have setup a transaction controller and inside it an http sampler.The assertion works just fine for the http sampler, but it also asserting the transaction controller and failing it with a "Response was null" error.This now forcing me to duplicate the assertion as a child of each relevant http sampler instead of having just one, at the root of the test plan.I don't see any reason for assertions to assert logic controller.	CreatedTestplan to reproduce the issueThe transaction controller haven't response data, thus Response Assertion can't check in this data.In Response Assertion, you can change option "Apply to:" Sub-samples only to work.I am not sure it is invalid.In more complex cases, I have both Transaction Controllers and (HTTP) Samplers (which are not in Transaction Controllers) in the same level in a thread group.The solution you suggested will not work in such cases.Perhaps we need to disable assertion test on Transaction controller when a Assertion Response passes inside.Thus, when we have some TC and HTTP Requests at same level than a Assertion Response with Apply to "Main and Subsamplers" there haven't errors (null data on tc) (and if no errors on req http)There is something odd happening here.If the TC is set to not generate a parent sample, then the RA works.Adding an Assertion Results Listener shows two entries.The RA should fail when applied to the TC sample result as that is empty.Hoewever it does not fail; looks like the RA somehow runs against the wrong data here. Yet the AR Listener shows TC in its list.	5.0	id=55375	10	False	False	sebb	1
id=55375	REOPENED	None	JMeter	Main (	2.9	All All	P2 normal	JMeter issues mailing list	2013-08-07 08:18 UTC by	None	2017-02-25 14:17 UTC (	2 users	CreatedThe test to reproduce the error. The log file.See attached a simple jmx file to reproduce the stackoverflow. There is no problem when i start the test in GUI mode. In Non-GUI i have the following output.An error occurred: nullerrorlevel=1Drücken Sie eine beliebige Taste . . .Here is how i start the test in Non-GUI mode.jmeter -n -t d:\development\workspaces\2.11.3\apache-jmeter-latest\bin\temp\stackoverflow\stackoverflow.jmx -l d:\development\workspaces\2.11.3\apache-jmeter-latest\bin\temp\stackoverflow\stackoverflow.jtl -j d:\development\workspaces\2.11.3\apache-jmeter-latest\bin\temp\stackoverflow\stackoverflow.log In the log (also attached) the last entry is the stacktrace of the java.lang.StackOverflowError.2013/08/07 10:04:45 FATAL - jmeter.JMeter: An error occurred: java.lang.StackOverflowError at java.util.HashMap.containsKey(HashMap.java:335) at org.apache.jorphan.collections.ListedHashTree.add(ListedHashTree.java:163) at org.apache.jmeter.control.ModuleController.getReplacementSubTree(ModuleController.java:170) at org.apache.jmeter.JMeter.convertSubTree(JMeter.java:883) at org.apache.jmeter.JMeter.convertSubTree(JMeter.java:885) at org.apache.jmeter.JMeter.convertSubTree(JMeter.java:885)...	Work-round is to rename the Module Controller so it has a different name from the target controller.May perhaps be related to?Thanks for the original report and Test Case.Turned out that the Module Controller (MC) was finding itself rather than the proper target node. Fixed by not allowing the target node to be a Module Controller.URL:Log:StackOverflowError with ModuleController in Non-GUI mode if its name is the same as the target nodeBugzilla Id: 55375Added: jmeter/trunk/bin/testfiles/.csv (with props) jmeter/trunk/bin/testfiles/.jmx (with props) jmeter/trunk/bin/testfiles/.xml (with props)Modified: jmeter/trunk/build.xml jmeter/trunk/src/components/org/apache/jmeter/control/ModuleController.java jmeter/trunk/xdocs/changes.xmlCreatedTest Plan showing another way for issue to occurIt's picking the wrong parent controller.If you look in the drop-down list in the GUI, there are two instances of the target:Test Plan > Thread Group > reloadSelecting the second one in the GUI results in a stack overflow error when running the GUI test.It looks like the non-GUI code happens to choose the wrong target - correct name, wrong instance.Note: the Test Plans that are created are identical, so the GUI test must be using additional information to identify the target controller.Not sure if it makes sense to allow multiple MC targets with the same name.If it does, then the JMX file (and drop-down list) will need to contain extra information to identify the instance. It may just be simpler to complain if the Test Plan contains multiple targets with the same name.CreatedPatch proposal to detect recursivityThe patch effectively converts the stack overflow into an illegal state exception.The benefit is that the message reports the name offending test element (though this may not be unique).However it does not solve the issue that the non-GUI code behaves differently from the GUI code.Ideally any fix should address the different behaviour and prevent recursive calls.If the MC refused to allow multiple controllers with the same name, that should solve the behaviour difference, but it could affect some test plans. Given that at present the non-GUI code does not have sufficient information to choose the correct controller in the case of duplicates, that change is probably acceptable.The other issue is that the user may choose a target which is a parent of the MC, causing stack overflow. If controller names must be unique then this should be less likely to occur. Since such targets are useless, ideally they should be excluded from the list. This would need to be done after duplicate checking.(In reply to Sebb from)Shall I commit it ?In fact both GUI and Non GUI behave identically except that sometimes GUI picks the first element instead of picking the second one. But if you fix it then you get a StackOverflow.Fix prevents recursive calls.Not sureCould be next step.To be clear, I think the GUI may have in fact changed the saved reference value of Module Controller. So I suspect another bug in GUI mode which gets confused when there are duplicated.I don't see any point in applying a temporary fix.The GUI seems to remember the correct instance whilst it is active.i.e. it will run which ever is selected from the list.However, when the JMX is saved, only the name is stored.So when the JMX is reloaded, it cannot possibly know which one was previously selected.This is a fundamental problem for both GUI and non-GUI test runs.One way to fix this is to insist that controller names are unique.Another way would be to store the instance number.This could either be done for every case, or only done where there are multiple matches, or the instance number could default to the first (i.e. only store it if not the first or only instance).Requiring unique names would be more likely to break existing test plans, as the default names are not unique, so I now think it would be better to store an instance number. Making the instance default to 1 would reduce the number of test plans that needed to be updated. However it would make it more difficult to determine whether the choice was deliberate or accidental, unless the test plan version is also taken into account. So it might be better to insist that the instance number was always saved.The question then arises - what should JMeter do if the user adds another controller with the same name after the MC entry has been selected?(In reply to Sebb from)Ok, let's wait for a better implementationOk, that's what I was saying.Maybe but could be tricky to develop and maintain.***has been marked as a duplicate of this bug. ***	11.0	id=55756	5	False	False	p.mouawad	1
id=55756	REOPENED	None	JMeter	HTTP (	2.10	All All	P2 enhancement	JMeter issues mailing list	2013-11-07 14:41 UTC by	Philippe Mouawad	2013-11-08 23:38 UTC (	1 user	Add X-SetHeaders special request header to make mirror return these headers.Separator for headers would be |Syntax would be:headerA=valueA|headerB=valueB	Date: Thu Nov 7 14:42:28 2013New Revision: 1539664URL:Log:- HTTP Mirror Server : Add ability to set HeadersBugzilla Id: 55756Modified: jmeter/trunk/src/protocol/http/org/apache/jmeter/protocol/http/control/HttpMirrorThread.java jmeter/trunk/xdocs/changes.xmlDate: Thu Nov 7 14:44:39 2013New Revision: 1539667URL:Log:- HTTP Mirror Server : Add ability to set HeadersDocumentBugzilla Id: 55756Modified: jmeter/trunk/xdocs/usermanual/component_reference.xmlDate: Thu Nov 7 20:36:32 2013New Revision: 1539805URL:Log:- HTTP Mirror Server : Add ability to set HeadersFix tests failureBugzilla Id: 55756Modified: jmeter/trunk/src/protocol/http/org/apache/jmeter/protocol/http/control/HttpMirrorThread.javaThis is basically a duplicate of.As discussed there, the facility is not particularly useful as it requires the browser (or other client) to send the X-headers in order to get them returned without the X- prefix.It would be better to implement the suggestion inwhich is to add a table to the mirror server GUI with the headers it should return.I'm finding it hard to imagine a use-case for the current implementation.	3.0	id=54481	12	False	False	shmulikk	1
id=56539	REOPENED	None	JMeter	Main (	2.11	PC All	P2 normal	JMeter issues mailing list	2014-05-17 09:05 UTC by	sergio	2014-07-18 19:46 UTC (	1 user	CreatedJMX script showing the behaviourSuppose to use IMAP and to set two messages to downloads.When you run the test, Results tree shows you a sample holding two leaves (one for each message).It looks like it were an HTML page holding 2 different component, which is very reasonable.BUT, if you go to the "Aggregate Report" and "Summary Report" you will see 2 different operations, each with its own time.Again, if you save the results to a .jtl file, only one results is saved, that holds the total time (which is the expected behaviour).THis happens only when downloading more than 1 message in a single sample.Java: Java(TM) SE Runtime Environment (build 1.7.0_55-b13).	Hello,This is because you checked save sub results.The only issue I find is that #Samples shows 2 while it should show only one.So I will fix this.Waiting for your feedback.Date: Sun Jul 6 14:26:03 2014New Revision: 1608206URL:Log:- Mail reader sampler: When Number of messages to retrieve is superior to 1, Number of samples should only show 1 not the number of messages retrievedBugzilla Id: 56539Modified: jmeter/trunk/src/protocol/mail/org/apache/jmeter/protocol/mail/sampler/MailReaderSampler.java jmeter/trunk/xdocs/changes.xml(In reply to Philippe Mouawad from)HI Philippe, IMHO the issue is a little bit more complex.It seems to me that "Aggregate Report" uses the data from each subsample to calculate the statistical result.E.g. if you have 2 subsample, and each subsample takes 100ms to run, one should see 200 ms as average response time, for 10 samples.In Jmeter 2.11, you will find 20 samples with 100 ms response time.HTH	3.0	id=58679	10	False	True	benoit.wiart	1
id=58704	REOPENED	None	JMeter	Main (	2.13	All All	P2 normal	JMeter issues mailing list	2015-12-08 10:42 UTC by	None	2016-01-17 13:45 UTC (	1 user	due to log in the file BatchTestLocal.logeg :2015/12/02 09:28:23 WARN - jmeter.config.CSVDataSet: Could not translate shareMode=All threads using Locale: frthe locale must be set to -Duser.language=en in the build.xml file	Issue related toAuthor: pmouawadDate: Sat Jan 16 13:53:08 2016New Revision: 1724976URL:Log:Non regression testing : Ant task batchtest fails if tests and run in a non en_EN locale and use a JMX file that uses a Csv DataSetBugzilla Id: 58704Modified: jmeter/trunk/build.xml jmeter/trunk/xdocs/changes.xmlAlthough fixing the locale avoids the test failure, I'm not sure that is the correct solution.JMX files should ideally be portable between systems with different locales.Another fix is to remove the label in the JMX files and replace it by shareMode.all, which is how it is stored sincehas been fixed.If you are ok , I can commit the change and revert the locale setting.Note that for me it is fine as the issue here should not happen for a user unless he has used locale A on his computed and save the file in a version whereis not fixed and opened it in a newer version under another locale, he will lose the sharing mode.I think there's another issue here:Seems to me that ideally JMeter ought to be able to understand the original sharemode setting even if the current Locale is different. As we have seen here, it's possible for old-style JMX files to be re-used on a different locale.	5.0	id=58704	9	False	False	p.mouawad	1
id=58679	REOPENED	None	JMeter	Main (	2.13	All All	P2 major	JMeter issues mailing list	2015-12-02 09:31 UTC by	None	2016-04-27 18:42 UTC (	1 user	in current jmeter version xstream use the xpp library to provide the xml pull parser functionality.Since Java6 it's possible to use the xstream staxDriver which does not need the xpp librarySeePR will follow.Testing was done with the jmeter test suite + local tests	PRDate: Wed Dec 2 12:23:11 2015New Revision: 1717619URL:Log:- Replace the xpp pull parser in xstream with a java6+ standard solutionBugzilla Id: 58679Removed: jmeter/trunk/licenses/bin/xmlpull-1.1.3.1.txt jmeter/trunk/licenses/bin/xpp3-1.1.4c.txtModified: jmeter/trunk/build.properties jmeter/trunk/build.xml jmeter/trunk/eclipse.classpath jmeter/trunk/lib/aareadme.txt jmeter/trunk/res/maven/ApacheJMeter_parent.pom jmeter/trunk/src/core/org/apache/jmeter/save/SaveService.java jmeter/trunk/xdocs/changes.xmlDate: Wed Dec 2 12:27:48 2015New Revision: 1717621URL:Log:- Replace the xpp pull parser in xstream with a java6+ standard solutionBugzilla Id: 58679Modified: jmeter/trunk/lib/ (props changed)Date: Fri Dec 11 19:49:27 2015New Revision: 1719512URL:Log:Replace the xpp pull parser in xstream with a java6+ standard solution.Bugzilla Id: 58679Modified: jmeter/trunk/test/src/org/apache/jmeter/JMeterVersionTest.javaI found a regression (I thinks) with this change.2016/04/14 19:16:43 WARN - jmeter.gui.action.Load: Unexpected error java.lang.IllegalArgumentException: Problem loading XML from:'/tmp/Scripts-JMeter/03_Issue.jmx', missing class com.thoughtworks.xstream.converters.ConversionException: : ParseError at [row,col]:[32,44]Message: Character reference "&# : : ParseError at [row,col]:[32,44]Message: Character reference "&#see log in attachment.I cannot open this script jmx (in attachment) with latest trunk (error above), but it's works with JMeter 2.13.If I return just before the commits about 58679, it's works.It I revert only the commits from trunk (with git), It's works too.This bug blocks the next release 3.0.CreatedLogs with exceptionCreatedSimple test plan with special data which don't parse with the commits of 58679This post seems to be about the same error:-back-from-xml&#x0; is not a valid xml 1.0 entity. So the parser has the right to refuse such a file. :(Hi,If you open it in Firefox for example it gives the same error.RegardsOk but why it's works with JMeter 2.13 (or without this change)It's was created by JMeter proxy element, 3.0 SNAPSHOT (20160406).Record and save the script works, but in the same version, open the script don't.Regression in the save action service? if the &#x0; isn't a good entity.?I have a recording session from the original record (in a jtl file)<httpSample t="0" it="0" lt="0" ct="0" ts="1459960789816" s="false" lb="162 /UploadData.aspx" rc="Non HTTP response code: java.lang.IllegalArgumentException" rm="Non HTTP response message: URLDecoder: Illegal hex characters in escape (%) pattern - For input string: &quot;&#x1;&#x0;&quot;" tn="" dt="text" de="" by="694" sc="1" ec="1" ng="0" na="0" hn="ender"> <responseHeader class="java.lang.String"></responseHeader> <requestHeader class="java.lang.String"></requestHeader> <responseData class="java.lang.String">java.lang.IllegalArgumentException: URLDecoder: Illegal hex characters in escape (%) pattern - For input string: &quot;&#x1;&#x0;&quot; at java.net.URLDecoder.decode(URLDecoder.java:194) at org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.sendPostData(HTTPHC4Impl.java:1270) at org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.handleMethod(HTTPHC4Impl.java:504) at org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.sample(HTTPHC4Impl.java:322) at org.apache.jmeter.protocol.http.sampler.HTTPSamplerProxy.sample(HTTPSamplerProxy.java:74) at org.apache.jmeter.protocol.http.sampler.HTTPSamplerBase.sample(HTTPSamplerBase.java:1146) at org.apache.jmeter.protocol.http.proxy.Proxy.run(Proxy.java:240)</responseData> <responseFile class="java.lang.String"></responseFile> <cookies class="java.lang.String"></cookies> <method class="java.lang.String">POST</method> <queryString class="java.lang.String"></queryString> <java.net.URL></java.net.URL></httpSample><httpSample t="787" it="0" lt="787" ct="325" ts="1459960789840" s="true" lb="163 /UploadData.aspx" rc="200" rm="OK" tn="" dt="" de="" by="178" sc="1" ec="0" ng="0" na="0" hn="ender"> <responseHeader class="java.lang.String">HTTP/1.1 200 OKCache-Control: privateContent-Length: 0Server: Microsoft-IIS/7.5X-AspNet-Version: 4.0.30319X-Powered-By: ASP.NETDate: Wed, 06 Apr 2016 16:39:49 GMT</responseHeader> <requestHeader class="java.lang.String">Connection: keep-aliveUser-Agent: MSDWContent-Type: application/x-www-form-urlencodedContent-Length: 5243Host: ssw.live.com</requestHeader> <responseData class="java.lang.String">Non-TEXT response data, cannot record: ()</responseData> <responseFile class="java.lang.String"></responseFile> <cookies class="java.lang.String"></cookies> <method class="java.lang.String">POST</method> <queryString class="java.lang.String">MSQMx%00%00%00%00%00%00%00%129%23%C3%B0%03%00%00%00%C3%92%06%00%00M%00%00%00%02%00%10%00%01%00%00%00%00%00%00%00%60m%C3%9F%C3%AF%22%C2%90%C3%91%01%00%00%00%00%00%00%00%00%60%C3%A7%02%C3%97+%C2%90%C3%91%01P%C3%95%C3%9C%C3%AF%22%C2%90%C3%91%01k%1C%2F+%C3%86%C3%B5%60I%C2%AC%02%C2%AD%C3%89a%3E%C3%91%1C%C2%A35%C2%98%7B%C3%9BQ%C3%B6%40%C2%BEYC3l%C2%A2%C3%B7=&amp;%00%00%00%00%02%00%00%00%00%00%00%00%00%00%00%00%00%00%00%00l%00%00%00%02%00%00%00%00%00%00%00%00%00%00%00%03%00%00%00%00%00%0A%00%00%00%00%00%04%00%00%00%01%00%00%00%00%00%00%00%05%00%00%00Z%29%00%00%00%00%00%00%06%00%00%00%00%00%00%00%00%00%00%00%07%00%00%000%00%00%00%00%00%00%00%10%00%00%00%01%09%00%0A%00%00%00%00%11%00%00%00%02%00%00%00%00%00%00%00%C3%A8%03%00%00%0F%00%00%00O%C2%BE%0D%00%03%00%00%00r%03%00%00%01%00%00%00%00%00%00%00+%00%00%008%003%00C%000%008%002%00D%00B%003%00E%001%005%004%00B%001%006%008%001%00B%00F%00B%005%008%001%008%003%003%000%00C%00E%00A%00C%00%00%00%00%00%08%00%00%00%00%00%00%00%05%00%00%00%28%00N%00%2F%00A%00%29%00%00%00%00%00%09%00%00%00%00%00%00%00%0E%00%00%001%007%00.%003%00.%006%003%000%002%00.%000%002%002%005%00%00%00%00%00%0A%00%00%00%00%00%00%00%05%00%00%00%28%00N%00%2F%00A%00%29%00%00%00%00%00%0B%00%00%00%00%00%00%00%05%00%00%00%28%00N%00%2F%00A%00%29%00%00%00%00%00%0C%00%00%00%00%00%00%00%05%00%00%00f%00r%00-%00F%00R%00%00%00%00%00%0D%00%00%00%00%00%00%00%05%00%00%00%28%00N%00%2F%00A%00%29%00%00%00%00%00%0E%00%00%00%00%00%00%00%05%00%00%00%28%00N%00%2F%00A%00%29%00%00%00%00%00%0F%00%00%00%00%00%00%00%05%00%00%00%28%00N%00%2F%00A%00%29%00%00%00%00%00%12%00%00%00%00%00%00%00%05%00%00%00%28%00N%00%2F%00A%00%29%00%00%00%00%00%13%00%00%00%00%00%00%00%05%00%00%00%28%00N%00%2F%00A%00%29%00%00%00%00%00%14%00%00%00%00%00%00%00%01%00%00%000%00%00%00%00%00%21%00%00%00%00%00%00%00%04%00%00%00p%00r%00o%00d%00%00%00%00%00=&amp;%00%00%00%00%00%00%00%24%00%00%00d%003%006%001%00a%00a%00c%009%00-%003%003%00c%000%00-%005%00a%00a%002%00-%003%006%009%007%00-%006%00f%009%006%008%002%00d%009%00c%004%000%003%00%00%00%00%00%C2%87%0F%03%00%00%00%00%00%01%00%00%00%23%00%00%00%00%00%C2%8C%0F%03%00%00%00%00%00%0E%00%00%001%006%00.%000%00.%006%007%000%001%00.%001%000%001%003%00%00%00%00%00%C2%A1%0F%03%00%00%00%00%00%24%00%00%00d%003%006%001%00a%00a%00c%009%00-%003%003%00c%000%00-%005%00a%00a%002%00-%003%006%009%007%00-%006%00f%009%006%008%002%00d%009%00c%004%000%003%00%00%00%00%00%C2%A2%0F%03%00%00%00%00%00%10%00%00%00O%00n%00e%00D%00r%00i%00v%00e%00C%00o%00n%00s%00u%00m%00e%00r%00%00%00%00%00%C2%A3%0F%03%00%00%00%00%00%24%00%00%000%000%000%000%000%000%000%000%00-%000%000%000%000%00-%000%000%000%000%00-%000%000%000%000%00-%000%000%000%000%000%000%000%000%000%000%000%000%00%00%00%00%00%C2%B0%0F%03%00%00%00%00%00%06%00%00%00%28%00n%00u%00l%00l%00%29%00%00%00%00%00%C2%B2%0F%03%00%00%00%00%00%05%00%00%00*%00*%00*%00*%00*%00%00%00%00%00%C2%B3%0F%03%00%00%00%00%00%05%00%00%00*%00*%00*%00*%00*%00%00%00%00%00%C2%B4%0F%03%00%00%00%00%00%06%00%00%00%28%00n%00u%00l%00l%00%29%00%00%00%00%00%05%00%00%00%C3%9C%02%00%00%C2%95%0F%03%00%02%00%00%00%3C%00%00%00%00%00%00%00%3F4%00%00%00%00%00%00%00%00%00%00%3F4%00%00%40u%00%00%00%00%00%00%7F%C2%A9%00%00%00%00%00%00%00%00%00%00%7F%C2%A9%00%00%40u%00%00%00%00%00%00%C2%AF%1E%01%00%00%00%00%00%00%00%00%00%C2%AF%1E%01%000u%00%00%00%00%00%00%C3%AE%C2%93%01%00%00%00%00%00%00%00%00%00%C3%AE%C2%93%01%00%3Fu%00%00%00%00%00%00.%09%02%00%00%00%00%00%00%00%00%00.%09%02%00%40u%00%00%00%00%00%00m%7E%02%00%00%00%00%00%00%00%00%00m%7E%02%00%3Fu%00%00%00%00%00%00%C2%9D%C3%B3%02%00%00%00%00%00%00%00%00%00%C2%9D%C3%B3%02%000u%00%00%00%00%00%00%C3%9Dh%03%00%00%00%00%00%00%00%00%00%C3%9Dh%03%00%40u%00%00%00%00%00%00%0D%C3%9E%03%00%00%00%00%00%00%00%00%00%0D%C3%9E%03%000u%00%00%00%00%00%00MS%04%00%00%00%00%00%00%00%00%00MS%04%00%40u%00%00%00%00%00%00%C2%8C%C3%88%04%00%00%00%00%00%00%00%00%00%C2%8C%C3%88%04%00%3Fu%00%00%00%00%00%00%C2%BC=%05%00%00%00%00%00%00%00%00%00%C2%BC%3D%05%000u%00%00%00%00%00%00%C3%AC%C2%B2%05%00%00%00%00%00%00%00%00%00%C3%AC%C2%B2%05%000u%00%00%00%00%00%00%2C%28%06%00%00%00%00%00%00%00%00%00%2C%28%06%00%40u%00%00%00%00%00%00l%C2%9D%06%00%00%00%00%00%00%00%00%00l%C2%9D%06%00%40u%00%00%00%00%00%00%C2%AB%12%07%00%00%00%00%00%00%00%00%00%C2%AB%12%07%00%3Fu%00%00%00%00%00%00%C3%AB%C2%87%07%00%00%00%00%00%00%00%00%00%C3%AB%C2%87%07%00%40u%00%00%00%00%00%00%1B%C3%BD%07%00%00%00%00%00%00%00%00%00%1B%C3%BD%07%000u%00%00%00%00%00%00Zr%08%00%00%00%00%00%00%00%00%00Zr%08%00%3Fu%00%00%00%00%00%00%C2%8A%C3%A7%08%00%00%00%00%00%00%00%00%00%C2%8A%C3%A7%08%000u%00%00%00%00%00%00%C2%BA%5C%09%00%00%00%00%00%00%00%00%00%C2%BA%5C%09%000u%00%00%00%00%00%00%C3%AA%C3%91%09%00%00%00%00%00%00%00%00%00%C3%AA%C3%91%09%000u%00%00%00%00%00%00*G%0A%00%00%00%00%00%00%00%00%00*G%0A%00%40u%00%00%00%00%00%00j%C2%BC%0A%00%00%00%00%00%00%00%00%00j%C2%BC%0A%00%40u%00%00%00%00%00%00%C2%9A1%0B%00%00%00%00%00%00%00%00%00%C2%9A1%0B%000u%00%00%00%00%00%00%C3%99%C2%A6%0B%00%00%00%00%00%00%00%00%00%C3%99%C2%A6%0B%00%3Fu%00%00%00%00%00%00%09%1C%0C%00%00%00%00%00%00%00%00%00%09%1C%0C%000u%00%00%00%00%00%00I%C2%91%0C%00%00%00%00%00%00%00%00%00I%C2%91%0C%00%40u%00%00%00%00%00%00%C2%89%06%0D%00%00%00%00%00%00%00%00%00%C2%89%06%0D%00%40u%00%00%00%00%00%00%C2%B9%7B%0D%00%00%00%00%00%00%00%00%00%C2%B9%7B%0D%000u</queryString> <java.net.URL></java.net.URL></httpSample>I can reproduce the ssw.live.com request (on a Win 10), just wait some minutes after have configure JMeter as the proxy of machine.With a trunk JMeter version *without* the commits of 58679, it's works, I can save, close and (re)open the script (in attachment)CreatedA new script generate by trunk without 58679 (but don't works for open with trunk)Hi,Issue is either in XStream using StaxDriver or in Java.I created a test case and submitted:-Meanwhile, I suggest we revert the bug code to avoid blocking release.RegardsHi,Jörg from XStream already answered:So I think we should for now revert then implement what's here:-URL:Log:Revert Bugzilla 58679 - A character of value 0 is not valid as part of XML, but it's was written by xpp3.This closes #197Bugzilla Id: 58679Added: jmeter/trunk/licenses/bin/xmlpull-1.1.3.1.txt (with props) jmeter/trunk/licenses/bin/xpp3-1.1.4c.txt (with props)Modified: jmeter/trunk/LICENSE jmeter/trunk/build.properties jmeter/trunk/build.xml jmeter/trunk/eclipse.classpath jmeter/trunk/lib/ (props changed) jmeter/trunk/lib/aareadme.txt jmeter/trunk/res/maven/ApacheJMeter_parent.pom jmeter/trunk/src/core/org/apache/jmeter/save/SaveService.java jmeter/trunk/test/src/org/apache/jmeter/JMeterVersionTest.java jmeter/trunk/xdocs/changes.xml	17.0	id=56539	8	False	False	p.mouawad	1
id=59052	REOPENED	None	JMeter	Main (	3.0	PC All	P2 minor	JMeter issues mailing list	2016-02-23 12:33 UTC by	None	2016-05-31 20:32 UTC (	0 users	Hi,You could set the JM_LAUNCH but this information is not used to detected the java version, so JM_LAUNCH and 'java' could have differents versionsrem JM_LAUNCH - java.exe (default) or javaw.exeset JM_LAUNCH=d:\java\jdk1.7but the java -version 2^>^&1 use the version in the PATH not the JM_LAUNCH...for /f "tokens=3" %%g in ('java -version 2^>^&1 ^| findstr /i "version"') do ( rem @echo Debug Output: %%g set JAVAVER=%%g)Regards.Vincent D.	It's clear from the comment that JM_LAUNCH is the executable name, and does not include the PATH.Other usage is not supported/I second this. It is usual that the JDK or JRE present in the PATH of the system is not the one used to run JMeter. And I know you can copy the jmeter.bat and modify it yourself but this can be easily fixed moving one line in the jmeter.bat file:Line 64: if .%JM_LAUNCH% == . set JM_LAUNCH=java.exe <-- Move this to line 33 And changing this:Line 34: for /f "tokens=3" %%g in ('java -version 2^>^&1 ^| findstr /i "version"') do (To:for /f "tokens=3" %%g in ('%JM_LAUNCH% -version 2^>^&1 ^| findstr /i "version"') do (So the new would look like:Line 33: if .%JM_LAUNCH% == . set JM_LAUNCH=java.exeLine 34: for /f "tokens=3" %%g in ('%JM_LAUNCH% -version 2^>^&1 ^| findstr /i "version"') do (Then you can set JM_LAUNCH to use a full path (i.e JM_LAUNCH=C:\myjdk\bin\java.exe) and it works perfectly (winxp, win7, winserver2012 at least).Is not a big deal but simplifies the execution when having multiple JDKs	2.0	id=58240	5	False	False	apache	1
id=58752	REOPENED	None	JMeter	HTTP (	2.13	All All	P2 enhancement	JMeter issues mailing list	2015-12-18 05:42 UTC by	Eagle Liu	2015-12-22 00:34 UTC (	1 user	Steps:1. Create a HTTP request into loop controller[Loop count 2] for a new test plan in JMeter Client.2. Add HTTP Cookie Manager into the HTTP sampler3. Add a dynamical cookies with Domain and path.Name: testValue: 123${__Random(100,999,)}Domain: www.google.compath: /4. Chose Clear cookies each iteration option.5. Run the test planExpected Result:the cookies value will be changed for each request.Actual Result:the cookies value is not changed in View Results Tree Listener	The cookie will be changed for each iteration not for each request (if for example you have 2 requests inside the Thread Group).It is working for me this way.Thank you for your explanation!Now, I realized that this behavior could help one thread keep cookies values in one session.Anyway, I still need to changed cookies values for each request in some special case. If HTTP Cookie Manager couldn't support the scenarios, I have to use BeanShell PreProcessor to deal with it.I reopen the bug importance to status enhancement and added my explanation above. Please consider to add a feature to support it.	3.0	id=59052	7	False	True	vdaburon	1
id=59973	REOPENED	None	JMeter	Main (	3.0	All All	P2 minor	JMeter issues mailing list	2016-08-11 10:09 UTC by	Andrey Pokhilko	2016-08-17 20:37 UTC (	1 user	When JMeter writes XML JTL files, it uses header with XML version 1.0declared()Here's example that I generated with ResultCollector, reproducing theissue from one of the users:<?xml version="1.0" encoding="UTF-8"?><testResults version="1.2"><sample t="0" it="0" lt="0" ts="1469356444856" s="false" lb="" rc=""rm="&#x9;" tn="" dt="" by="0" ng="0" na="0"/></testResults>If we will use strict XML parser, likeor other strict XML reading library, we will get the error for the abovetext, because &#x9 is a character entity, only allowed in XML 1.1.XStream writer that we use generates XML 1.1 data and there is FAQ entryfor this case:It is a bug of writing file body as XML 1.1 whiledeclaration says 1.0.	Date: Thu Aug 11 10:27:45 2016New Revision: 1755929URL:Log:- Issue with XML version in JTL filesModified: jmeter/trunk/bin/testfiles/BatchTestLocal.xml jmeter/trunk/bin/testfiles/BatchTestLocalRemote.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/HTMLParserTestFile_2.xml jmeter/trunk/bin/testfiles/TEST_HTTPS.xml jmeter/trunk/src/core/org/apache/jmeter/reporters/ResultCollector.java jmeter/trunk/xdocs/changes.xmlThe first commits introduce a regression I think.With the trunk version, if I try to open a JTL file (recorded by trunk version too) into a listenr, I have this error message (the listener dont display the results) :2016/08/17 10:11:28 WARN - jmeter.reporters.ResultCollector: Failed to load /directory/Results-20160816-1625.jtl using XStream. Error was: com.thoughtworks.xstream.io.StreamException: : only 1.0 is supported as <?xml version not '1.1' (position: START_DOCUMENT seen <?xml version="1.1"... @1:19)Funny... Xstream writes XML 1.1, but requires 1.0 declaration... I'll investigate it moreMilamber, which version of XStream library do you have? I have 1.4.8 and don't experience this. Also, I tried searching for this kind of error message in XStream sources of 1.4.8 and did not find it.The version from the trunk: xstream-1.4.8.jarTest case :- Compile JMeter from trunk (ant distribution)- unzip the binary, launch JMeter (with Java8)- Create a simple test plan:Thread Group |-- HTTP request |-- View Results Tree with a file to record into JTL (inside Configure button select all options)- Run- Close JMeter and reopen- Add a View Results Tree, a try to open the jtl file from the simple test.Thanks for the reproducing case. I have investigated it and found that issue is in xpp3 library we use. It is 10 years old and has xml 1.0 requirement hardcoded into it. There are no newer versions of this library that could load XML 1.1.So it's a deadlock: we write XML 1.1 as body, we state that it's 1.0 in header because reading flow is unable to accept 1.1 header. However, it perfectly eats 1.1 body, which means that xpp3 does not conform to XML standard.From their FAQ () I can assume that the only way is to rollback my changes. Also, we should document somewhere here () that JTL and JMX files produced by JMeter do not conform XML 1.0 standard and cannot be read by strict parsers. Users should be aware of this issue.I have reverted the modifications(In reply to Andrey Pokhilko from)See thisrelated to this issue, this would be the final fix.And thisDate: Wed Aug 17 20:11:30 2016New Revision: 1756681URL:Log:- revert all modificationsModified: jmeter/trunk/bin/testfiles/BatchTestLocal.xml jmeter/trunk/bin/testfiles/BatchTestLocalRemote.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/.xml jmeter/trunk/bin/testfiles/HTMLParserTestFile_2.xml jmeter/trunk/bin/testfiles/TEST_HTTPS.xml jmeter/trunk/src/core/org/apache/jmeter/reporters/ResultCollector.java jmeter/trunk/xdocs/changes.xmlDate: Wed Aug 17 20:24:21 2016New Revision: 1756682URL:Log:- document non-standard XML in docs and codeModified: jmeter/trunk/src/core/org/apache/jmeter/reporters/ResultCollector.java jmeter/trunk/xdocs/usermanual/component_reference.xml	12.0	id=58752	6	False	True	p.mouawad	1
id=59633	REOPENED	None	JMeter	Main (	3.0	HP All	P2 normal	JMeter issues mailing list	2016-05-25 14:08 UTC by	Ken Coppage	2016-06-06 19:22 UTC (	0 users	In an opened script if you perform "Save As" function using a new different name the opened file does not change to the new Save As file name like it does in Window. The old file name remains and you likely will overwrite the new changes to the old file.This is a problem when you are creating versions of an existing baseline file, in that you perform a SAVE AS, then and have to Close the existing file WITHOUT saving it and then open the new file name. If you save the open file the changes just made are applied corrupting the old file.Have seen this problem in version 2.13 and version 3.0.	There are three different "save as" entries. Only one changes the current testplan name. It is the one named "Save Test Plan as" and can be invoked with Ctrl+Shift+s.Feel free to re-open this entry, if you think there is still an error.the resolution you suggest is not on the toolbar of functions. So it does not default to Save As Test Plan, just Selection. This is very confusing for normal Windows platform users. If the function was added to the tool ribbon then ok that would help.I personally think it causes over-writes of existing scripts accidently.Now I know what to do, but I foresee problems with other team members who are new to Jmeter. Not many people save files by going FILE first, when the ribbon show normal options.Just speaking from a clarity standpoint.	2.0	id=60120	8	False	False	p.mouawad	1
id=38883	REOPENED	None	Log4j - Now in Jira	Other (	1.3alpha	Other other	P2 normal	log4j-dev	2006-03-07 15:41 UTC by	Scott Deboy	2008-08-02 18:13 UTC (	0 users	An entry in a log file like this:2006-02-10 16:09:36,977 [INFO] ejb.SecurityManagerBean - [MA Core connected toremote services...host: nlveus13.xxxx.comport: 8561repository: Foundationdeployment: Remote Services]Should be able to be processed with a logFormat field like this:TIMESTAMP [LEVEL] LOGGER - [MESSAGE]However, this results in a non-matching entry (see logging output, for exampleChainsaw's chainsaw-log tab).Multi-line entries work fine if the MESSAGE field is not delimited on bothsides. For example, this entry:2006-02-10 16:09:36,977 [INFO] ejb.SecurityManagerBean - MA Core connected toremote services...host: nlveus13.xxxx.comport: 8561repository: Foundationdeployment: Remote ServicesWill be processed correctly with this logFormat:TIMESTAMP [LEVEL] LOGGER - MESSAGE	Version 1.3 is discontinued, and LogFilePatternAppender does not exist (afaics) in 1.2. Marking issue WONTFIX.LogFilePatternReceiver is a part of the receivers companions..see	2.0	id=38883	4	False	False	thorbjoern	1
id=60120	REOPENED	None	JMeter	HTTP (	3.0	PC All	P2 normal	JMeter issues mailing list	2016-09-12 14:46 UTC by	Stuart Barlow	2016-09-27 10:34 UTC (	1 user	CreatedSimple test plan with HTTP POST RequestA HTTP Request configured to POST a parameter containing the @ symbol sees that parameter silently encoded. The @ symbol is incorrectly encoded as %40. This occurs whether I tick the 'Encode?' check box or not. To demonstrate the issue I have attached a simple .jmx that should be executed against the following HTML. <html><body><img alt="Captcha" class="captcha-image" width="100" height="50" src="/IqGo6EM1JEVZ+MSRJqUSo@qhjVMSFBTs/37/0/1"></img><input type="hidden" name="captchaCapiId" value="IqGo6EM1JEVZ+MSRJqUSo@qhjVMSFBTs"></body></html>	Hi Stuart,The attached test plan does not have a POST method.Can you provide the correct one ?Thank youCreatedTest Plan with POSTSorry. I attached the wrong .jmxThe correct one has been attached now.CreatedWorkaroundHello,Your issue is due to the fact that we use UrlEncodedFormEntity class from HTTPClient when posting a form.I'll investigate if it's a bug or not.Meanwhile attached a way to workaround your problem.RegardsThe workaround worked. Thanks.Personally I think it is a bug. I'd expect the Encode? checkbox to alter the paramter. At the moment it appears from a user perspective to have no affect.(In reply to Stuart Barlow from)Hi Stuart,If you feel it's a bug, then can you jointand participate to discussion "UrlEncodedFormEntity and parameter value encoding"Thank youRegards(In reply to Stuart Barlow from)Hi Stuart,In fact maybe you should check that the bug is not in your server stack.Encoding of @ should not break your code.Frankly the RFC is not very clear on that side so it's hard to tell if encoding should be done or not.Anyway JAva encodes it so I think it's not a bug.I am closing issue as WORKSFORME. Feel free to reopen if you find a reference telling it should not be.And I am of course interested if it ends up to be a bug in your server stack.RegardsHi PhilippeI'm not sure a Wikipedia page can be used definitively but I found this"When a character from the reserved set (a "reserved character") has special meaning (a "reserved purpose") in a certain context, and a URI scheme says that it is necessary to use that character for some other purpose, then the character must be percent-encoded."I think the RFCs leave it open and optional. It depends on the context. Likewise it should probably be optional for JMeter users too. Thanks for looking into it and I see you've put effort in but I would still disagree. I feel JMeter is defective. From the users' perspective, whether they click the 'encode?' checkbox or not should influence whether that parameter is percent-encoded or not. At the moment @ is encoded whether the check box is clicked or not. The user isn't aware or concerned what underlying API is used. Just because Java does it, doesn't mean it is right :)Regards	8.0	id=59973	9	False	False	apc4	1
id=42213	REOPENED	None	Log4j - Now in Jira	Appender (	1.2	Sun Solaris	P2 blocker	log4j-dev	2007-04-24 01:20 UTC by	Sudhanshu	2008-10-07 12:01 UTC (	1 user	The log4j is causing threads in weblogic server to STUCK.The thread dump prints the following: "[STUCK] ExecuteThread: '20' for queue: 'weblogic.kernel.Default (self-tuning)'" RUNNABLE java.io.FileOutputStream.writeBytes(Native Method) java.io.FileOutputStream.write(FileOutputStream.java:260) sun.nio.cs.StreamEncoder$CharsetSE.writeBytes(StreamEncoder.java:336) sun.nio.cs.StreamEncoder$CharsetSE.implFlushBuffer(StreamEncoder.java:404) sun.nio.cs.StreamEncoder$CharsetSE.implFlush(StreamEncoder.java:408) sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:152) java.io.OutputStreamWriter.flush(OutputStreamWriter.java:213) org.apache.log4j.helpers.QuietWriter.flush(QuietWriter.java:57) org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:315) org.apache.log4j.RollingFileAppender.subAppend(RollingFileAppender.java:236) org.apache.log4j.WriterAppender.append(WriterAppender.java:159) org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:230) org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:65) org.apache.log4j.Category.callAppenders(Category.java:203) org.apache.log4j.Category.forcedLog(Category.java:388) org.apache.log4j.Category.debug(Category.java:257) The environment details are:Java version:java version "1.5.0_04"Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_04-b05)Java HotSpot(TM) Server VM (build 1.5.0_04-b05, mixed mode)Weblogic version:Weblogic 9.2OS:Sun SolarisSunOS ncsv4sun 5.10 Generic_118833-03 sun4v sparc SUNW,Sun-Fire-T200	Log4j version is 1.2.14We are using RollingFileAppender. Multiple users(around 50) are accessing the application and logs are written very frequently. Log level is DEBUG.This looks like a duplicate of*** This bug has been marked as a duplicate of***We have same environment and same issue. Sudhanshu/Anyone who want to answer : How was this issue resolved finally.Duplicate bug(41214)mentioned that this may be jre1.5 solaris bug."[STUCK] ExecuteThread: '93' for queue: 'weblogic.kernel.Default (self-tuning)'" daemon prio=3 tid=0x00000001071bbea0 nid=0x5a0 runnable [0xfffffffe9a3fd000..0xfffffffe9a3ff6a8] at java.io.FileOutputStream.writeBytes(Native Method) at java.io.FileOutputStream.write(FileOutputStream.java:260) at sun.nio.cs.StreamEncoder$CharsetSE.writeBytes(StreamEncoder.java:336) at sun.nio.cs.StreamEncoder$CharsetSE.implFlushBuffer(StreamEncoder.java:404) at sun.nio.cs.StreamEncoder$CharsetSE.implFlush(StreamEncoder.java:408) at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:152) - locked <0xffffffff19c95178> (a java.io.OutputStreamWriter) at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:213) at org.apache.log4j.helpers.QuietWriter.flush(QuietWriter.java:49) at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:311) at org.apache.log4j.DailyRollingFileAppender.subAppend(DailyRollingFileAppender.java:343) at org.apache.log4j.WriterAppender.append(WriterAppender.java:150) at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:222) - locked <0xfffffffefa9acfe8> (a org.apache.log4j.DailyRollingFileAppender) at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:57) at org.apache.log4j.Category.callAppenders(Category.java:190) - locked <0xfffffffefa7ecfb0> (a org.apache.log4j.spi.RootCategory) at org.apache.log4j.Category.forcedLog(Category.java:375) at org.apache.log4j.Category.log(Category.java:868) at org.apache.commons.logging.impl.Log4JCategoryLog.debug(Log4JCategoryLog.java:165)	5.0	id=40251	4	False	False	genman	1
id=40251	REOPENED	None	Log4j - Now in Jira	Other (	1.2	Other other	P2 minor	log4j-dev	2006-08-14 14:30 UTC by	wing tung Leung	2007-02-01 14:26 UTC (	0 users	(based on log4j-1.2.13)Current JMX implementation has an hard coded domain name ("log4j") forregistering the MBean instances. This is not flexible enough when you need toconfigure multiple log4j repositories, all in different applications running ona J2EE server.I made some minor changes which extends the current interface and adds theoption to specify your own domain name.All comments are welcome.	CreatedExtended JMX MBean classes to allow alternate domain nameThis patch allows you to chose your own domain name. All comments welcome.Better would be for an MBean to create child MBeans within the same domain.I fixed this for 1.3 (SVN head) and added a test demonstrating it works.Elias committed changes against log4j/trunk in rev 500491Previous commit caused Gump failure on unit tests since build.jmx was not a precondition for building the newly added unit test. The newly added unit test is still not run, but rev 502375 at least fixes the build failure.The previous change also resulted in the following compatibility issues with log4j 1.2.8:o.a.l.jmx.HierarchyDynamicMBean Removed org.apache.log4j.spi.HierarchyEventListener from the set of implemented interfaceso.a.l.jmx.HierarchyDynamicMBean.addAppenderEvent Method 'public void addAppenderEvent(org.apache.log4j.Category, org.apache.log4j.Appender)' has been removedo.a.l.jmx.HierarchyDynamicMBean.removeAppenderEvent Method 'public void removeAppenderEvent(org.apache.log4j.Category, org.apache.log4j.Appender)' has been removedReopening bug until unit tests are integrated into tests/build.xml.Well except on JDK 1.3 and 1.4 since HierarchyMBeanTest depends on classes introduced in JDK 1.5. Rev 502381 suppresses compilation of that class for now.	5.0	id=59633	5	False	False	felix.schumacher	1
id=43282	REOPENED	None	Log4j - Now in Jira	Other (	1.2	Other other	P2 normal	log4j-dev	2007-08-31 13:56 UTC by	Curt Arnold	2011-10-14 04:39 UTC (	5 users	Jacob Kjome inWhat does everyone think about releasing OSGi ready Log4j jars? OSGi is quickly becoming popular. In order for OSGi apps to depend on Log4j in an OSGi environment, certain MANIFEST.mf entries need to exist. There's a Maven plugin that performs the OSGi packaging called the "maven-bundle-plugin".... (more follows)Also the following discussion on users@maven on 27-Aug-2007 regardingcommons-logging's pom may be helpful:	If it is only a question of a slight change of the maven configuration file, could you please provide a patch :)What do the Apache Felix folks think of this? Is the logging outside the OSGi container?Committed for log4j in rev 685646. Did not address companions. May want to askfor feedback.Unfortunately the new OSGi headers are broken, trying to install the Log4J bundle in Equinox yields the following error:org.osgi.framework.BundleException: Invalid manifest header Import-Package: "com.sun.jdmk.comm;resolution:="optional"" : Cannot import a package more than once "com.sun.jdmk.comm"Examining the manifest confirms this and another problem in the Import-Package header:'com.sun.jdmk.comm;resolution:=optional' is specified twicejavax.jmdns is a required package, even though it isn't a standard JDK package.I'm not sure if you wanted these exposed or not, but the Export-Package header is missing these packages:org.apache.log4j.lf5.*org.apache.log4j.chainsawI noticed one other issue, the org.apache.log4j packages exported via the Export-Packages header are missing an explicit version number.Thanks for the bug report, sorry about the delay in response. I have not yet investigated the reported issue with log4j 1.2.16, but made an initial attempt to add OSGi metadata to log4j-extras and would appreciate your review.There are two classes org.apache.log4j.xml.XSLTLayout and org.apache.log4j.varia.SoundAppender that are in packages also exported by log4j. I looked into split packages, but it looked highly likely that I would get it wrong, so I've left those classes unaccessible.Here is the generated manifest for extras, do you see anything troubling?Manifest-Version: 1.0Export-Package: org.apache.log4j.extras;uses:="javax.xml.parsers,org.w 3c.dom,org.xml.sax,org.apache.log4j.config,org.apache.log4j.or",org.a pache.log4j.filter;uses:="org.apache.log4j.extras,org.w3c.dom,org.apa che.log4j.rule",org.apache.log4j.rolling;uses:="org.apache.log4j.extr as,org.w3c.dom",org.apache.log4j.ruleBuilt-By: curtaTool: Bnd-0.0.357Bundle-Name: Apache Extras Companion for log4j 1.2.Created-By: Apache Maven Bundle PluginBundle-Vendor: Apache Software FoundationBuild-Jdk: 1.6.0_18Bundle-Version: 1.1.0.SNAPSHOTBnd-LastModified: 1275709540450Bundle-ManifestVersion: 2Bundle-Description: This companion provides additional appenders, filt ers and other capabilities for log4j 1.2. Several of these were bac kported from the abandoned log4j 1.3 development effort.Bundle-License:Bundle-DocURL:.htmlBundle-SymbolicName: log4j.apache-log4j-extrasImport-Package: javax.xml.parsers,javax.xml.transform,javax.xml.transf orm.dom,javax.xml.transform.sax,javax.xml.transform.stream,org.apache .log4j.config,org.apache.log4j.extras,org.apache.log4j.filter,org.apa che.log4j.or,org.apache.log4j.rolling,org.apache.log4j.rule,org.w3c.d om,org.xml.sax,org.xml.sax.helpersName: org.apache.log4jImplementation-Vendor: Apache Software FoundationImplementation-Title: Apache Extras Companion for log4j 1.2.Implementation-Version: 1.1-SNAPSHOTI've committed rev 953935 which results in a log4j.jar which can be installed and started in the Equinox OSGi framework supplied with Eclipse.For future ref:cd /usr/lib/eclipse/pluginsjava -jar org.eclipse.osgi_---.jar -consoleinstall file:///home/.../log4j-1.2.17-SNAPSHOT.jarstart BUNDLE_FROM_PREVIOUS_STEPBased on a suggest from the 2008 exchange on user@felix, I added a DynamicImport-Package directive via the maven-jar-plugin. Hopefully, that would allow resolving any user-supplied appender that was specified via a configuration file.javax.swing, com.ibm.uvm.tools and com.sun.jdmk.comm are explicitly blocked from being listed in the import list. Swing is used by the obsolete log viewing tools still in the java, but not exported. com.ibm.uvm.tools is dynamically loaded for some ancient VisualAge specific behavior. com.sun.jdmk.comm provides an http service for JMX. If needed, these classes can still be obtained via the DynamicImport-Package.I've listed three javax packages not included in Java SE 1.5 as optional. The javax.jmdns is not zeroconf implementation and despite the package name is not from Sun or part of the Java Community Process.All the other imported classes are either part of Java SE 1.5 or are exported by log4j.jar.The maven-bundle-plugin has enough info that it could provide the version qualifier on the exported classes, but it doesn't. I can bounce this off of users@felix, but is there a reason that you think that we should specify it?Specifying version numbers on export is a good practice in OSGi since it allows consumers to target specific versions of the package, which is often required when multiple versions of the same package exist within the same OSGi runtime.Sorry for not responding earlier, but I forgot to add myself to the CC list.Anyway, I agree with Craig, we need a version number on all of the exported packages. It will keep behavior predictable if a user accidentally (or, on purpose for some strange reason) loads multiple version of Log4J into an OSGi environment at once.Another enhancement that you might consider is packaging the NT appender DLLs inside the bundle. With headers like this:Bundle-NativeCode: NTEventLogAppender.dll;osname=Win32;processor=x86, NTEventLogAppender.amd64.dll;osname=Win32;processor=x86-64, *OSGi is smart enough to unpack the necessary DLL from the bundle and load it on demand.If 55K is too much bloat for the Log4J jar, then they could be packaged separately and added as a fragment bundle.WRT Log4J extras, again adding the version number for the exported packages is important. In addition, you can handle the split packages problem gracefully if you make it a fragment bundle. A fragment bundle will not function by itself in OSGi, but instead requires the log4j bundle to be loaded first. Then the contents of the fragment bundle become accessible to the classloader provided by host bundle. To do this you just add a header like so:Fragment-Host: log4j;bundle-version="1.2.9"This would instruct the log4j extras bundle to attach itself to the log4j bundle if the version number of the parent bundle is greater than or equal to 1.2.9.CreatedLoader.javaCreatedLog4jActivator.javaCreatedmanifest.bndMy team has created an OSGi friendly version of Log4j. Unfortunately, it's not as simple as adding the right headers to the manifest because there are classloader issues when using custom layouts or appenders.Fortunately, the fixes were not too hard. The Loader.loadClass() method in org.apache.log4j.helpers had to be updated to use a bundle context class loader, if available, and a bundle activator class had to be created to provide access to the bundle context class loader.We used the BND tool to generate the bundle with the appropriate manifest entries.I have attached the modified/new files for your review.Any thoughts on how to write tests around this? What specific things fail with the current class loader that would pass with the modified class loader? Are there potential other approaches?The specific situation that we had to address was logging via a customized JMSAppender that uses MDC parameters to set header properties in the JMS messages in order to simplify subscriber filtering. We also use a custom Layout class for simulations.When custom appenders or layout classes from a bundle are specified in the Log4j properties/xml file, there is no way to resolve them via Class.forName() in Loader.loadClass() because they aren't on the static class path, and the instantiation fails with a ClassNotFoundException. By modifying Loader.loadClass() to use the bundle context class loader, the OSGi runtime is given the chance to locate the classes in the bundle where they were loaded, and the creation of the logger succeeds.Testing should be fairly straightforward. You would need one or more custom layout and appender classes deployed in a bundle separate from the Log4j bundle. The unit tests would just create and use loggers using those classes. The tests would have to be deployed in a bundle. Knopflerfish and Felix provide JUnit integration to facilitate testing. I don't know about Equinox.We haven't managed to identify any other approaches. Personally, I don't think there is any other way to make this work.***has been marked as a duplicate of this bug. ***	15.0	id=42213	5	False	False	carnold	1
id=44386	REOPENED	None	Log4j - Now in Jira	Appender (	1.2	Other Windows Server 2003	P2 normal	log4j-dev	2008-02-10 04:56 UTC by	y360	2012-04-06 18:26 UTC (	2 users	A version of NTEventLogAppender.dll should be built for Windows 64 bit OS as wellSome discussion took place a while ago	CreatedAMD64 and Itanium support for NTEventLogAppenderHere is the patch. A few changes to the C code and a few changes to the java code.Everything seems to be working for me.The attached patch is for the consideration of log4j development to merge into the main tree. I have included a file in the attachment called patch_readme.txt that details the changes.Users who wanted to use this patch today could follow the following steps:1. obtain log4j-1.2.15.jar from the regular distribution2. compile the NTEventLogAppender.java against it using a simple javacjavac -cp log4j-1.2.15.jar NTEventLogAppender.java3. take that NTEventLogAppender.class and put it in a jar file (keeping the org.apache.log4j.nt path/directory structure up front).4. Make sure that the jar file you created in 3 is in the class path before log4j-1.2.15.jar so that it can override the NTEventLogAppender with the one from the patch.5. Add the .dll's in the patch to the current working directory of your application (wherever you app is started from).CreatedBetter AMD64 and Itanium support for NTEventLogAppender patchThis fixes a problem with the dll's being built by VS2005 to require a dll included in the .NET Framework 2.0I made updates to the readme inside the patch, but the long and short of this second patch is this:Visual Studio 2005 was forcing a library reference to a .dll in .NET Framework 2.0 which is really silly. So I built the dlls statically. They now work on systems without the .NET 2.0 framework.I also created an AppenderNT.jar so that users could take the contents of this patch and make their log4j work on 64 bit windows with nt event logging today, while we wait for this to be made part of a main release of log4j.Another minor change: there is no reason for the .dlls to be com dll's that need to be regsvr32'ed on the systems. So that has been removed.Should I assign this to someone specific to get them to review and add the changes to source control??I made updates to the readme inside the patch, but the long and short of this second patch is this:Visual Studio 2005 was forcing a library reference to a .dll in .NET Framework 2.0 which is really silly. So I built the dlls statically. They now work on systems without the .NET 2.0 framework.I also created an AppenderNT.jar so that users could take the contents of this patch and make their log4j work on 64 bit windows with nt event logging today, while we wait for this to be made part of a main release of log4j.Another minor change: there is no reason for the .dlls to be com dll's that need to be regsvr32'ed on the systems. So that has been removed.Should I assign this to someone specific to get them to review and add the changes to source control??As it is very hard to build the DLL's on other platforms I suggest that they should be placed in subversion so the distribution can be built on any platform.Committed a different take in bug 687885. The submitted patches changed the signatures on the native methods which could potentially result in undesirable behavior if the new Java class was mixed with an old DLL or vice-versa.In the committed change, there are separate set of native methods (registerEventSource64 et al) that take long for the message source that are used they appear in the DLL. If there is a UnsatisifedLinkError on ...64 method, the old "int" methods are called which from my testing allowed the new NTEventLogAppender.class work with an old DLL.From:The registry keys that point to the category files are attempted to be set at run time if they are not present, however that user may not have privileges to write to the registry. The DllRegisterServer allows the registry keys to be written at installation time.From:Building NTEventLogAppender.dll for x86 on Linux and similar should be as simple as installing mingw-dev prior to running Maven. However, it does look like MinGW's support for x86_64 is experimental at best (haven't looked at Itanium), so it does not seem practical attempt to build those DLL's with MinGW either on Windows or Linux.To build NTEventLogAppender.dll for x86_64, open the appropriate command prompt window from the Visual Studio Tools entry in the start menu and then run:mvn -Dntdll_target=msbuild packageThe NTEventLogAppender unit tests will require running under an x86_64 JVM to succeed. Though the DLL would be correctly build from either an x86 or x86_64 JVM.It would be helpful to include an x86_64 DLL in the release, however it is not practical since the MinGW cross tools seem too immature and moving back to building on Windows is undesirable. In addition the general ASF position is to not to provide platform specific binaries and let downstream packagers do that.The class initializer first attempts to load NTEventLogAppender.dll. If that fails (either missing or not the right type), it will then attempt to load NTEventLogAppender + System.getProperty("os.arch") + ".dll" which results in attempting to load NTEventLogAppenderamd64.dll when running with Sun's x86_64 JVM. I thought the lower casing was problematic (turkish i and similar problems) and unnecessary since Windows file systems are case-insensitive. Overriding the os.arch with a system property seemed overkill and potentially a security issue since you could redirect to an unexpected DLL. I've modified nteventlog.cpp so that it will still properly register itself if you rename the DLL. So if you want to rename the DLL from NTEventLogAppender.dll to NTEventLogAppenderamd64.dll it will still work.I thought about also providing a Visual Studio project file, however it would still require tweaking to include the right JVM include files and a custom step to invoke javah, etc.Would appreciate feedback.You can do what you want, but I just don't believe this is an issue at all and I would take the approach I took. I think it is unnecessary to create alternate 64 bit versions of the functions. My rationale is this: If you are using the Windows NT Event Logging, you know that you need to deploy the .dlls when you put out a new version of log4j. If you are not, it will never matter anyway as your app will never attempt to load the dll.I think now is the time to make a clean break and do it right. Keep the code simple and clean so that if someone was using this code as a model for some similar thing they were doing it would be obvious how to do it.I didn't think of that security privileges issue, and it is a valid point. However, the idea of doing it via the DllRegisterServer doesn't guarantee a fix. It only guarantees that you can write a default event source, so maybe named "log4j" or whatever you named it in your commit. But that forces all applications to write to the "log4j" event source. The user must specify that named event source, instead of choosing one that looks like his application. If my application is ABCServer I would rather have my event logs show up as ABCServer rather than the generic log4j -- along with every other app that uses log4j event log appender.The way I have it, all the user must do otherwise to initialize the event log is run the app as a privileged user one time so that it can create the event registry. He could even run a simpleton "initialization" class instead of the whole app. I don't know what the golden solution to this is. Personally, I think the DllRegisterServer is a misuse of the COM registration stuff, which is what regsvr32 is all about. But it is a clever way to get stuff setup outside of any Java code.Give me a break. This is going to murder the value of this whole exercise for the average user. The average log4j user is a java programmer -- not guaranteed at all to have the developer tools or know how to build a native stuff on Windows (especially since you need Visual Studio 2005 to build the 64 bit stuff and the itanium stuff and you don't have a reasonable mingw alternative now and probably for some time to come in the future). Further, the user may want to support 64 bit windows but not have access 64 bit machines anyway to build the binary on in the first place. Further yet, his working java logging application will cease to work when someone starts using a 64 bit JVM on 64 bit windows instead of 32 bit. And the log4j programmer's user may need 64 bit due to the extra memory capacity available (i.e. above 1280M JVM memory), so then he is straddled with the choice between two features that he wants.Just include the dlls. The old package had the 32 bit dll pre-built and no one was complaining. Just include the 64 bit dlls along with the source code and have done with the whole mess and get back to java development. This has always been a fringe module for log4j anyway since it is not pure java.Changing the argument list on the existing native methods is not an option. There are two many scenarios where you could have a mismatch between DLLs and log4j versions and bad things happen. However, it is not like we can't represent all the currently active event sources in 32-bits. What we could do is return the HANDLE in the 32-bit implementation and maintain a small (maybe 256 entry) array of HANDLEs in the 64-bit versions and return the index into the array instead of the direct handle. That would allow pre-1.2.16's log4j's to work on 64-bit JDK's as long as the NTEventLogAppender.dll on the path was a 64-bit DLL. If you installed the 32-bit NTEventLogAppender.dll in \Windows\SysWOW64 and the 64-bit in \Windows\System32 (where 64 bit DLL's live despite the name), things might just magically work for older versions of log4j. log4j 1.2.16 would be needed if you wanted to have a 32-bit dll and a 64-bit dll (NTEventLogAppender+ os.arch + .dll) both on the path. I'll give that a shot tomorrow.As for including the AMD64 and/or IA64 binaries. If we were to include them in the log4j release itself, we'd have to keep canned versions of them in the SVN. We are kind of doing the same thing now with the output of the message compiler since there isn't a cross-platform implementation of it. It is a little distasteful compared to building everything from source in a release, but maybe tolerable. I can build and test an AMD64 DLL and should be able to build an IA64, but I won't be able to test it. If someone is willing to test the IA64 build, I'd be willing to include it in the release, but I would not feel good about including it untested.Curt, as the one who opened the bug, I completely agree with David Bennion's sentiment on this. Please try to find a way to include the dlls.I could see maintaining an array of HANDLEs. Then the new dll returns an offset into the array instead of a pointer and the java side is none the wiser. That would work and you'd likely never have a problem. Now I can see why you are so concerned about the signature of the Dll's! It is because you are putting the DLL's in Windows\System32. That's not something that I ever do. I just leave them in the application path (with jars) and set java.library.path to load them. I would keep the different names for different arches though and based on the arch try to load the appropriate one. That way people like me can simply stick all the .dll's in the same directory as their application. Then their single application installation can switched JVM's in place. For instance, the user can come and choose a different JVM for my application and could go to a 64 bit from right where it is installed. My feeling is that my java based installation should be able to just chuck all the .dll's on there and it should just "work" regardless of which JVM they are using. I as a java application developer shouldn't have to put any energy into tracking whether I am set up for the 64 bit one or not. Just slap them all on at once and go.I think this model describes what at least a high percentage of other users will do as well.It is generally considered distasteful to make dll additions to the system directory. Microsoft wanted everyone to get away from that after so many applications experienced "dll hell" (which is what your signature changing concern is based on). I realize that not everyone plays by those rules.I agree. I know there was a customer of ours that asked about Windows on Itanium, but by and large I think this is basically a dead platform. I don't feel terribly bad about just leaving the code in a state where it builds on this platform. I think there are a few people out there who have Windows on Itanium. Maybe you could keep the IA64 .dll in an "already built" state and put a notation in the doc that if you are a developer and you wanted to support that platform you can download the IA64 .dll from such and such a link and try it to see if it works.All the best.David.I am moving our web serviceon Tomcat to 64 bit. So I found this bug and used the patch in attachment. I followed the instructions in your patch_readme,although the webservice and Tomcat 64bit both work fine, I don't see any logs output. Tomcat logs don't give any errors or complaints though. So would you please have a look of what I'm doing wrong? This is what I did:1) build.xml I put AppenderNT.jar before log4j-1.2.14.jar ... <pathelement location="${AppenderNT-jar}"/> <pathelement location="${log4j-jar}"/>2) put the NTEventLogAppender64.dll under tomcat\bin3) put AppenderNT.jar under tomcat\lib4) check tomcat\webapps\mine\WEB-INF\lib, verify that fileAppenderNT.jar is thereBTW, it's been almost a year since the last update on this bug. Any update? Will there be any official support of 64bit NTEventLogAppender from log4j?Thanks!In newer OS's, you may need to explicitly register the DLL as admin, but I think that failing to do that would only result in the messages being formatted.I think the DLL needs to be in the path of the process that launced java on on java.library.path (). I don't think placing the DLL on the java classpath is sufficient.I guess my Hurricane Ike excuse is getting old. Ike, marriage and a new job has been keeping me busy. Won't promise but hopefully a nice surprise soon..Maybe consider scrapping NTEventLogAppender altogether and replacing it with a JNA-based implementation (see).On Win2k8 x64 with x64 JDK and the included NTAppender file in the Windows folder, I get an error stating that it can't load the IA 64 bit library on an AMD 64bit platform. Shrug. Works fine using the 32bit jdkHow has this not been fixed yet?	15.0	id=42883	6	False	False	isurues	1
id=42883	REOPENED	None	Log4j - Now in Jira	chainsaw (	unspecified	Other other	P2 normal	log4j-dev	2007-07-12 20:04 UTC by	Isuru Suriarachchi	2007-08-23 20:59 UTC (	0 users	Normal Chainsaw log panels can be hidden or shown as the user needs. But thisfeature is not working with 'Welcome' and 'Drag & Drop' panels. Hiding thesepanels also can be useful for some users to concentrate only one one panel.	CreatedAdding ability to hide 'Welcome' and 'Drag & Drop' panelsThis patch contains the modifications in LogUI to hide and show 'Welcome' and'Drag & Drop' panels panels whenever needed.CreatedNew version of the previous patch with no commentsCommented lines are removed in this patchThis patch has already been applied, but it would be nice if this feature couldbe enhanced to remember if these tabs were hidden on app exit and hide them ifappropriate (with menu options to view them again?)This is also serving as a reminder to myself if you don't get to it.CreatedAdding ability to remember previous tab settings after restartsIn this patch, I have implemented the feature of saving the tab setting (whatare hidden and what are shown) at closing and loading it back at restartingChainsaw. I used an XML file to save the setting. Methods to save and load thissetting is included in the "ChainsawTabbedPane" class. I wonder whether it isthe correct place to include that code. But I couldn't fine somewhere moresuitable.I have used a separate class called "SavableTabSetting" to represent the tabsetting that should saved. "Welcome", "Drag & Drop" and "chainsaw-log" are themain log panels which are always running in Chainsaw. Existence of other logpanels depends on the situation. For example, the user may load a receiver onceand close Chainsaw and later restart and load some other receiver. I thinksaving the state of such panels will be useless. So I have made Chainsawremember only the state of the above mentioned main log panels.There may be some adjustments to be done. Hope this will help as an initialwork to get the job done.So please check this and comment...1) SavableTabSetting - Can this be a normal Java bean with getters/settings? It's stylistic, and I know this is a simple class but I'd prefer it withgetters/setters2) ChainsawTabbedPane - Can this implement the SettingsListener interface anduse the standard save/load call backs that provides?3) This patch might now be slightly out of date, it's not applying cleanly atall. The LogUI file has 2 patch chunks that don't apply cleanly:@@ -2055,8 +2073,10 @@ if ( getTabbedPane().getSelectedIndex() == getTabbedPane() .indexOfTab(ident)) {- getTabbedPane().setIconAt(- getTabbedPane().indexOfTab(ident), SELECTED);+ if(getTabbedPane().indexOfTab(ident) > 0){+ getTabbedPane().setIconAt(+ getTabbedPane().indexOfTab(ident), SELECTED);+ } newEvents = false; seenEvents = true; } else if (getTabbedPane().indexOfTab(ident) > -1) {and @@ -2094,7 +2114,7 @@ public void stateChanged(ChangeEvent event) { if (- getTabbedPane().indexOfTab(ident) == getTabbedPane().getSelectedIndex()) {+ getTabbedPane().indexOfTab(ident) == getTabbedPane().getSelectedIndex()&& getTabbedPane().indexOfTab(ident) > 0) { getTabbedPane().setIconAt(getTabbedPane().indexOfTab(ident), SELECTED); } }I'm not really sure why.. Can you double check ? Does it apply cleanly for you?CreatedAdding ability to remember tab settings improved versionThis patch includes all the improvements proposed by paul.(In reply to)OK. Done in the new patch...Yes. Done in the new patch..getTabbedPane().getSelectedIndex()) {There was a bug in those two statements and I had fixed it in the previouspatch. But after that someone has committed that fix. So that why it wasconflicting. These two statements are not included in the new patch as it isalready fixed.New patch applies without any problem. Please check it...(In reply to)OK. Done in the new patch...Yes. Done in the new patch..getTabbedPane().getSelectedIndex()) {There was a bug in those two statements and I had fixed it in the previouspatch. But after that someone has committed that fix. So that why it wasconflicting. These two statements are not included in the new patch as it isalready fixed.New patch applies without any problem. Please check it...Fixed in. THXI'm seeing quite weird behaviour with this feature now. It did look ok when Itested this yesterday but I tried the following:1) Start Chainsaw, run the tutorial and get the 3 tabs from the generators2) Hide all 3 of the generator tabs3) exit and restart Chainsaw5) Run the tutorial again with generators, they come back to life.So, the restore logic does hide any hidden tab that appears at app startup time,but does not hide any new tabs that turn up.(In reply to)time,Yes. I didn't try to save the configuration of tabs other than Welcome, Drag and Drop and chainsaw-log. This is because we don't know whether the user will start exactly the same tabs when chainsaw is restarted. This was done as an intial step for this feature. We can work on saving any tab's setting in the future. But there will be some problems comming up. I'll try it..	11.0	id=43282	11	False	False	thorbjoern	1
id=51766	REOPENED	None	Log4j - Now in Jira	Appender (	1.2	PC Linux	P2 normal	log4j-dev	2011-09-06 03:21 UTC by	Curt Arnold	2013-04-25 15:19 UTC (	0 users	Move org.apache.log4j.rewrite package and org.apache.log4j.helpers.UtilLoggingLevel from the to-be-discontinued receivers companion to log4j.	Committed rev 1165491.It seems that the DTD for log4j.xml was not updated properlyA xml fragment such as this<appender name="rewrite" class="org.apache.log4j.rewrite.RewriteAppender"> <appender-ref ref="console"/> <rewritePolicy class="logtest.MyRewriter"/></appender>Causes the following parse error on initialization:log4j:WARN Continuable parsing error 15 and column 50log4j:WARN Element type "rewritePolicy" must be declared.log4j:WARN Continuable parsing error 16 and column 14log4j:WARN The content of element type "appender" must match "(errorHandler?,param*,rollingPolicy?,triggeringPolicy?,connectionSource?,layout?,filter*,appender-ref*)".	2.0	id=44386	10	False	False	davidbennion	1
id=35142	REOPENED	None	Ant	Documentation (	1.6.4	Other All	P2 enhancement	Ant Notifications List	2005-05-31 16:35 UTC by	Daniel B.	2008-02-22 12:18 UTC (	0 users	In the Ant User Manual, in the subant task's page, the text does not wrap to fit the user's browser window (or pane within that window). Thisbehavior (and that page's formatting in general) is consistent with most ofthe rest of the Ant Manual pages.The text in the body of the page does not wrap to fit because the headerand the body are wrapped in a table. (That forces the browser to formatthe body to match the width of the header, instead of letting the browsershrink or stretch the body to fit the user's chosen browser window width.)(Also, something, possibly that wrapping in a table, prevents the browser from trying to narrow the header text column enough to make it and the Ant logo fit next to each other.)Additionally, the formatting of the page is inconsistent with the rest ofthe Ant manual. (It looks like someone cut and pasted from something otherthan another Ant task.)	This page was automatically generated using a means that was, at the time, under investigation for being the generation facility for the entire Ant manual. At present more options continue to be investigated. This may offend the occasional eye but you must agree the only possible harm here would be to aesthetic sensibilities.As I wrote for:------------------------------------------------------Wrong. That is not the only possible harm. The much more signficant harm is to usability and user efficiency. When the text doesn't fit in the user's chosen browser window width, the user has to scroll horizontally to read the text. Note that the user has to scroll right and then back left for EACH line of text. That is horrible.(Remember that some users have big screens so they can see _multiple_windows, say, an editor editing some file (.../build.xml, maybe?) and,say, a browser displaying relevant documentation (the Ant manual, anyone?).Don't treat users as if they're too stupid to be able to handle multiplewindows (by assuming they use full-screen browser windows).)HTML is designed so that by default the browser can wrap regular text tofit the user's chosen window width. Don't break that ability, by tying the width of the text column to something else, especially when breaking it is completely unnecessary.Sorry for the snippiness, but maybe you shouldn't claim that I must agreewith something that is clearly false and shouldn't summarily discard my bug report without showing any understanding of the problem.----------------------------See myfrom.Changing the title, as this is true forall the generated manual pages - subant, tempfile, whichresource,csc, ilasm, ildasm, importtypelib, jsharpc, setproxy, vbc,and wsdltodotnet.Reducing the severity to enhancement.***has been marked as a duplicate of this bug. ***	5.0	id=53976	8	False	False	p.mouawad	1
id=58062	REOPENED	None	Log4j - Now in Jira	Other (	unspecified	All All	P2 critical	log4j-dev	2015-06-20 15:22 UTC by	Tang Xinye	2015-06-25 04:02 UTC (	0 users	The problem here is that if an exception is thrown during the read process the method will exit without closing the stream and hence without releasing the file system resources, it may run out of resources before it does run.I will make a commit in github right now.	I've made a pull request into resolve the bug.Note that v1 is not actively maintained. I strongly encourage you to migrate to v2, which includes a v1.2 compatibility layer.	2.0	id=51766	4	False	False	carnold	1
id=32632	REOPENED	None	Ant	Core tasks (	unspecified	PC Linux	P2 enhancement	Ant Notifications List	2004-12-10 14:06 UTC by	Holger Engels	2008-02-22 12:18 UTC (	0 users	filters / filterchains / filterreaders should all allow for patterns and nestedpatternsets.<copy todir="${deploy.dir}"> <fileset refid="webapp"/> <filterset> <filter token="DATE" value="${TODAY}"/> <patternset id="sources"> <include name="**/*.xml"/> </patternset> </filterset></copy>This should apply the filters of the filterset only to files of the filesetmatching the patterns of the patternset.	You can do this with multiple copy tasks.(In reply to)I can use multiple coüpy tasks only, if I am the one, who defines the fileset.Unfortunately, I'm not. The fileset is an artifact, exported by another module.The documentation of filtersets says "copy operations will typically corruptbinary files". But there's no straight forward way to avoid that. Imagine youwrite a deploy target, that copies filesets to a deployment directory andthereby weaves the configuration into the deployables. From the perspective ofthe target, filesets are logical units. The target should be unconcerned aboutthe fileset's internals... just wanted to state my thoughts. I won't reopen the issue again.It still sounds as though you could work around this using multiple copyoperations if you had the ability to use an existing fileset as a template for amore-specific fileset, per your other bug report, correct?Correct. Then I could write something like:<copy todir="${deploy.dir}"> <fileset inherit="webapp"> <include name="**/*.xml"/> </fileset> <filterset> <filter token="DATE" value="${TODAY}"/> </filterset></copy><copy todir="${deploy.dir}"> <fileset inherit="webapp"> <exclude name="**/*.xml"/> </fileset></copy>	4.0	id=58062	5	False	False	tgttxy	1
id=35776	REOPENED	None	Ant	Core (	1.7.0	Other other	P2 enhancement	Ant Notifications List	2005-07-18 13:35 UTC by	Ralf Hauser	2008-02-22 12:18 UTC (	3 users	org.apache.tools.ant.util.FileUtils is a great tool to create platformindependent paths: FileUtils fu = FileUtils.getFileUtils(); return new Location(Locator.fromURI(fu.toURI(fileName))) .getFileName();Unfortunately, when a path is using cygwin symlinks, it still won't work.There is a resolveFile() that already does more than just new File(file,filename), but unfortunately, it wouldn't notice if a file doesn't exist (due tobeing a cygwin symbolic link).Once this exists, I guess this very useful tool "FileUtils" might find many moretakers and thus could move to jakarta-commons?	(In reply to)toCygwin symbolic links are a pecularity unto themselves. Further, I don't know that there is any guarantee that their implementation has never changed, and will never change. This means that any code we wrote, making assumptions about the implementation of cygwin symlinks might not work against older or future versions of cygwin. Beyond that, cygwin not being a true OS it would be a judgement call whether supporting its symlinks would be proper. More likely the "right" way to get this would be to build cygwin awareness into a JVM, since it is at this level that, e.g. Unix symlinks are handled.moreIIRC there is at least one jakarta-commons project [io] with a FileUtils class.thx to Matt for the pointer to the existing commons-io FileUtils (see Bug 35818).Agreed, having symlinks handled by the JVM might be the best solution. But sincewe all stumble daily over those and JVMs haven't delivered on this so far, whynot aiming for the second best and giving it a (temporary) home in the jakartafamily of projects? (instead of everybody making his ownhalf-hearted-quick-fixes all the time)Also, "This means that any code we wrote ... might not work against older orfuture versions of ..." - isn't that true for almost any interface/third-partycomponent one programs against/with?(In reply to)35818).sinceOne idea: extend oata.types.FileSet -> CygwinFileSet (e.g.), which returns an extended oata.DirectoryScanner that knows how to resolve cygwin symlinks. The result is a drop-in replacement for fileset (using typedef though I know Steve L. will complain ;) and any compatibility risks are undertaken voluntarily by the user.Indeed; however most third-party components do not masquerade as the filesystem.By the way, don't get me wrong here. I have been a loyal cygwin user for quite some years now.(In reply to)FWIW, one solution that would not involve modifying the JVM is a native methodshipped with a DLL that's linked with Cygwin. That way you can guarantee thatany path conversion will invoke the genuine Cygwin functionality, socompatibility will not be an issue.(In reply to)This sounds like an interesting project, but I personally don't have time toattempt it given my laughable C-Fu and complete lack of knowledge of the cygwinAPIs. I'm sure that when a patch implementing this solution is submitted itwill be given due consideration.I would not personally support adding cygwin support in ant at this level.Maybe the people who are interested by specific cygwin functionality shouldstart their own Java project ?	6.0	id=34403	4	False	False	bodewig	1
id=34403	REOPENED	None	Ant	Core tasks (	1.6.2	Other other	P2 normal	Ant Notifications List	2005-04-11 19:08 UTC by	Kohsuke Kawaguchi	2015-12-23 03:04 UTC (	0 users	Just like the unzip task, the zipgroupfileset should support a nested patternset so that users can control which part of the source zip files should becopied into the destination zip file.This can be used to avoid having duplicate entries.	Because of having problems with sign artefacts inside merged jar files using thezipgroupfileset, i created a workaround (repackaging), which simulated themissing "nested pattern specification" - maybe someone is interested in it:<tempfile property="templib" destdir="@{destination}"/><move tofile="${templib}" file="${libraryname}"/><zip file="${libraryname}"> <zipfileset src="${templib}"> <exclude name="META-INF/*.SF"/> <!-- EXAMPLE: excludes the existing sign information, which confuseJavaWebstart --> </zipfileset></zip><delete file="${templib}" quiet="true"/>put this snipped into a task and use it.*** This bug has been marked as a duplicate of***zipgroupfileset should support two separate pattern specs: one to identify jar/zip files and another to filter files within jars/zips (e.g. META-INF/*).	3.0	id=35142	7	False	True	mbenson	1
id=37127	REOPENED	None	Ant	Core tasks (	1.7.0	Other other	P2 enhancement	Ant Notifications List	2005-10-17 20:45 UTC by	Konstantin Komissarchik	2008-02-22 12:18 UTC (	0 users	We have been using the alpha Apt task quite successfully in our build scripts. Thanks for including it in this release!One thing that we have ran into is that it is not possible to define the options externally and then reference them in the apt task (something akin to path refs). We are basically trying to create a fully parameterizable build script that includes an Apt task.	Does <presetdef> or <macro> let you wrap up a new declaration with yourpreferred options?I concur; you should have success referencing an externally defined <presetdef> via either <import> or an antlib.I don't believe using macro or presetdef facility fits the bill for what I am trying to do. The usecase is almost identical to the path ref usecase. In particular, we want create a common build script that would get called from different places with different annotation processors. Depending on which annotation processors are in the processor path, the set of options needs to vary. So I want to allow the caller to define a named processor path (possible right now) and a named option set (not possible right now) and then refer to both when calling the Apt task.	3.0	id=38082	8	False	False	michael	1
id=38082	REOPENED	None	Ant	Optional Tasks (	1.6.5	All All	P2 normal	Ant Notifications List	2005-12-30 22:22 UTC by	Michael Montuori	2011-06-01 11:10 UTC (	0 users	The scp task does not handle password with special characters like "@", this cause a problem is used, I found out that the password is retrieved using an indexOf function as the following:setPassword(uri.substring(indexOfColon + 1, indexOfAt));this would fail is the secure password is user user:p@ssword@host:/dir.Consider using lastIndexOf method for it.	CreatedCorrect issue with password containing special charactersLine # 263 now containsint indexOfAt = uri.lastIndexOf('@');instead ofint indexOfAt = uri.indexOf('@');this gets around password with special characters like "p@ssword"Please submit changes as patches in unified diff format as outlined atCreatedDiff -u of the suggested changefixed in SVN HEAD.This may fix passwords with @ but break filenames with @Still does not work with '&' and ':' in password.Introduction of escape characters would fix this.	6.0	id=32632	5	False	True	mbenson	1
id=39164	REOPENED	None	Ant	Optional Tasks (	1.6.5	Other other	P2 enhancement	Ant Notifications List	2006-03-31 08:55 UTC by	Nikolay Metchev	2009-07-31 06:37 UTC (	0 users	It would be nice if the antlr task created the output directory automatically orhad an option allowing you to do so. At the moment if the output directorydoesn't exist you get a build failure which is a shame.	e.g., <javac> doesn't do this. Why should the (barely-supported) ANTLR task?javac is very different. The packages themselves do get created by the java compiler. Antlr on the other hand doesen't understand packages even though it is most commonly used to create files in a package. That is why the output directory has to be specified all the way including the package.Antlr is either supported or it isn't? If it is supported I don't see why it would be too much to ask to enhance it a little.Did you attach a patch with your desired enhancement?Createdpatch to create output directoryI have now attached a patchIn all honesty, I looked into this when the issue was first raised, but foundthat this task has a testcase that explicitly tests that output dirs are notcreated. I will reopen and if I or some other committer takes a notion to makethe change _and_ augment the junit test, who knows?	6.0	id=41280	8	False	False	stevel	1
id=40093	REOPENED	None	Ant	Core (	1.6.5	PC Linux	P2 normal	Ant Notifications List	2006-07-22 08:39 UTC by	Stefano Marsili	2009-07-30 06:19 UTC (	0 users	The reference 'ant.PropertyHelper' isn't passed correctly tothe target of an ant (task) call. If a new PropertyHelper (a subclass of it)is installed, it is not passed to the called build.It seems that property expansion is executed before the references are passed (with inherirefs="true"). This means that astandard PropertyHelper instance is created for the reference 'ant.PropertyHelper' and the new reference is therefore ignored.Maybe this reference should be treated separately?See:- the call to newProject.setInheritedProperty in Ant.execute()- overrideProperties() calling Property.execute()	Createdbuild.xml showing problemI see that passing the property helper as reference doesn't evenmake sense. Adding a property to the build would (AFAIK) also addit to the called build, which is wrong. A solution would beto clone the PropertyHelper if it's subclassed and if anattribute inheritPropertyHelper="true" is set.And I guess there are other tasks that would have to implement the same.Sorry for polluting bugzilla. I think the PropertyHelper isn'tconceived for being (cloned and) passed to a sub build, so I mark this bug as invalid and live with the limitation.CreatedPatch to clone PropertyHelper for subbuildsCustom property helper is not passed to subbuilds at all. This clones the PropertyHelper if inheritAllis true (but an additional attribute like inheritPropertyHandler may be better).CreatedCloneable PropertyHelperThis patch declares PropertyHelper as Cloneable.The implementation sets values as if it wherenewly constructed, since properties are setthrough the project.CreatedTest: custom property helper cloned in subbuildsThis test should be successful if patches are applied.Calls subbuilds with ant,antcall,subant.I know not many care about the property helper,but I think the current behaviour is wrong.The PropertyHelper class is there to be extendedand Ant should therefore handle it correctly.The patch implementation clones the PH when inheritAll is true, but a new attribute may be defined.CreatedPatches of Ant.java and PropertyFunction.java (patch.xml)Found out there is the very handy patch.xml buildto create patches for Ant.Added a missing setProject() in the code.CreatedPatches of sources, tests and docu, plus new filesPropertyHelper patches. Why (the problem): even though it is possible for a custom task to install a new property helper as a hook or even as the main property helper, at present there is no way to pass the helper to subbuilds. The additional data in the PropertyHelper subclass and the hooks are not inherited. Proposed solution: this patch extends the PropertyHelper interface to install and clone PropertyHelper instances in a way suitable to Ant.** Sources **PropertyHelper.java: - installPropertyHelper(project, newHelper) to install a custom PropertyHelper into a project - clonePropertyHelper(fromProject, newProject, inheritAll) to clone a PropertyHelper to a new project and optionally copy hooks' properties to their clones - areYouOverridableBy(newHelper) method that let's the main property helper decide whether it wants to give up control of the build's property handling to a new helper.Ant.java, CallTarget.java, SubAnt.java: - new "inheritph" attribute telling whether the property helper and its hooks should be cloned to the new project** Tests **PropertyHelperTest.java: - created new test case, testing installation of property helpers and hooks into a projectAntTest.java, CallTargetTest.java: - extended to test cloning of property helper and its hooks** Docu **Ant.html, AntCall.html, SubAnt.html: - added "inheritph" attribute with a small comment** Note **Modified and patched against latest 1.70alpha.The proposed changes should be backward compatible.CreatedCorrected PropertyHelper patch.txtNew in patch:- added PropertyHelper.installPropertyHelperHook(project, newHook) - added minimal SubAntTest.java- corrected some bugs (setProject on clones,...)Further corrections, comments, suggestions, motivated refusals are welcome.I know the bugzilla title no longer reflects the contents of the patch and sorry for the many files posted, but this was my first patch ever (I'm sure you noticed :-)Thank you,Stefano MarsiliThanks for the patch, unit tests and doc - this isgreat!.	11.0	id=37127	9	False	False	mbenson	1
id=41280	REOPENED	None	Ant	Core tasks (	1.7.0	Other other	P2 normal	Ant Notifications List	2007-01-02 12:45 UTC by	David Ehrlich	2009-07-10 08:11 UTC (	1 user	While processing a large number of files via an 'apt' ant task, apt is failingwith an out of memory exception. Altering the task to include a'memoryMaximumSize' argument does not rectify the situation. The following isemitted regardless of the 'fork=' setting in the ant script: [apt] Compiling 1881 source files [apt] Since fork is false, ignoring memoryMaximumSize setting. [apt] [apt] [apt] The system is out of resources. [apt] Consult the following stack trace for details. [apt] java.lang.OutOfMemoryError: Java heap spaceI haven't found a way to work around the issue via the script. However, I wasable to get the Apt task to do what I need by altering theorg.apache.tools.ant.taskdefs.Apt constructor as follows.Original constructor: public Apt() { super(); super.setCompiler(AptExternalCompilerAdapter.class.getName()); setFork(true); }Work-around constructor: public Apt() { super(); super.setCompiler(AptExternalCompilerAdapter.class.getName()); super.setFork(true); }However, the workaround causes the task to emit a different warning: [apt] Since compiler setting isn't classic or modern,ignoring fork setting. [apt] Compiling 1881 source files [apt] Since compiler setting isn't classic or modern,ignoring fork setting.I suspect this implies other changes need to be made to get the apt task torecognize the appropriate memory options.	good catch. we turned forking off because we want people to fork every time(classpath problems) but in fact hadn't fixed the problem because we'd stubbedout the setFork() operator. Expect a fix for Ant1.7.1fixed in SVN, you can check out and use the new version or stick with yourpatched edition until ant1.7.1 ships. Thank you for finding a bug that snuckpast our testing!It happens again even with ant 1.7.1 with 64bit JDK 5u16 on Ubuntu Intrepid. I am not able to say if it is specific to ant package on Ubuntu.[local]> ant -versionApache Ant version 1.7.1 compiled on October 3 2008 <target name="compile-jruby" depends="compile-tasks, compile-annotation-binder, check-for-optional-packages"> <!-- Generate binding logic ahead of time --> <apt factory="org.jruby.anno.AnnotationBinder" destdir="${jruby.classes.dir}" debug="true" source="${javac.version}" target="${javac.version}" deprecation="true" encoding="UTF-8"> <classpath refid="build.classpath"/> <classpath path="${jruby.classes.dir}"/> <src path="${src.dir}"/> <patternset refid="java.src.pattern"/> <compilerarg line="-XDignore.symbol.file=true"/> </apt> </target>compile-jruby: [apt] Since compiler setting isn't classic or modern,ignoring fork setting. [apt] Compiling 641 source files to /mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build/classes/jruby [apt] Since compiler setting isn't classic or modern,ignoring fork setting. [apt] warning: Annotation types without processors: [java.lang.Override, java.lang.SuppressWarnings, java.lang.Deprecated, org.jruby.anno.JRubyConstant, org.jruby.anno.JRubyModule, java.lang.annotation.Retention, java.lang.annotation.Target] [apt] Problem encountered during annotation processing; [apt] see stacktrace below for more information. [apt] java.lang.OutOfMemoryError: Java heap space [apt] at java.io.BufferedWriter.<init>(BufferedWriter.java:87) [apt] at java.io.BufferedWriter.<init>(BufferedWriter.java:70) [apt] at java.io.PrintStream.init(PrintStream.java:83) [apt] at java.io.PrintStream.<init>(PrintStream.java:100) [apt] at java.io.PrintStream.<init>(PrintStream.java:62) [apt] at org.jruby.anno.AnnotationBinder$AnnotationBindingProcessor$RubyClassVisitor.visitClassDeclaration(AnnotationBinder.java:90) [apt] at com.sun.tools.apt.mirror.declaration.ClassDeclarationImpl.accept(ClassDeclarationImpl.java:95) [apt] at com.sun.mirror.util.DeclarationScanner.visitClassDeclaration(DeclarationScanner.java:110) [apt] at com.sun.tools.apt.mirror.declaration.ClassDeclarationImpl.accept(ClassDeclarationImpl.java:95) [apt] at com.sun.mirror.util.DeclarationScanner.visitClassDeclaration(DeclarationScanner.java:125) [apt] at com.sun.tools.apt.mirror.declaration.ClassDeclarationImpl.accept(ClassDeclarationImpl.java:95) [apt] at org.jruby.anno.AnnotationBinder$AnnotationBindingProcessor.process(AnnotationBinder.java:60) [apt] at com.sun.mirror.apt.AnnotationProcessors$CompositeAnnotationProcessor.process(AnnotationProcessors.java:60) [apt] at com.sun.tools.apt.comp.Apt.main(Apt.java:454) [apt] at com.sun.tools.apt.main.JavaCompiler.compile(JavaCompiler.java:448) [apt] at com.sun.tools.apt.main.Main.compile(Main.java:1075) [apt] at com.sun.tools.apt.main.Main.compile(Main.java:938) [apt] at com.sun.tools.apt.Main.processing(Main.java:95) [apt] at com.sun.tools.apt.Main.process(Main.java:43) [apt] at com.sun.tools.apt.Main.main(Main.java:34) [apt] Note: Some input files use unchecked or unsafe operations. [apt] Note: Recompile with -Xlint:unchecked for details.[o.jruby.distro]> java -versionjava version "1.5.0_16-rev"Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_16-rev-b09)Java HotSpot(TM) 64-Bit Server VM (build 1.5.0_16-rev-b09, mixed mode)[o.jruby.distro]> javac -versionjavac 1.5.0_16-revI forgot to say: The same target passes on 32bit JDK.It probably only fails on a 64 bit VM because Sun 64-bit JVM heap requirements are 1.5X 32 bit due entirely to memory pointer length; it probably works on the JRockit JVM which uses short pointers when the heap is <4GB.What is the ant -verbose run?The "no forking" message implies that Apt is running in Ant's own process, so if you set the heap size there then it should propagate down. That would make the problem go away, but not fix any defect.I set ANT_OPTS to "-Xmx1024m -Dbuild.compiler.debug=true" but it does not help anyway. It would say this should be enough so there might be another problem specific to 64bit JDK. Verbose output for this target (I build o.jruby.distro module from NetBeans sources.) is ~5MB. If you wish I can send zip fileHopefully relevant part is (let me know if you want anything else): [apt] Since compiler setting isn't classic or modern,ignoring fork setting. [apt] Compiling 641 source files to /mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build/classes/jruby [apt] Using external apt compiler [apt] Since compiler setting isn't classic or modern,ignoring fork setting. [apt] Compilation arguments: [apt] '-deprecation' [apt] '-d' [apt] '/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build/classes/jruby' [apt] '-classpath' [apt] '/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build/classes/jruby:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/asm-3.0.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/asm-analysis-3.0.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/asm-commons-3.0.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/asm-tree-3.0.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/asm-util-3.0.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/backport-util-concurrent.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/bnd-0.0.249.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/bytelist-0.1.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/dynalang-0.3.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/emma.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/emma_ant.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/jarjar-1.0rc7.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/jline-0.9.93.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/jna-posix.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/jna.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/joda-time-1.5.1.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/joni.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/junit.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/jvyamlb-0.2.3.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/retroweaver-2.0.5.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/build_lib/retroweaver-rt-2.0.5.jar:/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/lib/bsf.jar:/usr/share/ant/lib/ant-launcher.jar:/usr/share/java/xmlParserAPIs.jar:/usr/share/java/xercesImpl.jar:/usr/share/ant/lib/ant-javamail.jar:/usr/share/ant/lib/ant-apache-oro.jar:/usr/share/ant/lib/ant-commons-logging.jar:/usr/share/ant/lib/ant-apache-bsf.jar:/usr/share/ant/lib/ant-apache-regexp.jar:/usr/share/ant/lib/ant-nodeps.jar:/usr/share/ant/lib/ant-jsch.jar:/usr/share/ant/lib/ant-jmf.jar:/usr/share/ant/lib/ant-apache-log4j.jar:/usr/share/ant/lib/ant.jar:/usr/share/ant/lib/ant-commons-net.jar:/usr/share/ant/lib/ant-apache-resolver.jar:/usr/share/ant/lib/ant-antlr.jar:/usr/share/ant/lib/ant-apache-bcel.jar:/usr/share/ant/lib/ant-trax.jar:/usr/share/ant/lib/ant-junit.jar:/usr/share/ant/lib/ant-jdepend.jar:/usr/share/ant/lib/ant-swing.jar:/usr/java/jdk1.5.0_16_64bit/lib/tools.jar' [apt] '-sourcepath' [apt] '/mnt/local/mslama/netbeans/hg-nbsrc/core-main/o.jruby.distro/unpatched_source/jruby-1.1.4/src' [apt] '-target' [apt] '1.5' [apt] '-encoding' [apt] 'UTF-8' [apt] '-g' [apt] '-XDignore.symbol.file=true' [apt] '-source' [apt] '1.5' [apt] '-factory' [apt] 'org.jruby.anno.AnnotationBinder' [apt] [apt] The ' characters around the executable and arguments are [apt] not part of the command. [apt] Files to be compiled:looking at the source, that warning about forking is invalid; its the superclass whining. The stack trace implies this is a new JVM (no ant classes in the trace) so we are forking. That leaves the question of whether the memory options are going down to the process or not.Can you now do an ant -debug run and paste in the [apt] bit of that log?I used memoryMaximumSize="256m" for apt task and it works now. Yes warning about forking is confusing and should be eliminated. Parent process memory settings are not propagated to forked apt task so maximum memory size must be increased explicitly. But it is documented to it is correct. Thank you for help. I assume ant -debug output is not necessary anymore. If you still need it let me know.Problem is that task does not fail when OutOfMemoryError happens in forked JVM. Is it correct? It makes diagnostic difficult in case of building big project.	9.0	id=43271	11	False	False	foamdino	1
id=44530	REOPENED	None	Ant	Optional Tasks (	1.7.0	PC Linux	P2 normal	Ant Notifications List	2008-03-04 09:42 UTC by	Michael Rosett	2011-11-17 00:56 UTC (	3 users	I am running the junit task with forkmode="once" and the xml formatter. It seemsthe xml output and subsequent report generation only contain system.out for the first test class that is ran. If I set showoutput="true" I have the same problem (system.out is not in every xml output) but the console output shows system.out for every test class. Here's my junit task definition:<junit fork="true" forkmode="once" showoutput="true">	I have tested this and do not see the problembuild file:<project name="x" default="run"> <target name="run"> <path id="t.path"> <path path="build/classes"/> <fileset dir="lib" includes="*.jar"/> </path> <mkdir dir="build/classes"/> <mkdir dir="build/test/reports"/> <javac srcdir="src" destdir="build/classes" target="1.5" source="1.5" debug="yes" classpathref="t.path"/> <junit fork="true" forkmode="once" showoutput="true"> <formatter type="brief" usefile="false"/> <classpath refid="t.path"/> <formatter type="xml"/> <batchtest todir="build/test/reports"> <fileset dir="src" includes="**/*Test.java"/> </batchtest> </junit> </target> <target name="clean"> <delete dir="build"/> </target></project>and the java files:peter/T1TEst.java:package peter;import org.junit.*;public class T1Test { @Test public void t1() { System.out.println("T1Test::t1"); }}andpackage peter;import org.junit.*;public class T2Test { @Test public void t2() { System.out.println("T2Test::t2"); }}The xml reports contain the stdout from each test.I have seen something similar with (shudder) Log4jAt the each of each test, the std output copy is closed*unless* one uses the follow attribute in the log4j.propertiesfile.# attributes for stdout appender# need to set follow for stdout as it gets moved around in junit testslog4j.appender.STDOUT.follow=trueSetting to NEEDINFO, it would be nice to have a completetar.gz or zip file showing the problem.Createdtestcase reproducing the problem using java.util.loggingIt is easy to reproduce this with JUL (java.util.logging). JUL also caches a reference to System.err.I just implemented a workaround for this bug in my own codebase by using a formatter created for the task (a la org.apache.tools.ant.taskdefs.optional.junit.TearDownOnVmCrash). My workaround is for users of java.util.logging.The code for the formatter is available at (I'll add it as an attachment as well):Use it by adding an additional formatter tag to your build.xml:<junit... ... <formatter classname="JUnitLogFixFormatter" usefile="false"/> ...</junit>At the beginning of each test, the code looks at the root Logger and replaces any ConsoleHandlers it finds with a new ConsoleHandler (and copies over the configuration). ConsoleHandler caches System.err during its constructor, so constructing a new instance is required. After the test completes, the original handlers are replaced back into the root Logger.For my codebase this seems to work, but YMMV.CreatedJUnitLogFixFormatter.javaWorkaround formatter for java.util.logging users	5.0	id=39164	5	False	False	mbenson	1
id=43271	REOPENED	None	Ant	Optional Tasks (	1.7.0	Other other	P2 enhancement	Ant Notifications List	2007-08-31 01:31 UTC by	Eric Jain	2015-10-14 04:18 UTC (	4 users	sendFileToRemote in org.apache.tools.ant.taskdefs.optional.ssh.ScpToMessageseems to set permissions to 0644 -- would be useful if this could be overridden!Also, when using sftp instead of ssh1, could 'chmod' files after transferring them?	It would be also very useful to specify the group ownership to create files with.There's a secondary problem, which is that the scp task docs claim that the umask on the remote system is honoured. It would be more helpful if the docs made it clear that the permissions were hardcoded. I have attached a patch which adds chmod and umask attributes to the scp task, in line with what the ftp task already supports.CreatedPatch to allow setting optional chmod and umask attributes on scp commandThanks a lot for the patch, some comments:* please don't use protected fields. There are some inside Ant's code base but they are only kept for backwards compatibility reasons.* usually "umask" applies to directories and files. Your patch uses "umask" for directories and "chmod" for files. This is confusing. Maybe it would be better to go with the already existing names of "dirmode" and "mode" like we are using for <zipfileset>?I've added fileMode and dirMode with git commit 2f0edba and tested it for scp - I don't have access to a server speaking sftp.Note the resulting permissions are still the intersection of the permossions set and the remote UMASK (i.e. if the remote UMASK is 022 you won't be able to create group- or world-writable files). I don't know enough about JSch to know whether there is a way around it.I have tested this with sftp and it causes the parent level directory permissions to be reset to 644, effectively breaking the directory.My task configuration:<scp todir="${ssh.userid}@${mainframe.host}:${classpathjar.mainframe.directory}" keyfile="${ssh.keyfile}" verbose="${ssh.verbose}" sftp="true" file="target/${ARM_PROJECTNAME}.jar"></scp>This task works perfectly fine against 1.8.2, but fails under 1.9.5Oh, my sftp server is running with umask 0002"Subsystem sftp /bin/sh -c 'umask 0002; /usr/lib/ssh/sftp-server'"I'm new to Ant bug reporting. Could someone let me know that they have seen my last updates. thanks.Hi Walter, yes, at least I've seen your comment, but haven't found the time to look into it, yet.	9.0	id=40093	11	False	False	efanomars	1
id=45135	REOPENED	None	Ant	Core (	1.7.0	PC Linux	P2 regression	Ant Notifications List	2008-06-05 03:39 UTC by	ivanveselovsky	2008-06-05 08:40 UTC (	0 users	regression against ant 1.6.5: a patternset cannot be redefined through itself.the problem may be illustrated using the following sample:----------------------------------<?xml version="1.0" encoding="iso-8859-1"?><project name="Show Bug" default="all" basedir="."> <target name="all"> <patternset id="x.id"> <include name="*.a"/> </patternset> <patternset id="x.id"> <patternset refid="x.id"/> <include name="*.b"/> </patternset> <mkdir dir="target"/> <mkdir dir="src"/> <copy todir="target"> <fileset dir="src"> <patternset refid="x.id"/> </fileset> </copy> </target> </project>----------------------------------execute this sample with "-debug" key with ant 1.6.5 and 1.7.0:the log line where the patternset is shown is:----Setup scanner in dir /tmp with patternSet{ includes: [*.b, *.a] excludes: [] }----for ant 1.6.5, and ----Setup scanner in dir /tmp with patternSet{ includes: [*.b] excludes: [] }----We can see that in 1.7.0 the previous content of the referred patternset is lost. This is the problem.	that is simply the way Ant references work. You could use a scriptdef to programmatically retrieve the original reference, then assign a new object which refers to the original object back to the same reference name. Another option would be to use the Ant 1.8 properties API to add a PropertySetter capable of setting a reference (so you use a different mechanism to set and get the reference). I will add this to the list of things to do for the props antlib as well.(In reply to)Hi, Matt,sure, this may be easily worked around using another reference, for example. But the point of this bug request is just *compatibility* with 1.6.5 release.Do you know, why that worked in 1.6.5 in a different way than in 1.7.0?Ah... you should then have marked that as a regression. That would require a little more research as I wouldn't have expected this ever to have worked.	3.0	id=44756	4	False	False	lateralcoder	1
id=47035	REOPENED	None	Ant	Core (	1.7.1	All All	P2 enhancement	Ant Notifications List	2009-04-15 05:20 UTC by	Juhos Csaba-Zsolt	2009-04-16 20:56 UTC (	1 user	Hi,I'm using Launch4j to wrap a JAR into a windows executable:<launch4j> <config> <jre> <opt>-Xms${java.initial.memory}m</opt> </jre> </config></launch4j>The problem is that the "opt" element doesn't expand properties.It seems that the developers didn't take care to expand them explicitly.()I wrapped the task into a macro like this:<macrodef name="create-exe"> <attribute name="java-initial-memory"/> <sequential> <launch4j> <config> <jre> <opt>-Xms@{java.initial.memory}m</opt> </jre> </config> </launch4j> </sequential></macrodef>And called it like:<create-exe java-initial-memory="${java.initial.memory}"/>in hope that the property will be expanded before being passed to the template.It wasn't :(I know that this is the expected behavior. What I'm asking for here is an option to expand macro arguments before passing them to the template.After some consideration I came to the conclusion that the best solution would be to add an optional attribute "expand" to the "element" and "attribute" child elements of macrodef.Modified example:<macrodef name="create-exe"> <attribute name="java-initial-memory" expand="true"/> <attribute name="java-maximum-memory" expand="true"/> <element name="vm-options" optional="true" expand="true"/> <sequential> <launch4j> <config> <jre> <opt>-Xms@{java.initial.memory}m</opt> </jre> </config> </launch4j> </sequential></macrodef><create-exe java-initial-memory="${java.initial.memory}" java-maximum-memory="${java.maximum.memory}"> <vm-options> <opt>${another.vm.option}</opt> </vm-options></create-exe>"expand" should be a boolean variable with a default value of "false". This wouldn't break any existing code.Another use case I'm seeing is the following:<macrodef name="create-exe"> <attribute name="expand-options" default="true"/> <attribute name="java-initial-memory" expand="@{expand-options}"/> <attribute name="java-maximum-memory" expand="@{expand-options}"/> <element name="vm-options" optional="true" expand="@{expand-options}"/>...This would make the macro more flexible.In case this isn't possible to implement, I'm sorry for my ignorance, I'm not familiar with Ant's internals at all.Thank you,CsabiP.S.: I filed a bug against Launch4j:	Uh... Red herring alert: I think the reason your attribute is not expanded is because you're not using it:<macrodef name="create-exe"> <attribute name="java-initial-memory"/> <sequential> <launch4j> <config> <jre> <opt>-Xms@{java.initial.memory}m</opt> </jre> </config> </launch4j> </sequential></macrodef>This should be:<macrodef name="create-exe"> <attribute name="java-initial-memory"/> <sequential> <launch4j> <config> <jre> <opt>-Xms@{java-initial-memory}m</opt> </jre> </config> </launch4j> </sequential></macrodef>Hi,It's a typo. In my real code I have used the attribute correctly.What happens is that "@{java-initial-memory}" gets expanded to "${java.initial.memory}", which doesn't get expanded by the "opt" element, because it doesn't expand properties in its text.What I'm asking for is an option for the expansion of "${java.initial.memory}" when calling the macro, like this:<property name="java.initial.memory" value="128"/><create-exe java-initial-memory="${java.initial.memory}"/>...@{java-initial-memory} should expand to "-Xms128m"Cheers,Csabi	2.0	id=44530	8	False	False	peterreilly	1
id=44756	REOPENED	None	Ant	Core (	1.6.5	PC Linux	P2 major	Ant Notifications List	2008-04-04 07:43 UTC by	Kevin Gilpin	2008-07-21 13:14 UTC (	0 users	CreatedAnt's -v log of the junit task, also the contents of my ant/libI am running the JUnit ant task. The output of the task is: [junit] Exception in thread "main" java.lang.NoClassDefFoundError: [junit] Tests FAILEDI've tried everything I can think of to get the class name or a stack trace without success, including:* -v and -debug flags* loading up a console and looking in the ClassLoader for duplicates of Ant classes (i'm not using anything suspect like weblogic.jar)* running Ant in a debugger. I can't set a breakpoint for NoClassDefFoundError because it's occurring in a forked processI'm at a loss to figure out what's going on. There may be a problem with my classpath but Ant doesn't give me enough information to figure out what it might be.Same problem occurs with Ant 1.6.2.	I agree it's not very obvious how to achieve this, but if you add an element<formatter type="plain">within your <junit> element, a text file will be created with the stack trace of your test case.I wrote a basic test class XXXTest (no package) that referred to a missing class; the output went to a file TEST-XXXTest.txt as follows:Testsuite: XXXTestTests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec------------- Standard Error -----------------This message went to syserr------------- ---------------- --------------- Caused an ERRORYYYTestjava.lang.NoClassDefFoundError: YYYTest at XXXTest.<clinit>(XXXTest.java:17) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:169)Caused by: java.lang.ClassNotFoundException: YYYTest at java.net.URLClassLoader$1.run(URLClassLoader.java:200) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:188) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at java.lang.ClassLoader.loadClass(ClassLoader.java:251) at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)Note the NoClassDefFoundError. If however you run it on a class that doesn't exist (e.g. XXXTest1), you get a ClassNotFoundException:Testsuite: XXXTest1Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec Caused an ERRORXXXTest1java.lang.ClassNotFoundException: XXXTest1 at java.net.URLClassLoader$1.run(URLClassLoader.java:200) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:188) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at java.lang.ClassLoader.loadClass(ClassLoader.java:251) at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:169)I hope this helps.The problem is not when the test class itself does not exist, it appears to be when some class referenced by the test class does not exist. These tests work fine in my IDE. I can't figure out how to get Ant to tell me what class it can't find, nor how to attach a debugger to a running Ant JUnitTestRunner.	2.0	id=35776	10	False	False	hauser	1
id=47433	REOPENED	None	Ant	Core tasks (	1.8.2	All All	P2 enhancement	Ant Notifications List	2009-06-25 18:11 UTC by	Jagadesh Munta	2014-01-06 15:11 UTC (	0 users	Email thread sent to thealias.Jagadesh Babu Munta wrote:>>>	Earlier code (calling execute after setting new URL) had issue on Linux and modified to call doGet() method.Below is the latest code (last line changed). int responseCode = httpConnection.getResponseCode(); log("Response Code="+responseCode, logLevel); // test for 401 result (HTTP only) if (responseCode == HttpURLConnection.HTTP_UNAUTHORIZED) { String message = "HTTP Authorization failure"; if (ignoreErrors) { log(message, logLevel); return false; } else { throw new BuildException(message); } } else if ((responseCode == HttpURLConnection.HTTP_MOVED_PERM) || (responseCode == HttpURLConnection.HTTP_MOVED_TEMP)) { String newLocation = httpConnection.getHeaderField("Location"); String message = "HTTP URL Moved to "+newLocation; log(message, logLevel); setSrc(new URL(newLocation)); return doGet(logLevel, progress); }I have adapted the patch to avoid redirect loops etc, furthermore I refactored the code to reach a cleaner solution.Test show however that redirects are followed by default. This is also the default setting of the HttpURLConnection.When researching why the redirect does not work for the hudson download I found that the HttpURLConnection does not switch protocols. The Sun Java Networking Engineers reached the conclusion it would be unsafe to switch protocols, this has been listed in:Before I progress applying the changes, I would like to pose the question whether we would want this behavior in ant, even if the Sun Networking Engineers concluded this would be unsafe. For this reason, personally I would tend to say no to the patch with the protocol change. If Sun deems this change to be safe, this change should be applied in the HttpURLConnection, I could imagine that a switch from http to https could be allowed, while a change in the other direction is not. Switches to yet other protocols pose completely new questions.If such change is deemed safe, the change should probably be made in the HttpURLConnection.I agree with Martijn.If <get> followed redirect that Java wouldn't follow on its own it shouldn't do so without the user explicitly asking for it (i.e. with a new attribute to enable it).Thanks Martin and Stefan for quickly looking into this.I looked at the jdk bug, ""and it says at the end "Check response code and Location header field value for redirect information. It's the application's responsibility to follow the redirect."I think we should take care of the same in the ANT for the above issue because it is an application. Because having a solution to the problem is better than nothing! I agree that it is better to have an optional attribute to have this new behavior so that by default it simply follows HttpURLConnection and on demand (when no security issue) user can use this new attribute.If it is not safe in the platform, it is not safe in an application that cannot, and will not pop up an indication >>>are you sure<<< (and probably not even when it would pop up an indication it would still not be safe)It does not get any safer by doing this from an application, instead of from the java platform, it just becomes the responsibility of the application if it does follow the redirect. If it is unsafe to switch protocols from a redirect, we should not be doing this unless we do understand the implications.Createdget task that seizes control of redirection itself allowing a redirect from HTTP to HTTPSD:\data\eclipseworkspace\ant-trunk\testje>..\dist\bin\ant -file testget.xmlBuildfile: D:\data\eclipseworkspace\ant-trunk\testje\testget.xmlbug: [get] Getting:[get] To: D:\data\eclipseworkspace\ant-trunk\testje\dl2.html [get]moved to[get] Getting:[get] To: D:\data\eclipseworkspace\ant-trunk\testje\dl.html [get]permanently moved toIs there any way we can set up redirects on our webserver to allow official testcases?Not sure who is the administrator on ant.apache.org. I don't know the location of tests too;)If you are looking for information on how to do the same, see atThere is forum question as sample -Jagadesh,could you inquire within Sun why it has been chosen not to allow a redirect between protocols in the HttpURLConnection in general, and why not from http to https (in that direction) in particular?Because still if it is not safe we should not do it.And if it is safe, why shouldn't it be done in the HttpURLConnection instead of within ant?Martijn,Ok. I will inquire about HttpURLConnection and update once I get the details.About the ANT, I still think it is an application w.r.t Java (platform) and also the usage for ANT is different than Java, where in java secured applications might be developed. With ANT I think, the usage is under control with the users.Thanks.Martijn,The following are the responses that I got from JDK folks:- --- Michael McMahon wrote:>>>>-----I have created and applied a patch to allow a redirect from http to https, but no other protocol switches.svn revision:Seeing this problem on 1.8.2Got a redirect and the file was not downloaded - instead this error message is stored in the fileIf you are not automatically redirected use this url:John, could this berather than this one? Any chance you could give 1.9.x a try?	15.0	id=47433	8	False	True	bodewig	1
id=48746	REOPENED	None	Ant	Core tasks (	1.8.2	PC Linux	P2 normal	Ant Notifications List	2010-02-15 21:05 UTC by	Jay Berkenbilt	2010-12-27 11:12 UTC (	1 user	I'm running apache ant 1.8.0 on Red Hat Enterprise Linux 5.4, x86_64, with the 64-bit version of java 1.6.0_18 from sun (the latest available at least as of last week). I can also see this behavior with 1.5.0_15, which is used in some of our development environments for various reasons.With ant 1.8.0, there is a fairly high probability that ant's exec task will generate extraneous newlines in command output. For example, with this build.xml file:<project default="all"> <target name="all"> <exec executable="cat"> <arg value="/tmp/a"/> </exec> </target></project>and the following /tmp/a:0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789"most" of the time, I get the expected output:Buildfile: /tmp/build.xmlall: [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789BUILD SUCCESSFULTotal time: 0 secondsbut sometimes I get variations with an extraneous line break in an indeterminate spot. For example: [exec] 01234567890123456789012345 [exec] 678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789 [exec] 01234567890123456789012345678901234567890123456789BUILD SUCCESSFULTotal time: 0 secondsIf I run ant 50 times in a loop, I see this or similar behavior (the line break is not always in the same place) about 50% of the time. I'm running ANT like this:while true; do if JAVA_HOME=~/tmp/jdk1.6.0_18 $ANT_HOME/bin/ant -f /tmp/build.xml | diff - /tmp/a1; then echo yes >> /tmp/a2; else echo no >> /tmp/a2; donewhere /tmp/a1 just contains what the ant command is supposed to contain.I never see this behavior with 1.7.0, and I don't believe I've seen it with 1.7.1 either.This problem is causing false test failures for us and will thus prevent us from upgrading to 1.8.0 pending a solution. A quick diff of ExecTask.java between versions doesn't reveal any relevant changes. Any suggestions on where in the code to check? Let me know if there's anything you need me to test or submit.	Hi,I have tried this on my mac and i can confirm this.I have this java version :java version "1.6.0_17"Java(TM) SE Runtime Environment (build 1.6.0_17-b04-248-10M3025)Java HotSpot(TM) 64-Bit Server VM (build 14.3-b01-101, mixed mode)and MacOS Snow Leopard.My guess is that the bits of codes which would cause this must be handling input output, probably PumpStreamHandler and the classes it delegates to.AntoineCreatedpatch of PumpStreamHandlerThis patch fixes the issue. I will probably commit it soon, but if meanwhile some one can explain what is the justification of a special wait behavior in StreamPumper for Windows systems ? The change makes all operating systems behave the same in this respect, so looping to wait for input in the streamPumper#waitForInput method.I do not have yet a unit test for this issue. I am also wondering whether this change might not make some unit tests which are not behaving consistently behave more consistently.committed my change.We have an antunit test case testRedirector13 in src/tests/antunit/taskdefs/exec/apply-test.xml which was failing inconsistently. I bet that this change in PumpStreamHandler will prevent this test from failing.The patch causes a regression in stream handling inside NetBeans: under some circumstances, an <exec> gobbles up all subsequent output from Ant! I recommend this patch be rolled back for 1.8.2.I am not sure if there is any reliable fix for the originally reported bug; Ant tries to log messages corresponding to complete lines, even in case the final \n is missing; but if it receives some characters without a newline it simply cannot know whether or not more characters are coming in the same line, so it just waits a short while to see if any arrive. Increasing StreamPumper.POLL_INTERVAL might reduce the likelihood of problems (untested).Why would a test be checking the output of Ant's log, anyway? This should be considered informational. You can capture the output of <exec> to a file if it must be exact.Just to answer the question of why a test would be looking at ant's output, the test suite is for a build tool that invokes ant (as well as doing many other things). I already have filters in the test suite to compensate for unpredictable aspects of ant's output, such as durations. If it is deemed that the output of <exec> is not reliable when output to the screen, I will just modify my test suite to normalize it in some way. For my original purposes, I don't care about it in the general case. I just need to make sure ant is doing what it is supposed to do when invoked from my other tool.In other words, running ant is the objective in this case, not a means to an end. So I care about ant's output. But since I'm not testing ant, I can work around its quirks without damaging the integrity of my open application's test suite.Hello Jesse,could you explain in more details the problem that you have in Netbeans ? Maybe the issue that you have in NetBeans can be resolved in the netbeans integration.there is a comment in your Netbeans bug report saying this :"I've switched to a local (non-bundled) installation of Ant 1.8.1 (originalrelease from apache). This fixes the bug."The change which I did for thiswas to systematically set useAvailable=true in o.a.t.a.t.StreamPumper. Before, useavailable was set to true on Windows and false on other platforms.I was able to reproduce the problem described in thisof lines being cut in the log of ant. I reproduced this on my MAC. If I did what is described in the bug report 50 times, I would get 30 times approximatively lines broken.I did not create a JUnit testcase for the original issue, which I should do to prevent regression.Regards,Antoine(In reply to)I think the URL field explains it in detail. Under 1.8.1, with that example, output from <exec> is never displayed, nor is any subsequent output from Ant.It is possible, and I will try to investigate the root cause, but issues involving stdio are especially nasty to track down so reverting the problematic patch seems safest.Right. And if I set it back to false on Linux, everything is fine. I will double-check on XP, but I think platform-specific differences in stdio handling meant that there was no problem on Windows to begin with.Oddly enough, the bug in the test case is reproducible in XP in 1.8.1 (not in 1.8.0). This fixes it for both Linux and XP:Index: src/main/org/apache/tools/ant/taskdefs/PumpStreamHandler.java===================================================================--- src/main/org/apache/tools/ant/taskdefs/PumpStreamHandler.java ()+++ src/main/org/apache/tools/ant/taskdefs/PumpStreamHandler.java (working copy)@@ -244,7 +244,7 @@ final Thread result = new ThreadWithPumper(new StreamPumper(is, os, closeWhenExhausted,- true));+ false)); result.setDaemon(true); return result; }But I have not yet checked the effect on the test cases from.So, I'm just a user and am not familiar with the internals of ant, but isn't it pretty clear that if changing this parameter to true breaks one thing and changing it to false breaks something else, that neither code is correct? I hope we don't fall into the trap here of choosing which bug we'd rather have but instead continue to investigate why the change to fix the extraneous newline problem causes the other issue. Sorry if I'm stating the obvious here.Caused by PumpStreamHandler.finish interrupting the thread, which can cause a listener based on java.nio.channel to abruptly terminate with a ClosedByInterruptException. I think somehow timing-dependent; the extra waiting for output caused by useAvailable probably causes the pumper thread to hit the JOIN_TIMEOUT, thus falling through to the backup behavior of interrupting it until it dies.Workaround on listener side is to forcibly clear the thread interrupt status immediately before writing to the channel. Still subject to a race condition but seems to work.CreatedHandy diagnostic patchThere also iswhich somehow may also be related.Of the suggested changes over there we have performed one (join timeout) butneglected two that we may want to rethink:* unconditionally call process.destroy and don't close the streams at all (rely on process.destroy to do that). I'm not convinced this is going to help or even poses a problem. At the point where we close the streams either process.waitFor has succeeded or we have invoked process.destroy* first close the streams and then stop the threads At first glance I'd expect this change to make it more likely to miss output written by the process but at the same time it may force buffers to get flushed.I'll look into bringing real testcases forinto Ant's trunk so we cantest a few things. I don't think I ever checked in the ones I used by theend of 2008 and don't have them anymore, so I'll have to recreate them.I did turn Peter's example code into an AntUnit test back then insvntestDoesntWaitForChildren in src/tests/antunit/taskdefs/exec/exec-test.xmltakes three seconds on my Win7 machine when run with Ant 1.8.[01] and21 seconds with Ant 1.7.1.If I apply the suggested change ofI'm back at 22 seconds again,swapping close/stopThreads doesn't change that either. But if I swap them,I don't even see the "finished" output the original process executedso it would actually make things worse.So whatever we do, the flag has to remain true on Windows.I have been experiencing the LeadPipeInputStream Broken Pipe error with Ant 1.8.1 and Antoine pointed me at this issue. I am running on Windows XP and my use case is the Java Task with a LineCOntainsRegExp output filter chain redirector. In Ant 1.7.1 I would see the broken pipe error very infrequently, once every 5 - 10 builds. In Ant 1.8.1 I see it with every build.I have tried the 6/28/2010 trunk source and still see the problem. I have also tried changing the flag in createPumper and still see the problem.This bug appears unresolved in ant 1.8.1.using 1.8.1 release and Jay's original test build.xml I can easily see it.Mac OS X 10.6.4, java version "1.6.0_20"	15.0	id=48746	15	False	False	antoine	1
id=48008	REOPENED	None	Ant	Core tasks (	1.7.1	PC Windows XP	P2 normal	Ant Notifications List	2009-10-15 16:34 UTC by	Alexander Pogrebnyak	2009-11-17 10:54 UTC (	0 users	When I use a touch task with a mapper<touch mkdirs="true" verbose="${TALK}"> <fileset dir="${orig-dir}"/> <globmapper from="*" to="${shadow-dir}/*"/></touch>Produces empty files that are OLDER than sources.After running this task and using ls --time-style=full-iso, I get the following timestamp for source: 2009-09-30 07:44:26.290894100 -0400And this timestamp for target 2009-09-30 07:44:26.290000000 -0400Notice that target now is older than source.This breaks <ant-contrib:outofdate> task later on.	Ant uses the lastModified and setLastModified methods in java.io.Filewhich doesn't allow a granularity smaller than millisecond.I don't understand why outofdate seems to be rounding up here but as far as I can see there isn't anything touch could do differently as it set the last modified time to the one read from the source file.CANTFIX rather than WONTFIX.(In reply to)How about setting it tonewFile.setLastModified( origFile.lastModified() + 1 )This will definitely round it in the right direction. Plus it would probably integrate better with the tools external to ant (e.g. make), that could have used the newly created file.Is it possible to add "roundup" attribute to Touch task, similar in concept to one in Zip task.Then if the attribute is set, you can do newFile.setLastModified( origFile.lastModified() + 1 ) as proposed in	3.0	id=47035	5	False	False	csaba.juhos	1
id=49891	REOPENED	None	Ant	Documentation (	1.8.0	All All	P2 regression	Ant Notifications List	2010-09-07 09:22 UTC by	Ben Hale	2014-06-25 09:19 UTC (	1 user	When using <subant/> more than one level deep, properties are not overridden properly after the first level.The test case works as follows:Level 1: * A property is defined * <subant/> is called redefining the value of the propertyLevel 2: * The property is expected to be the redefined value (and is for 1.7.x and 1.8.x) * <subant/> is called redefining the value of the property again.Level 3: * The property is expected to be the redefined value (and is for 1.7.x but is *not* for 1.8.x)Using the attached build.xml file you'll see an output of the following with 1.7.1Buildfile: build.xmllevel-1:level-2: [echo] Expecting 'bravo', actual 'bravo'level-3: [echo] Expecting 'charlie', actual 'charlie'BUILD SUCCESSFULTotal time: 0 secondsYou'll see the following with 1.8.1.Buildfile: /Users/benhale/Desktop/build.xmllevel-1:level-2: [echo] Expecting 'bravo', actual 'bravo'level-3: [echo] Expecting 'charlie', actual 'bravo'BUILD SUCCESSFULTotal time: 0 seconds	I assume the build file looks similar to this<project default="level-1"> <target name="level-1"> <property name="p" value="alpha"/> <subant target="level-2"> <file file="subant.xml"/> <property name="p" value="bravo"/> </subant> </target> <target name="level-2"> <echo>Expecting 'bravo', actual '${p}'</echo> <subant target="level-3"> <file file="subant.xml"/> <property name="p" value="charlie"/> </subant> </target> <target name="level-3"> <echo>Expecting 'charlie', actual '${p}'</echo> </target></project>In the twisted way Ant properties are supposed to work Ant 1.8.x's behavior is correct.If you take the <ant> task manual page it saysin the "Description" section andin the section describing the nested property element of <ant>.What happens is that starting with the project executing level-2 the propertyp (in my build file) looks as if it had been set on the command line andso the next subant call is unable to override its value.Sorry about the attachment being missing, not sure what happened there. In addition, I don't dispute that it's working per those instructions it's just that it's a undocumented change from the 1.7 line behavior.so what we need is better documentation, I'll look into it.It is a bit too late to note this as a breaking change, maybe we can do that as part of the FAQ. My guess is we've "fixed" subant by accident and didn't realize the change at all.svnCreatedAdding checks in addAlmostAllI came across the same problem and think I found the behaviour-changing cause between 1.7.1 and 1.8 (till 1.9.3).In 1.8.0 a new method addAlmostAll() for copying properties was introduced in Ant.java. In the past that task was delegated to PropertyHelper.java and its copyXYProperties() methods. These methods check whether the target project already contains the property and don't override existing values. AFAIK these checks are missing in addAlmostAll().Here's the implementation in copyUserProperties() with it's containsKey-check: public void copyUserProperties(Project other) { //avoid concurrent modification: synchronized (userProperties) { Enumeration<String> e = userProperties.keys(); while (e.hasMoreElements()) { Object arg = e.nextElement(); if (inheritedProperties.containsKey(arg)) { continue; } Object value = userProperties.get(arg); other.setUserProperty(arg.toString(), value.toString()); } } }And here's the snippet from addAlmostAll() ... } else if (type == PropertyType.USER) { newProject.setUserProperty(key, value); } ...BTW, you can produce the same effect in 1.7.1, if you remove "continue".The attached patch adds the same checks in addAlmostAll(): } else if (type == PropertyType.USER) { if (!PropertyHelper.getPropertyHelper(this.getProject()).getInheritedProperties().containsKey(key)) { newProject.setUserProperty(key, value); } }I am using this patch for several months without any (other) problem, but perhaps there are effects I don't see.Here's the output of your example with the patched version:level-1:level-2: [echo] Expecting 'bravo', actual 'bravo'level-3: [echo] Expecting 'charlie', actual 'charlie'Recently, at our office (Realworld Systems) we experienced issues with the same underlying problem. We usually use a build.properties file to override or introduce properties. However, on our CI (Jenkins) we encode these properties into the Jenkins environment.This time, we had a certain property, which was altered with subant. On the command-line, on the developer's box, this was alright. However, the behavior of Ant inside the CI was quite different.After experimenting a bit, we found out that these properies are set through the command-line, creating the same problem as already described.Aside from the change to addAlmostAll, we would like to be able to have a tertiary property, indicating whether command-line properties should take precedence, or at least a way to indicate which command-line properties should be excluded from precedence.To our understanding, the behavior, although documented, is illogical and contradicts with the normal expectation of the properties to be reset.	7.0	id=53347	7	False	False	jglick	1
id=53347	REOPENED	None	Ant	Core tasks (	1.8.3	PC Linux	P2 major	Ant Notifications List	2012-06-01 19:11 UTC by	Jesse Glick	2015-04-23 06:26 UTC (	2 users	(in 1.8.3) added some JDK 8 support. Unfortunately running <javac> by default fails with this error: Class not found: javac1.8Workaround is to pass -Dbuild.compiler=javac1.7.	Committed.Apache Ant(TM) version 1.9.2 compiled on July 8 2013javac 1.8.0_40java.exe -versionjava version "1.8.0_40"Java(TM) SE Runtime Environment (build 1.8.0_40-b26)Java HotSpot(TM) 64-Bit Server VM (build 25.40-b25, mixed mode)Not fixed ..ANT does not work with JDK 1.8M-CreatedJava8-Sample: SourcecodeCreatedJava8-Sample: BuildfileI did an example which works for me.Ant: 1.9.4Java: 1.8.0-ea (I know, I should update ....)Buildfile: C:\temp\ant-test\build.xml [delete] Deleting directory C:\temp\ant-test\build [mkdir] Created dir: C:\temp\ant-test\build\classes [echo] Java-Home: C:\seu\jdk1.8.0_b87\jre [echo] Java-Version: 1.8.0-ea [javac] Compiling 1 source file to C:\temp\ant-test\build\classes--> wait for the swing gui--> and click on the button [java] Click Detected by Lambda Listner [java] Click Detected by Anon ClassBUILD SUCCESSFULTotal time: 7 seconds	4.0	id=48008	4	False	False	bodewig	1
id=54473	REOPENED	None	Ant	Core tasks (	1.9.1	PC Linux	P2 normal	Ant Notifications List	2013-01-23 11:53 UTC by	René Krell	2014-04-30 08:47 UTC (	1 user	In rare cases, but reproducable on certain testsystems, I get the following stacktrace:com.izforge.izpack.installer.InstallerException: The following error occurred while executing this line:/tmp/buildfile_resource7846034884221422628xml:7: The following error occurred while executing this line:/usr/local/app/bin/database_contents.xml:5: The following error occurred while executing this line:Unable to find jar:file:/usr/local/app/lib/ant/ant-contrib/1.0b3/ant-contrib.jar!/net/sf/antcontrib/antlib.xml at com.izforge.izpack.event.AntActionInstallerListener.performAllActions(Unknown Source) at com.izforge.izpack.event.AntActionInstallerListener.afterPacks(Unknown Source) at com.izforge.izpack.installer.UnpackerBase.informListeners(Unknown Source) at com.izforge.izpack.installer.Unpacker.run(Unknown Source) at java.lang.Thread.run(Thread.java:736)Caused by: The following error occurred while executing this line:/tmp/buildfile_resource7846034884221422628xml:7: The following error occurred while executing this line:/usr/local/app/bin/database_contents.xml:5: The following error occurred while executing this line:Unable to find jar:file:/usr/local/app/lib/ant/ant-contrib/1.0b3/ant-contrib.jar!/net/sf/antcontrib/antlib.xml at org.apache.tools.ant.ProjectHelper.addLocationToBuildException(ProjectHelper.java:551) at org.apache.tools.ant.taskdefs.Ant.execute(Ant.java:395) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:48) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:37) at java.lang.reflect.Method.invoke(Method.java:600) at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) at org.apache.tools.ant.Task.perform(Task.java:348) at org.apache.tools.ant.Target.execute(Target.java:392) at org.apache.tools.ant.Target.performTasks(Target.java:413) at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399) at org.apache.tools.ant.Project.executeTarget(Project.java:1368) at com.izforge.izpack.event.AntAction.performAction(Unknown Source) at com.izforge.izpack.event.AntAction.performInstallAction(Unknown Source) ... 5 moreCaused by: /tmp/buildfile_resource7846034884221422628xml:7: The following error occurred while executing this line:/usr/local/app/bin/database_contents.xml:5: The following error occurred while executing this line:Unable to find jar:file:/usr/local/app/lib/ant/ant-contrib/1.0b3/ant-contrib.jar!/net/sf/antcontrib/antlib.xml at org.apache.tools.ant.ProjectHelper.addLocationToBuildException(ProjectHelper.java:551) at org.apache.tools.ant.taskdefs.ImportTask.importResource(ImportTask.java:238) at org.apache.tools.ant.taskdefs.ImportTask.execute(ImportTask.java:163) at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:48) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:37) at java.lang.reflect.Method.invoke(Method.java:600) at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) at org.apache.tools.ant.Task.perform(Task.java:348) at org.apache.tools.ant.Target.execute(Target.java:392) at org.apache.tools.ant.helper.ProjectHelper2.parse(ProjectHelper2.java:180) at org.apache.tools.ant.ProjectHelper.configureProject(ProjectHelper.java:82) at org.apache.tools.ant.taskdefs.Ant.execute(Ant.java:393) ... 17 moreCaused by: /usr/local/app/bin/database_contents.xml:5: The following error occurred while executing this line:Unable to find jar:file:/usr/local/app/lib/ant/ant-contrib/1.0b3/ant-contrib.jar!/net/sf/antcontrib/antlib.xml at org.apache.tools.ant.ProjectHelper.addLocationToBuildException(ProjectHelper.java:551) at org.apache.tools.ant.taskdefs.Definer.loadAntlib(Definer.java:445) at org.apache.tools.ant.taskdefs.Definer.execute(Definer.java:292) at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:48) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:37) at java.lang.reflect.Method.invoke(Method.java:600) at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) at org.apache.tools.ant.Task.perform(Task.java:348) at org.apache.tools.ant.Target.execute(Target.java:392) at org.apache.tools.ant.helper.ProjectHelper2.parse(ProjectHelper2.java:169) at org.apache.tools.ant.taskdefs.ImportTask.importResource(ImportTask.java:229) ... 29 moreCaused by: Unable to find jar:file:/usr/local/app/lib/ant/ant-contrib/1.0b3/ant-contrib.jar!/net/sf/antcontrib/antlib.xml at org.apache.tools.ant.taskdefs.Antlib.createAntlib(Antlib.java:68) at org.apache.tools.ant.taskdefs.Definer.loadAntlib(Definer.java:440) ... 40 moreCaused by: java.io.FileNotFoundException: JAR entry net/sf/antcontrib/antlib.xml not found in /usr/local/app/lib/ant/ant-contrib/1.0b3/ant-contrib.jar at sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:125) at org.apache.tools.ant.taskdefs.Antlib.createAntlib(Antlib.java:66) ... 41 morein the definition: <taskdef resource="net/sf/antcontrib/antlib.xml"> <classpath> <pathelement location="${install.prefix}/lib/ant/ant-contrib/1.0b3/ant-contrib.jar"/> </classpath> </taskdef>The File and the resource in it definitely exist.Ant 1.8.4 is used built-in here from an Izpack installer, but this doesn't play any role here, IMHO.The important point is, that it fails just on the first try, but on the second on it suceeds, although there hasn't been changed anything on the appropriate location in between time.Note:At the links--I found some discussion about switching off caching in such cases for UrlConnection. Maybe this could be useful also for Ant.	OS, where this bug occurs:Linux 2.6.32.12-0.7-default #1 SMP 2010-05-20 11:14:20 +0200 x86_64 x86_64 x86_64 GNU/LinuxJRE, which this bug occurs with:- Oracle JRE 6u38, Linux 64 bit- IBM J9 VM (build 2.4, JRE 1.6.0 IBM J9 2.4 Linux amd64-64 jvmxa6460sr7-20091214_49398 (JIT enabled, AOT enabled)Createdpatch for ProjectHelper2suggested patch based on the information in the bug reportCreatedPatch for the ANT_18_BRANCHHere is another patch for the Ant 1.8 branch which solves the problem for me. Are you able to apply this? ThanksI've combined both patches and also fixed some other places that looked suspicious.svnamended withWould appreciate a release with this fix, this still occurs in 1.9.3 for me.let's discuss a schedule for the next release on the user listUnfortunately I got the reopen this for the following reason:The most latest suggested patch didn't apply for me, the problem reoccurred. The is necessary to apply this patch for solving it:Thus, using conn.setDefaultUseCaches(false), not just setUseCaches(false), for whatever reason.	9.0	id=54835	23	False	True	andrew	1
id=54547	REOPENED	None	Ant	svn Antlib (	1.9.1	PC Windows XP	P2 normal	Ant Notifications List	2013-02-11 13:52 UTC by	kYm	2013-05-22 03:39 UTC (	1 user	build.xml:19: Failed to set 'info' properties19<svn svnkit="true" username="${svn.username}" password="${svn.password}" javahl="false">20 <info target="" verbose="true" />21 <wcversion path="." prefix="svn.wc." processUnversioned="true" />22 </svn>	I'm afraid the svn task you use is not the one of the svn antlib but some other third party task. The snippet doesn't match the task of Ant's own sandbox svn antlib.I am new to coldfusion/eclipse/ant/subversion. Can you tell me what the svn antlib task should look like? I am stuck on my build and not sure what to do at this point and dont know enough about the software.thanks.I don't have any idea about coldfusion or eclipse, I don#t use either of them.The svn antlib has never made it out of Ant's sandbox, so I'm pretty sure you must be using an svn task provided by a different project (and one I don't know). And I'm sure you want to stick with your svn task as the sandbox antlib isn't really supported.There must be a typedef somewhere in your build file that defines the svn task - and this typedef should give you a hint whom to tel about the bug.I'm sorry, I don't have any more clues to offer.	3.0	id=54473	8	False	False	renda.krell	1
id=32886	REOPENED	None	Slide	WebDAV client (	2.1	PC Windows XP	P2 critical	Slide Developer List	2004-12-30 11:07 UTC by	canrull	2006-10-06 05:26 UTC (	2 users	I have a mature application working with client webdavlib 2.0, with the webdavserver that comes with tomcat 5.0.28. Client webdavlib 2.1 seams to be broken.Method listWebdavResources() doesn't return child collections, but the parentcollection itself.This is the directory structure I tested:parent/ child1/ child2/ file1.txt file2.txtThis is my code:HttpURL url = new HttpURL("]");url.setUserinfo("user", "pass");WebdavResource wdResource = new WebdavResource(url);WebdavResource[] webdavList = wdResource.listWebdavResources(); System.out.println("Display Name: " + wdResource.getDisplayName()); WebdavResource[] webdavList = null; webdavList = wdResource.listWebdavResources(); System.out.println("Children returned: " + webdavList.length); for (int idx = 0; idx < webdavList.length; idx++) { System.out.println("\t" + webdavList[idx].getDisplayName()); System.out.println("\t" + webdavList[idx].getHttpURL()); }This is the output:Display Name: parentChildren returned: 3 file1.txtchild2file2.txtThe third result is the URL of the parent, with display name "child2"!So nothing about child1, although it comes from the server (see trace).And child2 URL is bad, because it shows parent URL.In general, I see that collection childs are never shown.And this is the http tracing I got. There are two requests involved, the firstusing depth 0 and the second using depth 1.FIRST REQUEST===============PROPFIND /webdav/parent HTTP/1.1Authorization: Basic anVncmVnbzE6cGFkZW50cm8=Content-Type: text/xml; charset=utf-8User-Agent: Jakarta Commons-HttpClient/2.0finalHost: localhost:8080Content-Length: 207Depth: 0<?xml version="1.0" encoding="utf-8"?><D:propfind xmlns:D="DAV:"> <D:prop> <D:displayname/> <D:getcontentlength/> <D:getcontenttype/> <D:resourcetype/> <D:getlastmodified/> <D:lockdiscovery/> </D:prop></D:propfind>FIRST RESPONSE=================HTTP/1.1 207 Multi-EstadoContent-Type: text/xml;charset=UTF-8Content-Length: 436Date: Thu, 30 Dec 2004 09:51:45 GMTServer: Apache-Coyote/1.1<?xml version="1.0" encoding="utf-8"?><multistatus xmlns="DAV:"> <response> <href>/webdav/parent/</href> <propstat> <prop> <displayname><![CDATA[parent]]></displayname> <resourcetype> <collection/> </resourcetype> </prop> <status>HTTP/1.1 200 OK</status> </propstat> <propstat> <prop> <getcontentlength/> <getcontenttype/> <getlastmodified/> <lockdiscovery/> </prop> <status>HTTP/1.1 404 Not Found</status> </propstat> </response></multistatus>SECOND REQUEST================PROPFIND /webdav/parent HTTP/1.1Authorization: Basic anVncmVnbzE6cGFkZW50cm8=Content-Type: text/xml; charset=utf-8User-Agent: Jakarta Commons-HttpClient/2.0finalHost: localhost:8080Content-Length: 207Depth: 1<?xml version="1.0" encoding="utf-8"?><D:propfind xmlns:D="DAV:"> <D:prop> <D:displayname/> <D:getcontentlength/> <D:getcontenttype/> <D:resourcetype/> <D:getlastmodified/> <D:lockdiscovery/> </D:prop></D:propfind>SECOND RESPONSE=================HTTP/1.1 207 Multi-EstadoContent-Type: text/xml;charset=UTF-8Content-Length: 2028Date: Thu, 30 Dec 2004 09:51:45 GMTServer: Apache-Coyote/1.1<?xml version="1.0" encoding="utf-8"?><multistatus xmlns="DAV:"> <response> <href>/webdav/parent/</href> <propstat> <prop> <displayname><![CDATA[parent]]></displayname> <resourcetype> <collection/> </resourcetype> </prop> <status>HTTP/1.1 200 OK</status> </propstat> <propstat> <prop> <getcontentlength/> <getcontenttype/> <getlastmodified/> <lockdiscovery/> </prop> <status>HTTP/1.1 404 Not Found</status> </propstat> </response> <response> <href>/webdav/parent/file2.txt</href> <propstat> <prop> <displayname><![CDATA[file2.txt]]></displayname> <getcontentlength>10</getcontentlength> <getcontenttype>text/plain</getcontenttype> <resourcetype/> <getlastmodified>Mon, 27 Sep 2004 11:13:07 GMT</getlastmodified> </prop> <status>HTTP/1.1 200 OK</status> </propstat> <propstat> <prop> <lockdiscovery/> </prop> <status>HTTP/1.1 404 Not Found</status> </propstat> </response> <response> <href>/webdav/parent/file1.txt</href> <propstat> <prop> <displayname><![CDATA[file1.txt]]></displayname> <getcontentlength>10</getcontentlength> <getcontenttype>text/plain</getcontenttype> <resourcetype/> <getlastmodified>Mon, 27 Sep 2004 11:13:07 GMT</getlastmodified> </prop> <status>HTTP/1.1 200 OK</status> </propstat> <propstat> <prop> <lockdiscovery/> </prop> <status>HTTP/1.1 404 Not Found</status> </propstat> </response> <response> <href>/webdav/parent/child2/</href> <propstat> <prop> <displayname><![CDATA[child2]]></displayname> <resourcetype> <collection/> </resourcetype> </prop> <status>HTTP/1.1 200 OK</status> </propstat> <propstat> <prop> <getcontentlength/> <getcontenttype/> <getlastmodified/> <lockdiscovery/> </prop> <status>HTTP/1.1 404 Not Found</status> </propstat> </response> <response> <href>/webdav/parent/child1/</href> <propstat> <prop> <displayname><![CDATA[child1]]></displayname> <resourcetype> <collection/> </resourcetype> </prop> <status>HTTP/1.1 200 OK</status> </propstat> <propstat> <prop> <getcontentlength/> <getcontenttype/> <getlastmodified/> <lockdiscovery/> </prop> <status>HTTP/1.1 404 Not Found</status> </propstat> </response></multistatus>As you can see from the second response, all the resources are returnedcorrectly from the server.	I have a very similar problem: when I listWebdavResources() on a collection,only the first child collection is returned, and its path is wrong (it is thepath of the parent). This worked with 2.0 but not in 2.1.What is the status of this bug? If it is genuine and not an error in usage, Iam suprised anyone can use this version.(In reply to)I am afraid few people use other WEBDAV servers than Slide. I guess this is nota problem with Slide server...Are you Rob using Slide server?I'll patiently wait for a new release of this library, 2.1 is not usable for me.(In reply to)No, I'm using the Apache webdav server.2.1 isn't usable for me either. I was hoping it may fix some problem withescaped url's ()(In reply to)I have the same problem accessing a DAV server running on IIS 5.1.In addition, list() also returns the parent, rather than listing its contentsI experienced the same problem using the slide WebDAV client library against subversion 1.1.3.I think I had reported this too. I traced this down to the following:In WebdavResource, line 1086: URIUtil.getName(href) is used to get the name ofthe item; however as per the javadocs this method returns an empty string ifhref ends with a slash. At least with subversion, all hrefs to collections arereported as ending with a slash; therefore no sub-collections are ever reportedby the WebdavResource.listWebdavResources() or associated methods.My fix: replace URIUtil.getName(href) with WebdavResource.getName(href) thisseems to work fine for me; although as I said I am not sure what, if any,implications this might have on other functionality.(In reply to)I tried this solution because I saw your post before reporting the bug.It didn't solve my problem.(In reply to)Thanks for this suggestion. I too looked into the source code and found this tobe the problem. But the solution I used was to add the following lineshref = href.endsWith("/") ? href.substring(0,href.length() - 1) : href;href=href.replaceAll("\\[","%5B");href=href.replaceAll("\\]","%5D");href=href.replaceAll("\\|","%7C");href=href.replaceAll("\\^","%5E");href=href.replaceAll("\\`","%60");This I found to work perfectly. And the reason for the replaces statments is toencode all special charaters that the exchange server might return. I supposethat this would be better done with the URLEncoder.This seems to be a long standing issue and because the fix is so simple I do notknow why it has not been implemented yet.I will be using my modified class until the changes are implemented in theofficial source code.Fix committed to HEAD. If no one finds any problems with it I'll backport it tothe 2.1 branch.I used Robert's fix, but I tweaked the getName() method a bit. It was decodingthe name which made anything with spaces in it invalid.***has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ***Thanks indeed, today's cvs version of webdavclient runs smoothly (2.2pre1).I think I'll wait for 2.2 to be released, instead of getting 2.1 patched (I'lljump over it!) regards,canrullWith regard toI was wondering what version of the Slide client API the fix was put in, as I get the same error described in this bug with the 2.1 binary version I downloaded off the website today. I assume I'd need to use a 2.2 beta or release candidate(?)More info:Folder structure is below ('+' denotes folder, '-' denotes file. Note: these symbols are not part of the file name):+NewFolder1 +A New Folder +SubSubFolder -MyDocsOnline.txt -TestFileUpload.txt -MyDocsOnline.txt -TestFileUpload.txt +NewFolder3 -MyDocsOnline.txt -TestFileUpload.txt -MyDocsOnline.txt -TestFileUpload.txt +NewFolder -MyDocsOnline.txt -TestFileUpload.txtThe code I used to test is below: HttpURL url = new HttpURL(""); url.setUserinfo("", "TUSu_618"); WebdavResource wdr = new WebdavResource(url); recurseResources(wdr, 0); public void recurseResources(WebdavResource parent, int depth) throws Exception { int MAX_DEPTH = 3; if(depth > MAX_DEPTH || parent == null) return; WebdavResource[] wdrsArray = parent.listWebdavResources(); for(WebdavResource r : wdrsArray) { String s = r.isCollection() ? "[COLLECTION] " : "[FILE] "; System.out.println(repeat("\t", depth) + s + r.getDisplayName()); if(r.isCollection()) recurseResources(r, depth + 1); } } public String repeat(String s, int n) { StringBuffer sb = new StringBuffer(); for(int i = 0; i < n; ++i) sb.append(s); return sb.toString(); }The output was:[FILE] TestFileUpload.txt[FILE] MyDocsOnline.txt[COLLECTION] NewFolder2 [FILE] TestFileUpload.txt [FILE] MyDocsOnline.txt [COLLECTION] NewFolder2 [FILE] TestFileUpload.txt [FILE] MyDocsOnline.txt [COLLECTION] NewFolder2 [FILE] TestFileUpload.txt [FILE] MyDocsOnline.txt [COLLECTION] NewFolder2I bumped into the problem as well. 2.1 didn't work for me.A solution was to change a part of WebdavResource.setWebdavProperties afterif (!itself) {String myURI = httpURL.getEscapedURI();...toif (!itself) { String myURI = httpURL.getEscapedURI(); final String adjustedHref = href.endsWith("/") ?href.substring(0, href.length() - 1) : href; final String name = URIUtil.getName(adjustedHref); char[] childURI = (myURI + (myURI.endsWith("/") ? "" : "/") + name).toCharArray();the problem is that children's hrefs can end with '/' and in this caseURIUtil.getName returns empty string. Because of that childURI is the same asmyURI (bug!!!)I bumped into the problem as well. 2.1 didn't work for me.A solution was to change a part of WebdavResource.setWebdavProperties afterif (!itself) {String myURI = httpURL.getEscapedURI();...toif (!itself) { String myURI = httpURL.getEscapedURI(); final String adjustedHref = href.endsWith("/") ?href.substring(0, href.length() - 1) : href; final String name = URIUtil.getName(adjustedHref); char[] childURI = (myURI + (myURI.endsWith("/") ? "" : "/") + name).toCharArray();the problem is that children's hrefs can end with '/' and in this caseURIUtil.getName returns empty string. Because of that childURI is the same asmyURI (bug!!!)(In reply to)Thanks Igor! Everything works fine now. Also thanks to Robert and Philip. Forthose who want a fast cut-and-paste solution, simply replace the entire protected void setWebdavProperties(Enumeration responses)method in org.apache.webdav.lib.WebdavResource with the following code:// START: FIX ///////////////////////////////////////////////////// /** * Set WebDAV properties following to the given http URL. * This method is fundamental for getting information of a collection. * * @param responses An enumeration over {@link ResponseEntity} items, one * for each resource for which information was returned via PROPFIND. * * @exception HttpException * @exception IOException The socket error with a server. */ protected void setWebdavProperties(Enumeration responses) throws HttpException, IOException { // Make the resources in the collection empty. childResources.removeAll(); while (responses.hasMoreElements()) { ResponseEntity response = (ResponseEntity) responses.nextElement(); boolean itself = false; String href = response.getHref(); if (!href.startsWith("/")) href = URIUtil.getPath(href); href = decodeMarks(href); /* * Decode URIs to common (unescaped) format for comparison * as HttpClient.URI.setPath() doesn't escape $ and : chars. */ String httpURLPath = httpURL.getPath(); String escapedHref = URIUtil.decode(href); // Normalize them to both have trailing slashes if they differ byone in length. int lenDiff = escapedHref.length() - httpURLPath.length(); int compareLen = 0; if ( lenDiff == -1 && !escapedHref.endsWith("/")) { compareLen = escapedHref.length(); lenDiff = 0; } else if ( lenDiff == 1 && !httpURLPath.endsWith("/")) { compareLen = httpURLPath.length(); lenDiff = 0; } // if they are the same length then compare them. if (lenDiff == 0) { if ((compareLen == 0 && httpURLPath.equals(escapedHref)) || httpURLPath.regionMatches(0, escapedHref, 0, compareLen)) { // escaped href and http path are the same // Set the status code for this resource. if (response.getStatusCode() > 0) setStatusCode(response.getStatusCode()); setExistence(true); itself = true; } } // Get to know each resource. WebdavResource workingResource = null; if (itself) { workingResource = this; } else { workingResource = createWebdavResource(client); workingResource.setDebug(debug); } // clear the current lock set workingResource.setLockDiscovery(null); // Process the resource's properties Enumeration properties = response.getProperties(); while (properties.hasMoreElements()) { Property property = (Property) properties.nextElement(); // ------------------------------ Checking WebDAV properties workingResource.processProperty(property); } String displayName = workingResource.getDisplayName(); if (displayName == null || displayName.trim().equals("")) { displayName = getName(href); } /** BUGGY CODE if (!itself) { String myURI = httpURL.getEscapedURI(); char[] childURI = (myURI + (myURI.endsWith("/") ? "" : "/") + URIUtil.getName(href)).toCharArray(); HttpURL childURL = httpURL instanceof HttpsURL ? new HttpsURL(childURI) : new HttpURL(childURI); childURL.setRawAuthority(httpURL.getRawAuthority()); workingResource.setHttpURL(childURL, NOACTION, defaultDepth); workingResource.setExistence(true); workingResource.setOverwrite(getOverwrite()); } */ /** FIX ********/ if (!itself) { String myURI = httpURL.getEscapedURI(); /** Checks if href contains trailing '/', and if so removes it. This ensures URIUtil.getName does not return an empty String when we don't want it to. Seefor more information. */ String fixedHref = href.endsWith("/") ? href.substring(0, href.length() - 1) : href; char[] childURI = (myURI + (myURI.endsWith("/") ? "" : "/") + URIUtil.getName(fixedHref)).toCharArray(); HttpURL childURL = httpURL instanceof HttpsURL ? new HttpsURL(childURI) : new HttpURL(childURI); childURL.setRawAuthority(httpURL.getRawAuthority()); workingResource.setHttpURL(childURL, NOACTION, defaultDepth); workingResource.setExistence(true); workingResource.setOverwrite(getOverwrite()); } /**************/ workingResource.setDisplayName(displayName); if (!itself) childResources.addResource(workingResource); } }// END: FIX /////////////////////////////////////////////////////The code is based on Igor's solution and should work fine. Please post here ifyou stumble upon any issues the fix causes. In the meantime however, itsworking perfectly for me.Cheers!Mike N. ChristoffI am still having problem with this solution. I now have deleted folders beeinglisted with the listWebdavResources call...(In reply to)(In reply to)	17.0	id=49891	9	False	False	bodewig	1
id=54835	REOPENED	None	Ant	Optional Tasks (	1.9.7	All All	P2 regression	Ant Notifications List	2013-04-12 22:36 UTC by	andrew cooke	2016-12-04 13:49 UTC (	5 users	I don't fully understand the history of classpath loading, but atitem 5 says "Leave ant-junit.jar in its default location in ANT_HOME/lib but include junit.jar in the <classpath> passed to <junit>. (since Ant 1.7)" - that appears not to work.I have a detailed SO question atbut to reproduce simply:hg clone ssh:///isti/java-examplecd java-exampleant testand you should see:BUILD FAILED/home/andrew/project/guice/hg/build.xml:33: java.lang.NoClassDefFoundError: junit/framework/TestListener at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:791)... at org.apache.tools.ant.launch.Launcher.run(Launcher.java:280) at org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)Caused by: java.lang.ClassNotFoundException: junit.framework.TestListener at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355)...(the above project uses ant; alternatively copy junit-4.8.2.jar to ./lib and delete the ivy task from the build.xml).	Correction: "(the above project uses ant; alternatively" should be "(the above project uses IVY; alternatively..."I'm not sure this is a good bug report, sorry - the details are mainly at the SO question. If you'd prefer me to repeat them here I can do so. Basically - I provide a classpath to the junit task, but it still gives a class not found error.I am now working on this repo. The example referred to above is tagged as "classpath problem" so to check out please do:hg clone ssh:///isti/java-examplecd java-examplehg update -r "classpath problem"Thanks, AndrewThings are clearer now. The following:hg clone ssh:///isti/java-examplecd java-examplehg update -r "works with 1.8.4"ant bootstrap testworks (gives an error because my test is missing a constructor) with Ant 1.8.4 but fails with Ant 1.9.0.This is with a "virgin" Ant install. The "bootstrap" copies the Ivy jar into ~/.ant/lib and then "test" downloads Junit jar to the local lib directory.With Ant 1.8.4, having the Junit jar in the classpath for the junit task works just fine. As documented at(item 5). But with Ant 1.9.0 this code DOES NOT WORK.Michael?It looks like the class search is being delegated to the System's classloader rather than being handled by the Ant Classloader (the system Classloader has no knoweldge of JUnit since JUnit isn't on the core Ant Classpath, but is added to the JUnit task by Ivy). Given some JUnit classes must have already been loaded to have reached this point, the Ant Classloader can see the JUnit jars loaded by Ivy, but the Split Classloader seems to be delegating incorrectly when trying to load classes used by the JUnit Runner.I suspect the list of 'split classes' in JUnitTask needs expanded to include the classes added for the @Ignore changes introduced in Ant 1.9.0. I'll do a bit more investigation on this issue to see if there are any other issues caused by JUnit being inherited from an external source rather than being in the ANTLIB directory whilst I fix this issue.In the meantime, copying your JUnit JAR into your ANTLIB directory will allow Ant to pickup the dependent classes required to launch the JUnit task if you want to continue using Ant 1.9.0.Thanks. I've switched back to 1.8 for now (I know that I can fix it by moving jars, but the motivation is to have the build bootstrap itself completely to simplify the process for other users).Fixed under SVN.Looks like this bug only affected calls to <junit> with fork="false" (the unfortunate default), which should be considered deprecated as far as I am concerned; with fork="true" it is reported to work in 1.9.0.I'm still seeing this problem as of ant 1.9.1I have a unit test flagged as such:@RunWith(SpringJUnit4ClassRunner.class)and an ant test target that contains <junit printsummary="yes" haltonfailure="yes" fork="true" forkmode="once" tempdir="${report.tests.dir}"> <classpath refid="test.classpath.run"/> <formatter type="plain"/> <batchtest fork="yes" todir="${report.tests.dir}"> <fileset dir="${src.tests.dir}"> <include name="**/*IntegrationTestCase*.java"/> </fileset> </batchtest></junit>my classpath is built via ivy.The ant test run does not honor the 'RunsWith' annotation. A stack trace demonstrates this. Running in eclipse works fine.Actually, I haven't the vaguest idea. I've tried using ant 1.8.4 with and without the junit4 jar in the lib directory, as well as ant 1.9.1. I'm flumoxed. This may very well be my stupidity someplace.Totally my bad.. I didn't see that this particular case I was fiddling with extended junit3 TestCase. Time to go shave heads.. mine and someone else's.It looks like this fix actually broke JUnit4 for me. Everything up to (and including) 1.9.0 works fine, 1.9.1-1.9.3 do not.I created a fairly minimal test case at my git repositorygit clonecd ant-1.9-junit.gitant test(I tested 1.9.0 by adding it to the test-lib directory and setting includeantruntime="false" for the JUnit task in the ant build file)actually, only providing ant-junit-1.9.0.jar in the test-lib folder is sufficient to make it work againRe-closing issue since it can't be replicated with the test case in #13 on a clean build of Ant 1.9.6.As per ant versionC:\Users\fandre\Documents\git\javacc>ant -versionApache Ant(TM) version 1.9.7 compiled on April 9 2016this issue is always present with the script below <junit printsummary="yes" haltonfailure="no"> <classpath> <pathelement location="classes" /> <pathelement location="junit" /> <pathelement location="lib/junit.jar" /> </classpath> <batchtest fork="no" todir="."> <fileset dir="junit"> <include name="**/*Test.*" /> </fileset> <formatter type="failure"/> <!-- I want to see something ... --> <formatter type="plain" usefile="false"/> </batchtest> </junit>unittest: [delete] Deleting directory C:\Users\fandre\Documents\git\javacc\test.tmp [mkdir] Created dir: C:\Users\fandre\Documents\git\javacc\test.tmp [delete] Deleting directory C:\Users\fandre\Documents\git\javacc\junit [mkdir] Created dir: C:\Users\fandre\Documents\git\javacc\junit [javac] Compiling 6 source files to C:\Users\fandre\Documents\git\javacc\junitBUILD FAILEDC:\Users\fandre\Documents\git\javacc\build.xml:229: Using loader AntClassLoader[C:\Apache Software Foundation\apache-ant-1.9.7\lib\ant-launcher.jar;C:\Apache Software Foundation\apache-ant-1.9.7\lib\ant.jar;C:\Apache Software Foundation\apache-ant-1.9.7\lib\ant-junit.jar;C:\Apache Software Foundation\apache-ant-1.9.7\lib\ant-junit4.jar;C:\Users\fandre\Documents\git\javacc\classes;C:\Users\fandre\Documents\git\javacc\junit;C:\Users\fandre\Documents\git\javacc\lib\junit.jar] on class org.apache.tools.ant.taskdefs.optional.junit.FailureRecorder: java.lang.NoClassDefFoundError: junit/framework/TestListener at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at org.apache.tools.ant.AntClassLoader.findBaseClass(AntClassLoader.java:1407) at org.apache.tools.ant.AntClassLoader.loadClass(AntClassLoader.java:1085) at org.apache.tools.ant.util.SplitClassLoader.loadClass(SplitClassLoader.java:58) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.apache.tools.ant.taskdefs.optional.junit.FormatterElement.createFormatter(FormatterElement.java:287) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeInVM(JUnitTask.java:1617) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:1021) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:2105) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:832) at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:293) at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) at org.apache.tools.ant.Task.perform(Task.java:348) at org.apache.tools.ant.Target.execute(Target.java:435) at org.apache.tools.ant.Target.performTasks(Target.java:456) at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1405) at org.apache.tools.ant.Project.executeTarget(Project.java:1376) at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41) at org.apache.tools.ant.Project.executeTargets(Project.java:1260) at org.apache.tools.ant.Main.runBuild(Main.java:854) at org.apache.tools.ant.Main.startAnt(Main.java:236) at org.apache.tools.ant.launch.Launcher.run(Launcher.java:285) at org.apache.tools.ant.launch.Launcher.main(Launcher.java:112)Caused by: java.lang.ClassNotFoundException: junit.framework.TestListener at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 49 moreCreatedjunit.jar: contentCreatedjunit.jarWjunit classpath resolution for junit.jar is working with ant 1.9.3.Did not test with 1.9.4 or 1.9.5 or 1.9.6.Wjunit classpath resolution for junit.jar is working with ant 1.9.3.Did not test with 1.9.4 or 1.9.5 or 1.9.6.	20.0	id=54547	8	False	False	kymberly.mack	1
id=36184	REOPENED	None	Batik - Now in Jira	SVG Rasterizer (	1.8	Macintosh other	P3 normal	Batik Developer's Mailing list	2005-08-15 03:51 UTC by	Joshua Randall	2007-10-30 18:19 UTC (	1 user	When the same clipPath element is applied to multiple graphics objects, every object after the first one has the clip-path applied translated in the same amount in both the x and y axes. Thus, trying to use a clip-path to "cut-out" a region from a stack of graphics objects results in the bottommost object showing through on the bottom and right edges of the cut-out portion.This occurs in Squiggle in both the release version 1.6 and the 1.6+ build made on 05-08-06.	CreatedTestcase to demonstrate bugThis SVG should appear as a green 32x24 rectangle centered inside a blue 128x96rectangle. In Batik's squiggle browser, red appears (from an underlying rectangle) alongthe left and top edges of the blue rectangle and along the right and bottomedges of the green rectangle. Also, the blue rectangle overflows over theright and bottom edges and part of the green rectangle is covered by the bluerectangle.I've tried your example on windows and on Mac OS 10.3 (With JDK 1.4.2_05-141.4)It displayed flawlessly on both.If you want to provide additional information to assist me in reproducing it pleasereopen the bug.This is actually on Mac OS X 10.4.2 (was not on the list of choices, and I forgot to add it in the comment).I was using JDK build 1.4.2_07, but I have just tried with 1.3.1_15 and 1.5.0_02 and they all seem to exhibit the same problem.I also tried it on a windows machine, and did _not_ have the problem, so it seems to be platform specific to OS X 10.4.I will try to find another 10.4.2 machine to verify it on so that I can be sure it isn't something about my specific configuration.This bug has been verified on a colleague's machine also running 10.4.2.I have verified that this bug is also present on 10.4.1The 1.5 JRE on OS X 10.5 now uses Sun's rendering pipeline instead of Quartz, so there's a good chance this bug does not occur on Leopard. I don't have 10.5 to test out with, though, but if you do it would be good to know if it resolves this issue.	6.0	id=56460	11	False	False	gius92	1
id=43411	REOPENED	None	Batik - Now in Jira	SVG Viewer (	1.8	PC All	P3 minor	Batik Developer's Mailing list	2007-09-17 14:37 UTC by	Archie Cobbs	2009-03-14 19:32 UTC (	1 user	Running squiggle on this input exposes some kind of rendering bug.--------------------- CUT HERE ------------------------<?xml version="1.0" encoding="UTF-8"?><svg xmlns="" xmlns:xlink="" width="480.0" height="240.0" viewBox="0 0 48.0 24.0" version="1.0"> <text> <tspan fill="black" font-size="20" id="node" y="20"font-family="'Arial'">55:55</tspan> </text></svg>--------------------- CUT HERE ------------------------See attached screenshot.Runtime information: $ java -version java version "1.5.0_06" Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_06-b05) Java HotSpot(TM) Client VM (build 1.5.0_06-b05, mixed mode, sharing)	CreatedScreenshot of the rendering problemthis is indeed an interesting bug: 1.) it is reproducible - good2.) it shows up only with the example's height of viewport3.) it shows up only with 'Arial' font of that size4.) it shows up only where a '5' is involved.i think, it is a numeric precision problem, which confuses the winding-rule whenthe scan-line passes through that corner of the glyph.i work on it.CreatedTest caseConverted the copy+paste example to a test case (which can be, for example, browsed directly using Squiggle's "Open Location..." option of "File" menu). :-)I took the opportunity to confirm the issue's reproducibility using updated software versions: - Batik: trunk (rev. 718269) - Java 1.6 update 10 - Windows XP SP3More suggestions regarding this particular issue: - Is this a font-related issue? Can this be reproduced simply using a straight SVG path (oblique line followed/preceded by a curve) simulating the corner present in the "5" glyph? - Is this issue completely operating-system independent? At least, it was reported as being Linux, I've also confirmed it using Windows (MacOS anyone?) Changing to "All" until proved otherwise, as there is no "Linux+Windows" OS option. ;-) - Is this issue located within Batik, external libraries or may it be a Java issue?Also, lowering severity and priority to more reasonable (IMHO, hope I'm not sounding too intrusive!) values, given the difficulty to reproduce (fades away just by slightly change the zoom level or Squiggle's window size) and the minor visual glitch it causes.fixed with svn 731265(In reply to)I suspect that this simply moved where the problem occurs. This is almost certainly a bug in the JDK rasterizer. Thesesorts of bugs almost always occur only under a very specificcombination of values. Going to double changed the valuesjust slightly so the problem didn't happen in this case.I'm certain the problem still exists (it may be almostimpossible to find but it's there somewhere). Ideally we would print out the relevant information(Scale value we passed and the path of the '5' char)which might allow us to construct a standalone exampleof the bug outside of Batik so it could be submittedto Sun.tja - somehow you might be right. and i might collect the data and construct a simplified case for their big bugbase. However:- do we keep our bugs open for sun's ?- i studied the values going into graphics2d: the glyphvector holds only float-values, and the scale given previously just holds float-precision. so when the path-elements are finally converted to their internal coordinate-system to have the scan-lines probing the outlines, it doesnt have enough 'resolution' to produce correct results.- it is an interesting mind-puzzle to imagine if there is a constallation, in which the scale-value with just float-precision is the 'correct' value. This might exist, but it is (i assume) far away from our test-case: we have scaling in the font-size and in the canvas, and both influence the rasterizer. i would close this until someone comes up with another example.(In reply to)The fill of a shape should _never_ result is this rendering.A fill is always closed if it wasn't originally. So that single pixelhigh solid fill line is a bug in the coverage calculation in the rasterizer.Trust me I've worked on rasterizers and that failure is a clear indicationof incorrect coverage calculation.Generally neither the float or double is correct, the double maybe closer but is rarely correct. Since the source content is a TrueTypefont in the case it only has ~11 bits of precision anyway (em square of2048) a float value as ~25 bits of mantissa.I'd rather back the commit out (as it involves a fairly large number offloat casts "to no end") and close the bug saying it's not a bugin Batik but a bug in the JDK - if you really want to close the bug. All the commit has done is move the bug from a where we can reproduce it to someplace we can't reproduce it. If Archie wants to keep the patch because it helps him he is welcome to.CreatedStandalone example of bug.This is a standalone example of the bug thatshows the problem is with the JDK not Batik.hmm - what a nice program...anyway - when you correct the scale-calc in line 48 to a double - therendering problem goes away.if you dont want to change your code - there are other ways to fix it:- the rendering-problem is reproducible (with float in 48):jdk142_13 yesjdk150_14 yesjdk160_04 yesjdk160_10 NO !(on windows XP).So, seems their major revamp in u10 fixed that too.(In reply to)Except that "correct" isn't the right term. If you "change" thecalculation in line 48 the rendering problem goes away, but for thecombination of glyph, scale and translate that is used in that examplethe rendering problem exists (the double generated by 'casting' the float calculation is a perfectly valid double). The point here is that the rendering must work for any scale factor and it clearly doesn't, so it's a bug in the renderer. Also I am 100% certain that if it fails here it fails for any numberof other sets of glyphs, scales and translates. That number is smallcompared to the combinations that it works for but I'm sure there aremany that is fails on.That is good news (I had tried 1.6 04 earlier).I think this is the real solution. To be clearchanging the calculation simply fixes 'this'version of the problem it doesn't come close toaddressing the root cause.(In reply to)I've also confirmed that this issue doesn't reproduce in JDK 1.6.0_12-b04.Recently[1] was closed as "works for me" based on this discussion. Should this one also be marked "works for me" or invalid (as it apparently has nothing to do with Batik)?[1](In reply to)I'd also vote for rolling back the changes () and close this issue as invalid.	13.0	id=40826	7	False	False	sean.mullan	1
id=48634	REOPENED	None	Batik - Now in Jira	GVT (	1.7	PC Windows XP	P3 major	Batik Developer's Mailing list	2010-01-28 00:27 UTC by	Nicolas Labrot	2010-02-15 18:54 UTC (	0 users	I'm using JPEGTranscoder in a webapp.I call the LRUCache.setsize at init of my webapp. It flushed the LRUCache.used list and set the LRUCache.maxsize variable to 0. The LRUCache.free list remains at 50 (which it initizialed by TileCache) which is in my opinion not logic.The LRUCache.add method test the size of the LRUCache.free list to cache new item to the LRUCache.used list.The LRUCache.used list grows up to 50. The LRUCache.setSize method is not able to flush the LRUCache.used list because there is if statement (inferior strict) on the maxsize (at 0) value against the new size (at 0).If think a workaround would be to call LRUCache.setSized twice, the first with value > 50 and the second with 0	(In reply to)Could you help checking whether this issue is still present in the current trunk code [1]?If the issue is still present, as you seem to have enough knowledge on how to fix it, I'd invite you to attach a patch into this bug report. That would definitely help towards getting the bug fixed in the next release! :-)BTW: I have little knowledge regarding this code: Thomas [2] is probably the person which can actually provide valuable feedback on this issue. I'm trying to put things moving in the meantime, though. ;-)Thanks, Helder[1][2]Just noticed this was first discussed/reported in the mailing list. The issue's URL now points at the thread. :-)(In reply to)I will post a patch. Does a test cover the LRUCache class ?(In reply to)Great! Thanks for considering a contribution. :-)A quick find-in-files didn't show up anything related so I guess not. Of course if you can make up one (intuitively, to be setup in "test-resources/org/apache/batik/ext/awt/image/rendered") I guess that would be valuable. ;-)Cheers, HelderCreatedCorrect the setSize and the add methodsHere is the patch. I hope the format is correct.The complete regard test do not run correctly because of an exception :Error while running test suite : Cannot build Test instance of class : . Got exception of class : java.lang.IllegalArgumentException with messages test-references/../../beSuite/rendering-orderGr-BE-01.svg and stack trace : java.lang.IllegalArgumentException: test-references/../../beSuite/rendering-orderGr-BE-01.svg at org.apache.batik.test.svg.AbstractRenderingAccuracyTest.resolveURL(Unknown Source)Hi nithril,Why did you mark the issue as fixed? The path didn't get commit yet! ;-) Or am I missing something...?I was hoping Thomas could take a glance at this, because I really can't even give valuable feedback to the patch.(In reply to)(In reply to)Sorry I do not know the workflow... :)I thought I need to mark it resolved and after somebody verfied the resolution and closed the issueI made a mistake, VCS need to be updated before.(In reply to)Exactly [1]! ;-)Let's hold for some extra feedback (possibly from Thomas) in order to proceed. :-)[1](In reply to)Can you fix the patch so that it doesn't include changes in indention?The only changes in a patch should be the real code changes. Also canI suggest that you drop 'free' nodes in preference to 'used' nodeswhen setting the size? I took a look at the new regard test and it looks pretty good. Also do you have an Apache Contributors License on file?(In reply to)I will fix the patch and implement your suggestion.I have send today the CLA by fax.CreatedPatch on LRUCacheMy IDE automaticaly right trim lines. The patch contains trim modification for regard.xml with the new test.If there is any new modifications to do, please ask me(In reply to)I still noticed a few whitespace modification but they now point only towards trimmed trailing spaces, which I believe to be good. :-)Without a deep idea on what's involved, I did notice a few small things:1. typo: "inconsistant" (should be "inconsistent", found in properties file and related source code. ;-)2. Whitespace issues found in "sources/org/apache/batik/ext/awt/image/rendered/LRUCache.java": The file seems to be using 8 spaces for indenting instead of the usual 4 spaces. This is somehow inconsistent with the framework but, IMHO, should be fixed maybe in a follow-up commit and *not* as part of this patch. :-)I've also fixed the bug title and bug keywords for grammar and to reflect the availability of a patch. ;-)(In reply to)Also, I just noticed "Nicolas LABROT" listed in the "Unlisted CLAs" [1] so, whenever this patch is ready... :-)[1](In reply to)Funny because I reconfigure my IDE to reformat the code with 8 spaces like the original file ;)Thank you ;)	15.0	id=32886	6	False	False	masonjm	1
id=8867	REOPENED	None	Apache httpd-2	Build (	2.0.46	Sun Solaris	P3 critical	Apache HTTPD Bugs Mailing List	2002-05-07 13:10 UTC by	Jeff Stavsky	2012-07-26 11:39 UTC (	15 users	I am successful doing configure (without any additional modules) and make with httpd 2.0.36. The problem occurs when doing make install. Here is part of the output:exports.c:900: `ap_hack_apr_xml_quote_string' previously defined hereexports.c:2097: redefinition of `ap_hack_apr_xml_quote_elem'exports.c:901: `ap_hack_apr_xml_quote_elem' previously defined hereexports.c:2098: redefinition of `ap_hack_apr_xml_insert_uri'exports.c:902: `ap_hack_apr_xml_insert_uri' previously defined heremake: Fatal error: Command failed for target `exports.lo'Current working directory /aleph1/product/httpd-2.0.36/servermake: Fatal error: Command failed for target `install-recursive'Current working directory /aleph1/product/httpd-2.0.36/servermake: Fatal error: Command failed for target `install-recursive'This output is from the gcc command that follows the nawk command:nawk -f /aleph1/product/httpd-2.0.36/build/make_exports.awk /aleph1/product/httpd-2.0.36/include/*.h /aleph1/product/httpd-2.0.36/os/unix/*.h /aleph1/product/httpd-2.0.36/srclib/apr/include/*.h /aleph1/product/httpd-2.0.36/srclib/apr-util/include/*.h /aleph1/product/httpd-2.0.36/modules/http/*.h > exports.c/bin/bash /aleph1/product/httpd-2.0.36/srclib/apr/libtool --silent --mode=compile gcc -g -O2 -pthreads -DSOLARIS2=8 -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT-DAP_HAVE_DESIGNATED_INITIALIZER -I. -I/aleph1/product/httpd-2.0.36/os/unix -I/aleph1/product/httpd-2.0.36/server/mpm/prefork -I/aleph1/product/httpd-2.0.36/modules/http -I/aleph1/product/httpd-2.0.36/modules/proxy -I/aleph1/product/httpd-2.0.36/include -I/aleph1/product/httpd-2.0.36/srclib/apr/include -I/aleph1/product/httpd-2.0.36/srclib/apr-util/include -I/aleph1/product/httpd-2.0.36/modules/dav/main -I/aleph1/product/httpd-2.0.36/srclib/apr-util/xml/expat/lib -prefer-non-pic -static -c exports.c && touch exports.loFollowing is some info on the relevant machine:ram12=M505>>gcc -vReading specs from /usr/local/lib/gcc-lib/sparc-sun-solaris2.8/2.95.2/specsgcc version 2.95.2 19991024 (release)ram12=M505>>uname -aSunOS ram12 5.8 Generic_108528-03 sun4u sparc SUNW,UltraSPARC-IIi-EngineThanks for your coop,Jeff	Could you please attach your copy of exports.c?Thanks!***has been marked as a duplicate of this bug. ***Could you please attach the broken exports.c?Thanks!CreatedAttached it exports.c for Apache 2.0.36 - Same problem found with Apache 2.0.39I am having a similar problem building 2.0.40 under SGI IRIX 6.5.My make process does not get as far, and reports the following :Making all in prefork /bin/sh /data1/local/apache3/srclib/apr/libtool --silent --mode=compile cc -g -D_POSIX_THREAD_SAFE_FUNCTIONS -I/data1/local/apache3/srclib/apr/include -I/data1/local/apache3/srclib/apr-util/include -I/data1/local/apache3/srclib/apr-util/xml/expat/lib -I. -I/data1/local/apache3/os/unix -I/data1/local/apache3/server/mpm/prefork -I/data1/local/apache3/modules/http -I/data1/local/apache3/modules/filters -I/data1/local/apache3/modules/proxy -I/data1/local/apache3/include -I/data1/local/apache3/modules/dav/main -prefer-non-pic -static -c exports.c && touch exports.lo cc-1144 cc: ERROR File = exports.c, Line = 1473 The variable "ap_hack_apr_allocator_create" has already been initialized. const void *ap_hack_apr_allocator_create = (const void *)apr_allocator_create; ^ MANY MORE ERRORS ALONG THE SAME LINES "already been initialised"....until:cc-3452 cc: ERROR File = exports.c, Line = 1635 The compilation is aborted due to the number of errors. 101 errors detected in the compilation of "exports.c". *** Error code 1 (bu21) *** Error code 1 (bu21) *** Error code 1 (bu21) It appears that make_exports.awk pulls in a number of header files, some from the apr and some from the server subsystems, that have the same objects defined.I don't know enough about these inclusions to attempt changing them - Please help...Thanks,Gary Farley[This is a mass bug update.]This bug reports a problem in an older version of Apache 2.Could you please update to the most recent version and seeif you can reproduce this problem. If the bug still exists,please update the bug with the latest version number. If the bug no longer exists, please close the bug report.Sorry for this impersonal response, but we get many more bugreports than our volunteers can keep up with.Thanks for using Apache!Not applicable anymore.Have a later version installed.I'm trying to upgrade to 2.0.45 as per the instructions to do so before 8th andhaving the error messages appear under Solaris 8. I'm having no problemscompiling 2.0.39, 2.0.43 and 2.0.44 on the same machine using the exact samesettings so this is new to 2.0.45. I read the changelog and it said there's anew apr included with the .45, is this related?Sorry, didn't change the version to target 2.0.45 in the previous submit.Ok I found the cause of the problem.If the path to Apache source has a symlink, the code that generates exports.cuses both the absolute path (symlink parsed out) and the path with the symlinkduring the generation process. The end result is that some files (namely aprstuff) is included twice into exports.c.You can replicate this by downloading the source, decompress the source, thensymlink the source "ln -s httpd-2.0.45 httpd", "cd http", "./configure", "make".The make fill fail and complain about duplicate apr symbols in exports.c. If youlook at exports.c, you'll see all apr-related files have been included usingboth the "httpd-2.0.45" and the "httpd" paths.It's probably just a line or two that needs to be changed to fix this.***has been marked as a duplicate of this bug. ***I've also some problems with Apache 2.0.45. I am trying to build in my homedirectory and after some investigations, i've noticed that APR_INCLUDEDIR andAPU_INCLUDEDIR include my home directory twice with two different pathname(because the users directory are in /users and are also symlinked in /home)This problem may also occur when building on NFS. (See bug report #19229).On my box the problem is the same. Here my system:SunOS ife.ee.ethz.ch 5.9 Generic_112233-04 sun4u sparcgcc 3.2.2 and Forte cc 7.0I wrote a little perl script that converts the defect exports.c. After applyingthis script httpd compiled fine:#!/usr/bin/perl -wuse strict;$|++;# httpd 2.0.45/server/exports.cmy %hash; # Zeile -> Nummerprint "/* $0: ", scalar(localtime), " */\n";while(<STDIN>){ if (/^const/) { chomp; if (defined $hash{$_}) { print "/* $hash{$_}: $_ */\n"; } else { $hash{$_} = $.; print "$_\n"; } } else { print; }}With 2.0.46 this is even messier: Up to 2.0.45 a workaround was cd `/bin/pwd -P`(cd to the physical path) before running configure. This does no longer help.***has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ***This bug still occurs on Solaris 8 with httpd-2.0.46. It seems that the /bin/shbuiltin version of 'pwd' automatically resolves symlinks. I do *not* have'realpath' installed, as indicated might help in apr-config:----# If we have the realpath program, use it to resolve symlinks# Otherwise, being in a symlinked dir may result in incorrect output.if test -x "`which realpath 2>/dev/null`"; then...----What seems to be happening to cause *both* paths to get included is this:() $thispath gets set to the symlink-resolved path() $thispath is compared with $APR_SOURCE_DIR, which is set by configure and isnot symlink-resolved. While they should match, they do not.() because of the mismatch, $location is set to 'build' while it should be 'source'.() checking under the --includes case in the command-line switch, we see thatthis choice of $location is interpreted to be a VPATH build, so both$thisdir/include and $APR_SOURCE_DIR/include are returned, giving duplicate headers.This is fairly easy to work around by just compiling in /tmp or something. Ifthere are issues (beyond this one) with compiling in a symlinked directory,perhaps a warning in INSTALL or during configure would be enough to resolve thisbug.***has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ***I found out that the problem is also if httpd source dir is in a symlinked dir: on Solaris, by default /usr/src is a symlink to /usr/share/src.So, exports.c makes a problem with duplicate definitiopns, as said.The solution: have httpd source unpacked freshly in a normal directory, that is not a symlink to anything (such as /tmp).From the autoconf manual:--POSIX 1003.1-2001 requires that pwd must support the `-L' ("logical") and `-P'("physical") options, with `-L' being the default. However, traditional shellsdo not support these options, and their pwd command has the `-P' behavior.Portable scripts should assume neither option is supported, and should assumeneither behavior is the default. Also, on many hosts `/bin/pwd' is equivalent to`pwd -P', but POSIX does not require this behavior and portable scripts shouldnot rely on it.--Perhaps a configure test could be added that tries -L, and uses it wheresupported (for example, on Solaris 8!)Email/comment if you'd like me to do this and submit a patch.***has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ***CreatedProposed workaround for duplicate definitions in exports.cFixing the apr-config problem is difficult, fixing the symptom which causes thisissue is relatively simple though: can someone with the symlinked build problemtry the patch to make_exports.awk attached above, or download the pre-patchedversion from:***has been marked as a duplicate of this bug. ***Tried the new make_exports.awk it with Apache 2.0.48, again on Solaris 8 (same machine, as a matter of fact). Worked like a champ. What a nasty, ugly hack! You're not really going to put that into a release, are you? :)Sometimes nasty, ugly hacks are required to get the ugly police off theirbehinds to fix it correctly. Go Joe!!!!***has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ***Joe Orton's modifed make_exports.awk fixed the problem for me as well, buildinghttpd-2.0.50 on Solaris 9.I wasted some time tracking this down yesterday. It was frustrating. Sulkadiscovered the cause of this problem over a year ago. Symlinks are not uncommon,especially with automounting. At the very least, perhaps the INSTALL file couldbe updated to say "NOTE: Apache will not compile if the sources are located in apath with a symlink."CreatedProposed change to INSTALL file.The proper fixes have been backported to the stable branch now, and should makeit into the 2.0.51 release.I checked out trunk, and I stepped into the same issue trying to compile it on my laptop with OsX ... the perl hack above solved it, but still, googling I found lot of people with the same issue but they probably never reached the informations in this ticketI dare to reopen it... feel free to shoot me :)	36.0	id=45135	6	False	True	ivan.veselovsky	1
id=11580	REOPENED	None	Apache httpd-2	mod_dir (	2.0.39	All All	P3 enhancement	Apache HTTPD Bugs Mailing List	2002-08-09 06:20 UTC by	Mark Nottingham	2008-06-13 09:18 UTC (	1 user	The Content-Location HTTP header indicates an alternate location for the entity associated with a message. This much underused facility could be very useful to agents and automated systems like Google, in order to get a richer understanding about the relationship between resources.For example, if mod_dir were to generate a C-L header for requests to "/": Content-Location: /index.htmland likewise requests for /index.html: Content-Location: /Google would know that the representations it stores for each are equivalent, rather than just assuming so by using a heuristic.This is a low-cost way to add some intelligence to the Web. Please consider adding C-L to mod_dir. Thanks!	I don't believe, that it's so useful at all. RFC 2616, 14.14 says: A cache cannot assume that an entity with a Content-Location different from the URI used to retrieve it can be used to respond to later requests on that Content-Location URI.But that is actually, what you want. Further it's probably not desired to sendsomething likeContent-Location: /cgi-bin/generate-index.cgiIf the resource (say, index.html) is content-negotiated, the Content-Locationwill be set anyway (by mod_negotiation, of course).I didn't imply that C-L would have any effect on caches at all, so am confusedby your first remark.As to the interaction with mod_negotiation, I think that's beside the point; ifmod_dir serves content at one location ('/') that's available at another('/index.html'), it should insert a C-L header (configurable, of course).This would also be a smarter way to inform of the preferred absolute URI. Some prefer their websites to be access from www.example.tld, where others prefer having it shortened to example.tld.If I were to prefer www.example.tld, I would normally do a 301 redirect for anyone not accessing my website from the www. prefix domain. Instead, what I should do is to inform users and search engines that my preferred URI for this resource is Content-Location:. When accessed from www.example.tld, a realative URI would suffice.This could maybe be added as a option to mod_negotiate, as that module is used when generating the Content-Location. (I think?)Createdadds new directive to mod_dir for returning CL on URIs that end on '/'(patch against 2.2.8)This patch adds a new directive to mod_dir for returning the Content-Location for URIs that end in '/'. The feature is turned off by default and can be used together with mod_setenvif.Description: Returns the Content-Location for URis that end in '/'Syntax: DirectoryRevealContentLocation on:off [env=var]Default: DirectoryRevealContentLocation offContext: server config, virtual host, directory, .htaccessModule: mod_dirAn example on making the C-L return conditional to the User-Agent:BrowserMatch lwp reveal-clDirectoryRevealContentLocation on env=reveal-clI'll resubmit it against 2.2.9 and trunk in a couple of days. There shouldn't be too much change.-jose	4.0	id=48634	12	True	False	nithril	1
id=17497	REOPENED	None	Apache httpd-2	mod_mime_magic (	2.0.44	All All	P3 normal	Apache HTTPD Bugs Mailing List	2003-02-27 16:18 UTC by	ABE Hiroshi	2005-02-25 08:16 UTC (	1 user	Apache-2.0.44 with mod_mime_magic generates incorrect responses for some filesin Japanese. The problem seems to happen when the requested file includes escape character(Japanese files encoded in ISO-2022-JP always include it) and does not have anyextensions. The following procedure reproduces this problem: 1. generate a file with escape character: % perl -e 'print "Nagoya, the 3rd largest city in Japan, is written as\x1b\$BL>8E20\x1b(B in Japanese."' > file_with_escapes, 2. copy it to somewhere accessible via Apache 3. and request it through Apache with mod_mime_magic. When I tried the procedure above, I got--- snip ----HTTP/1.1 200 OKDate: Apache/2.0.44 (Unix) DAV/2Last-Modified: Sun, 16 Feb 2003 15:05:28 GMTETag: "92611-4e-319e00"Accept-Ranges: bytesContent-Length: 78Connection: closeContent-Type: text/plain ; charset=ISO-8859-1Content-Encoding: (with--- snip ---as HTTP response. I think the value of Content-Encoding header is wrong.Regards, ABE Hiroshi	mod_mime_magic does more or less black magic, which is known not to work alwaysas desired ;-)Sorry, it's the nature of mod_mime_magic to _guess_ the file type. So if theguess is wrong, you have to change the magic file or, even better, configure thefiletype explicitely. So it's all in all a configuration problem, not a bug.Thanks for using Apache.Createdpatch to show the mod_mime_magic.c has a bugI understand there are some case the mod_mime_magic does not work as I want to. But still, I think there is a bug in mod_mime_magic because I think theContent-Encoding header may not have `` (with'' as its value in this case. To show this is bug, I tried the procedure in the first report with attachedpatch. Then I got---- snip ----HTTP/1.1 200 OKDate: Thu, 06 Mar 2003 03:36:07 GMTServer: Apache/2.0.44 (Unix) DAV/2Last-Modified: Thu, 27 Feb 2003 16:15:04 GMTETag: "8f8ba-4e-415b7200"Accept-Ranges: bytesContent-Length: 78Connection: closeContent-Type: text/plain_(with_escape_sequences); charset=ISO-8859-1---- snip ---- The value of ``Content-Type'' header is slightly changed and there is no``Content-Encoding'' header. In my opinion, whitespaces in the header causes this.***has been marked as a duplicate of this bug. ***	4.0	id=8867	30	False	False	rbb	1
id=22484	REOPENED	None	Apache httpd-2	mpm_prefork (	2.0.47	HP HP-UX	P3 major	Apache HTTPD Bugs Mailing List	2003-08-16 19:42 UTC by	vtmue	2006-01-06 19:44 UTC (	2 users	Hello,Basically I run into the problem which is discussed here:But the proposed fix (rising semaphore-related kernel parameters) does not help.We run 11.11 at a fairly recent patch level. A full trace of Apache 2.0.47 untilit's "suicide" is available (given httpd is alive...) at:And the main error log holds:tons of: [emerg] (22)Invalid argument: couldn't grab the accept mutexsome: [emerg] (36)Identifier removed: couldn't grab the accept mutexfew: [emerg] (28)No space left on device: couldn't grab the accept mutexThe symptom does not appear to be related to the number of children. I watchedthe parent die with 46 and another time with 17 child processes. The load of themachine is around 0.2 all the time.To put it straight: I'm stuck and hoping some good soul out there can help!Any hint is appreciated - TIAvt	We worked around the problem by temporarily setting AcceptMutex to fcntl.***has been marked as a duplicate of this bug. ***BTW, the trace provided is just of the parent process, so it doesn't show thesemaphore errors encountered in the children.I don't think this is an Apache or APR problem. (The APR codebase has the codethat uses SysV semaphores.) While there have been at least a few peopleencountering this on HP-UX, semaphore problems that could not be resolved bysystem tuning haven't been reported elsewhere, and presumably many other HP-UXusers are running Apache successfully. Maybe there is further tuning necessaryon your system, maybe you have a bad level of some kernel code, maybe I don'tknow what I'm talking about :)If you want to pursue this further with us, we need a trace that showsApache+APR doing something invalid with the semaphores. If you have OS supportfrom HP, you might describe to them what tuning you performed already and see ifthey have additional recommendations.Jeff,I read about other HP-UX und Solaris users who appear to face the very samesymptom. HP suppplies a compiled binary so many users will stick with this Isuppose.My trace has the capability to follow forks but there are a couple ofshowstoppers here on my side: the affected server is productive and we arerather in the process of downgrading back to 1.3 . Then there are about 170vhosts configured; httpd has approximately 50-60 concurrent active childs duringthe day.One of our first thoughts here was that one of the vhosts may generate an errorthat causes the parent to shut down but we could not confirm this when searchingthe logs. And I have to admit we haven't got the time to trace down this anyfurther right now.We are about to set up an 11i system with current patch level during the nextweek. We can possibly set up 2.0.47 there and see if httperf can reproduce theproblem.Cheers,vtIf there is some fix for this in the HP-supplied binary but not in Apache orAPR, we'd love to hear about it :) I hope that isn't the situation.In the case that a child returned a fatal error which forced a shutdown, thereshould be a message in error_log written by the parent by this code: ap_log_error(APLOG_MARK, APLOG_ALERT, 0, ap_server_conf, "Child %" APR_PID_T_FMT " returned a Fatal error..." APR_EOL_STR "Apache is exiting!", pid->pid);In all likelihood the fatal error was simply the first unexpected ENOSPC fromattempting to acquire the mutex, then that child returned a fatal error, thenthe semaphore got cleaned up, then remaining children that hadn't already dieddue to shutdown started getting EINVAL on their semaphore operations.Hi Jeff,Ok, I have to admit we have those:[Sat Aug 16 16:08:16 2003] [notice] Apache/2.0.47 configured -- resuming normaloperations[Sat Aug 16 16:38:09 2003] [emerg] (28)No space left on device: couldn't grabthe accept mutex[Sat Aug 16 16:38:09 2003] [alert] Child 16480 returned a Fatal error...Apache is exiting![Sat Aug 16 16:38:10 2003] [emerg] (36)Identifier removed: couldn't grab theaccept mutex[...]Unfortunately a colleague deleted the client' logs of that day so... :(Then hp: from what I see in their relasenotes they fixed a bug related tosemaphores/modssl/dbm in 2.0.43 so it seems that is s/th different. Besides Itake it for granted that they'll report problems once they find/fix them.At this time, I'm a bit clueless because I see no way how we could track thisdown. Can you give me a hint where I can read about what could cause a child toproduce an "Fatal error"? (I googled but didn't find s/th hot). I'm willing toinvestigate, but I can't trace 170 vhosts one after the other - many of themusing PHP.Thanks, vtThis first error message from your last error log submission is the entire story:[Sat Aug 16 16:38:09 2003] [emerg] (28)No space left on device: couldn't grabthe accept mutexThe kernel failed the semaphore acquire. If you can't fix it with OS tuning,than avoid it with "AcceptMutex fcntl" or some other mutex type.***has been marked as a duplicate of this bug. ***Not a problem in httpd or APR as far as anyone can tell... If OS tuning can'tresolve the problems, then use AcceptMutex directive to try a different mutexmechanism.The not-uncommon occurrences with mutex problems that defy easy resolution oreven explanation is why there is an AcceptMutex directive to start with :)This bug is cropping up in a gentoo build of 2.0.54 (, if anyone cares).I've tried removing user limits on apache without success. Is there a definitivesolution for Linux which doesn't involve switching the AcceptMutex directive?	10.0	id=36184	5	False	False	deweese	1
id=14104	REOPENED	None	Apache httpd-2	mod_ssl (	2.0.54	All other	P3 enhancement	Apache HTTPD Bugs Mailing List	2002-10-30 22:30 UTC by	Jim Lippard	2016-04-15 13:54 UTC (	6 users	I run my local machine with a hardware time of GMT and a local time ofUS/Arizona (Mountain Standard), which is GMT -7.When I generate a new CRL, the time in the CRL for last update and no update isin GMT. Output from openssl crl -text -noout -in crl.pem shows: Last Update: Oct 30 17:21:57 2002 GMT Next Update: Nov 29 17:21:57 2002 GMTfor a CRL created at 10:21:57 MST on October 30. However, mod_ssl says allclient certificates are invalid until 17:21:57 MST:[Wed Oct 30 15:14:01 2002] [warn] Found CRL is expired - revoking allcertificates until you get updated CRL[Wed Oct 30 15:14:01 2002] [error] Certificate Verification: Error (12): CRL hasexpired[Wed Oct 30 15:14:01 2002] [error] Re-negotiation handshake failed: Not acceptedby client!?mod_ssl seems to be taking the CRL date as MST instead of GMT.(System is OpenBSD 3.1.)	I've misdiagnosed the problem. The issue is not a time zone discrepancy, theissue appears to be that the server needs to be restarted in order to load thenew CRL, similar to the need to restart to load a new server certificate. Thisis somewhat less intuitively obvious for the CRL than for the server certificate.Is this documented anywhere?The server needs to be restarted for just about any change to take effect. It'snot clear that this is explicitly documented anywhere, and I'm not really surewhere that information should be placed where it would be effective. Any changeof configuration requires a server restart. This is not SSL specific. Perhaps anentry in the FAQ is warranted? Thoughts?I would actually like to see some way of enabling/forcing Apache to re-read thecrl file episodically.This could either be based on the "next update" field within the CRL itself, ormore flexibly, some interval specified in a new mod_ssl Directive. I favour thelatter as basing it on the "next update" field in the CRL raises issues aboutwhat to do once that date/time has passed and the CRL file has not been updated.Reclassified as a feature request against mod_ssl. Thanks for the suggestion.Periodically reloading the CRL file from within httpd does not really soundfeasible (would you re-load and reparse it in each child? what if the childrengot out of synch? what if the re-load failed? what about thread-safety issuessince the CRL is stored in the server-global config structure).If the CRL changes relatively infrequently over time, you could cron a(graceful) restart to pick up changes. If it is updated so frequently thatrestarting to pick up changes is not practical, you need OCSP (or something likeit).-> WONTFIXI, too have been tripped up by this. Please reconsider.It seems to me that the current behavior is undesirable, and that the problems joe raises are all soluble. The CRL is unlike other configuration changes; it has a expiration date and is expected to require periodic refresh.I update my crl daily with a lifetime of several days - more on general principles than because it's highly volatile. However, if something bad happens, I'd like a reasonable latency till the crl is refreshed.I agree that polling "just in case" could a lot of extra synchronization, and is probably overkill.But it does not seem friendly or robust to have apache stop service when it knows what's wrong & the data it needs is sitting on the disk where the config file says it is.Apache seems to have sufficient synchronization to "revoke all certificates until you get updated CRL". It also has sufficient smarts to do a graceful restart.So, why not do this:When a thread finds that the CRL is out of date, it synchronizes on a CRL update lock. Under that lock, it looks to see if there's a new CRL. If there is, it schedules a graceful restart, placing the request that detected the problem back on the service queue. The request will be picked up by the new generation of the configuration DB after the restart.This way, the update only happens when there is a problem; existing mechanisms are used. The only delay is to the requests at time of crl expiration. And by adjusting the expiration time, an administrator can minimize the impact.The work-around of apachectl -k graceful in the crl rebuild script should work on a single system, single server. But in a more interesting environment (say, multiple systems with the crl on a networked disk), it's a lot more work.But at an absolute minimum, update the documentation for the SSLCARevocationFile directive to indicate that a restart is required when the file changes. As an experienced system manager, but new to apache, it was by no means obvious to me.Hi All,I have also the same problem.You seems to forget the exact role of a CRL. Remember : CRL X509 format is a list of Revoked Certificates. Thus, the goal here is to stop the access to someone that has a revoked certificates.For a security point of view, waiting until the CRL Expiration date is not a good solution (can be 2 days or more). You put your business at risk. In fact, According to some PKI Policies (CSP - Cerificate Security Policies), depending of your working environment, (as in my case), the Revoked certificates must be blocked maximum 10 seconds after the effective revoke. Thus in my case, soon as the CRL has been updated, you have to reload it, and to block any access. This is not only special to my case, any companies (like insurrance,financial, ...) has these types of rules.More : A crl, on our case is published every 30 min, even if no revoke occurs (to avoid overwritte of our CRL and ensure that all chains is working). or immediately after a revoke. His expiration date (next update) is at least 48 hours (this is only for business continuity, to have time to make intervention in case of CRL distribution problem or whatever).More : We are also using Appliance Reverse proxy hardware, XML security Gateway, Software Application Firewall Hardware. All of them has these types of feature about the CRL. It load it, else based on a regular verification time (ie every 5 seconds), or immediatly after it detects the change. It's depends of the product. Why this will be different in Apache ? IIS of microsoft is working also like that.RegardsI agree that reloading of CRLs when necessary is a highly desirable feature.OpenSSL 0.9.9 does have some improved CRL support but adding generic reloadingto cover all cases into OpenSSL isn't really practical. OpenSSL 0.9.8 doesn'thave reloading support but its handling isn't as broken as mod_ssl manual CRLhandling.As a general solution there are several options.One is to run a local OCSP responder which makes use of CRLs to providerevocation information. Then mod_ssl can determine certificate status over OCSPand the responder can deal with CRLs in an appropriate manner. I did write sucha responder for a similar situation but never got round to getting theimplementation into a publicly usable form.Another option is to have a database of CRL information in mod_ssl. A bit likethe session cache but for revocation information. Note that I say "revocationinformation" as opposed to storing full CRLs because CRLs can be quite large anddecoding on each use is a considerable overhead. It is better to just store theset of revoked certificates serial numbers (CRL entries) and have a lookup mechanism which each thread could use.CreatedAutomatically reload CRL when the previous one expires and a new one is availableWith this patch applied, Apache will reload a certificate revocation list (CRL) file, when* previous CRL, stored in memory expired* a new CRL file is available (based on file mtime)It only works with CRLs loaded with SSLCARevocationFile, but if there's interest, I'll extend it to support SSLCARevocationPath as well.It doesn't require any additional options; Apache's behavior will not change if you don't supply fresh CRLs. If you do, it will automagically reload them when needed.I've tested the fromagainst httpd 2.2.15I'm having here a setup with multiple sub-CAs (each with its own CRL) - and could successfully login with revoked certs from the sub-CAs after the patch above was applied.So, this patch seems to have following bug:If you have multiple CRLs within one file the patch only loads the first one.Still interested in this.I'd like to see the patch inwork with revocationpath, and the multiple CA bug reported infixed as well.9 years after this was first reported, X.509 certificates are even more important...and CRLs are part of the support. I still consider the current behavior a bug, not a new feature since httpd is ignoring the CRL's expiration date.This just bit me today. I'm using client-certificate authentication on a web server that I admin for my company, and yesterday I had to revoke one of the certificates due to a termination of an employee, and today I decided to verify that the revocation actually worked by temporarily revoking my own certificate, and surprise(!), I was still able to authenticate to the site. I had to reload Apache before it would reject my authentication. This is not the behavior I expected. It's not as though the contents of the CRLs is conceptually being "included" into the configuration like a modular config file would be; no, the CRL is a piece of volatile data that the configuration *references*, and the server needs to notice when the file changes. At the very least, the Apache mod_ssl documentation needs to note that any changes to the CRL files at SSLCARevocationPath will require a reload of the server configuration in order to take effect. This could have been disastrous if I hadn't thought to double check that Apache was actually rejecting the revoked certs.I would also like to see the patch inimplemented in standard code. I agree withthat this is ancillary information and should NOT require a restart when the CRL file is modified. It is an unnecessary and error prone extra step.Please rate the comments related to this onfor updating the documentation, at least.	14.0	id=12033	22	False	True	info	1
id=56460	REOPENED	None	Ant	Core tasks (	1.9.4	Macintosh All	P2 major	Ant Notifications List	2014-04-25 15:32 UTC by	Giuseppe	2016-04-07 19:22 UTC (	1 user	When I run Ant through Eclipse using JDK 1.8, you receive the following error:BUILD FAILED Class not found: javac1.8 Instead, by setting the use of JDK 1.6, the execution works correctly. This bug should be fixed since version 1.9.0 (:). But I have tried both versions, and the error is still present.	I think we need a little bit more of a stacktrace.Eclipse plugs in its own compiler AFAIK and we need to be sure it is Ant and not Eclipse's compiler throwing the exception.No additional infos are supported (asked for stacktrace). One year of silence and a hint on a solution - I close this issue.I am getting the same error now. Trying to run ant build with ant 1.9.6 and java 1.8.0_77. This is all on command line, no Eclipse is involved.Please provide us with the output ofant -disgnosticsa snippet of build file that you use and the full error message when your run the build file with ant -verbose.	4.0	id=43411	10	True	True	archie	1
id=22686	REOPENED	None	Apache httpd-2	All (	2.2.4	PC Linux	P3 normal	Apache HTTPD Bugs Mailing List	2003-08-25 04:29 UTC by	Bojan Smojver	2007-11-13 18:03 UTC (	1 user	With Apache 2.0.47, when I try to use ab on Red Hat Linux 9, it comes up withthe above message. The revision of ab is 1.121.2.1.This is the command I run and the stdout:-----------------------------------------------# strace ab -n 12>strace.outThis is ApacheBench, Version 2.0.40-dev <$Revision: 1.121.2.1 $> apache-2.0Copyright (c) 1996 Adam Twiss, Zeus Technology Ltd,Copyright (c) 1998-2002 The Apache Software Foundation,Benchmarking localhost (be patient)...-----------------------------------------------I will also attach the strace output.	Createdstrace output from the runSorry, the Summary should read apr_poll, not ap_poll :-(*** This bug has been marked as a duplicate of***Sorry fellows :-( I actually searched Bugzilla before I posted, but foundnothing. Shame on me.Same error with Apache 2.2.4 on Red Hat AS 3.0. The error happens with thefollowing command and always near the 40000th request:./ab -n 100000 -c 5 -p/exec/applis/snuproxitaltel/current/sh/server-test-req-real.xmlmore /etc/redhat-release : Red Hat Enterprise Linux AS release 3 (Taroon Update 6) uname -a : Linux opsnu1t1 2.4.21-37.ELsmp #1 SMP Wed Sep 7 13:28:55 EDT 2005i686 i686 i386 GNU/Linux)../ab -V : This is ApacheBench, Version 2.0.40-dev <$Revision: 1.146 $> apache-2.0Copyright 1996 Adam Twiss, Zeus Technology Ltd,Copyright 2006 The Apache Software Foundation,-------------------------I never encounter the problem on an other Red Hat machine :more /etc/redhat-release : Red Hat Enterprise Linux AS release 3 (Taroon Update 3)uname -a : Linux dflp0ee9 2.4.21-20.0.1.ELsmp #1 SMP Wed Nov 24 20:34:01 EST2004 i686 i686 i386 GNU/LinuxSame here, on Debian Etch, ApacheBench 2.0.40-dev <$Revision: 1.146 $>I still see this time out using the latest (or at least very recent) ab / apr /apu combination:http revision:apr revision: SVNapr-util revision: SVNWhen I try to benchmark a page that takes approximately 30 to 60 seconds toload, I (almost always) get an apr_poll timeout after 30 seconds:v@kuskanook:~/src/httpd_20071113171835/support$ time ./ab -r -t 120 "$SLOWURL"This is ApacheBench, Version 2.3 <$Revision: 573101 $>Copyright 1996 Adam Twiss, Zeus Technology Ltd,Licensed to The Apache Software Foundation,Benchmarking localhost (be patient)apr_poll: The timeout specified has expired (70007)real 0m30.004suser 0m0.000ssys 0m0.001s	7.0	id=11580	5	False	False	nd	1
id=23911	REOPENED	None	Apache httpd-2	Core (	2.2.13	All All	P3 critical	Apache HTTPD Bugs Mailing List	2003-10-18 17:33 UTC by	David Cook	2011-09-14 17:11 UTC (	1 user	With the latest version of Apache (2.0.47) we are seeing occassional defunct/zombie cgi's. The cgi's can't be killed (won't die with kill -9 pid) but do die if we kill the parent httpd process.This seems to me to be identical to bug report 21737 --- but with the 2.0.47 version (21737 was for the 1... version). Since their fix was to alloc.c, and I don't see an alloc.c to compare too, I'm seeking help on fixing this problem.The zombies are not taking up cpu cycles, of course... but do tend to deplete the process count pool. We've counted as high as 60 zombies in one situation. Last night there were 8.	suexec or not?mod_cgi or mod_cgid?probably doesn't matter, but which MPM?configure:12836: checking whether to enable mod_suexecconfigure:12888: result: noconfig.log:MPM_LIB='server/mpm/prefork/libprefork.la'config.log:MPM_NAME='prefork'config.log:MPM_SUBDIR_NAME='prefork'config.log:#define APACHE_MPM_DIR "server/mpm/prefork"./httpd -lCompiled in modules: core.c mod_access.c mod_auth.c mod_include.c mod_log_config.c mod_env.c mod_setenvif.c mod_ssl.c prefork.c http_core.c mod_mime.c mod_status.c mod_autoindex.c mod_asis.c mod_cgi.c mod_negotiation.c mod_dir.c mod_imap.c mod_actions.c mod_userdir.c mod_alias.c mod_rewrite.c mod_so.cDoes this happen even for simple CGIs such as printenv (in cgi-bin dir ofdefault install), or only for setuid binaries, or what?Also, can you get a truss of a CGI request, including both the web server childhandling the request and the CGI itself?Start the server like this:# truss -o outfile -f ./httpd -DONE_PROCESSand run a couple of CGI requests, then use ps to see whether or not the zombieproblem occurs, then interupt truss+httpd. If this run exhibited the zombieproblem, send in the truss. If not, you may need to start the server normally,run truss against one of the children (truss -o outfile -f -p PID) and keepdoing CGI requests until the truss-ed process handles it and we can see the trace.It is not specific to any cgi. It is difficult for us to reproduce this because we can't predict when it will happen and these are public/commercial servers with which we don't have the luxury of playing with.Is there something I can do once I get zombies? The zombies usually belong to one or two parents. If there is information that I can get from that parent for you that would be useful, let me know (just killed 27 zombies in fact).I don't know what the next step is, unfortunately.I've been testing 2.0.47 with default config (prefork, mod_cgi, no suexec) thisafternoon and using printenv as the example cgi. No long-term zombies. printenv goes through zombie state temporarily but Apache cleans it up very soonafter.I'm curious about how you can tell it isn't specific to some cgi. All I seefrom ps for zombies is trawick 6872 29703 0 0:00 <defunct>Is it possible that the zombie represents a child process that the CGI scriptcreated, and not the CGI script itself?Apache parent -> Apache child process -> CGI script -> some command invoked by the CGIMaybe there is some infrequent condition where the Apache child processterminates the CGI script before it has reaped status from the command it runs,and then the Apache child process becomes the parent of the command invoked bythe CGI. Since the Apache child process doesn't call waitpid() to collectstatus from arbitrary processes, then the zombie never gets cleaned up.Apache will terminate the CGI script with SIGTERM (and later SIGKILL) if the CGIscript keeps running for a while after the client connection drops.nothing easy that I know of...I wasn't able to recreate any zombies in this scenarioApache parent -> Apache child process -> CGI script -> some command invoked by the CGIwhen the CGI script exited without reaping status from its child. (just the wayUnix works I guess)If you set MaxRequestsPerChild relatively low, won't that take care of zombies?Another VERY stray thought is to write a simple module that calls waitpid(-1,,)to try to reap status from any stray child process remaining for any reason. Since this is prefork, it shouldn't interfere with any other requests.Is this reproducible in 2.0.54?I run Debian stable with apache2 2.0.54. I can confirm that this version leavesdefuncts every now and then. It does this every few days, and what's happeningis all the defuncts lock up all the apache processes and the server isunresponsible and has to be restarted.Does this affect 2.2.x?Nearly three years in NEEDINFO, closing old 2.0 report. If it's not fixed in 2.0.latest, it won't get fixed in 2.0.any.Still there in 2.2.13-1fc11.I have isolated it: it can be simply reproduced with a cgi containing sleep 9999 >/dev/null &and fixed by redirecting the stderr of the child: sleep 9999 >/dev/null 2> /dev/null &(the stdout redir is needed anyway for the HTTP request to complete)So it boils down to: CGI exits with the stderr dup'ed over to a lingering child.I assume this is linked to Apache's capture of CGI's stderrs (for error_log), not expecting their lifecycle to be decoupled from the CGI process's.Apologies if this is not the proper place to reopen. Spank me in that case :)	11.0	id=22484	9	False	True	vt	1
id=22898	REOPENED	None	Apache httpd-2	mod_cgi (	2.5-HEAD	All Windows XP	P3 normal	Apache HTTPD Bugs Mailing List	2003-09-02 21:39 UTC by	Marek Chlup	2015-02-23 14:52 UTC (	3 users	Hi!my conf:--------AddHandler application/x-httpd-eperl .phtmlAction application/x-httpd-eperl /cgi-bin/nph-testmy request:-----------telnet myhost 80GET /test/t1.phtml HTTP 1.1Host: myhostapache send me:---------------HTTP/1.1 200 OKContent-Type: text/plainHello!HTTP/1.1 200 OKDate: Tue, 02 Sep 2003 21:27:38 GMTServer: Apache/2.0.47 (Debian GNU/Linux) mod_perl/1.99_09 Perl/v5.8.0mod_ssl/2.0.47 OpenSSL/0.9.7bContent-Length: 0Connection: closeContent-Type: application/x-httpd-phpmy script nph-test:-------------------#!/usr/bin/perlprint "HTTP/1.1 200 OK\n";print "Content-Type: text/plain\n\n";print "Hello!\n";where is bug?-------------My script noprint seccond HTTP header. Why apache generate second header?ByeMarek	Hmm. e-perl is mod_perl, isn't it? I'd guess, mod_perl doesn't supportnph-scripts at all, you'd have to use mod_cgi instead.mod_perl no problem. I tested my example without loading mod_perl module too.Problem is mod_cgi and mod_cgid. My example don't use e-perl.I am getting this problem too, with Meta-HTML. I have not been able to reproduceby telnetting to port, b/c the second header seems to be sent after theconnection is closed! but all my pages are served with a second header at theend. For example,telnet shows the Meta-HTML headers at the beginning where one would expect them:get /welcome.mhtmlHTTP/1.0 200 OKServer: MHttpd/4.1 (bfox; i686-linux; Meta-HTML/6.11)Date: Fri, 31 Oct 2003 23:08:24 GMTExpires: Thu, 30 Oct 2003 23:08:24 GMTLast-modified: Fri, 31 Oct 2003 23:07:24 GMTContent-length: 9585Meta-HTML-Engine: MHttpd/4.1 (bfox; i686-linux; Meta-HTML/6.11)Content-type: text/htmlthen at the end, the page displays the apache2 header (from view source):HTTP/1.1 200 OKDate: Fri, 31 Oct 2003 23:05:58 GMTServer: Apache/2.0.48 (Gentoo/Linux)Content-Length: 0Keep-Alive: timeout=15, max=100Connection: Keep-AliveContent-Type: metahtml/interpretedMy conf:--------AddType metahtml/interpreted .mhtmlAction metahtml/interpreted /cgi-bin/nph-engineScriptAlias /cgi-bin/ /www/metasystema.net/cgi-bin/I've verified this occurs in 2.0.48 as well as 2.0.47.sounds really like a nasty bug ;-)***has been marked as a duplicate of this bug. ***I have a similar problem, using an "nph-" script via mod_cgi, being redirectedvia mod_rewrite. If I call the script directly, I do not get the additonalheader at the end, but if I go through mod_rewrite, I do. This is with Apache2.0.54 and 2.0.55.This bug persists on Apache 2.2.It is caused in generators/mod_cgi.c, line 760 nph = !(strncmp(argv0, "nph-", 4)); Add the following just before it (recompile, start with debug logging on):ap_log_rerror(APLOG_MARK, APLOG_DEBUG, 0, r, "Identifying cgi script:%s",r->filename);We can see that the strncmp will fail, because the cgi script will include afull path: e.g. "Identifying cgi script:/home/www/site/test/bin/nph-foobar.cgi"The conf lines for this are as follows: ScriptAlias /x/ /home/www/site/test/bin Action foo-file /x/nph-foobar.cgi AddHandler foo-file .fooSo - changing line 760 to:nph = ( strstr(argv0,"nph-") != NULL );(which of course means that any "nph-" will return positive...)now identifies nph scripts as such. However.. the additional headers STILLappear... at which point.. I am stumped.There is a work-around, which is to change the return line of any cgi scriptfrom being e.g.HTTP/1.1 200 OKtoStatus 200 OK- and dropping nph- status. Of course this does NOT fix the legacy issue, and probably remains a typicalreason why so many people carry on using 1.3.x (slaps wrist).I've done some more RA on this.The dupe header always shows Content-Length: 0It looks like a default / stub header is being pushed out.Interestingly - this bug ONLY appears to occur when using the Action directive,(It makes no difference if one uses AddType or AddHandler or SetHandler ) ..guess where I will look next...Output filters are being correctly stripped by mod-cgi - though it seems like itwould be easier just to do something like rv = ap_pass_brigade(r->output_filters, bb);.. but hey, what do I know?CreatedThis is a drop in replacement that fixesThis adds some APLOG_DEBUG messages, which are useful for nph- bug issues.There are 2 major bugs that are now 'fixed' here. Of course, the fixes arekludges, but then I'm not a core Apache programmer. Note that identification ofnph- now uses strstr, rather than strncmp! Also note that the nph- code partstrips out output_filters from it's immediate caller!This bug is essentially fixed - though it needs to be folded into the main cvs.There were two basic bugs here - one was the faulty (ie not working)identification of nph- cgi scripts. The other was the misconception that annph-cgi request will be the initial request; something patently not true whenbeing called via Action.The fix that I have done is lame. It only deals with one previous process;however, this seems to be the primary problem, so it works. Basically, we nowcall ap_is_initial_req(r) and if it is !=1, then we also strip theoutput_filters of the previous/container request.I have posted the entire amended file. sorry. I don't understand the patch system..Createdmod_cgi diff file that fixes 22898 and related bugsThe Apache2.x fails to behave properly when nph- are used in Actions.This patch includes some debugging messages, and fixes two related severe bugswhich cause nph- cgi scripts to fail when cgi,_handler is called via an Actiondirective. The bugs are (1) apr_filepath_name_get(r->filename) returns a fullpath when cgi_handler is accessed via Action, which means that the strncmp testfails to identify the nph- prefix. (2) The clear up of the bucket brigade failsunless the request is the ap_is_initial_req. The fix in the patch for (1) is touse strstr instead of strncmp - though there may be a better fix for that. Thefix in the patch for (2) is on false for ap_is_initial_req to clear the bucketbrigade at r->prev.After a year, the patch below has still not been folded into the 2.2 trunk.Until it is, nph- scripts addressed via AddHandler or Action fail on Apache 2.x!I am aware that nph- is considered a dead area - but it's necessary for legacycode and regardless, Apache 2.x claims support for it (and indeed does supportit when it is not referred to via Action or AddHandler)	12.0	id=14104	18	False	True	trawick	1
id=24890	REOPENED	None	Apache httpd-2	Runtime Config (	2.0.48	PC Linux	P3 normal	Apache HTTPD Bugs Mailing List	2003-11-21 13:29 UTC by	ismail 'cartman' donmez	2004-11-16 19:05 UTC (	0 users	httpd-std-conf.in contains the following directive : <IfModule mod_mime_magic.c> MIMEMagicFile @rel_sysconfdir@/magic </IfModule> But looking at modules/metadata/mod_mime_magic.c the directive must be like <IfModule mod_mime_magic.c> MimeMagicFile @rel_sysconfdir@/magic </IfModule> Please notice the s/MIMEMagicFile/MimeMagicFile . This fixes the problem for me with the mod_perl module loading and according to the mod_mime_magic.c its the \ right way ( TM ). Please fix this in Apache 2.0.49 release.	Please note that apache config directives are generally case insensitive. Ifyour script cannot handle this you should fix your script.Thanks for using Apache.My script? I use apachectl start to start apache. And if httpd.conf contains MIMEMagicFile this error shows up in logs : <error> Syntax error on line 436 of /opt/apache/conf/httpd.conf: Invalid command 'MIMEMagicFile', perhaps mis-spelled or defined by a module not included in the server configuration </error> and this only happens if I add LoadModule perl_module modules/mod_perl.so to httpd.conf. Looks a like a bug in apache rather than my commands.Then it's obviously a mod_perl problem. mod_mime_magic isn't loaded, butmod_perl seems to try to interpret the directive (?). Stas?Ok I found the exact problem. If I do export LC_ALL=POSIX && apachectl start apache works fine. If I do export LC_ALL=tr_TR && apachectl start it gives parsing error that I posted earlier. So whats going on is apache configuration parsing is local aware and it should not be so. Because on turkish locales tolower('I') != i or toupper('i') != I . This is a widely known property but guess apache developers missed it :-). Solution is making config parsing non-local aware.Hrm, ok. But I'm wondering how mod_perl is involved with it...However, locales are iirc implementation defined (?), we'd need to define(a) that config directives can only contain word character [a-z_0-9](b) compare them using your own tables.Any opinions about this?Really no idea why this works if I do not load mod_perl. Any mod_perl developer around can shed a light?André can you tell me where is the config parser in apache source? So I can look at it and maybe fixor it.Mostly in server/config.c. You may find <> useful aswell. (Though it's for 2.1 branch but there are only minor changes in the configpart, if any).Thanks!Workaround is adding LC_ALL=POSIX to apachectl script. Fixing this bug involves apr-* internals which I have no idea.	9.0	id=17497	5	False	False	nd	1
id=31302	REOPENED	None	Apache httpd-2	mod_cgi (	2.0.55	All All	P3 critical	Apache HTTPD Bugs Mailing List	2004-09-19 21:36 UTC by	Michael Disserman	2008-06-06 02:15 UTC (	5 users	(I have apache 2.0.51 but can't select it from the version list)I have the following problem:There's some virtualhost with suexec enabled and vh directory with includes enabled.I've created test.shtml page and trying to execute cgi command from it:<!--#exec cmd="sample.cgi" -->this one works oksuexec log output:uid: (1000/divisor) gid: (45000/45000) cmd: sample.cgithen I move sample.cgi to the some dir i.e. called '5' and change test.shtmlcontent to<!--#exec cmd="./5/sample.cgi" -->this one stops workingsuexec log output:uid: (1000/divisor) gid: (45000/45000) cmd: sample.cgi[2004-09-20 00:18:00]: cannot stat program: (sample.cgi)but works fine if disable suexec in the virtualhost config.virtualhost config example:<VirtualHost 172.16.1.1:80>ServerAdminDocumentRoot /home/divisor/domain.comServerName domain.comSuexecUserGroup divisor divisor<DIRECTORY //home/divisor/domain.com>OPTIONS Indexes FollowSymLinks IncludesAllowOverride All</DIRECTORY>IndexOptions FancyIndexing</VirtualHost>	In 1.3.x suexec executes programs in subdirs well if there's a correct relativepath in the ssi command.I noticed that apache 2.0.51 gives to suexec just a program name, without apath, i.e. example.cgi instead of ./somedir/example.cgi when 1.3.x gives tosuexec a program name together with a path.Maybe this is a feature of 2.0.x :) but many sites have such includes and can'tbe transferred to 2.0.x if the system is configured to work with suexec.If the problem executing programs in subdirs via ssi+suexec isn't going to besolved in 2.0.x could you plz advice how to configure or manually patch apache2.0.x to transfer such sites? I would be greatly appreciated. Thanks you.I've figured this out writting such patch:-- FILE START ----- os/unix/unixd.c.orig Mon Sep 20 04:09:08 2004+++ os/unix/unixd.c Mon Sep 20 04:09:27 2004@@ -308,15 +308,7 @@ return apr_proc_create(newproc, progname, args, env, attr, p); } - argv0 = ap_strrchr_c(progname, '/');- /* Allow suexec's "/" check to succeed */- if (argv0 != NULL) {- argv0++;- }- else {- argv0 = progname;- }-+ argv0 = progname; if (ugid->userdir) { execuser = apr_psprintf(p, "~%ld", (long) ugid->uid);-- FILE END --what was that check for? security feature to avoid running external programs? Ifso they can be executed anyway under the same permissions directly from theprogram executed from the current dir. I guess it was unusual. Please comment.Thanks.WORKSFORME is used to indicate that the problem can't be replicated with thecurrent source code. I don't think that is what you intended.Same problem here, still present in 2.0.53. Michael's suggested fix effectively reverses the changes to os/unix/unixd.c added by Ryan Bloom on 2002-06-26 (for 2.0.40), see?&msgNo=21396"Fix a long-standing bug in 2.0, CGI scripts were being called with relative paths instead of absolute paths. Apache 1.3 used absolute paths for everything except for SuExec, this brings back that standard."I'm not completely sure about possible side effects of reversing the changes to unixd.c, so would appreciate if a current Apache HTTP server project member could have a look at this issue (Ryan is now listed as an emeritus member).BTW, #29534 seems to be a duplicate of this bug (or the other way round, since 29534 was actually filed earlier).Thanks,KasparMy personal opinion, without any carful study is that this is the wrong fix. The original patch was put in for valid reasons and should stay. However,mod_include needs a patch that turns CGI script paths into absolute paths. Thatwill keep everything working.Createdproposed fixWell, nobody seems to have picked up so far, so here's my try.That's what my proposed fix tries to do. The code handling the SSI exec tag has actually been moved to mod_cgi (before 2.0.10 already), so that's where the patch goes.In handle_exec(), I have added a check to make sure an absolute pathname is used when calling include_cmd()/run_cgi_child(). For suexec, the important point is that in run_cgi_child() the directory for the new child is set to the parent of "command", not the one of r->filename (they differ if the "exec" target is not in the same directory as the shtml file).I've tested this both with relative and absolute file names for the "cmd" parameter, and running with and without suexec. It works for me (as predicted by Ryan), but further testing is welcome, of course. And finally, I'd appreciate to see this fix (or a similar one) in 2.0.54... thanks!***has been marked as a duplicate of this bug. ***PatchAvailable keyword addedI can still confirm this issue for 2.0.55. I think it's getting time to bringthis fix to HEAD.The version 2.0.63 of apache server was released but this bug was not fixed. Will this problem be resolved in the nearest future?	11.0	id=22898	16	False	False	sdhill	1
id=40826	REOPENED	None	Security - Now in JIRA	Signature (	unspecified	Other other	P2 normal	XML Security Developers Mailing List	2006-10-27 04:14 UTC by	Fillipe Lima	2010-07-28 20:41 UTC (	2 users	HelloI am having a problem. My application is an applet that generate a xml and signit with PrivateKey from SmartCard. The first time, it works fine! however when i try to sign again (second time), iget this exception: java.security.InvalidKeyException: Private keys must be instance ofRSAPrivate(Crt)Key or have PKCS#8 encodingThe exception occurs when calling the method: DOMSignContext dsc = newDOMSignContext(pk, doc.getDocumentElement()); This method is part of thefollowing code:Does any body know what could i do to solve this problem ?======= code that generates the xml signatureString providerName = System.getProperty("jsr105Provider","org.jcp.xml.dsig.internal.dom.XMLDSigRI");XMLSignatureFactory fac = XMLSignatureFactory.getInstance("DOM",(Provider) Class.forName(providerName).newInstance());Reference ref = fac.newReference("",fac.newDigestMethod(DigestMethod.SHA1, null),Collections.singletonList(fac.newTransform(Transform.ENVELOPED,(TransformParameterSpec) null)),null,null);SignedInfo si = fac.newSignedInfo(fac.newCanonicalizationMethod(CanonicalizationMethod.INCLUSIVE_WITH_COMMENTS,(C14NMethodParameterSpec) null),fac.newSignatureMethod(SignatureMethod.RSA_SHA1, null),Collections.singletonList(ref));KeyInfoFactory kif = fac.getKeyInfoFactory();X509Data x509 = kif.newX509Data(Collections.singletonList(cert));KeyInfo ki = kif.newKeyInfo(Collections.singletonList(x509));DOMSignContext dsc = new DOMSignContext(pk, doc.getDocumentElement());XMLSignature signature = fac.newXMLSignature(si, ki);signature.sign(dsc);return doc; }======= code that get de PrivateKey and Certificate from Smart Card:String configuracao = "name = SmartCard\n" +"library = c:\\windows\\system32\\aetpkss1.dll";byte[] configuracaoBytes = configuracao.getBytes();ByteArrayInputStream configuracaoStream = newByteArrayInputStream(configuracaoBytes);sun.security.pkcs11.SunPKCS11 provider = newsun.security.pkcs11.SunPKCS11(configuracaoStream);Security.addProvider(provider);this.nomeProvider = provider.getName();this.repositorio = KeyStore.getInstance("PKCS11", provider);repositorio.load(null, pin.toCharArray());this.inicializarDados(pin);String keyEntry = null;boolean ok = false;Enumeration en = repositorio.aliases();while(en.hasMoreElements()) {keyEntry = (String)en.nextElement();if(repositorio.isKeyEntry(keyEntry)){ok = true;break;}}if(ok){certificado = (X509Certificate) repositorio.getCertificate(keyEntry);chavePrivada = (PrivateKey) repositorio.getKey(keyEntry, pin.toCharArray());	(In reply to)This exception indicates that you are trying to use the key that is stored on the smart card with a software based crypto provider. It won't work. Seefor some more information about that.However, we need to figure out why you are getting that exception.Can you attach the full exception stack trace?(In reply to)signtime), iwork.I will try to do this:"It is recommended that applications only call getProvider() after they have called the relevant initialization method. "Im using xml signature... i) Is this my initialization method "XMLSignature signature = fac.newXMLSignature(si, ki)"; ?ii) Is my getProvider() this part? "XMLSignatureFactory fac = XMLSignatureFactory.getInstance("DOM",(Provider) Class.forName(providerName).newInstance());Reference ref = fac.newReference("",fac.newDigestMethod(DigestMethod.SHA1, null),Collections.singletonList(fac.newTransform(Transform.ENVELOPED,(TransformParameterSpec) null)),null,null);" ?The complete stack trace:AssinaturaXMLException: java.security.InvalidKeyException: Private keys must be instance of RSAPrivate(Crt)Key or have PKCS#8 encoding at AssinaturaXMLEnveloped.assinar(AssinaturaXMLEnveloped.java:86) at AssinadorDigital.AssinarDados(AssinadorDigital.java:133) at AssinadorDigital.btnAssinar_actionPerformed(AssinadorDigital.java:189) at AssinadorDigital_btnAssinar_actionAdapter.actionPerformed(AssinadorDigital.java:201) at javax.swing.AbstractButton.fireActionPerformed(Unknown Source) at javax.swing.AbstractButton$Handler.actionPerformed(Unknown Source) at javax.swing.DefaultButtonModel.fireActionPerformed(Unknown Source) at javax.swing.DefaultButtonModel.setPressed(Unknown Source) at javax.swing.plaf.basic.BasicButtonListener.mouseReleased(Unknown Source) at java.awt.Component.processMouseEvent(Unknown Source) at javax.swing.JComponent.processMouseEvent(Unknown Source) at java.awt.Component.processEvent(Unknown Source) at java.awt.Container.processEvent(Unknown Source) at java.awt.Component.dispatchEventImpl(Unknown Source) at java.awt.Container.dispatchEventImpl(Unknown Source) at java.awt.Component.dispatchEvent(Unknown Source) at java.awt.LightweightDispatcher.retargetMouseEvent(Unknown Source) at java.awt.LightweightDispatcher.processMouseEvent(Unknown Source) at java.awt.LightweightDispatcher.dispatchEvent(Unknown Source) at java.awt.Container.dispatchEventImpl(Unknown Source) at java.awt.Component.dispatchEvent(Unknown Source) at java.awt.EventQueue.dispatchEvent(Unknown Source) at java.awt.EventDispatchThread.pumpOneEventForHierarchy(Unknown Source) at java.awt.EventDispatchThread.pumpEventsForHierarchy(Unknown Source) at java.awt.EventDispatchThread.pumpEvents(Unknown Source) at java.awt.EventDispatchThread.pumpEvents(Unknown Source) at java.awt.EventDispatchThread.run(Unknown Source)(In reply to)This stack trace doesn't help. The AssinaturaXMLException is swallowing thestack trace of the cause: InvalidKeyException. I need to have that the stacktrace of the InvalidKeyException to be able to help.(In reply to)must beOk. I think this may help:javax.xml.crypto.dsig.XMLSignatureException: java.security.InvalidKeyException: Private keys must be instance of RSAPrivate(Crt)Key or have PKCS#8 encoding at org.jcp.xml.dsig.internal.dom.DOMXMLSignature.sign(DOMXMLSignature.java:370) at AssinaturaXMLEnveloped.assinar(AssinaturaXMLEnveloped.java:80) at AssinadorDigital.AssinarDados(AssinadorDigital.java:134) at AssinadorDigital.btnAssinar_actionPerformed(AssinadorDigital.java:190) at AssinadorDigital_btnAssinar_actionAdapter.actionPerformed(AssinadorDigital.java:202) at javax.swing.AbstractButton.fireActionPerformed(Unknown Source) at javax.swing.AbstractButton$Handler.actionPerformed(Unknown Source) at javax.swing.DefaultButtonModel.fireActionPerformed(Unknown Source) at javax.swing.DefaultButtonModel.setPressed(Unknown Source) at javax.swing.plaf.basic.BasicButtonListener.mouseReleased(Unknown Source) at java.awt.Component.processMouseEvent(Unknown Source) at javax.swing.JComponent.processMouseEvent(Unknown Source) at java.awt.Component.processEvent(Unknown Source) at java.awt.Container.processEvent(Unknown Source) at java.awt.Component.dispatchEventImpl(Unknown Source) at java.awt.Container.dispatchEventImpl(Unknown Source) at java.awt.Component.dispatchEvent(Unknown Source) at java.awt.LightweightDispatcher.retargetMouseEvent(Unknown Source) at java.awt.LightweightDispatcher.processMouseEvent(Unknown Source) at java.awt.LightweightDispatcher.dispatchEvent(Unknown Source) at java.awt.Container.dispatchEventImpl(Unknown Source) at java.awt.Component.dispatchEvent(Unknown Source) at java.awt.EventQueue.dispatchEvent(Unknown Source) at java.awt.EventDispatchThread.pumpOneEventForHierarchy(Unknown Source) at java.awt.EventDispatchThread.pumpEventsForHierarchy(Unknown Source) at java.awt.EventDispatchThread.pumpEvents(Unknown Source) at java.awt.EventDispatchThread.pumpEvents(Unknown Source) at java.awt.EventDispatchThread.run(Unknown Source)Caused by: java.security.InvalidKeyException: Private keys must be instance of RSAPrivate(Crt)Key or have PKCS#8 encoding at sun.security.rsa.RSAKeyFactory.translatePrivateKey(Unknown Source) at sun.security.rsa.RSAKeyFactory.engineTranslateKey(Unknown Source) at sun.security.rsa.RSAKeyFactory.toRSAKey(Unknown Source) at sun.security.rsa.RSASignature.engineInitSign(Unknown Source) at sun.security.rsa.RSASignature.engineInitSign(Unknown Source) at java.security.Signature$Delegate.init(Unknown Source) at java.security.Signature$Delegate.chooseProvider(Unknown Source) at java.security.Signature$Delegate.engineInitSign(Unknown Source) at java.security.Signature.initSign(Unknown Source) at org.jcp.xml.dsig.internal.dom.DOMRSASignatureMethod.sign(DOMRSASignatureMethod.java:134) at org.jcp.xml.dsig.internal.dom.DOMXMLSignature.sign(DOMXMLSignature.java:367) ... 27 morejava.security.InvalidKeyException: Private keys must be instance of RSAPrivate(Crt)Key or have PKCS#8 encoding at sun.security.rsa.RSAKeyFactory.translatePrivateKey(Unknown Source) at sun.security.rsa.RSAKeyFactory.engineTranslateKey(Unknown Source) at sun.security.rsa.RSAKeyFactory.toRSAKey(Unknown Source) at sun.security.rsa.RSASignature.engineInitSign(Unknown Source) at sun.security.rsa.RSASignature.engineInitSign(Unknown Source) at java.security.Signature$Delegate.init(Unknown Source) at java.security.Signature$Delegate.chooseProvider(Unknown Source) at java.security.Signature$Delegate.engineInitSign(Unknown Source) at java.security.Signature.initSign(Unknown Source) at org.jcp.xml.dsig.internal.dom.DOMRSASignatureMethod.sign(DOMRSASignatureMethod.java:134) at org.jcp.xml.dsig.internal.dom.DOMXMLSignature.sign(DOMXMLSignature.java:367) at AssinaturaXMLEnveloped.assinar(AssinaturaXMLEnveloped.java:80) at AssinadorDigital.AssinarDados(AssinadorDigital.java:134) at AssinadorDigital.btnAssinar_actionPerformed(AssinadorDigital.java:190) at AssinadorDigital_btnAssinar_actionAdapter.actionPerformed(AssinadorDigital.java:202) at javax.swing.AbstractButton.fireActionPerformed(Unknown Source) at javax.swing.AbstractButton$Handler.actionPerformed(Unknown Source) at javax.swing.DefaultButtonModel.fireActionPerformed(Unknown Source) at javax.swing.DefaultButtonModel.setPressed(Unknown Source) at javax.swing.plaf.basic.BasicButtonListener.mouseReleased(Unknown Source) at java.awt.Component.processMouseEvent(Unknown Source) at javax.swing.JComponent.processMouseEvent(Unknown Source) at java.awt.Component.processEvent(Unknown Source) at java.awt.Container.processEvent(Unknown Source) at java.awt.Component.dispatchEventImpl(Unknown Source) at java.awt.Container.dispatchEventImpl(Unknown Source) at java.awt.Component.dispatchEvent(Unknown Source) at java.awt.LightweightDispatcher.retargetMouseEvent(Unknown Source) at java.awt.LightweightDispatcher.processMouseEvent(Unknown Source) at java.awt.LightweightDispatcher.dispatchEvent(Unknown Source) at java.awt.Container.dispatchEventImpl(Unknown Source) at java.awt.Component.dispatchEvent(Unknown Source) at java.awt.EventQueue.dispatchEvent(Unknown Source) at java.awt.EventDispatchThread.pumpOneEventForHierarchy(Unknown Source) at java.awt.EventDispatchThread.pumpEventsForHierarchy(Unknown Source) at java.awt.EventDispatchThread.pumpEvents(Unknown Source) at java.awt.EventDispatchThread.pumpEvents(Unknown Source) at java.awt.EventDispatchThread.run(Unknown Source)java.security.InvalidKeyException: Private keys must be instance of RSAPrivate(Crt)Key or have PKCS#8 encoding at sun.security.rsa.RSAKeyFactory.translatePrivateKey(Unknown Source) at sun.security.rsa.RSAKeyFactory.engineTranslateKey(Unknown Source) at sun.security.rsa.RSAKeyFactory.toRSAKey(Unknown Source) at sun.security.rsa.RSASignature.engineInitSign(Unknown Source) at sun.security.rsa.RSASignature.engineInitSign(Unknown Source) at java.security.Signature$Delegate.init(Unknown Source) at java.security.Signature$Delegate.chooseProvider(Unknown Source) at java.security.Signature$Delegate.engineInitSign(Unknown Source) at java.security.Signature.initSign(Unknown Source) at org.jcp.xml.dsig.internal.dom.DOMRSASignatureMethod.sign(DOMRSASignatureMethod.java:134) at org.jcp.xml.dsig.internal.dom.DOMXMLSignature.sign(DOMXMLSignature.java:367) at AssinaturaXMLEnveloped.assinar(AssinaturaXMLEnveloped.java:80) at AssinadorDigital.AssinarDados(AssinadorDigital.java:134) at AssinadorDigital.btnAssinar_actionPerformed(AssinadorDigital.java:190) at AssinadorDigital_btnAssinar_actionAdapter.actionPerformed(AssinadorDigital.java:202) at javax.swing.AbstractButton.fireActionPerformed(Unknown Source) at javax.swing.AbstractButton$Handler.actionPerformed(Unknown Source) at javax.swing.DefaultButtonModel.fireActionPerformed(Unknown Source) at javax.swing.DefaultButtonModel.setPressed(Unknown Source) at javax.swing.plaf.basic.BasicButtonListener.mouseReleased(Unknown Source) at java.awt.Component.processMouseEvent(Unknown Source) at javax.swing.JComponent.processMouseEvent(Unknown Source) at java.awt.Component.processEvent(Unknown Source) at java.awt.Container.processEvent(Unknown Source) at java.awt.Component.dispatchEventImpl(Unknown Source) at java.awt.Container.dispatchEventImpl(Unknown Source) at java.awt.Component.dispatchEvent(Unknown Source) at java.awt.LightweightDispatcher.retargetMouseEvent(Unknown Source) at java.awt.LightweightDispatcher.processMouseEvent(Unknown Source) at java.awt.LightweightDispatcher.dispatchEvent(Unknown Source) at java.awt.Container.dispatchEventImpl(Unknown Source) at java.awt.Component.dispatchEvent(Unknown Source) at java.awt.EventQueue.dispatchEvent(Unknown Source) at java.awt.EventDispatchThread.pumpOneEventForHierarchy(Unknown Source) at java.awt.EventDispatchThread.pumpEventsForHierarchy(Unknown Source) at java.awt.EventDispatchThread.pumpEvents(Unknown Source) at java.awt.EventDispatchThread.pumpEvents(Unknown Source) at java.awt.EventDispatchThread.run(Unknown Source)AssinaturaXMLException: java.security.InvalidKeyException: Private keys must be instance of RSAPrivate(Crt)Key or have PKCS#8 encoding at AssinaturaXMLEnveloped.assinar(AssinaturaXMLEnveloped.java:87) at AssinadorDigital.AssinarDados(AssinadorDigital.java:134) at AssinadorDigital.btnAssinar_actionPerformed(AssinadorDigital.java:190) at AssinadorDigital_btnAssinar_actionAdapter.actionPerformed(AssinadorDigital.java:202) at javax.swing.AbstractButton.fireActionPerformed(Unknown Source) at javax.swing.AbstractButton$Handler.actionPerformed(Unknown Source) at javax.swing.DefaultButtonModel.fireActionPerformed(Unknown Source) at javax.swing.DefaultButtonModel.setPressed(Unknown Source) at javax.swing.plaf.basic.BasicButtonListener.mouseReleased(Unknown Source) at java.awt.Component.processMouseEvent(Unknown Source) at javax.swing.JComponent.processMouseEvent(Unknown Source) at java.awt.Component.processEvent(Unknown Source) at java.awt.Container.processEvent(Unknown Source) at java.awt.Component.dispatchEventImpl(Unknown Source) at java.awt.Container.dispatchEventImpl(Unknown Source) at java.awt.Component.dispatchEvent(Unknown Source) at java.awt.LightweightDispatcher.retargetMouseEvent(Unknown Source) at java.awt.LightweightDispatcher.processMouseEvent(Unknown Source) at java.awt.LightweightDispatcher.dispatchEvent(Unknown Source) at java.awt.Container.dispatchEventImpl(Unknown Source) at java.awt.Component.dispatchEvent(Unknown Source) at java.awt.EventQueue.dispatchEvent(Unknown Source) at java.awt.EventDispatchThread.pumpOneEventForHierarchy(Unknown Source) at java.awt.EventDispatchThread.pumpEventsForHierarchy(Unknown Source) at java.awt.EventDispatchThread.pumpEvents(Unknown Source) at java.awt.EventDispatchThread.pumpEvents(Unknown Source) at java.awt.EventDispatchThread.run(Unknown Source)(In reply to)HelloI solved the problem just changing the way i get the instance of the KeyStore: It was: "this.repositorio = KeyStore.getInstance("PKCS11", provider);"Now is: "this.repositorio = KeyStore.getInstance("PKCS11");"I singed the xml many times, without closing the application and it´s is workingfine now.But i still have some doubts....1) If i use KeyStore.getInstance("PKCS11", provider) The Exception doesnt occurs at the first time and the xml document is signed normally. However, i if try tosign again, the exception occurs. why ? is it a Bug ?2) What´s the difference between each one? When use each one ?TksI don't think so. The only thing that makes sense to me is that the first key issoftware based, and then the subsequent keys are hardware based and used witha cached Signature object that is software based. Are you reusing the XMLSignature objects each time you sign? Have you tried extracting the keycontents to see if it is hardware or software (ex: Key.getFormat should return "RAW" if it is hardware).I don't think I understand the question.(In reply to)occurstry toisNo. I am not reusing the XMLSignature objects each time i sign. Yes. I tried extracting the key contents: "SunPKCS11-SmartCard RSA private key, 1024 bits (id 1, token object, not sensitive, unextractable)"the question 2 is: what's the difference between using KeyStore.getInstance("PKCS11", provider) and using KeyStore.getInstance("PKCS11"). When use each one ?I know that to use KeyStore.getInstance("PKCS11") i must have to call Security.addProvider() before. but...which use ?I'm experiencing the same kind of weird behavior when I try to sign twice. Thecode does something like:Document document = loadDocument();smartCard.open(...); // registers SunPKCS11 providerPrivateKey privateKey = smartCard.getPrivateKey(); // from the keystorexmlSign(document, privateKey, ...);smartCard.close(); // removes SunPKCS11 providersmartCard.open(...); // registers SunPKCS11 provider againPrivateKey privateKey = smartCard.getPrivateKey();xmlSign(document, privateKey, ...);The second sign operation gives me:Caused by: org.apache.xml.security.signature.XMLSignatureException: Private keymust be instance of RSAPrivate(Crt)Key or have PKCS#8 encodingOriginal Exception was org.apache.xml.security.signature.XMLSignatureException:Private key must be instance of RSAPrivate(Crt)Key or have PKCS#8 encodingOriginal Exception was java.security.InvalidKeyException: Private key must beinstance of RSAPrivate(Crt)Key or have PKCS#8 encoding at org.apache.xml.security.signature.XMLSignature.sign(Unknown Source)In my smart card code it doesn't matter whether to useKeyStore.Builder builder = KeyStore.Builder.newInstance("PKCS11", this.pkcs11Provider, callbackHandlerProtection);or...("PKCS11", null, callbackHandlerProtection);I always get the exception. The funny thing is that I can sign twice via anon-XML signature, i.e., usingSignature.getInstance("SHA1withRSA");but, when using xmlsec, it's throwing the exception.Ok, I looked into this a little bit more and still need more details.First, if you remove the SunPKCS11 provider and create a new instance, then anyPrivateKey objects from the old SunPKCS11 instance will not be usable with thenew instance (and this exception will be thrown). If the privateKey object is from the new provider, it should work. Can you let me know if you are doing this?(In reply to)Each time i sign, i add the provider (Security.addProvider(p)) and create a newPrivateKey object. Im not reusing nothing.. is it correct ? or i must have toremove and add the provider each time? from what i know...when we use theaddProvider, it add the provider in the available slot, but we can add in aspecific slot, just using addProviderAt(p, 2) for instance.Ahh, here's what I think is happening:The first time you call Security.addProvider(p), your provider is added correctly, you create a new PrivateKey object using that same provider andthe xml signing operation works ok.The next time you create a new provider and call Security.addProvider(p), I bet the provider is not added because it is already installed (from the APIsstandpoint, it is the same provider even though it is a new instance). Check the return value of addProvider to see if it returns -1. *But* (and this is a major but), you then use the new provider instance tocreate a PrivateKey object. There's the problem. When you use this PrivateKeyto sign, the xml signature cannot find the provider that you used to create it(since it is not installed, it only finds the old provider but that won'twork with the new PrivateKey) and therefore the signing operation fails becauseit cannot find a provider that the key can be used with.Workarounds:Either, 1) explicitly remove the provider before you reinstall it each time, or 2) only create and install the provider once and always use this provider to create the PrivateKeys. Please confirm if this is the case.(In reply to)Exactly! the addProvider is returning -1 at the second time! (the first returneda high position: 7). however i am not having problems anymore because im usinglike this: Security.addProvider(p);KeyStore.getInstance("PKCS11");If i use KeyStore.getInstance("PKCS11", p), then i get the exception the secondtime!Sean Mullan, Thank you very much for your clarifications! i am doing this as apart of my monograph project (Information Systems course). I will put your namein the gratfulness area! :))Closing this out, as it looks like my explanation is correct and the problem has been resolved.As PKCS#11 has so great design, a separate instance must be instantiate for each slot of each provider.I tried to do what you requested, registered the provider to system, and passed null to KeyStore.Builder as a provider and it does work.But I don't think this is a valid solution, as there is no reason why all providers should be added to the global scope.Thought about this again...And I don't think this solution is valid.By convention all crypto functions throughout the SDK API accept explicit provider.If you can find another exception to this rule I may understand and drop this...But if xmlsec is the only exception, than it should sync up.(In reply to)xmlsec/jsr 105 are at a higher layer than the JCE APIs.I still think there should be a workaround to your problem. Something is happening in your code that causes the PrivateKey's provider to be different than what is in the JRE's list ofProviders. Have you tried the solution in this bug report, that is:Security.addProvider(p);KeyStore.getInstance("PKCS11");I seem to be experiencing the same problem reported in this bug, but no workaround seems to be effective, apart form restarting the application altogether.The situation I have is this: my application makes use of several different types of KeyStore. The user can use a configuration window to select which one to use, and even change some properties (e.g. the .dll module for the PKCS11 provider, or the slot number).The application uses the selected provider to send SOAP requests using Axis/Rampart 1.3. I have written a replacement Crypto class for Rampart that gets the keys and certificates from the correct keystore/provider. While everything works fine with JKS keystores, the PKCS11 provider only works until it gets reconfigured. Please note that I remove the previous Provider instance entirely, discard all previously obtained keys and keystores and recreate everything anew, but only the very first instance of the PKCS11 provider works.Code excerpt:----------------------------------------------------StringBuilder sb = new StringBuilder();sb.append("name = ");sb.append(providerName);sb.append("\nlibrary = ");sb.append(ConfigManager.getInstance().getConf().getDllPkcs11()); // Get the selected PKCS11 DLLsb.append("\nslot ="); sb.append(ConfigManager.getInstance().getConf().getPkcs11Slot()); // Get the selected slotsb.append("\ndisabledMechanisms = { CKM_SHA1_RSA_PKCS }\n"); String pkcs11config = sb.toString(); byte pkcs11configBytes[] = pkcs11config.getBytes();ByteArrayInputStream configStream = new ByteArrayInputStream(pkcs11configBytes);if (Security.getProvider(fullProvName)!=null) // Provider name + SunPKCS11 prefix Security.removeProvider(fullProvName);prov = new SunPKCS11(configStream);Security.insertProviderAt(prov, 1); // This is done to have Rampart select this provider for signature operations.ks = KeyStore.getInstance("PKCS11");-----------------------------------------------------Any key obtained after the first ever Provider instance gets replaced leads to this exception:-----------------------------------------------------org.apache.ws.security.WSSecurityException: Signature creation failed; nested exception is: org.apache.xml.security.signature.XMLSignatureException: Private key must be instance of RSAPrivate(Crt)Key or have PKCS#8 encodingOriginal Exception was org.apache.xml.security.signature.XMLSignatureException: Private key must be instance of RSAPrivate(Crt)Key or have PKCS#8 encodingOriginal Exception was java.security.InvalidKeyException: Private key must be instance of RSAPrivate(Crt)Key or have PKCS#8 encoding at org.apache.ws.security.message.WSSecSignature.computeSignature(WSSecSignature.java:663) at org.apache.rampart.builder.AsymmetricBindingBuilder.doSignature(AsymmetricBindingBuilder.java:611) at org.apache.rampart.builder.AsymmetricBindingBuilder.doSignBeforeEncrypt(AsymmetricBindingBuilder.java:385) at org.apache.rampart.builder.AsymmetricBindingBuilder.build(AsymmetricBindingBuilder.java:95) at org.apache.rampart.MessageBuilder.build(MessageBuilder.java:131) at org.apache.rampart.handler.RampartSender.invoke(RampartSender.java:64) at org.apache.axis2.engine.Phase.invoke(Phase.java:292) at org.apache.axis2.engine.AxisEngine.invoke(AxisEngine.java:212) at org.apache.axis2.engine.AxisEngine.send(AxisEngine.java:377) at org.apache.axis2.description.OutInAxisOperationClient.send(OutInAxisOperation.java:374) at org.apache.axis2.description.OutInAxisOperationClient.executeImpl(OutInAxisOperation.java:211) at org.apache.axis2.client.OperationClient.execute(OperationClient.java:163)-----------------------------------------------------Is there any way to replace the PKCS11 provider without having to restart the application? Note that since I have to change the configuration parameters at run-time I cannot leave the first provider instance alone...Have you tried the workaround implemented in:In order to use a specific provider, you must pass the Provider objectas a property to the XMLSignContext or XMLValidateContext, ex:signContext.setProperty ("org.jcp.xml.dsig.internal.dom.SignatureProvider", new MyProvider());Unfortunately I do not have access to the signature contexts, as they are entirely handled by Rampart. In fact to 'encourage' Rampart to use the right provider I force it to the highest priority, by using Security.insertAt().However from my testing it seems that Rampart does select the right provider, but the provider fails to work if it's not the first time that it has been instanced. If I don't try to remove and recreate it, it works any number of times. If I have to change any parameter (for example the slot number) I have to restart the application (and the virtual machine).Hi,I am experiencing the same problem.My application uses org.apache.xml.security.signature.XMLSignature for signing with software and hardware based keys. Each time when signing with PKCS#11 hardware, the appropriate security provider is created, added and then removed after the signing.The first time everyting works fine. However, if I try to sign again, the org.apache.xml.security.signature.XMLSignatureException with message "Private key must be instance of RSAPrivate(Crt)Key or have PKCS#8 encoding" is thrown:org.apache.xml.security.signature.XMLSignatureException: Private key must be instance of RSAPrivate(Crt)Key or have PKCS#8 encodingOriginal Exception was java.security.InvalidKeyException: Private key must be instance of RSAPrivate(Crt)Key or have PKCS#8 encoding at org.apache.xml.security.algorithms.implementations.SignatureBaseRSA.engineInitSign(SignatureBaseRSA.java:181) at org.apache.xml.security.algorithms.SignatureAlgorithm.initSign(SignatureAlgorithm.java:276) at org.apache.xml.security.signature.XMLSignature.sign(XMLSignature.java:497) ...java.security.InvalidKeyException: Private key must be instance of RSAPrivate(Crt)Key or have PKCS#8 encoding at sun.security.pkcs11.P11RSAKeyFactory.implTranslatePrivateKey(P11RSAKeyFactory.java:84) at sun.security.pkcs11.P11KeyFactory.engineTranslateKey(P11KeyFactory.java:115) at sun.security.pkcs11.P11KeyFactory.convertKey(P11KeyFactory.java:48) at sun.security.pkcs11.P11Signature.engineInitSign(P11Signature.java:375) at java.security.Signature$Delegate.engineInitSign(Unknown Source) at java.security.Signature.initSign(Unknown Source) at org.apache.xml.security.algorithms.implementations.SignatureBaseRSA.engineInitSign(SignatureBaseRSA.java:179) at org.apache.xml.security.algorithms.SignatureAlgorithm.initSign(SignatureAlgorithm.java:276) at org.apache.xml.security.signature.XMLSignature.sign(XMLSignature.java:497) ...The interesting thing is, if I retry the signing after catching the exception, it succeeds. I found out that this "recovery" is caused by the "cache cleanup" (XMLSignature:510):try { // initialize SignatureAlgorithm for signing sa.initSign(signingKey); ...} catch (XMLSecurityException ex) { sa.clearSignatureCache(); throw ex;}And the problem itself is caused by the "cache" ("SignatureAlgorithm.instancesSigning" variable). This map holds instances of SignatureAlgorithmSpi descendants. SignatureBaseRSA, which extends SignatureAlgorithmSpi, has "private java.security.Signature _signatureAlgorithm" variable, which, obviously, is initialised only once, and later gets associated with the first "suitable" security provider.During the second invocation, this provider (which is already removed BTW, but it doesn't matter - instance of java.security.Signature still holds a reference to it) is fed with a PrivateKey obtained from the newly added provider. This leads to futile attempts to "convert" the key and finally the InvalidKeyException.There are several possible workarounds (tested and confirmed to work):1. try-catch and then retry, letting the "recovery" magic to do the work :)2. Manual "cache cleanup" before the signing, for example:new SignatureAlgorithm(doc, signature.getSignedInfo().getSignatureMethodURI()).clearSignatureCache();(actually, clearSignatureCache() should be static, if it is intended for a "public" use - creating an instance of the SignatureAlgorithm is pointless here)Or, the "instancesSigning" can be cleared directly, using the reflection (this is for research only, of course):Field f = SignatureAlgorithm.class.getDeclaredField("instancesSigning");f.setAccessible(true);ThreadLocal t = (ThreadLocal)f.get(null);Map instancesMap = (Map)t.get();instancesMap.clear();3. Since the "cache" is implemented per-thread, the problem can be avoided by calling the sign() from a newly-created thread.Thinking of the possible fixes, one solution could be implementing SignatureBaseRSA.reset() (like in IntegrityHmac), which would assing a new instance to the _signatureAlgorithm. If it's worth having such a "cache" at all.I have a coworker who is experiencing this problem.The error doesn't happen in 1.3.0. Something changed in the interim. We first saw this in 1.4.1, and reproed it in 1.4.3(In reply to)I am not sure if the cause of the problem is the same, but regarding my case (see) the optimization with the "SignatureAlgorithm.instancesSigning" variable was introduced inon Jun 11 2006 (as "instances" variable, later split into "instancesSigning" and "instancesVerify"). Version 1.3 was released in October 2005; Version 1.4 - in January 2007. So yes, that happened in the interim.	22.0	id=23911	11	False	False	jorton	1
id=58935	REOPENED	None	Tomcat 8	Manager (	8.0.30	All All	P2 enhancement	Tomcat Developers Mailing List	2016-01-29 03:01 UTC by	bernard	2017-02-09 23:08 UTC (	0 users	It appears that it is not possible to re-deploy a web application while the server is running without Tomcat cleaning up the context in /conf/Catalina/localhost/.We want in server.xml<Host name="localhost" appBase="webapps" unpackWARs="true" autoDeploy="false">Currently, the context file /conf/Catalina/localhost/appName.xml is deleted on the undeploy part of the re-deploy cycle.We do not want the deletion of this file. We want a genuine update of the application that can be executed via the manager console by just overwriting the existing .war file. Currently, the manager application requires an undeployment first which cleans up the configuration.Or in other words, we want some means of updating the running application in a controlled fashion for production use. With controlled fashion we mean that we don't want to be exposed to the complexity that comes with autoDeploy="false". From our perspective, the manager application should have the capability to upload and overwrite an existing .war file, explode it and reload the application without deleting the context. We do not want to put the context into the war file because it contains installation specific configuration.	Sorry I should have written:With controlled fashion we mean that we don't want to be exposed to the complexity that comes with autoDeploy="true".You should be able to use the Manager text interface to do that. The update option isn't avaialble in the HTML UI. We should look at adding that.Thanks Mark for the info.I looked again atWithFAIL - Deployed application at context path /mypath but context failed to startI could not figure out how to use the update parameter in any of the commands without getting an undeploy and subsequent context xml file loss.I think the docs need fixing. Try something like this:This works for me. It swaps between test1.war and test2.war deployed as test.war and $CATALINA_BASE/conf/Catalina/localhost/test.xml remains untouchedI've updated the docs to remove the (very) out of date references to using jar:file: URLs.The fix has been applied to 9.0.x for 9.0.0.M3, 8.0.x for 8.0.33, 7.0.x 7.0.68 and 6.0.x for 6.0.46 onwards.Thanks for the command line options. Would you please clarify in which release the update option will be available in the HTML UI. Many thanks.Could we have juste an option to prevent it from happening ?So there will be no probleme of retrocompatibility.	7.0	id=24437	16	False	True	jessh	1
id=31418	REOPENED	None	Apache httpd-2	mod_ssl (	2.0.54	All All	P3 normal	Apache HTTPD Bugs Mailing List	2004-09-25 03:58 UTC by	Kevin Bentley	2012-04-18 07:05 UTC (	5 users	Because SSLUserName is implemented by a fixup hook, and fixup hooks happen after auth and access check hooks, other modules (like mod_authz_svn) cannot use this variable, since it isn't set until after they are done with authentication. A simple solution would be to move this block of code from the fixup hook to the end of the ssl_hook_Access method: /* * Set r->user if requested */ if (dc->szUserName) { val = ssl_var_lookup(r->pool, r->server, r->connection, r, (char *)dc->szUserName); if (val && val[0]) { r->user = val; } }Then, the module can register the hook after mod_ssl.c and get the user variable.	I don't think anyone really wanted SSLUserName to be supposed to be a substitutefor really doing authentication. If you want that, you can use FakeBasicAuth. SSLUserName was really just supposed to fix the logging problem.If you use FakeBasic auth, your username is very long and ugly. Subversion's authz_svn module uses the username for logging changes. It really sucks to have a log file that is the entire certificate subject.The reason I filed this is that I was trying to patch Subversion's authentication module to allow the use of SSLUserName. I expected that I'd only have to ensure that the module was hooked after mod_ssl, but because the SSLUserName is only filled in in the fixup hook, that isn't possible.I'm currently running a server with this patch on it, with the Subversion authz_svn module modified to support it as well. This works very well, and is much better than FakeBasicAuth. Several people on the Subversion mailing list have asked for a feature just like this.I totally agree with Kevin. FakeBasicAuth might be working for cases where youdon't care about the username. But having the full certificate subject asusername is definitely a problem with subversion. Not only that the log filehas the dn everywhere, it also makes ViewCVS output bloated, and results inbroken commit-log mails, since the username is again used as From: header in themails.using apache+ssl+mod_dav_svn+authz+client_certificates is the only way to get adecently secure subversion repository with per-file granular permissions andstrong crypto running...Yes, this could all be fixed up within svn or any other later module, rewritingthe certificate subject, replacing it with the email address contained within.But I think the source of this ugliness is FakeBasicAuth in the beginning. Itstarts with certificate subjects in the 'passwd' files.OK sorry, I didn't understand this properly; this is done as you suggest on thetrunk, I'll propose for backport.[One of the really, really amazing things of this small world is, thatyou always meet the same people. :-) ]Two things: - the backport is wrong (and also the code on the trunk). The reason for this: ATM, the setting of r->user only happens if nothing in the ssl_hook_access returns DECLINED, FORBIDDEN or anything else before the subroutine actually comes to the setting of r->user, some hundreds of lines below the start of the subroutine (That is why "one method, one exit" is a good thing).- the only way to actually get this to work (at least what I found) in all corner cases is to factor out the user name setting (which should run all the times anyway, no matter what other parts of ssl_hook_access are run or not run) and add it as a separate hook to the processing chain.I built a patch against 2.0.54. I run a slightly older version of this (forFedora Core 1 2.0.51-1.6.legacy) with Subversion 1.1.4 and client certificateauthentication and mod_authz_svn) and it ran fine in all three tests that I did.:-) Side-nit: One of the good thing of the countless code and style checkers forother programming languages besides C is that they keep your line-per-methodcount down. ssl_hook_access is _screaming_ for a refactoring and a breaking downinto smaller parts.CreatedPatch against 2.0.54 to implement a working SSLUserName feature.For unknown reasons, bugzilla decided to attach this file to another,completely innocent bug report where it should not go. Sorry about this, don'tknow how it happened.Can you please describe a precise scenario where the current code is notsufficient? Most of the earlier exits from the function are for fatal errorse.g. SSL_get_peer_certificate(ssl) == NULL etc.I admit that I've skimmed over the ssl_hook_access method (it has > 600 lines)and yes, most return pathes end in HTTP_FORBIDDEN (There is a DECLINED when SSLis off but this case is not really interesting for setting the username from a SSL certificate... ;-) ) So yes, you are correct, the current (2.0.54) version of the httpd does work inmy scenario (I was testing with 2.0.51 which does not have SSLUserName at alland 2.0.52 from Fedora Core 3, which has it but not in the ssl_hook_access butin ssl_hook_fixup). I still think that this method should be reworked, but this is more of abikeshed painting issue. I close the bug.The fix for this bug appears to have somewhat broken things, as described in theDebian bug here:This bit from the commit seems suspect to me:* However, if FakeAuth is being used then this isn't the case so* we need to postpone setting the username until later.Then you go and test if FAKEBASICAUTH is enabled, and don't perform any magic ifit is... But then "later" never comes, and the username is never set.I'm not convinced by that. If FakeBasicAuth is used then mod_ssl should letr->user be set up by the normal means, as it was is already. SSLUsername is tobe used in the cases where you *don't* get r->user set at all, and yetauthentication has taken place; i.e. using client certs. That's the problem itsolves.I'll admit to having not read the section of code in question terribly carefullyto see why it's doing what it's doing, it was merely a 20-second glance throughto see why the user's installation appears to be behaving differently. If youcould look at the Debian bug referenced in my last comment and either give a"oh, that new behaviour is expected and we'll update the docs" or "that's a bug,oops", that'd be great. This isn't a feature I personally use, nor one I havetime to debug right now, so I was just passing the reported breakage along.Let me add my voice to this one... since I just fought with this very problemmyself, and came to a similar solution before being directed to this issue.The problem is that FakeBasicAuth gives the user _no_ ability to set whatr->user will become; it forces it to the DN from the certificate and that is it.I think it's reasonable to let SSLUserName have the desired effect even inFakeBasicAuth mode; I've patched my mod_ssl (from 2.0.54) to set theAuthorization header based on dc->szUserName instead of clientdn if SSLUserNamewas specified. This appears to work fine, and allowed me to work with anunpatched mod_authz_svn (and the remote user name shows properly in access_logas well). This also means that the usernames are 'proper' in the htpasswd fileand everywhere else that httpd would normally see/use them.I've attached my patch for this behavior; please let me know if there is somereason why mod_ssl should _not_ behave this way.CreatedPatch against 2.0.54 to let SSLUserName control the username for FakeBasicAuth(In reply to)Is this 2.0-specific? I'm running up against the same limitation in 2.2.14.+1 to the following summary: Rather than overriding SSLUserName if set, +FakeBasicAuth should use the user name from SSLUserName if it is set, defaulting to the current one-line subject representation only if SSLUserName is not set.***has been marked as a duplicate of this bug. ***	15.0	id=24890	5	False	False	nd	1
id=24437	REOPENED	None	Apache httpd-2	mod_auth_ldap (	2.0.47	PC All	P3 normal	Apache HTTPD Bugs Mailing List	2003-11-05 16:58 UTC by	Jess Holle	2015-07-07 08:50 UTC (	0 users	In order to authenticate against ActiveDirectory-style uids with backslashes in them the authentication user name contains this character.Unfortunately the LDAP logs are showing that Apache is over-escaping the backslash -- at least on Windows (2000 SP4, all security updates applied). I suspect this above the LDAP SDK layer and thus applies to all platforms, but I could be proven wrong, of course.This is a serious issue to anyone needing to support use of such (existing) directories. Both workarounds and pointers to code areas to investigate (other than "look at the mod_auth_ldap and/or util_ldap sources" -- I know that much) would be greatly appreciated!	It turns out that the Microsoft LDAP SDK escapes these characters on its own!I found the filter escape code in mod_auth_ldap_build_filter in mod_auth_ldap.cand observed that its input and output was exactly what I'd expect. I also notethat it's output is what is passed to the LDAP SDK.I commented out the escape code (in the simplest fashion, i.e. yes, I could usestrncpy at this point) and then (and only then) am able to authenticate with \,), (, and * in my user name.I assume this is a Microsoft LDAP SDK feature as my other LDAP SDK experiencesuggests the escaping done by mod_auth_ldap is required.All the same, I believe we should #if out this filter code when using theMicrosoft LDAP SDK -- as it only currently serves to prevent that which it isintended to allow.My change is to add the comments in the code excerpt below taken frommod_auth_ldap.c (sorry, I'm not creating a patch as strncpy would be better,etc, etc): filtbuf_end = filtbuf + FILTER_LENGTH - 1; for (p = user, q=filtbuf + strlen(filtbuf); *p && q < filtbuf_end; *q++ = *p++) {/* Microsoft LDAP SDK does this automatically (!); doing this here causesdouble-escaping!!! The following code block must therefore be removed when using Microsoft'sLDAP SDK.*//* if (strchr("*()\\", *p) != NULL) { *q++ = '\\'; if (q >= filtbuf_end) { break; } }*/ }I believe your patch is correct, however we need to use a #ifdef to make this determination, to prevent clashes with folks building under OpenLDAP or the Netscape LDAP sdk.Yes, that's a big reason why I did not provide a patch -- I had not taken thetime to investigate the proper #ifdef's, etc, to use.Patch committed to v2.1.0-devPlease test if this solves this problem.Doh!I'm sure that will work as it is identical to my fix apart from a proper #if block.I should have taken the 2 extra minutes when I patched this on my own copy tolook in apr_ldap.h and see #define APR_HAS_MICROSOFT_LDAPSDK 1on Windows and done a proper #if block around this.Thanks for rounding out this fix and getting it in. A merge back to 2.0 wouldbe good too :-)Fixed in v2.0.50-dev.As the person who provided the patch, I must regretfully re-open this SPR.The patch works great *except* when the character immediately following the \ isa valid hexidecimal character. I'm not sure how to handle this case with theMicrosoft LDAP SDK.Hopefully we're missing some simple "behave correctly" flag on the MicrosoftLDAP SDK...CreatedPatch to address remaining issues (\[0-9,a-f], e.g. \a, etc)I discovered Microsoft documentation atthat essentially states "RFC 1960 says...", followed by an accurate quote ofthis IETF RFC, followed by section titled "Special characters" where they statea completely different means by which they require you to handle all the specialcharacters from the IETF RFC -- without actually stating that they're requiringother syntax than that indicated by the RFC much less why they require this!I produced a patch wherein the Microsoft documentation is obeyed when usingtheir LDAP SDK. This seems to fix the remaining issues, so I've attached thepatch (see above).Is it possible to create a patch for v2.1 also? The patch does not apply cleanlyto mod_authnz_ldap.c, not sure why.CreatedPatch against HEAD rather than against 2.0.52I've attached a patch against mod_authnz_ldap.c as requested.Note that I've only tested this on Windows against Microsoft's LDAP at thispoint (the problematic case). After pre-processing, there should be nodifferences in the code for any other platform resulting from my changes.Committed to HEAD, backport proposed to v2.0The LDAP code in v2.0 is effectively abandoned, as it's too difficult to fix atthis point (most of the fixes have involved major rewrites and have gone inhttpd v2.1 and apr v1.1).If this is broken in httpd v2.1, please reopen this bug.I guess I'll check again in 2.2 whenever it is "stable".I have to support a redistribution of a stable 2.x release, so at this time thatwould be 2.0.52. Thus I'll keep applying my patch to 2.0.x.Hello,I just found same problem using 2.2.22-1ubuntu1.4 apache2 package.Seems like this bug still needs to be patched.Any help with this problem will be appreciated.	16.0	id=25469	9	False	False	nd	1
id=41676	REOPENED	None	Apache httpd-2	mod_proxy (	2.5-HEAD	All All	P3 enhancement	Apache HTTPD Bugs Mailing List	2007-02-22 01:06 UTC by	rahul	2007-08-24 02:44 UTC (	0 users	Refactoring mod_proxy_ftp to do these:1) organize the heavy mod_proxy_ftp_handler to be similar to mod_proxy_http (use the same functions as the mod_proxy_http where possible)2) Split the mod_proxy_ftp_handler to smaller functions organized as a state machine so that the code flow is easier to follow3) Avoid the network traffic to ftpserver where possible (and instead Intelligently guess the required values.)	Createdmod_proxy_ftp.c diffThe patch contains refactored mod_proxy_ftp.c diffed against 2.3 trunkThanks for the patch. It is rather huge and thus hard to review. In order to getit applied please use the following procedure:1. Split the patch in smaller more consumable chunks (at least into three chunksthat address each of the 3 issues you try to solve separately).2. Attach the patches here and post them toto drawdeveloper attention to them and to have a discussion about them.3. Be PPP (patient, persistent and polite) :-) here and on dev@.As people are usually very busy smaller and easier patches increase thelikelihood that someone finds time to review them. Furthermore sometimes people(including me :-)) just forget about cool patches because they were not able tohandle them immediately, so a gentle ping sometimes helps to bring this back onthe agenda.Createdmod_proxy_ftp.c refactoring patch (removes redundant functions)This is a patch to do:- organize the heavy mod_proxy_ftp_handler to be similar to mod_proxy_http (use the same functions as the mod_proxy_http where possible)The proxy_ftp_handler has been modeled after proxy_http_handler, and uses thesame functions that is used in that. This allows us to remove the portion thatdoes - acquire-connection, determine-connection, connect-backend andconnection-create.A new function ap_proxy_ftp_request is added which implements the exclusivelyftp logic. This function contains the rest of proxy_ftp_handler code.	3.0	id=22686	7	False	False	bojan	1
id=41763	REOPENED	None	Apache httpd-2	mod_cache_disk / mod_disk_cache (	2.2.3	HP Linux	P3 normal	Apache HTTPD Bugs Mailing List	2007-03-05 03:00 UTC by	Eric Suran	2007-03-14 14:32 UTC (	0 users	I run httpd 2.2.3 on Linux 2.4.21-47.0.1.ELsmp (RedHat ES3) and I have enabledmod_disk_cache for several sites. Directives CacheDirLength and CacheDirLevels are respectively set to 1 and 2.Problem : subdirectories under CacheRoot directory are 2 characters long insteadof 1, and 3 subdirectories are created.Httpd processes have been stopped and restarted after configuration changes andall files and directories under CacheRoot directories deleted.The same problem is reproduced with httpd 2.2.4.	Sorry but I cannot reproduce this problem with 2.2.4. I guess something is wrongwith your configuration such that you fall back to the default values. Have youset some of mod_disk_caches directives inside a virtual host and some outside ofa virtual host?The involved directives are not redefined inside the virtual hosts configurationbut this is done for CacheRoot directive.Our configuration looks like this :CacheRoot /a/CacheDirLength 1CacheDirLevels 2...<VirtualHost xxx>CacheRoot /a/xxx</VirtualHost xxx><VirtualHost yyy>CacheRoot /a/yyy</VirtualHost yyy>Then we have performed more tests and the problem is corrected if we redefinedCacheDirLength and CacheDirLevels inside the virtual host configuration.So, we guess the problem is CacheRoot directive override the previousconfiguration with the default values.(In reply to)Well analysed. This is exactly the case. The reason is that there is no functionto merge per-server configs.OK, thank you. Then we will update our configuration...Do you know if a such function will be developed ?Additionally, maybe default setting could be changed accordingly to therecommended values as describe in "caching guide".The lack of a merge function is a real bug, not really something that should bedocumented.	5.0	id=31418	16	False	True	laforge	1
id=32051	REOPENED	None	Apache httpd-2	Other Modules (	2.1-HEAD	All other	P3 normal	Apache HTTPD Bugs Mailing List	2004-11-04 03:35 UTC by	Paul Querna	2013-06-26 12:34 UTC (	1 user	mod_example is not thread safe. This is a tracking bug for making mod_examplethread safe for all MPMs.Originally reported as part of bug @29709	Should be fixed in, by removing all use of global variables to store the backtrace prose. The remaining startup backtrace should be read-only after the server forks and spawns threads.This issue was fixed in trunk/2.4, but not in 2.2.***has been marked as a duplicate of this bug. ***	3.0	id=31302	16	False	False	slive	1
id=43997	REOPENED	None	Apache httpd-2	mod_ssl (	2.2.6	All All	P3 minor	Apache HTTPD Bugs Mailing List	2007-11-29 14:43 UTC by	Marc W. Mengel	2011-05-10 05:25 UTC (	0 users	The httpd server currently issues a warning: [warn] Init: SSL server IP/port conflict: vhost1.example.domain:443(/.../vhosts.conf:14) vs. vhost2.example.domain:443 (/.../vhosts.conf:71)This warning is only accurate if those two vhosts have differing SSLcertificates. If they have the same one (i.e. a '*.example.domain' wildcard)this warning should be suppressed.To reproduce:1) self-sign a certificate with CN=*.your.domain2) setup two NameVirtualHosts on the same IP & port using that certificate & key3) start httpd4) check error_log	No this behaviour is correct as only the SSL settings from the first virtualhost are used. It has been said frequently here: Even with wildcard certs namedbased virtual hosting is a bad idea and has many pitfalls. So a warning is due here.(In reply to)Then the warning is only correct if the SSL settings in any other virtual hostare different from the first one.This could be a Really Useful Warning if it actually differentiated the casethat works from the one that doesn't; instead it just whines that you're doingname based virtual hosting with SSL, which hopefully you already knew.But if the goal is just to complain about SSL name-based-vhosting rather than tocomplain when someone does it wrong, then please reclose the ticket and I'll drop it.Yeah, I agree with Marc. There should be a way to suppress this error message. Or drop its loglevel down to notice, instead of warn. I'm a big boy. I know what I'm doing.	3.0	id=41763	10	False	False	rpluem	1
id=44221	REOPENED	None	Apache httpd-2	mod_speling (	2.4.10	All All	P3 critical	Apache HTTPD Bugs Mailing List	2008-01-14 02:10 UTC by	Rainer Perske	2016-02-01 09:39 UTC (	10 users	From Documentation of "CheckCaseOnly Directive": "When set, this directivelimits the action of the spelling correction to lower/upper case changes. Otherpotential corrections are not performed."But if I have e.g. the files "foo.gif" and "foo.jpg", a request on "foo.bar"still returns a Multiple Choices page instead of a Not Found page even with"CheckCaseOnly On".Proposed bug fix: Disable search for files with "common basename" ifCheckCaseOnly is set to On.Proposed alternative, even better bug fix: Make the WANT_BASENAME_MATCHavailable as a configuration directive with context "server config, virtualhost, directory, .htaccess".Thank you in advance	CreatedJust make an else into else if... Simplest of fixes.Done. This should fix the issue.I just noticed that this problem is fixed in the current version, but still having status NEW.I'm testing this on a apache 2.2.16 Unix fresh install but I'm still running into exact the same problems as submitted in 2008.It's unexpected (and not wanted) behaviour when apache lists a few "multiple choices" (status code 300) when the checkCaseOnly directive is on!!I have this in my httpd.conf:CheckSpelling onCheckCaseOnly onfirst directive to use the mod_spelingsecond directive to limit only to case correctionsI looked at code, nothing has been changed to address the issue. Rainer, not sure what you saw as being "fixed".My patch would change what some are used to, but bring it to what I consider expected behavior.If I remember correctly, Rob, I thought that your fix was an official one.I'm experiencing this issue in version 2.2.17.My config:<IfModule mod_speling.c>CheckCaseOnly OnCheckSpelling On</IfModule>Example:1. Navigate to non-existing resource -2. Results in 300 Multiple Choices - Similar documents found with a single folder found - /js/jQuery (common basename) I have reviewed the source of mod_speling.c from version 2.2.15 up to 2.3-trunk and there has been no change/patch implemented; all versions are affected.If CheckCaseOnly was 'Off' I'd expect this result; however with it set to 'On' this is incorrect behavior and goes against what CheckCaseOnly implies.In addition, I like Rainer's suggestion of making WANT_BASENAME_MATCH a configuration directive with support for context; as this would be a great addition to this module. It could be defaulted to On/true for backward compatibility and allow for those experiencing issues to set it accordingly.I was just about filing a new bug report about this, when I found it was already discovered four years ago. Cmon, this should be really easy to fix, and it can also be considered a bit of security related, as it may lead to disclosure of information.Fixed in trunk.I will also propose a new option CheckBasenameMatch as suggested by Rainer Perske.I'm still seeing this problem in 2.2.22 on Ubuntu. I don't see any mention of mod_speling fixes in the change log up through 2.2.27 with the last mention being back in 2.2.9.Did (or will) this patch make it into the 2.2 branch? Or am I missing something?It has been proposed to 2.4 but refused becasue it changed the default behaviour of the module, which is not what is expected on a stable branch.It has not been proposed for backport for 2.2, but it would have been refused for the same reason.I have planned to rework the patch to avoid the change of default behaviour but have not done it yet.Anyway, I doubt it would be backported in 2.2, even reworked.To clarify the issue, it's expected that apache gives a "Multiple Choices" list with CheckCaseOnly directive on, if there is "sOmE.file" and "SoMe.file" in the same directory. I believe most people didn't expect a list with "foo.gif" and "foo.jpg" when requesting "foo.bar", but that's the current behaviour and the proposed patch breaks it.The proposed alternative, make WANT_BASENAME_MATCH a configuration directive, is a better solution. It is a controversial option, and the only way to solve it and make everyone happy is let each one choose.CreatedMake "WANT_BASENAME_MATCH" configurableHere is a patch to add the proposed "CheckBasenameMatch" config directive.It replaces the hard-coded WANT_BASENAME_MATCH and defaults to "on" so it maintains compatibility.Note that this patch is not compatible (and obsoletes/replaces) the one contributed by Rob on 2009-08-25. Setting "CheckBasenameMatch" to "off" will have the same effect.The new patch is actually against 2.4.10 code but that shouldn't be an issue, I presume.I'm willing to provide a patch against the docs as well, if there is a reasonable chance that the source code patch will be accepted.As this bug is also present in 2.4.10, I changed the version number accordingly. It seems at 2.2.x there is nobody home anymore.The patch byseems to be totally adequate to fix this issue once and for all in a backwards compatible manner. Any reason why it hasn't been applied yet? The current behavior is simply broken and counterintuitive.Hi!Once patch is applied the issue could be verified by this:(- is easy way to verify some existed apache issues against your environment ).At my environment tests suite fails ( which is expected ? )vagrant@Debian-jessie-amd64-netboot:~/my/apache-swat$ swat -t 44221/home/vagrant/.swat/.cache/9836/prove/44221/FOO.bar/00.GET.t ...ok 1 - GET 127.0.0.1/44221/FOO.bar succeeded# http headers saved to /home/vagrant/.swat/.cache/9836/prove/xeu0b4EQHn.hdr# body saved to /home/vagrant/.swat/.cache/9836/prove/xeu0b4EQHnok 2 - output match '200 OK'ok 3 - output match /Location: \S+/ok 4 - 'Location:' match 'foo.bar'1..4ok/home/vagrant/.swat/.cache/9836/prove/44221/foo.html/00.GET.t ..ok 1 - GET 127.0.0.1/44221/foo.html succeeded# http headers saved to /home/vagrant/.swat/.cache/9836/prove/ipywRk4nnn.hdr# body saved to /home/vagrant/.swat/.cache/9836/prove/ipywRk4nnnok 2 - output match /HTTP\/(\S+) (\d+) \S+/not ok 3 - 'HTTP/1.1 300 Multiple Choices' match /404 /# Failed test ''HTTP/1.1 300 Multiple Choices' match /404 /'# at /usr/local/share/perl/5.20.2/swat.pm line 218.not ok 4 - output match 'Not Found'# Failed test 'output match 'Not Found''# at /usr/local/share/perl/5.20.2/swat.pm line 218.1..4# Looks like you failed 2 tests of 4.Dubious, test returned 2 (wstat 512, 0x200)Failed 2/4 subtests/home/vagrant/.swat/.cache/9836/prove/44221/foo.bar/00.GET.t ...ok 1 - GET 127.0.0.1/44221/foo.bar succeeded# http headers saved to /home/vagrant/.swat/.cache/9836/prove/MuDUZpFoR3.hdr# body saved to /home/vagrant/.swat/.cache/9836/prove/MuDUZpFoR3ok 2 - output match '200 OK'1..2ok/home/vagrant/.swat/.cache/9836/prove/44221/foo.baz/00.GET.t ...ok 1 - GET 127.0.0.1/44221/foo.baz succeeded# http headers saved to /home/vagrant/.swat/.cache/9836/prove/jLZttcgKYZ.hdr# body saved to /home/vagrant/.swat/.cache/9836/prove/jLZttcgKYZok 2 - output match '200 OK'1..2ok/home/vagrant/.swat/.cache/9836/prove/44221/foo.BAR/00.GET.t ...ok 1 - GET 127.0.0.1/44221/foo.BAR succeeded# http headers saved to /home/vagrant/.swat/.cache/9836/prove/J91YGHyu9m.hdr# body saved to /home/vagrant/.swat/.cache/9836/prove/J91YGHyu9mok 2 - output match '200 OK'ok 3 - output match /Location: \S+/ok 4 - 'Location:' match 'foo.bar'1..4okTest Summary Report-------------------/home/vagrant/.swat/.cache/9836/prove/44221/foo.html/00.GET.t (Wstat: 512 Tests: 4 Failed: 2) Failed tests: 3-4 Non-zero exit status: 2Files=5, Tests=16, 1 wallclock secs ( 0.03 usr 0.01 sys + 0.30 cusr 0.01 csys = 0.35 CPU)Result: FAILMy apache build info:vagrant@Debian-jessie-amd64-netboot:~/my/apache-swat$ sudo ~/apache/bin/apachectl -VServer version: Apache/2.4.18 (Unix)Server built: Jan 30 2016 10:17:37Server's Module Magic Number: 20120211:52Server loaded: APR 1.5.1, APR-UTIL 1.5.4Compiled using: APR 1.5.1, APR-UTIL 1.5.4Architecture: 64-bitServer MPM: event threaded: yes (fixed thread count) forked: yes (variable process count)Server compiled with.... -D APR_HAS_SENDFILE -D APR_HAS_MMAP -D APR_HAVE_IPV6 (IPv4-mapped addresses enabled) -D APR_USE_SYSVSEM_SERIALIZE -D APR_USE_PTHREAD_SERIALIZE -D SINGLE_LISTEN_UNSERIALIZED_ACCEPT -D APR_HAS_OTHER_CHILD -D AP_HAVE_RELIABLE_PIPED_LOGS -D DYNAMIC_MODULE_LIMIT=256 -D HTTPD_ROOT="/home/vagrant/apache/" -D SUEXEC_BIN="/home/vagrant/apache//bin/suexec" -D DEFAULT_PIDLOG="logs/httpd.pid" -D DEFAULT_SCOREBOARD="logs/apache_runtime_status" -D DEFAULT_ERRORLOG="logs/error_log" -D AP_TYPES_CONFIG_FILE="conf/mime.types" -D SERVER_CONFIG_FILE="conf/httpd.conf"	15.0	id=43997	4	False	False	rpluem	1
id=19569	REOPENED	None	Lenya	Miscellaneous (	1.0	Other other	P3 major	Lenya Developers	2003-05-02 12:15 UTC by	Gregor J. Rothfuss	2007-07-16 02:13 UTC (	1 user		Michael Wechner: "still todo"any volunteers for this?i agree with a. kuckartz, later is badI did a load test using jmeter and the content of one of our live sites(). I did two identical test: 1. Site completely in lenya using tomcat to deliver the pages2. Static export of the whole site (wget) and then using apache to deliver thepages. Jmeter testplan:50 concurrent user access 10 times 2 pages (1000 Request)The first page was the homepage and the second one a page somewhere deeper inthe sitetree.Results (summarized):No error Tomcat: 1.5 request/secondApache: 100 request/secondMaybe we can add a module to generate JMeter test plans, e.g. visit each page ofthe publication etc.?+1, worth considering for 1.4.1 imho. or maybe it's a topic for the hackathon?should not be too hard to do...	6.0	id=41676	5	False	False	jim	1
id=51962	REOPENED	None	Fop - Now in Jira	page-master/layout (	1.0	PC Linux	P2 normal	None	2011-10-05 14:59 UTC by	Mehdi Houshmand	2012-07-23 18:38 UTC (	2 users	If force-page-count="odd" and the first page has an overflowing region onto a second page, then the 3rd and last page should use the page-position="last" simple-page-master in the conditional-page-master-reference. However, it was using the "rest".I'm having a few issues with the apache git server, so I'll attach the patch when the issues have been resolved.	CreatedPage breaker patchThis work is a collaboration between myself and Peter Hancock{Patch applied in rev 1181660Thanks Mehdi***has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ***Hi Mehdi, Peter,I believe that there is a small bug inside the new PageSequenceLayoutManager.getForcedLastPageNum() method:You're handling "even" and "end-on-even" the same way. I think that this is not entirely correct, because "even" only forces an even number of pages inside the current page-sequence, but not for the whole document. Since lastPageNum refers to the total amount of pages, the mod check should be done like that:(lastPageNum - startPageNum + 1) % 2 != 0The same applies to "odd" and "end-on-odd".Could you please check that?ThanksMatthias	6.0	id=32051	5	False	False	sander	1
id=12033	REOPENED	None	Apache httpd-2	All (	2.4.10	PC FreeBSD	P3 normal	Apache HTTPD Bugs Mailing List	2002-08-26 10:25 UTC by	Sander Holthaus	2015-10-01 11:22 UTC (	8 users	A gracefull restart results in the following message:[Fri Aug 23 10:30:21 2002] [notice] Graceful restart requested, doing restart[Fri Aug 23 10:30:25 2002] [info] mod_unique_id: using ip addr 127.0.0.1[Fri Aug 23 10:30:26 2002] [notice] Digest: generating secret for digest authentication ...[Fri Aug 23 10:30:26 2002] [notice] Digest: done[Fri Aug 23 10:30:26 2002] [notice] Apache/2.0.40 (Unix) configured -- resumingnormal operations[Fri Aug 23 10:30:26 2002] [info] Server built: Aug 16 2002 01:39:38[Fri Aug 23 10:30:26 2002] [warn] long lost child came home! (pid 62461)[Fri Aug 23 10:30:26 2002] [warn] long lost child came home! (pid 62460)[Fri Aug 23 10:30:26 2002] [warn] long lost child came home! (pid 62459)[Fri Aug 23 10:30:26 2002] [warn] long lost child came home! (pid 62458)[Fri Aug 23 10:30:26 2002] [warn] long lost child came home! (pid 62457)[Fri Aug 23 10:30:26 2002] [warn] long lost child came home! (pid 62456)Server is FreeBSD 4.6 with Apache 2.0.40. A similiar problem was present in previous versions, however they seemed to be related to the use of cronolog. Currently, the logfilerotation tool which comes with Apache is used.Server is a basic low activity server with several virtual hosts.	someone told something like that:apache uses PIDs for each of its childs... and it occurs, that apache uses PIDs more than one time...and if this occurs, apache write: long lost child came home!Could be related, here is what I see with linux (apache-2.0.41-dev): [Mon Sep 23 11:03:35 2002] [notice] Apache/2.0.41-dev (Unix) PHP/4.2.3 DAV/2 configured -- resuming normal operations [Mon Sep 23 11:07:05 2002] [error] [client 127.0.0.1] File does not exist: /srv/www/default/html/server-status [Mon Sep 23 11:07:21 2002] [notice] Graceful restart requested, doing restart [Mon Sep 23 11:07:54 2002] [notice] seg fault or similar nasty error detected in the parent processSorry, konqueror is not bugzilla-friendly, reformating my latest post:[Mon Sep 23 11:03:35 2002] [notice] Apache/2.0.41-dev (Unix) PHP/4.2.3 DAV/2configured -- resuming normal operations[Mon Sep 23 11:07:05 2002] [error] [client 127.0.0.1] File does not exist:/srv/www/default/html/server-status[Mon Sep 23 11:07:21 2002] [notice] Graceful restart requested, doing restart[Mon Sep 23 11:07:54 2002] [notice] seg fault or similar nasty error detected inthe parent processI'm trying to download 2.0.42 to test if this persists, but for some reason I'munable to reach httpd.apache.org. Will try again later.Tried 2.0.42, and it still happens, but ONLY if the php module (currently 4.2.3)is loaded. Without the PHP module, apachectl graceful works just fine. Bummer.This bug has nothing to do with PHP. It's just a "regular" (with_experimental) install. Did not try 2.0.42 yet.Well, for me apachectl graceful only works if php is not loaded OR if php fromtoday's CVS is used. So, for me, it's a PHP issue that is fixed in CVS.seems to be no(t| longer) an httpd problem. Fine! ;-)[Mon Feb 10 01:20:54 2003] [notice] Graceful restart requested, doing restart[Mon Feb 10 01:20:57 2003] [info] mod_unique_id: using ip addr 127.0.0.1[Mon Feb 10 01:20:58 2003] [notice] Digest: generating secret for digest authent[Mon Feb 10 01:20:58 2003] [notice] Digest: done[Mon Feb 10 01:20:58 2003] [notice] Apache/2.0.43 configured -- resuming normal[Mon Feb 10 01:20:58 2003] [info] Server built: Oct 7 2002 23:56:44[Mon Feb 10 01:20:58 2003] [warn] long lost child came home! (pid 4173)[Mon Feb 10 01:20:58 2003] [warn] long lost child came home! (pid 4172)[Mon Feb 10 01:20:58 2003] [warn] long lost child came home! (pid 4171)[Mon Feb 10 01:20:58 2003] [warn] long lost child came home! (pid 4170)[Mon Feb 10 01:20:58 2003] [warn] long lost child came home! (pid 4169)[Mon Feb 10 01:20:58 2003] [warn] long lost child came home! (pid 4168)[Mon Feb 10 01:20:58 2003] [warn] long lost child came home! (pid 4167)No PHP, no experimental modules loaded. OS: FreeBSD 4.7-stableSander,Do you have any piped log programs or perhaps mod_rewrite external processes,or any child processes created by httpd (other than those serving httprequests)? I have a hunch (perhaps a bad one :( ) that when some suchprocesses go away during restart some accounting is messed up and the MPMcan issue such a log message.Thanks!The only child processes created by the httpd are the piped logs. No external rewrite_engine processes or other child processes.Relevant entry from my vhosts.conf: CustomLog "|/usr/local/sbin/rotatelogs /usr/local/www/domain.nl/logs/access_%Y%m%d.log 86400" combinedI checked with ps aux, and it are the piped logs that cause the problem. (the PID's belong to them).still present in 2.0-HEAD, 2.1-HEAD, etc.MPMs handle other children differently for graceful restart vs. hard restart,resulting in these confusing log messages.ap_reclaim_child_processes() is called for apachectl stop and apachectl restart,but not apachectl graceful.apr_proc_other_child_refresh_all(APR_OC_REASON_RESTART) is called fromap_reclaim_child_processes() but nowhere else. That is the code that killsthe piped loggers from the server generation that is currently shutting down.This cleaner way of cleaning them up isn't done for graceful restart. Forgraceful restart, the loggers get whacked by the plog pool going away.There are a couple of ways to fix this. One way is for the MPM to callapr_proc_other_child_refresh_all() for both types of restarts. I believe thatanother way is to change piped log maintenance function to terminate the pipedlogger when it gets the UNREGISTER call. That is nice because whenever the poolgoes POOF the piped logger will terminate cleanly, irrespective of what the MPMdoes (not tested :) ). This assumes that during pool cleanup, the UNREGISTERcall will be made before the apr_proc_t-based process termination occurs.The problem is still present in 2.2.4.CC myself on FreeBSD related bugs[Sat Nov 30 19:20:28 2013] [notice] Apache/2.2.24 (Unix) configured -- resuming normal operations[Mon Dec 02 18:04:04 2013] [notice] Graceful restart requested, doing restart[Mon Dec 02 18:04:05 2013] [notice] Apache/2.2.24 (Unix) configured -- resuming normal operations[Mon Dec 02 18:04:05 2013] [warn] long lost child came home! (pid 1937)[mpm_prefork:warn] [pid 19070] AH00167: long lost child came home!(pid 19074)[Sun Dec 07 03:21:01.970843 2014] [mpm_prefork:notice] [pid 3380] AH00171: Graceful restart requested, doing restart[Sun Dec 07 03:21:03.008332 2014] [mpm_prefork:notice] [pid 3380] AH00163: Apache/2.4.10 (Unix) OpenSSL/1.0.1e-fips configured -- resuming normal operations[Sun Dec 07 03:21:03.008344 2014] [core:notice] [pid 3380] AH00094: Command line: '/usr/sbin/httpd'[Sun Dec 07 03:21:03.008378 2014] [mpm_prefork:warn] [pid 3380] AH00167: long lost child came home! (pid 3384)every sunday i gets susch messagesThis error still occurs in version 2.4.6:[Wed Aug 26 10:41:45.668897 2015] [auth_digest:notice] [pid 4416] AH01757: generating secret for digest authentication ...[Wed Aug 26 10:41:45.670339 2015] [lbmethod_heartbeat:notice] [pid 4416] AH02282: No slotmem from mod_heartmonitor[Wed Aug 26 10:41:45.709307 2015] [jk:warn] [pid 4416] No JkShmFile defined in httpd.conf. Using default /etc/httpd/logs/jk-runtime-status[Wed Aug 26 10:41:45.714202 2015] [mpm_prefork:notice] [pid 4416] AH00163: Apache/2.4.6 (CentOS) mod_jk/1.2.41 configured -- resuming normal operations[Wed Aug 26 10:41:45.714214 2015] [core:notice] [pid 4416] AH00094: Command line: '/usr/sbin/httpd -D FOREGROUND'[Wed Aug 26 10:41:45.714269 2015] [mpm_prefork:warn] [pid 4416] AH00167: long lost child came home! (pid 5230)[Wed Aug 26 10:41:45.714283 2015] [mpm_prefork:warn] [pid 4416] AH00167: long lost child came home! (pid 5231)[Wed Aug 26 10:41:45.714309 2015] [mpm_prefork:warn] [pid 4416] AH00167: long lost child came home! (pid 5232)all the pids correspond with stream loggers we run to our elk stack:0 0 5230 4416 20 0 49832 2652 poll_s S ? 0:00 \_ /usr/bin/nc -u elk.host.local 122020 0 5230 4416 20 0 0 0 exit Z ? 0:00 \_ [nc] <defunct>Still occurs in 2.4.10 and 2.4.12 (Solaris 10 SPARC)Using piped logs to rotate them throug rotatelogs:[adm@test log]$ ps -fu httpd UID PID PPID C STIME TTY TIME CMD httpd 1857 2139 0 Aug 14 ? 6:39 /pub/site/httpd/sbin/httpd -k start httpd 8763 2139 0 17:16:58 ? 0:00 /pub/site/httpd/sbin/rotatelogs -L log/transfer.log -l log/transfer.l httpd 8762 2139 0 17:16:58 ? 0:00 /pub/site/httpd/sbin/rotatelogs -L log/access.log -l log/access.log.%Y-%m-%d 86 httpd 8756 2139 0 17:16:58 ? 0:00 /pub/site/httpd/sbin/rotatelogs -L log/error.log -l log/error.log.%Y- httpd 5150 2139 0 Jul 28 ? 9:19 /pub/site/httpd/sbin/httpd -k start httpd 19310 2139 0 Aug 07 ? 8:21 /pub/site/httpd/sbin/httpd -k start httpd 2801 2139 0 Jul 14 ? 8:19 /pub/site/httpd/sbin/httpd -k start httpd 6118 2139 0 Jul 29 ? 9:26 /pub/site/httpd/sbin/httpd -k start httpd 19359 2139 0 Aug 07 ? 8:46 /pub/site/httpd/sbin/httpd -k start httpd 19136 2139 0 Jul 28 ? 9:10 /pub/site/httpd/sbin/httpd -k start httpd 8765 2139 0 17:16:59 ? 0:00 /pub/site/httpd/sbin/httpd -k start httpd 2749 2139 0 Jul 14 ? 8:08 /pub/site/httpd/sbin/httpd -k start httpd 8764 2139 0 17:16:58 ? 0:00 /pub/site/httpd/sbin/rotatelogs -L log/transfer.log -l log/transfer.l httpd 2139 1283 0 Jul 14 ? 109:37 /pub/site/httpd/sbin/httpd -k start httpd 2179 2139 0 Jul 14 ? 8:32 /pub/site/httpd/sbin/httpd -k start httpd 6131 2139 0 Jul 29 ? 9:43 /pub/site/httpd/sbin/httpd -k start httpd 8754 2139 0 17:16:58 ? 0:00 /pub/site/httpd/sbin/rotatelogs -L log/error.log -l log/error.log.%Y-%m-%d 8640 httpd 19151 2139 0 Jul 28 ? 8:50 /pub/site/httpd/sbin/httpd -k start httpd 2154 2139 0 Jul 14 ? 8:55 /pub/site/httpd/sbin/httpd -k start httpd 1845 2139 0 Aug 14 ? 6:52 /pub/site/httpd/sbin/httpd -k start httpd 2176 2139 0 Jul 14 ? 8:40 /pub/site/httpd/sbin/httpd -k start httpd 2751 2139 0 Jul 14 ? 9:03 /pub/site/httpd/sbin/httpd -k start[appadm@pine-test log]$ svcadm refresh httpd[appadm@pine-test log]$ tail error.log[Thu Oct 01 17:19:01.707625 2015] [mpm_worker:notice] [pid 2139:tid 1] AH00297: SIGUSR1 received. Doing graceful restart[Thu Oct 01 17:19:02.304683 2015] [socache_shmcb:info] [pid 2139:tid 1] AH00830: Shared memory socache initialised[Thu Oct 01 17:19:02.306327 2015] [ssl:info] [pid 2139:tid 1] AH01887: Init: Initializing (virtual) servers for SSL[Thu Oct 01 17:19:02.313674 2015] [ssl:info] [pid 2139:tid 1] AH01876: mod_ssl/2.4.12 compiled against Server: Apache/2.4.12, Library: OpenSSL/1.0.1i[Thu Oct 01 17:19:02.332053 2015] [mpm_worker:notice] [pid 2139:tid 1] AH00292: Apache/2.4.12 (Unix) OpenSSL/1.0.1i configured -- resuming normal operations[Thu Oct 01 17:19:02.332344 2015] [mpm_worker:info] [pid 2139:tid 1] AH00293: Server built: Jun 8 2015 17:07:18[Thu Oct 01 17:19:02.332533 2015] [core:notice] [pid 2139:tid 1] AH00094: Command line: '/pub/site/httpd/sbin/httpd'[Thu Oct 01 17:19:02.395953 2015] [mpm_worker:warn] [pid 2139:tid 1] AH00291: long lost child came home! (pid 8762)[Thu Oct 01 17:19:02.396639 2015] [mpm_worker:warn] [pid 2139:tid 1] AH00291: long lost child came home! (pid 8763)[Thu Oct 01 17:19:02.396867 2015] [mpm_worker:warn] [pid 2139:tid 1] AH00291: long lost child came home! (pid 8764)	19.0	id=44221	27	True	True	rmackenzie	1
id=25469	REOPENED	None	Apache httpd-2	mod_auth (	2.2-HEAD	All All	P3 enhancement	Apache HTTPD Bugs Mailing List	2003-12-12 11:06 UTC by	Matus "fantomas" Uhlar	2008-02-25 02:55 UTC (	1 user	Directives that define paths fo auth files (AuthUserFile, AuthGroupFile,AuthDBUserFile, AuthDBGroupFile, AuthDBMUSerFile, AuthDBMGroupFile,AuthDigestFile, AuthDigestGroupFile, and probably others) search for filesstarting in ServerRoot.This is easy to implement (if I understand corretcly, apache switches toServerRoot on startup and access fiels from there), however, it is not easy towork with it. We have multiple users, for whose we want to make access easier,and we would invite a directive AuthRoot, which would define for current context(server, virtualhost, directory, htaccess), where all the auth files arelocated, if they are defined with relative path.	Seems to be a good idea! However, I'd consider this as a new feature for 2.1.I'm afraid that upgrading to 2.1 is for us a long time to go.I'd like to have this in 1.3...***has been marked as a duplicate of this bug. ***I reopened this and changed from 1.3 to 2.2, since (not only) I am stillinterested in this feature	4.0	id=19569	15	True	True	gregor	1
id=43925	REOPENED	None	Tomcat 8	Jasper (	8.0.9	All All	P3 enhancement	Tomcat Developers Mailing List	2007-11-21 06:23 UTC by	Brian Remmington	2014-07-22 10:07 UTC (	1 user	BodyContentImpl buffers all output from a custom tag ready to be written whenthe tag execution ends. However, the way in which it grows this buffer isextremely inefficient and has two undesirable effects:- garbage collection is triggered very frequently to tidy up the waste.- CPU load ramps up as large, unnecessary array copies take place.All that's needed is a more intelligent buffer-management algorithm. I haverewritten this class and can forward it if that would be useful (can't see a wayof attaching it here).	There is an option to attach patches just above comment box on the bugzilla pagefor each bug.CreatedProposed changes to BodyContentImplThe attachment "buzilla43925.jar" contains a new implementation of theBodyContentImpl class (named "NewBodyContentImpl") and a new class on which itdepends named "CharBuffer". I have also included two JUnit test cases.This new implementation performs 45% faster than the current implementation onmy test machine and is substantially more efficient in memory usage, causing nocharacter arrays to be left to the garbage collector unnecessarily.I would greatly appreciate it if people would review this code, and let me haveany comments or suggestions.Ok, so I not sure I got everything, but:- I don't see where memory usage is bounded in the code (if it's not, you shouldcompare performance with the unbounded mode of Jasper, which will neverreallocate anything); that's a problem- The opportunity is the ability to share the buffer list per thread, so theCharBuffer.bufList should be ThreadLocal<LinkedList> (and should be limited insize, which should solve item 1); as a configuration option, it could be aconcurrent static structure, for use with thread intensive connectors- CharBuffer.toArray is expensive (no other option, though), so you should tryto show a (real) test resultThanks very much, Remy.Perhaps you could help me here. The memory usage doesn't appear to be bounded inthe current implementation of BodyContentImpl which *always* appears toreallocate exponentially-larger blocks (in the reAllocBuff method at thebottom). I must be missing something, but am not sure what. I'm looking at thehead of 5.5, which is the version affected by this change.This would require CharBuffer objects to be pooled to be of use. At the momentthey aren't (and neither are BodyContentImpl objects, as far as I can see). If Iunderstand you correctly, this would have the undesirable effect of holding ontoblocks of memory once allocated rather than making them available for garbagecollection. This would certainly require a bounded memory pool approach, but itsdebatable whether that would be preferable. It would certainly be a bigger change.I will put together a suitable test case for this. Is there a set of test casesfor the current implementation? If so I could just add new ones to it.RegardsBrianAbout the upper memory bound, I had missed the clear method. I thought theoriginal purpose was to keep the list of buffers around, now it looks to me thememory usage is equivalent to the LIMIT_BUFFER mode.What is the status of this "bug"? This causes problems in our jsf based application with a couple of hundreds users. I tried Brians fix with no improvement, memory is still occupied and not gc:d. However, I deployed the application in Glassfish and there the charcater array gets gc:d. Any ideas what to do?*** This bug has been marked as a duplicate of***This is not a duplicate of 37793.That one is related to the unbounded growth of cb without the ability for the garbage collector to kick in to collect it. The LIMIT_BUFFER option goes some way to resolve that problem. Thomas () does seem to be suffering from 37793 rather than this one.*This* problem, however, is about the way in which, while cb grows, large numbers of exponentially-larger char arrays are reallocated, copied, and discarded.Every time chars are written that cause cb to grow, the following process occurs:a. Allocate a new char array that is twice as large as the current "cb";b. Copy the contents of cb into the new array;c. Assign the new array to cb;d. Garbage collector collects the old cb array.This, I think, is obviously inefficient. It fills up the heap with large, unwanted character arrays and forces the garbage collector to activate more frequently, thus reducing the CPU available for doing useful work. It also uses CPU to copy exponentially-growing arrays from one block of memory to another. It is all unnecessary.My suggested resolution is to improve the way this buffering is managed. No array copies occur. No unnecessarily large heap allocations occur. Nothing is left to the garbage collector that doesn't have to be. The performance of this part of the code is improved. The drain that this code places on system resources is reduced.Given that widespread use of various tag libraries, I still feel that this is a very worthwhile change. RegardsBrianThis Tomcat 5 enhancement request has been moved to Tomcat 7 (the latest version) since Tomcat 5 development is limited and focussed on bugs and security issues whereas Tomcat 7 is still seeing new feature development.Hi,For this issue,- is there already a chosen new architecture for the new version of the buffering ?or- should it be discussed ?or- you let the numbers speak for any new implementation ?I'm willing to spend time on this for Tomcat 7, so answers from Tomcat devs are expected.Thx--IssaI don't recall any solutions being discussed but you should search the archives and review any discussions that there may have been about this feature.It is up to you whether to discuss it first. Generally, the more complex and invasive the change the more useful early discussion is.A patch for this enhancement will be judged like any other. It isn't just about raw performance we also take into account the complexity of the resulting code, the invasiveness of the patch, the usefulness of the feature and so on.Larger changes are best reviewed as a series of smaller patches. See my WebSocket compression changes earlier today as an example of breaking a large patch down into a series of smaller patches to make it easier to review.Note that you should provide patches against tomcat trunk and they will then be back-ported to the release branches as appropriate.Brian, I've taken a look into the code you've submitted but I failed to understand how this code reduce the number of array copies in comparison with Tomcat's current implementation. I've ran the test cases with different values for the iteration and can't see real improvements.Calls to write(String) [1] will always induce a array copy, unfortunately, because they are immutable.Calls to write(char[]) [2] are not improvable because we don't know if the caller will reuse the char array - so we need to copy it.I don't know if calls to [1] are more frequent in a web app than calls to [2]. If it is the case, then trying to avoid the array copy from the String would be good overall. In fact, having to manage both char[] and Strings makes it harder to improve the whole class!So as it seems that reducing the number of array copies is difficult while calling write() (even doable ?), maybe improving the way the buffer grows is the way to go - reducing the number of array copies and garbage creation.Any comment on my reflections ?Hi IssaIt's been a while since I raised this (6 years), so I had to refresh my memory.I've just checked out the trunk of Tomcat 8, and the same problem exists, and I still think my code addresses it.Your final sentence describes exactly what my suggested solution does: it improves the way the buffer grows to reduce array copies and reduce garbage collection. Your other comments don't relate to my proposal.In the current Tomcat class (org.apache.jasper.runtime.BodyContentImpl), the problem is with the method reAllocBuff(int) which is right at the bottom of the class. This is called when the buffer array (cb) needs to grow, and it's terribly inefficient.My code replaces cb with an instance (named "buffer") of a new class CharBuffer which manages a LinkedList of char arrays to ensure that, upon growth (CharBuffer.grow), no array copy occurs, and nothing is left to the garbage collector.I've just run the tests I provided again against Tomcat 8, ramping up the iterations in the BodyContentTest steadily to 1500000 (hardware is *so* much better now...).My version is consistently 40% faster than the current version, however both are pretty quick these days. My machine can chew through 1000000 iterations in 1378ms with the current version (BodyContentImpl) and 824ms with my version (NewBodyContentImpl).Speed isn't the main point of this, however. It's the very wasteful use of memory that was causing the real problem when I initially raised this issue. The garbage collector was kicking in constantly, completely draining CPU cycles.For example, take the iterations up to 1500000 on my test machine, and the current version blows up with an OutOfMemory error. My version continues to work and scales fairly linearly (1341ms). Take it up further to 2000000 iterations, and mine starts to plateau (2393ms), but it still continues to work. At 3000000 iterations, my version completes in 4543ms.I made some changes to the test case you provided to excluded the random (so that the random numbers are always the same for both cases) and run the test for one target only. JVM is Java HotSpot(TM) 64-Bit Server VM (25.5-b02) for linux-amd64 JRE (1.8.0_05-b13), ran with settings -server -Xloggc:gc.log -Xmx4G.I don't get the same results as yours and GC is running a lot more on your class than Tomcat's. (I can send you the results if you'd like)It's true I can increase the number of iteration with your class before getting a OOE more than with Tomcat's.But is this test case emulating real usage ? My next tests will be to run both class under Tomcat with a test case of several pages with tags triggered by jmeter. This will take me some time so I will not be back before several weeks.ThxCreatedTest case to compare default and suggested implementationI've ran some tests with JMeter 2.11 against some small custom tags running under Tomcat 8.0.9.The results from the Jmeter test in file show that the default implementation is a little bit faster than suggested one (number of jmeter samples being higher for the same amout of running time).Can someone run the tests I provided and report the results ? If any improvement or suggestion is found for the tests I provide, please report as well. Thx!	16.0	id=30453	8	False	False	ps	1
id=34994	REOPENED	None	Lenya	Default Publication (	2.0	Other AIX	P3 enhancement	Lenya Developers	2005-05-20 23:05 UTC by	Doug Chestnut	2007-11-21 08:45 UTC (	1 user	This patch allows for basic WebDAV functionality. Using DreamWeaver you cancheckout, edit/checkin documents to the authoring area. XP web folders don'tappear to have lock functionality, but I was able to list, get, and putdocuments with a network web folder (easy enough to force checkout on get, putalready forces a checkin). This patch requires apache xmlsec jar for decodingof auth basic userid/password (Auth header).I hope to have schema validation, document creation, copy|paste|move, and ifpossible publishing to live area worked out next.	Createdpatches/new files for WebDAV interface***Word of warning***WebDAV client must announce itself with a USER_AGENT header that contains "DAV"Opps, one more thing that I forgot to mention. You need to rebuild cocoon withwebdav block (and all dependant blocks).To use dreamweaver use:server:userid: youridpass: yourpassemail: youridi took a look at your patch, very nice work!there are a couple issues with it:* it uses the authenticator action which does no longer exist:+<map:match pattern="**">+ <map:match type="agent-match" pattern="*DAV*">+ <map:act type="authenticator">+ <map:act type="authorizer">+ <map:mount uri-prefix="" src="global-sitemap.xmap" check-reload="true"reload-method="synchron"/> + </map:act>+ <map:act type="set-header">+ <map:parameter name="WWW-Authenticate" value="Basic Realm=lenya" />+ <map:generate type="html" src="401error.html"/>+ <map:serialize type="xhtml" status-code="401"/>+ </map:act>+ </map:act>+ <map:act type="set-header">+ <map:parameter name="WWW-Authenticate" value="Basic Realm=lenya" />+ <map:generate type="html" src="401error.html"/>+ <map:serialize type="xhtml" status-code="401"/>+ </map:act>+ </map:match>+</map:match>an alternative will have to be found. the same can be said for the dicovercheckout action: actions are deprecated and should not be used for new development.i also rewrote your patch to be more self-contained (putting all files underdavmap into default/lenya/usecases/webdav/) and also renamingdavmap/sitemap.xmap to webdav.xmapthe dependency chain for required cocoon blocks is:webdav -> repository -> eventcache -> jmsalso, it was a lot of work to integrate. please provide a unified diff in thefuture. you can do this by right-clicking in eclipse and team-> create patch.this is definitely something we want asap, i hope the devs chime in withimprovements. i have attached a unified diff with my changes.CreatedAdds webdav support (not yet working)Thanks Gregor, it was exciting to see dreamweaver checkin an edit! :)I will apply your patch and take a look. I was just going to post the questionabout the use of actions (one of which doesn't exist in 1.4) but you beat me to it.One thing to note is that the webDAV clients that I tried didn't care forredirects, and didn't seem to care for GET parameters. This seemed to throw awrench into the current usecase model, well perhaps not, no steps are needed.Sorry about the patch, I posted before I installed eclipse.(In reply to)no doubt, this has been a loooong time coming.to it.note that i didnt run it :) so i do not expect it to run, but you should get theidea. i wanted to keep the new files togetherright, since HTTP verbs like PROPFIND substitute for itno problemCreatedworking version of Gregor's cleanupI had to add authenticator action to 1.4 to get it to work. Use of this actionin 1.4 needs to be hashed out on the dev mailing list since the action has beenretired in favor of the usecase.Webdav clients don't like redirects, so usecase solution might be tricky. Gregor suggested looking at servlet filters to handle authentication andauthorization.are you sure your version works?this part in global-sitemap.xmap looks problematic: <map:pipeline> <map:match pattern=""> <map:generate type="html"src="fallback://lenya/usecases/webdav/401error.html"/> <map:serialize type="xhtml" status-code="401"/> </map:match>first, this would give everyone hittinga 401 error page,no? second, it tries to resolve the fallback, but since 401error is not in thecore, and no publication chosen, it fails.i also got rid of the additional cocoon blocks, they do not seem to be needed.finally, i tried to login with multiple clients, and always got a 401can you provide some more information?CreatedBasic webDAV interfaceDid away with the use of cocoon repo block (and dependants)Using lenya usecases, but calling them differently (no redirect or GET param)This patch adds DelegatingAuthenticatorActionDiscoverCheckoutAction should be removed in the near future, and functionalityshould be added to webdav usecasesput usecase needs to validate (should just be a matter of looking at oneformusecase)I didn't test this with pub templating (yet), so it most likely don't work ;) I only tested this on my PC, don't know if that makes a diffI made this for the default pub, and it doesn't take into account other doctypes.You should be able to open the default publication in a webDAV client that hasDAV in it's useragent header value. PROPFIND, LOCK, GET, OPTIONS, HEAD, PUT,DELETE should return something that a webDAV client MIGHT recognize ;). DELETEreturns the same error that it does when I try it from the regular webinterface, so it might work when the problem is fixed.Here are steps to follow to look at the Default pub with an XP web folder:-Go to "My Network Places"-click on "Add a network place"-click next on the wizard dialog-click next with "Choose another network location..." selected-enter "" into the Internet or network addressinput-enter user name "lenya" and password "levi" and click ok button-name your folder something creative like "default on localhost" and click next-You should get "You have successfully created this network place" message,click on finish-I was able to open documents with openoffice, but for some reason or anothercould not open the web folder to save the doc back to lenya.-To save my changes, I saved my doc to a folder, and then draged and dropped,not ideal, but a start ;)I will post an image of the DreamWeaver MX configuration that I used to test.--DougCreateddreamweaver webdavapplied, thanksIs this bug still valid?Nope, this patch was applied. I don't know if webdav is currently workingthough, I will test and open new bugs if it is broken.Checked out trunk, did a build, deployed it in Tomcat and WebDAV is not working.Cadaver:dav:!> open localhost:3000/lenya/default/webdavimmedeatly returns with 500 error, caused by an exception around line 103 inDocumentPolicyManagerWrapper:String authoringUrl = prefix + Publication.AUTHORING_AREA + webappUrl.substring(prefixWithArea.length());...with prefixWithArea=="default/authoring" and webappUrl="/default/webdav/".Temporarily hacking it: try { authoringUrl = prefix + Publication.AUTHORING_AREA + webappUrl.substring(prefixWithArea.length()); } catch (Exception e) { authoringUrl = prefix + Publication.AUTHORING_AREA; }...causes login in Cadaver to work fine, and apparently control now enter theWebDAV module and some flowscript errors (but that might be my hack so...)What's the state of this issue?Createdtcp dumbcadaverCould not open collection:500 Internal Server ErrorSeems the same as described by Dag.As I understand it the patches are applied to 1.4. Further the stacktrace fuels the assumption that the ac refactoring messed withthe dav support."org.apache.lenya.ac.AccessControlException:java.lang.StringIndexOutOfBoundsException: String index out of range: -2 atorg.apache.lenya.cms.ac.DocumentPolicyManagerWrapper.getPolicyURL(DocumentPolicyManagerWrapper.java:121) atorg.apache.lenya.cms.ac.DocumentPolicyManagerWrapper.getGrantedRoles(DocumentPolicyManagerWrapper.java:320) atorg.apache.lenya.ac.impl.PolicyAuthorizer.authorizePolicy(PolicyAuthorizer.java:107) at org.apache.lenya.ac.impl.PolicyAuthorizer.authorize(PolicyAuthorizer.java:137)"Createdfixing the first bugsrc/modules-core/ac/java/src/org/apache/lenya/cms/ac/DocumentPolicyManagerWrapper.java===================================================================---src/modules-core/ac/java/src/org/apache/lenya/cms/ac/DocumentPolicyManagerWrapper.java ()+++src/modules-core/ac/java/src/org/apache/lenya/cms/ac/DocumentPolicyManagerWrapper.java (working copy)@@ -104,7 +104,7 @@ String prefixWithArea = prefix + ((area != null) ? area : ""); // String prefixWithArea = prefix + area; String authoringUrl = prefix + Publication.AUTHORING_AREA- + webappUrl.substring(prefixWithArea.length());+ +((webappUrl.length()>prefixWithArea.length())?webappUrl.substring(prefixWithArea.length()):"/"); if (map.isDocument(authoringUrl)) { Document authoringDoc = map.getFromURL(authoringUrl);This patch brings back the auth dialog but now it fails to find the flow.Renaming Lenya 1.4 to 2.0	20.0	id=34994	16	False	False	gregor	1
id=30453	REOPENED	None	Lenya	Site Management (	1.0	PC Linux	P3 normal	Lenya Developers	2004-08-03 18:51 UTC by	Peter Shipley	2007-04-23 07:41 UTC (	1 user	reproducible (always) by:- going to the demo site- logging in to the default pub as the lenya- goto admin section- goto ipranges- select localhost iprange- click edit group affiliationresultant stacktrace:Original Exception: ConversionError: The undefined value has no properties.(file:/root/stuff/incubating-lenya-1.2-bin/build/lenya/webapp/lenya/content/admin/ipranges/iprange-admin.js;line 260) at org.mozilla.javascript.NativeGlobal.constructError(NativeGlobal.java:581) at org.mozilla.javascript.NativeGlobal.constructError(NativeGlobal.java:541) at org.mozilla.javascript.ScriptRuntime.getProp(ScriptRuntime.java:716) atorg.mozilla.javascript.continuations.ContinuationInterpreter.interpret(ContinuationInterpreter.java:694) atorg.mozilla.javascript.continuations.ContinuationInterpreter.interpret(ContinuationInterpreter.java:190) atorg.mozilla.javascript.continuations.ContinuationInterpreter.interpret(ContinuationInterpreter.java:138) atorg.mozilla.javascript.continuations.InterpretedFunctionImpl.call(InterpretedFunctionImpl.java:121) at org.mozilla.javascript.ScriptRuntime.call(ScriptRuntime.java:1244) atorg.apache.cocoon.components.flow.javascript.fom.FOM_JavaScriptInterpreter.callFunction(FOM_JavaScriptInterpreter.java:702) atorg.apache.cocoon.components.treeprocessor.sitemap.CallFunctionNode.invoke(CallFunctionNode.java:130) atorg.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:49) atorg.apache.cocoon.components.treeprocessor.sitemap.PreparableMatchNode.invoke(PreparableMatchNode.java:130) atorg.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:72) atorg.apache.cocoon.components.treeprocessor.sitemap.PipelineNode.invoke(PipelineNode.java:126) atorg.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:72) atorg.apache.cocoon.components.treeprocessor.sitemap.PipelinesNode.invoke(PipelinesNode.java:101) atorg.apache.cocoon.components.treeprocessor.TreeProcessor.process(TreeProcessor.java:336) atorg.apache.cocoon.components.treeprocessor.TreeProcessor.process(TreeProcessor.java:277) atorg.apache.cocoon.components.treeprocessor.sitemap.MountNode.invoke(MountNode.java:103) atorg.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:49) atorg.apache.cocoon.components.treeprocessor.sitemap.PreparableMatchNode.invoke(PreparableMatchNode.java:130) atorg.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:72) atorg.apache.cocoon.components.treeprocessor.sitemap.PipelineNode.invoke(PipelineNode.java:126) atorg.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:72) atorg.apache.cocoon.components.treeprocessor.sitemap.PipelinesNode.invoke(PipelinesNode.java:101) atorg.apache.cocoon.components.treeprocessor.TreeProcessor.process(TreeProcessor.java:336) atorg.apache.cocoon.components.treeprocessor.TreeProcessor.process(TreeProcessor.java:277) atorg.apache.cocoon.components.treeprocessor.sitemap.MountNode.invoke(MountNode.java:103) atorg.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:49) atorg.apache.cocoon.components.treeprocessor.sitemap.ActTypeNode.invoke(ActTypeNode.java:138) atorg.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:49) atorg.apache.cocoon.components.treeprocessor.sitemap.PreparableMatchNode.invoke(PreparableMatchNode.java:130) atorg.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:72) atorg.apache.cocoon.components.treeprocessor.sitemap.PipelineNode.invoke(PipelineNode.java:126) atorg.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:72) atorg.apache.cocoon.components.treeprocessor.sitemap.PipelinesNode.invoke(PipelinesNode.java:101) atorg.apache.cocoon.components.treeprocessor.TreeProcessor.process(TreeProcessor.java:336) atorg.apache.cocoon.components.treeprocessor.TreeProcessor.process(TreeProcessor.java:277) at org.apache.cocoon.Cocoon.process(Cocoon.java:639) at org.apache.cocoon.servlet.CocoonServlet.service(CocoonServlet.java:1098) at javax.servlet.http.HttpServlet.service(HttpServlet.java:853) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:354) atorg.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:294) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567) at org.mortbay.http.HttpContext.handle(HttpContext.java:1808) atorg.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:525) at org.mortbay.http.HttpContext.handle(HttpContext.java:1758) at org.mortbay.http.HttpServer.service(HttpServer.java:879) at org.mortbay.http.HttpConnection.service(HttpConnection.java:790) at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:952) at org.mortbay.http.HttpConnection.handle(HttpConnection.java:807) at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:197) at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:289) at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:501)	That problem can still be reproduced on the demo site.this has been fixedStill can be reproduced on the demo site.This works in 1.4, also on the demo site.	4.0	id=45260	11	True	True	w.schindling	1
id=45260	REOPENED	None	POI	HSSF (	3.0-dev	PC Windows XP	P3 normal	POI Developers List	2008-06-23 08:36 UTC by	Werner Schindling	2015-11-07 18:11 UTC (	1 user	Hi!We're having a rather big problem at my company with a moderately large Excel report (6 sheets, 15 columns, maximum 102 rows) using POI.We do a lot of formatting using about 28 different font styles (arial, 10pt, 12pt, colored, bold, italic ... - half of the fonts are strikeout as we use it to mark old entries). We have up to 6 different fonts in one cell.Now, usually the POI-generated Excel-file is readable without any problem, but SOMETIMES MS Excel can't open it saying that there's "unreadable content". Our customer needs this 100% working all of the time and I can't figure out the exact reason. It looks like it has something to do with the applyFont() method. Because of this I've built in a switch to leave out text formatting. While the formatted file sometimes can be read, the unformatted ALWAYS can be.One bugfix that I hoped was to be the solution was to add several blank spaces at the end of each cell String (I thought that corrupt font indices were the reason for the error). This worked for some time but now turned out to not always work. I've also tried to make BIFF-dumps of the files but can't open the resulting files with the viewers I've tried out (BIFF-Workbench etc.) ... ?I hope someone can help me with this, I've been goofing around for ages with this and have become pretty frustrated.I don't want to add the generated files as an attachment (this is pretty sensitive data) but could mail it to someone who can help me. URGENT HELP is needed here!! :)Regards, Werner(btw text formatting on an assembled HSSFRichTextString is quite annoying to do as you can't append to it ... therefore you have to remember the indices for applying the fonts and do that after the result string has been pieced together)PS opening the file with Open Office always works and saving it with OO makes the file readable with MS Office (I wish I could convince our customer to use OO ...)	(In reply to)One thing to try then is run a BiffViewer on the poi output file, and the oo output file. See if you can spot what open office has done differently (and presumably correctly) with the formatting stuffAlso, can you generate a problem file with dummy data in? Without a test file to work against for unit tests etc, I can't see anyone being able to help(In reply to)Hi!Thanks for the quick reply. :)The POI-generated file has only 20 font records in it (some created in the code seem to be missing) while the open office file has 32.Also, the POI file has 2 format records while the open office one has 8.I'll look into this in a moment.I'm adding two BIFF-Dumps, one of the corrupted POI-generated file and one of the same file resaved with Open Office 2.2.corrupt POI-generated file dump:working open office file dump:I made both files using BIFF-Workbench.Ok, here comes a corrupt excel file (though not the one from the dump I've posted before) with masked data:CreatedMy code for initialising fonts and stylesNick, or someone else, do you have an idea why some of the fonts are missing in the corrupted file?Maybe there's something wrong in how i create the fonts and styles (see attachment above).The font creation code looks fine, though being only a code snippet I can't say anything elseWith such large files, finding the cause probably won't be quickI take it that if you apply the same fonts to a smaller file, it all works fine? Have you noticed any particular file complexity point where the problem manifests itself? Can you produce a file that works just fine, but adding another few rows or another sheet causes it to break?(The more information you can supply on this, especially things like two very similar files where one works and one doesn't the better. It's much easier, and hence more likely, to fix a problem of the type "fonts not correctly written once 10 ???? Records present" than "fonts sometimes fail on my really large file")I ran into the same problem, and determined that it was only happening on files larger than 5MB, but sometimes large files of over 7MB worked fine and I couldn't narrow it down further. Depending on the data set, it either happened or did not, but once it did it was 100% reproducible. Reducing the number of applyFont calls would cure the problem in the offending sheets. It only seems to happen when you mix fonts within a single RichText object. Lastly, mixing fonts within a single RTS seems to bloat the XLS file incredibly. If I don't apply the fonts to the single column that has the mixed fonts, the XLS reduces from 7.8MB to 1.5MB.Please try the latest 3.5-beta5. There was a bug in how applyFont handled overlapping regions and regions with formatting applied twice. The fix was submitted inand applied inYegorGreat!However, with Beta3.5 there's only 4 or 5 fonts im my Excel output file now instead of 30. Is the formatting run cancelled as soon as there's a problem with overlapping regions or something like that?I need to switch back to Beta3.3.(In reply to)Sounds like a bug. If I had it right, 3.5-beta3 and earlier versions produce files that can't be opened by Excel. After read-write using OpenOffice the file becomes readable. With 3.5-beta5 the output can be read by Excel, but some fonts are missing. Can you upload a file with a dummy data generated by 3.5-beta5? Ideally, it would be the same file as you posted previously. YegorCreatedDifferent files generated with beta3 and beta5Of course I can.Since I found out the generated report is open to public anyway, here are two files, generated with beta 3 (has all fonts) and beta 5 (has only some of the fonts).Is there anything special in the way you create / set rich text strings? Can you post the code you used to generated the attached files? Compare two code snippets: HSSFRichTextString str1 = new HSSFRichTextString("Andritz AG, Graz Short Name: AAG"); str1.applyFont(0, 16, FONT_CELL_HEADER_BLUE.getIndex()); str1.applyFont(17, 29, FONT_ITALIC.getIndex()); str1.applyFont(29, 32, FONT.getIndex()); cell.setCellValue(str1); HSSFRichTextString str2 = new HSSFRichTextString("Andritz AG, Graz Short Name: AAG"); cell2.setCellValue(str2); str2.applyFont(0, 16, FONT_CELL_HEADER_BLUE.getIndex()); str2.applyFont(17, 29, FONT_ITALIC.getIndex()); str2.applyFont(29, 32, FONT.getIndex());They are not equivalent. The second one is incorrect and can produce wrong results. Once a HSSFRichTextString is assigned, it is supposed to be immutable. I would say in the latter case the code should throw IllegalStateException. We don't do that only because of backwards compatibility. One of the problems with beta5fileMissingFonts.xls is that the internal cache of strings is incorrect. There are duplicate entries and as result, beta5fileMissingFonts.xls is 504 KB while beta3fileAllFonts.xls is just 331 KB. Internally, Excel stores all strings in a shared string table which is shared across workbook. Text cells just contain indexes into the string table as the value of a cell, instead of the full string. If the string cache is broken, it is very likely to be the reason why some fonts are missing. Regards,Yegor(In reply to)Hi Yegor,thanks for taking the time on this!In my code setCellValue(text) is called after all the fonts have been applied. The biggest problem for me is that you just can't append to a HSSFRichTextString. I have to run 2 identical loops, one for piecing the String together and then another one to apply the fonts to the finished HSSFRichTextString.Here's the code ;) ... (I've tried to remove unimportant lines)StringBuffer buffer = new StringBuffer(); for (POICellContent content : this.content){ for (int i = 0; i < content.indent; i++){ // insert leading whitespaces buffer.append(" "); } // insert content buffer.append(content.toString()); if (this.companyReport.showHistory && content.getInterval() != null){ String dateString = companyReport.createDateString(content.getInterval()); if (!"".equals(dateString)){ int dateLength = dateString.length(); int textLength = 0; if (buffer.indexOf("\n") != - 1){ textLength = (buffer.length() - 1) - buffer.lastIndexOf("\n"); } else textLength = buffer.length(); // factor: variable to determine, whether the date string fits in the same line // or a new line is needed double fontHeight = content.getFont().getFontHeightInPoints(); if (fontHeight == 9) fontHeight = SMALL_FONT_MAGNIFICATION; double widthFactor = CompanyReportXlsGenerator.LINE_BREAK_FACTOR * fontHeight; double approxColumnWidth = column.getWidth() / widthFactor; boolean makeNewLine = content.dateWrap && textLength < approxColumnWidth && textLength + dateLength + 1 > approxColumnWidth; if (makeNewLine){ buffer.append(CompanyReportXlsGenerator.NEW_LINE); if (content.dateIndent > 0){ for (int i = 0; i < content.dateIndent; i++){ dateString = " " + dateString; } } else if (content.indent > 0){ for (int i = 0; i < content.indent; i++){ dateString = " " + dateString; } } buffer.append(dateString); } else { buffer.append(" " + dateString); } } } } HSSFRichTextString resultString = new HSSFRichTextString(buffer.toString()); // build the cell String again to apply fonts buffer = new StringBuffer(); int end = 0; int start = 0; for (POICellContent content : this.content){ start = end; end = start + content.toString().length(); for (int i = 0; i < content.indent; i++) { // insert leading whitespaces buffer.append(" "); } end += content.indent; // insert content buffer.append(content.toString()); HSSFFont font = content.getFont(); if (!font.getStrikeout() && row.getEntry().isStrikeOut() && content.strikeOutEnabled){ // use strikeOut Font font = companyReport.getStrikeOutFont(content.getFont()); } HSSFFont indentFont = companyReport.getNoStrikeOutFont(font); if ((companyReport.useFormatting || row.getEntry().isStrikeOut()) && content.indent > 0){ resultString.applyFont(start, start + content.indent, indentFont); resultString.applyFont(start + content.indent, end, font); } else if (companyReport.useFormatting || row.getEntry().isStrikeOut()){ resultString.applyFont(start, end, font); } if (this.companyReport.showHistory && content.getInterval() != null){ String dateString = companyReport.createDateString(content.getInterval()); if (!"".equals(dateString)){ int dateLength = dateString.length(); if (dateLength > 0){ start = end; end = end + dateLength; // + 1 because of whitespace/new line end++; int textLength = buffer.length(); if (buffer.indexOf("\n") != - 1){ textLength = (buffer.length() - 1) - buffer.lastIndexOf("\n"); } if (content.dateIndent > 0){ for (int i = 0; i < content.dateIndent; i++){ dateString = " " + dateString; end++; } } else if (content.indent > 0){ for (int i = 0; i < content.indent; i++){ dateString = " " + dateString; end++; } } buffer.append(dateString); HSSFFont dateFont = this.companyReport.FONT_DATE; if (font.getStrikeout() || row.getEntry().isStrikeOut()) { dateFont = this.companyReport.FONT_DATE_STRIKE; } if (companyReport.useFormatting || row.getEntry().isStrikeOut()){ try { if (makeNewLine && content.dateIndent > 0){ resultString.applyFont(start, start + content.dateIndent, companyReport.FONT_DATE); resultString.applyFont(start + content.dateIndent, end, dateFont); } else if (makeNewLine && content.indent > 0){ resultString.applyFont(start, start + content.indent, companyReport.FONT_DATE); resultString.applyFont(start + content.indent, end, dateFont); } else { resultString.applyFont(start, end, dateFont); } } catch (IllegalArgumentException e){ logger.error("Company Report error: font indices messed up: " + e.getMessage()); } } } } } } return resultString;Potentially related to: Unreadable content in workbook when using applyFont on RichTextString in XSSFWorkbok.	15.0	id=40230	5	True	False	jeremias	1
id=40288	REOPENED	None	Fop - Now in Jira	images (	0.92	Other other	P3 normal	None	2006-08-18 20:01 UTC by	None	2012-04-07 01:52 UTC (	0 users	A file:// url in the <base> element in fopconfig.xml must end with a slash on Linux. If it does not, images (specified by "url(foo)" where foo is a relative file name) won't be found and will thus not be added to the PDF. FOP should probably either accept both ways to specify the url or display an ERROR that refers to this setting. This would make it much easier to debug this problem.	(In reply to)Well, this is the normal behavior of URLs (and more generally URIs). Whenresolving a relative reference against a base URI, all the characters of thebase URI which are to the right of the rightmost slash are removed, and replacedby the relative path:file://path/to/images + myImage.png -> file://path/to/myImage.pngfile://path/to/images/ + myImage.png -> file://path/to/images/myImage.pngSo the ending slash is important!The best Fop can do IMO is display the full resolved URI in the error message,so that the user immediately sees that there is something wrong with URI resolution.Hey, I didn't want to change the severity of this bug! Sorry :-\Displaying the full URL in the error message sounds like a good solution to me.This bug should be fixed/addressed by [PATCH]The full URL isn't yet displayed when the resolution of a resource failed.Createdlogs full url - not just relative part if there is a failed resolutionComment onlogs full url - not just relative part if there is a failed resolutionIndex: C:/workspace/fop/src/java/org/apache/fop/apps/FOURIResolver.java===================================================================--- C:/workspace/fop/src/java/org/apache/fop/apps/FOURIResolver.java ()+++ C:/workspace/fop/src/java/org/apache/fop/apps/FOURIResolver.java (working copy)@@ -155,7 +155,7 @@ //Note: This is on "debug" level since the caller is supposed tohandle this log.debug("File not found: " + effURL); } catch (java.io.IOException ioe) {- log.error("Error with opening URL '" + href + "': " +ioe.getMessage());+ log.error("Error with opening URL '" + effURL + "': " +ioe.getMessage()); } return null; }@@ -202,6 +202,9 @@ * @returns the base URL as java.net.URL */ private URL toBaseURL(String baseURL) {+ if (!baseURL.endsWith("/")) {+ log.warn("Base url does not end with a '/'. This could causeresolution problems");+ } try { return new URL(baseURL == null ? new java.io.File("").toURL().toExternalForm()Createdlogs full url - not just relative part if there is a failed resolutionnot warns about base urls not ending with '/' charsNot sure I like the warning. There is no requirement for a base URL to end with a /.Createdlogs full url - not just relative part if there is a failed resolutionAmmends base url as necessary with trailing '/'. I can't think of anyreason/instance when you would want to lose the end part of your base urlbecause you neglected to append a trailing slash.(In reply to)Patch applied with some modifications. Thanks Adrian!Please leave bugs open when submitting patches. We will close them when applyingthe patches.I re-open this bug because the problem isn't entirely solved yet. The full URLdoesn't appear in strict user config validation mode. Also, when I specify awrong base-font URL (pointing to a non-existing directory), this throws acatched FileNotFoundException (FOURIResolver.java, l.159), and null is returned.Thus FontSetup displays an error message (or throws a ConfigurationException)with the relative URL (FontSetup.java, l.291).VincentIsn't a full URL, that is a URL pointing to a file resource a legal base URLwhich will now fail? For example if I set the base URL to the file I amprocessing, lets say 'file:///home/mm/test.xml' the previous version wouldaccept that and resolve references relative to that while now we are appending a/ and it will fail.(In reply to)Indeed, and I was also concerned by that at the beginning. But that code is usedonly to handle the <base> and <font-base> elements in the user config file,where it would be really unusual to see URLs pointing to files instead ofdirectories. And anyway such cases would happen with power users having a goodunderstanding of relative URL resolution. If they have the full URL in the errormessage, they will understand that they have to remove the last part of it tomake the thing work. So this change looks safe to me.Now if a majority of the developers have concerns about that I can revert it. Wecan also move that piece of code to the place where base/font-base are retrieved.WDYT?VincentAdmittedly I haven't looked at the code in detail but wouldn't the same logicapply when setting the base URL programatically. I can easily envisage anenvironment where the XML/FO to process comes from some source whose locationvaries and the programmer dynamically sets the base URL to the input file inquestion. It boils down to that a base URL is something different than a basedirectory or base file system path.I would prefer to strengthen our documentation, add appropriate comments to oursample config file and may be add a hint as part of the error message comparedto making a FOP base URL meaning something different than in the rest of the URLresolution world.But in the end its really a very minor issue and I be happy to go with whateverthe FOP committer majority prefers.resetting P2 open bugs to P3 pending further review	15.0	id=40288	14	True	True	vhennebert	1
id=46883	REOPENED	None	Fop - Now in Jira	images (	0.95	All All	P3 enhancement	None	2009-03-20 03:04 UTC by	Jeremias Maerki	2012-04-07 01:52 UTC (	0 users	I've just stumbled over a serious performance hotspot in the GOCA part when rendering a DataMatrix barcode using Barcode4J. I haven't had time, yet, to investigate if other graphics also exhibit that. Here's some profiling data:org.apache.fop.afp.goca.AbstractGraphicsDrawingOrderContainer.getDataLength()Time: 277'609ms (51%)Own Time: 100'437ms (18%)Invocation Count: 178'560GraphicsBox.getDataLength() also seems to be involved here.Just noting this mostly for myself (to revisit later).	Createdpossible fixQuickly looking at the method in question shows one way of reducing the performance hit: instead of iterating over the collection every time, keep a cached dataLength member, and return that. Haven't tested whether the synchronization overhead outweighs going over the collection repeatedly, though...Note:Just thought of another way to go about it. Define the dataLength member, and simply update it upon every add() or remove() operation. It will make those operations just a tiny bit slower, but at least it also avoids having to iterate over the entire collection to determine the data length.CreatedRevised patchPatch in attach attempts at resolving the time spent in the mentioned method by adding dataLength as a member, and increasing/decreasing it with every add or remove operation.Patch applied with rev807010.I had to revertbecause it caused a regression. Even very simple SVGs (for example [1]) converted to GOCA caused faulty documents. Due to time pressure, I'm afraid I cannot currently invest more time than I already have on fixing this, so I'm just reverting the change and reopening the bug. Sorry.[1]resetting P2 open bugs to P3 pending further review	6.0	id=46048	7	True	True	mhilpert	1
id=53558	REOPENED	None	Fop - Now in Jira	page-master/layout (	trunk	All All	P3 normal	None	2012-07-17 09:25 UTC by	Jeremias Maerki	2012-07-20 06:45 UTC (	0 users	Since the unit tests were changed to JUnit 4, a failing layout engine test is no longer identifiable, since the individual tests just have an index but no name. I currently get:Testcase: runTest[555] took 0.017 secTestcase: runTest[556] took 0.022 secTestcase: runTest[557] took 0.023 secTestcase: runTest[558] took 0.037 secTestcase: runTest[559] took 0.032 sec Caused an ERRORExpected XPath expression to evaluate to 'true', but got 'false' (XPath: boolean(/areaTree/*))java.lang.RuntimeException: Expected XPath expression to evaluate to 'true', but got 'false' (XPath: boolean(/areaTree/*)) at org.apache.fop.layoutengine.TrueCheck.doCheck(TrueCheck.java:78) at org.apache.fop.layoutengine.TrueCheck.check(TrueCheck.java:59) at org.apache.fop.layoutengine.LayoutEngineTestCase.doATChecks(LayoutEngineTestCase.java:256) at org.apache.fop.layoutengine.LayoutEngineTestCase.checkAll(LayoutEngineTestCase.java:190) at org.apache.fop.layoutengine.LayoutEngineTestCase.runTest(LayoutEngineTestCase.java:171)Testcase: runTest[560] took 0.021 secTestcase: runTest[561] took 0.022 secTestcase: runTest[562] took 0.023 sec	This is a known issue of JUnit 4. However, rev. 1360665 contains a change to LayoutEngineTestUtils that will display the list of correspondences between test number and test files by switching the DEBUG boolean to true.I believe this provides a reasonable solution to this issue, feel free to re-open if not.I agree with Jeremias on this, and further, that something else needs to be done to better report the failing test without having to do something special (i.e., switching a DEBUG boolean).I reported this problem just after JUnit 4 was adopted, and I seem to recall that Mehdi was going to look into a better solution. FWIW, when I encounter this problem, I've been using Eclipse to catch the assert, which then allows one to discover the culprit by walking up the stack a few levels.In any case, I'm reopening this (as P3) as I'd like to see a better, long-term solution.Hmm... I swear I replied to this this morning... Anyways, I have been looking into this periodically and hadn't found anything 'til today:I don't have time to look into this at the moment, but I will look into it in the next week or so so watch this space.(In reply to)It looks like a patch fixing the issue has been committed recently:IIC it should make it in JUnit 4.11. So we just have to wait.So I've been playing around with this this morning and with the junit-4.11 snapshot, it seems like all we need to do is change the "@Parameters" to "@Parameters(name = "Test file = {0}")". That seems to do the trick and expresses the file name when a failure occurs.Ohh and I did some perusing and my sleuthing skills suggest we shouldn't have to wait to long for the 4.11 release. Looking at the 4.x release dates, they seem to be releasing annually (or there abouts) and 4.10 was released 10months ago... Watch out Columbo, there's a new kid in town.	6.0	id=46883	8	True	True	adelmelle	1
id=40230	REOPENED	None	Fop - Now in Jira	page-master/layout (	trunk	All All	P3 normal	None	2006-08-11 08:08 UTC by	Vincent Hennebert	2012-04-07 01:51 UTC (	0 users	When there is an <fo:block break-after="page"/> and nothing after it in theflow, a new page is still created, whereas section 7.19.1 of the recommendationstates that it should be the case only if there is material to typesetafterwards. See the attached fo sample.The result is the same if we replace break-after by break-before.Same result also if we remove the indenting such that all the closing tags afterthe block are sticked together, so this is not a whitespace handling issue.I guess that a Knuth penalty of value -infinity is generated for such a block,and this doesn't play well with the (infinite glue, -infinite penalty) pairwhich is probably added at the end of the page sequence. The penalty shouldprobably be only generated if there is also some box after it, and before theending pair.If nobody takes this bug I'll have a look at it after the GSoC.	CreatedSample file demonstrating the problem(Here I go again :))I think this could be handled before layout even kicks off...FOTreeBuilder could keep track of whether the currentFObj has area-generating content --shouldn't prove too hard to come up with a set of conditions that have to be satisfied.I'm thinking much in the same direction as the 'inMarker' instance member I recently added to FOEventHandler: a switch that is updated in the start- or endElement() event for each FObj. If it does not have any content, and the currentPropertyList contains a break-before/break-after property, instruct the current or the previous FO to discard that property (maybe log a warning, because it's not incorrect to specify it)Maybe we'd have to add a reference to the previousFObj/previousPropertyList to FOTreeBuilder as well, but that would be a small price to pay, and may open up perspectives in other areas.Sorry if I keep nagging about these 'normalizations' in the FOTree... :/I guess my underlying goal is: whatever layoutengine/renderer combination processes our FOTree, we should try to offer it a representation of the tree that would be difficult to 'misrender'.Should be fixed by:(In reply to)This change introduces a regression: if a block has break-after="odd-page" and a child block that has break-after="page", the following content will be on the next page instead of the next odd page. See upcoming attachment.CreatedTestcase showing the regressionvincent, should this stay open, or is it waiting for an assignment to a dev to fix?It (In reply to)It still needs to be fixed.resetting P2 open bugs to P3 pending further review	8.0	id=45494	16	False	False	bojan	1
id=29603	REOPENED	None	JMeter	Main (	Nightly (Please specify date)	Other other	P3 enhancement	JMeter issues mailing list	2004-06-16 14:14 UTC by	Mike Stover	2011-11-14 01:16 UTC (	1 user	Anyone writing a custom jmeter component has to make resource strings. WithTestBean, it's easy enough, but for regular gui components, their only currentoption is modding the messages.properties file and recompiling JMeter.External developers should have the option of specifying a custom resourcefile/location that jmeter tries to get properties from first, and then fall backon the given jmeter resources. This would also allow anyone to override themessages.	Gui components can override "getStaticLabel()" to retrieve the message from a private resources file.*** This bug has been marked as a duplicate of***This is not the same as, which is about upgrade.properties, not message.properties	3.0	id=47541	6	True	False	vhennebert	1
id=13099	REOPENED	None	Log4j - Now in Jira	Configurator (	1.2	PC other	P3 normal	log4j-dev	2002-09-27 20:50 UTC by	Douglas Jackson	2007-03-22 15:23 UTC (	2 users	When using the default heirarchy i.e. DOMConfigurator.configure(Element)the <categoryFactory> tag has no effect. The parseCategory method callsgetLogger(String) which will use the default LoggerFactory and not thecategory factory specified in the <categoryFactory> tag.The <categoryFactory> tag appears to have no effect whatsoever.It would be nice if the defaultFactory in the Heirarchy class could bemodified via a setter or make it protected so an extending class can setit.	I've noticed it too and this causes serious problems for me because I've subclassed the Logger class. When the DOMConfigurator runs and tries to set the log level for specific categories, it ignores the categoryFactory and creates default log4j.Logger objects. These choke my code when I go to load the appropriate Logger and cast it to my own Logger sub-class. Not only is not setting the defaultFactory, it's not even using the LoggerFactory to create the Logger objects it needs during configuration.I made some minor changes that helped at least in my case:1. Make the org.apache.log4j.DefaultCategoryFactory and its constructor public (alternatively, implement your own CategoryFactory in the xml package).2. Make the following changes to org.apache.log4j.xml.DOMConfiguratornew: // WK (26.08.2003): Replacement for the deprecated CATEGORY_FACTORY_TAG static final String LOGGER_FACTORY_TAG = "loggerFactory";new: /** * WK (26.08.2003): Method parameter list for retrieving a Logger using * a specific LoggerFactory as specified in the XML configuration */ static final Class[] STRING_LOGFACTORY_PARAM = new Class[] {String.class,LoggerFactory.class};new: /** * WK (26.08.2003): LoggerFactory which is used when retrieving a Logger. * The LoggerFactory may be set using the CategoryFactory XML */ protected LoggerFactory loggerFactory = new DefaultCategoryFactory();[changed in parseErrorHandler()] String loggerName = currentElement.getAttribute(REF_ATTR); // WK (26.08.2003): get the logger with the help of the loggerFactory Logger logger = repository.getLogger(loggerName, loggerFactory);[changed]: protected void parseCategory (Element loggerElement) { // Create a new org.apache.log4j.Category object from the <category> element. String catName = subst(loggerElement.getAttribute(NAME_ATTR)); Logger cat; String className = subst(loggerElement.getAttribute(CLASS_ATTR)); if(EMPTY_STR.equals(className)) { LogLog.debug("Retreiving an instance of org.apache.log4j.Logger."); // WK (26.08.2003): get the logger with the help of the loggerFactory cat = repository.getLogger(catName, loggerFactory); } else { LogLog.debug("Desired logger sub-class: ["+className+']'); try { // WK (26.08.2003): get the logger method by reflection which takes // a LoggerFactory as an argument Class clazz = Loader.loadClass(className); Method getInstanceMethod = clazz.getMethod("getLogger", STRING_LOGFACTORY_PARAM); cat = (Logger) getInstanceMethod.invoke(null, new Object[] {catName,loggerFactory}); } catch (Exception oops) { LogLog.error("Could not retrieve category ["+catName+ "]. Reported error follows.", oops); return; } }[changed in parseCategoryFactory()]: // WK (26.08.2003): set the LoggerFactory to a user-defined one loggerFactory =(LoggerFactory)OptionConverter.instantiateByClassName(className, LoggerFactory.class, null); PropertySetter propSetter = new PropertySetter(loggerFactory);[changed in parse(Element)]: if (tagName.equals(CATEGORY_FACTORY_TAG) || tagName.equals(LOGGER_FACTORY_TAG)) {3. Change the log4j.dtd<!ELEMENT log4j:configuration (renderer*, appender*,(category|logger)*,root?, (categoryFactory|loggerFactory)?)><!ELEMENT logger (param*,(priority|level)?,appender-ref*)><!ATTLIST logger class CDATA #IMPLIED name ID #REQUIRED additivity (true|false) "true" >4. New in log4j.dtd<!-- WK (26.08.2003): as categories are deprecated a loggerFactory element --><!-- should be available as a replacement for the categoryFactory element --><!ELEMENT loggerFactory (param*)><!ATTLIST loggerFactory class CDATA #REQUIRED>Hope this helps and might find its way in the next log4j release (following 1.2.8)***has been marked as a duplicate of this bug. ***Will this one ever get fixed? Is there a know workaround?Checked in for 1.3 (trunk SVN)rev 500155 addressed the problem in the trunk, however made some unnecessary name changes and did not include tests. See log4j-dev discussion on 2007-01-30. After those are resolved, backporting the changes to log4j 1.2 should be considered.tests/src/java/org/apache/log4j/xml/LoggerFactoryTest added rev 500161.CreatedPatch to fix this problem on the v1_2-branchI don't know if this is similar to the fix in 1.3 since I'm only using 1.2, butthis change fixes a similar problem I had like this bug, except when using thePropertyConfigurator.I don't use XMLConfigurator, so I didn't look into using this change for that,but I'd imagine it'd be similar.Can't add a new method to the LoggingRepository interface (as suggested in the 2007-03-07 patch) as it would break any existing implementation that did not implement the newly introduced setDefaultFactory method. Looking at the existing code and trying to keep the change minimal, it seems like the loggerFactory setting should likely only affect the current configuration() call and not affect the current LoggerRepository, otherwise you'd need to add a new LoggerRepositoryEx interface and methods on Hierarchy and still could not guarantee that a call from getLogger("foo") would return the expected class (since it might have been created prior to the configure() call).	9.0	id=53558	5	True	False	vhennebert	1
id=4913	REOPENED	None	Log4j - Now in Jira	Other (	1.2.18	All All	P3 minor	log4j-dev	2001-11-16 02:27 UTC by	J	2014-08-29 11:44 UTC (	4 users	It is impossible to interrupt() a FileWatchdog from outside.In method run the catch does nothing and should be enhanced by two lines: public void run() { while(!interrupted) { try { Thread.currentThread().sleep(delay); } catch(InterruptedException e) { // no interruption expected interrupted = true; // to be added! break; // to be added! } checkAndConfigure(); } }	The Watchdog architecture will be revisited in log4j 1.3.CreatedWebapp to demonstrate the filewatchdog leakTen years on, this is still an issue. I've attached a test case to reproduce the problem, and hopefully accelerate a fix.The test case is a webapp that I deploy to Tomcat (v7.0.26 in my case) and configures logging. The webapp can be undeployed, but results in a leak because Log4j's Filewatchdog thread is still running, despite the explicit call to LogManager.shutdown() in the ContextListener.To Reproduce:1) Install Tomcat with the manager application (not covered here)2) Build the attached webapp (Ant)3) Deploy the LeakyWeb.war file to Tomcat, either via Tomcat's manager application, or by dropping the war file into the "webapps" directory.4) [Optional] Visitto view the "Success" method.5) Use Tomcat's manager application to undeploy the webapp.5a) Note this error message from Tcomat:May 1, 2012 11:43:08 AM org.apache.catalina.loader.WebappClassLoader clearReferencesThreadsSEVERE: The web application [/LeakyWeb] appears to have started a thread named [FileWatchdog] but has failed to stop it. This is very likely to create a memory leak.6) Alternatively, press the "Find Leaks" button in Tomcat's manager application to see the same information.DESIRED OUTCOME:Invoking LogManager.shutdown() should interrupt and shut down any watchdog threads that were started by the associated configurer (PropertyConfigurer in this case.)Naturally this same problem applies to the XML config files as well.wow... ten years and seems as it could be fixed with 2 lines?When I ever manage to release 1.2.17 I will put this issue on my list for 1.2.18The two-line change is a good start, but wouldn't cause the watchdog threads to stop when LogManager.shutdown() is invoked.Thanks. All suggestions etc from your side welcome.I will look at the shutdown method pretty soonish as I suspect it should clear the MDC stuff too. I can then try to dig into the filewatchdog too.Anyway, if you want to help, you are welcome :-)As the originator of this bug I'm very glad to see that someone is working on it after more than ten years ...And I'll have a look at the old bugs assigned to me.Jürgen M. Regel, TUI-InfoTec, Hannover, GermanyThank you Jürgen.Feel free to ping me / us on the log4j-dev mailinglist if you want to discuss.CreatedPatch to end FileWatchdog threads when log4j is shut downI've attached a patch that works for me. Patch is against the 1.2.17 tag in svn.As I see in Pete Feltham's, a solution much better than my quick-and-dirty fix ten years ago has been found. Even if my server, where the problem arose, is still running without changes and issues for some years, I am not able to verify this now and have confidence in your solution. I'll migrate to the new version of log4j when my program has to be changed again.'s target milestone is 1.2.18. Is there any target release date for 1.2.18?not yet, i hope i can find some time in mayPlease?! 12 years for such a small issue? What is the problem? Is it so hard to commit the patch and retest? How can any developer be so ignorant toward users? The people in my project (I am ta Scrum coach) actually decided to switch from Log4J to LogBack just because of this issue because SEVERE log messages in Tomcat which are created by this blocking thread keep ringing alarm bells in our operations team. They get woken up at night or on weekends whenever this happens during Tomcat shutdown. Compiling out own version of L4J is not an option, so we need to switch to a whole new logging framework.Sorry for the typos, I cannot edit my previous message.Hi,Are there any plans of fixing this issue?Thank you.RománCan you switch to Log4j 2?Log4j 1 is not getting much attention as all our focus is on 2?Hi all.I faced out with this problem today and migrating to the log4j 2, is too expensive for me... so I preferred to look the code and try to fix the problem.So I added a method, that shutdown all FileWatchDog threads, callable from: PropertyConfigurator.shutdown().For me this works, and so I hope it will for you. Look at the attachment as log4j-1.2.18.jar.Best regardsR. A.CreatedTerminate the FileWatchDog threads from PropertyConfigurator.To use the new function, call the PropertyConfigurator.shutdown() before exit from the application.Note: I suppose you called PropertyConfigurator.configure...(...) at the init of your application.Good morning to all!I apologize for yesterday's post, but the rush to get a solution, I did not read all the posts and have found the solution to this only after my post. I think the best solution is previously provided, although the one contained in the library provided by me works. Have a good day and job. R.A.Keep in mind that the 1.x line is not actively maintained in favor of concentrating our efforts on 2.x.You are correct, version 2.0.2 does not support the properties file format for configuration.	21.0	id=29603	5	False	False	p.mouawad	1
id=17887	REOPENED	None	Log4j - Now in Jira	Appender (	1.2	Sun Solaris	P3 major	log4j-dev	2003-03-11 17:44 UTC by	Natalie Wang	2008-09-02 05:51 UTC (	3 users	RollingFileAppender does not work for 10 threads.1. It does not roll over2. It truncated the file when it should roll over, but never appends it again.3. It happened very often	I make the file size to 2MB or 4MB. Is it a limitation for the file size?The RollingFileAppender is designed to work in a multi-threaded setting. Would it be possible for you to supply a test case? Many thanks in advance.In the abscense of further information, I am marking this bug report as WORKSFORME.I also have this problem but are unable to generate a test scenario. It occursonly under heavy load after a few hours of running.Could problem be related to the code in subAppend method? It seems to berelated to the null-pointer exceptions issues in the subAppend method.If one thread rolls over, the fileName might temporarily be reset to null. Asecond thread will write using a null filename.I added a test in subAppend before calling the super method. It resolves mycase but it is not a full solution because there is still some theoretical problem. protected void subAppend(LoggingEvent event) { if(fileName != null) super.subAppend(event); if((fileName != null) && ((CountingQuietWriter) qw).getCount() >= maxFileSize) this.rollOver(); }From an initial reading of the bug report this might be that two threads contend for rolling over the files (i.e. the triggering happens in two threads so the operations get mixed).Perhaps a synchronized block around the renaming-bit could help?Suggestions?May be the reason :The SocketServer TCP/IP server is designed to manage 10 (or 11 ?) maximum threads .See org.apache.log4j.net.SocketServer.java constructor :public SocketServer(File directory) { this.dir = directory; hierarchyMap = new Hashtable(11); }There must be some mixing or hidden bugs between the clients if exceeding 10 clients.Thanks for your feedback if this is the right reason.Seefor (many..) extra bugs in log4j(In reply to)11 is the initial allocation size for hierarchMap, of course, which is the default anyway. May be a lack of memory + unmanaged allocation error ?(In reply to)It is unusual to provide the initialCapacity to the HashTable constructor (especially for such a low value) but not illegal.)Could you please elaborate on why your tool thinks these two cases are related?	8.0	id=4913	10	False	False	ceki	1
id=17498	REOPENED	None	Log4j - Now in Jira	Appender (	1.2	Sun other	P3 normal	log4j-dev	2003-02-27 16:28 UTC by	Andrew De Bona	2014-11-24 11:57 UTC (	4 users	I have an unusual problem with Logging and DailyRollingFileAppender. I have configured Log4J as follows:# The debug log configurationlog4j.appender.A7=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.A7.DatePattern='.'yyyy-MM-ddlog4j.appender.A7.File=${debug_log_file} log4j.appender.A7.layout=org.apache.log4j.PatternLayoutlog4j.appender.A7.layout.ConversionPattern=%d - %m%n#This log can be turned off by changing the priority lower than ERROR which is DEBUG or INFO or WARNlog4j.category.com.cibc.ibas.logger.debug=DEBUG, A7The Problem is that when I look in the logs directory I see the following:-rw-rw-r-- 1 proj12 proj12 829643 Feb 27 11:11 ibas.log-rw-rw-r-- 1 proj12 proj12 747099 Feb 27 11:15 ibas.log.2003-02-26as you can see today's log (for Feb.27th) and yesterday's log have the same timestamp which means that Log4J is still writing to both logs files. When I look at the log ibas.log.2003-02-26 it has logging activity for the 27th of February. It seems that the log files are always a day behind. (i.e. ibas.log.2003-02-25 has logging activity for the 26th.. ibas.log.2003-02-24 has logging activity for the 25th etc..)It seems as though log4j rolls over the file however keeps writing to both the new log file and the backup log file. Then repeats the same pattern the following day. We are running WebLogic 6.1 SP3 in Sun Solaris 5.8 O/S.Any help would be much appreciated,	This is probably a variant of.We are also experiencing the same problem on solaris. We are running iPlanet App Server 6.0 SP4 which uses J2SDK 1.2.2. This isrunning on Solaris 8. This problem appears for us inside a stateless sessionbean using the daily rolling file appender.refers to similar symptoms, but is on Windows NT. So I'm not sure ifit's related.We are also experiencing this problem, with iPlanet on Solaris.I think this may be the same problem as/31458Are you running in a clustered environment?I am also experiencing a similar problem.I am running j2sdk1.4.2 on Sun Solaris 5.8.We have pure java single threaded processes running in the same machine. We have a Log file for each process and each have their own properties file.I am also using the DailyRollingFileAppender.Below is a sample of my properties file.log4j.rootCategory=debug, Rlog4j.appender.R=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.R.File=/cdd/apps/logs/XProcess.loglog4.appender.LOG.DatePattern='.'yyyy-MM-ddlog4j.appender.R.layout=org.apache.log4j.PatternLayoutlog4j.appender.R.layout.ConversionPattern=%d [%t] %-5p %c - %m%nThe issue is that the process is writing into the present day’s log as well as the previous days log.This pattern continues and the following day’s log as well as the previous day’s log.-rw-rw-r-- 1 proj12 proj12 829643 Jan 27 11:11 XProcess.log-rw-rw-r-- 1 proj12 proj12 747099 Jan 27 11:15 XProcess.log.2005-01-16As a result I am loosing some of the data that is overwritten by the following day’s logs.Note the process does not write the same data in to the two logs but instead writes part of the log in the present day log and part of the log in the previous day logs.Can anyone Help me on this…..Do you guys have a solution for this? if so what is it. I want to know since we are facing the same situationWe have the same problem on Linux/i386 using log4j 1.2.8. As far as I can see,there was no relating bug fix up to the current version log4j 1.2.13. Am I right?Forget about my. We had an configuration problem with our tomcat,different web applications loaded instances of log4j and wrote unsynchronized tothe logfile.I am also running into this problem on a Solaris 8 box. We are using version1.2.12 of log4j.The log4j properties we are using are as follows:# Configure the name of the logout for the rolling appenderlog4j.appender.rolling.File=/dir/webapp.loglog4j.appender.rolling.layout.ConversionPattern=%d [%t] - %p %c - %m%n# Configure the Layout pattern and conversion pattern for the rolling appenderlog4j.appender.rolling.layout=org.apache.log4j.PatternLayout# Configure the rolling appender to be a RollingFileAppenderlog4j.appender.rolling=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.rolling.DatePattern='.'yyyy-MM-dd# Configure the stdout appender to use the PatternLayoutlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout# Pattern to output the caller's file name and line number.log4j.appender.stdout.layout.ConversionPattern=%5p [%t] (%F:%L) - %m%n# Configure the stdout appender to go to the Consolelog4j.appender.stdout=org.apache.log4j.ConsoleAppender# Create two appenders, one called stdout and the other called rollinglog4j.rootLogger=DEBUG, rolling# Set the logger level for everything under "org.apache" to WARNlog4j.logger.org.apache=WARN# Set the logger level for "org.apache.commons.validator.ValidatorUtil" to# FATAL because we know that errors will be generated here that we want to# ignore (due to the fact that it is unknown whether a consumer is personal# or business and the validation.xml file is not set up to accomodate the# differences)log4j.logger.org.apache.commons.validator.ValidatorUtil=FATALlog4j.logger.com.harland.collective.config.ApplicationConfig=WARNlog4j.logger.com.harland.choice.web.tags.CustomLink=WARNlog4j.logger.com.harland.choice.web.tags.html.HarlandLinkTag=WARNlog4j.logger.com.harland.choice.web.filters.PrivacyPolicyFilter=WARNlog4j.logger.com.harland.choice.web.util.resources.ChoiceMessageResources=WARNThe webapp is running under iPlanet and no other applications use this log4j.Only one JVM.This behaviour is clobbering the rolled file on us, so seems quite critical innature because we are losing yesterdays logs.CreatedAn alternate appender for daily rolling log files.DailyRollingFileAppender has issues when running in a multi-threadedenvironment, as a servlet container in it's nature is.The decision to roll over the file is handled within the appender, which inmany cases may exist in several instances, possibly due to placement in theclassloader hierarchy.Many of the appender instances may be unaware that the name of the file undertheir open file descriptor has changed.We have also experiences loss of logging data in this context, as DRFA deletesthe log file if it already exists when performing the roll over.Have you tried placing log4j in the boot classpath, ensuring that it sits ontop of the classloader hierarcy?In a web container, the .war files should NOT bundle log4j by themselves.I've attached an alternate appender we use for daily rolling. It avoids theissue by simply creating a dated log file in the first place.Marking as invalid as it is likely due to multiple instances of DRFA with same configuration.***has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ***I am also facing this problem .Please refer	14.0	id=13099	6	False	False	yoavs	1
id=28059	REOPENED	None	Log4j - Now in Jira	Configurator (	1.2	All All	P3 enhancement	log4j-dev	2004-03-30 18:26 UTC by	Sean C. Sullivan	2012-10-25 17:00 UTC (	0 users	class: org.apache.log4j.xml.DOMConfiguratorPlease add this method:public static void configure(java.io.InputStream)	Can you explain why configure(java.new.URL) doesn't work for you? If you canget an InputStream, you can get a URL, at least if you are loading your configfile off the classloader. What specific case do you have that doesn't work withthe existing functionality?Jake...of course I meant java.net.URLJakeThis is being discussed at the moment on log4j-dev, for the new JoranConfigurator and for PropertyConfigurator, though not for the deprecated DOMConfigurator. Please feel free to chime in to the list's discussion and explain your use-case there.In the future, please provide an explanation of why you need a particular method when you open a Bugzilla item for it. Alternatively, you can discuss you needs first on the mailing list(s).class: org.apache.log4j.xml.DOMConfiguratorPlease add this method:public static void configure(java.io.InputStream)because I want to load log4j.xml from a library jar in my war app.this log4j.xml will be served as various config example and also be used as template of my log configuration file.	4.0	id=17498	10	False	False	apache	1
id=6606	REOPENED	None	Ant	Core (	1.4	All All	P3 enhancement	Ant Notifications List	2002-02-21 10:09 UTC by	Stefan Bodewig	2016-10-10 10:49 UTC (	18 users	This META-BUG is here to collect all reports of the "<style> ignores <classpath>"or "<junit> ignores <classpath>" type, which are symptoms of a common rootproblem.I'll try to describe the problem here and show how to work around the problem, thegoal is to gather feedback on this description and add it to the FAQ and themanual after that.	***has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ***OK, first of all let's state that Ant adds all .jar files from ANT_HOME/libto CLASSPATH, therefore when I say "in CLASSPATH" in the rest, it means "eitherin your CLASSPATH environment or ANT_HOME/lib".This bug collects a common type of problem: A task needs an external library andit has a nested classpath element so that you can point it to this externallibrary, but that doesn't work unless you put the external library into theCLASSPATH.The root of the problem is that the class that needs the external libraryis on the CLASSPATH.When you specify a nested <classpath> in Ant, Ant creates a new class loaderthat uses the path you have specified. It then tries to load additionalclasses from this classloader.In most cases - for example the two cases above - Ant doesn't load the externallibrary directly, it is the loaded class that does so.In the case of <junit> it is the task implementation itself and in the caseof <style> it is the implementation of theorg.apache.tools.ant.taskdefs.XSLTLiaison class.Ant's class loader implementation uses Java's delegation model, seethe paragraphThis means, Ant's class loader will consult the bootstrap class loader first,which tries to load classes from CLASSPATH. The bootstrap class loaderdoesn't know anything about Ant's class loader or even the path you have specified.If the bootstrap class loader can load the class Ant has asked it to load,this class will try to load the external library from CLASSPATH as well - itdoesn't know anything else - and will not find it unless the library is inCLASSPATH as well.To solve this, you have two major options:(1) put all external libaries you need in CLASSPATH as wellthis is not what you want, otherwise you wouldn't have found this bug report.(2) remove the class that loads the external library from the CLASSPATH.The easiest way to do this is to remove optional.jar from ANT_HOME. If you doso, you will have to <taskdef> all optional tasks and use nested <classpath>elements in the <taskdef> tasks that point to the new location of optional.jar.Also, don't forget to add the new location of optional.jar to the <classpath>of your <style> or <junit> task.If you want to avoid to <taskdef> all optional tasks you need, the only otheroption is to remove the classes that should not be loaded via the bootstrapclass loader from optional.jar and put them into a separate archive. Add thisseparate archive to the <classpath> of your <style> or <junit> task - and makesure the separate archive is not in CLASSPATH.In the case of <junit> you'd have to remove all classes that are in theorg/apache/tools/ant/taskdefs/optional/junit directory, in the <style> caseit is one of the *Liaison classes in org/apache/tools/ant/taskdefs/optional.If you use the option to break up optional.jar for <junit>, you still have touse a <taskdef> with a nested <classpath> to define the junit task.Does this description make any sense? Would it help people who face the sameproblem you have seen? If not, how can we improve it?***has been marked as a duplicate of this bug. ***offers what I think is a better workaround for JUnit, namely the fork=yes option. True, it's not exactly what I want, but neither is it as complex (in my way of thinking) as disassembling and manually reassembling the ant optional framework. I did come across an aside in the docs mentioning that the classpath wouldn't work unless we fork, but better documentation would help with the understanding so folks like me don't spend hours trying to figure out why the classpath tag isn't working in this excellent new build tool that I'm just beginning to figure out :-) A warning from JUnit when it encounters a classpath tag in a non-fork task that wasn't taskdef'ed would also help to bring attention to the problem and help folks with the workaround without resolving it.How to resolve fully? I'll have to cogitate more on that one.Thanks Jim,yes, fork="true" helps in the case of <junit>, I forgot about it.***has been marked as a duplicate of this bug. ***Thanks for this very clear description of the problem from ant's point of view.Now let me expose some of the very concrete problems we run because of this bug.We use ant scripts to build our product on various software platform (somelinux, some forte/netbeans on windows with the integrated ant). Since ourexternal dependencies shift a lot they are downloaded on the build computer byant at the beginning of the build process. This is done via xslt analysis of xmldescriptors bundled with each of our modules. Installation documentation is alsoprocessed with xslt at build time.Putting all libraries in the classpath is therefore not possible since most ofthe external jars used in a typical build do not exist on the build systembefore ant start, and if they do they might not have the correct version for theversion of the product we are building (moreover the maturity and behaviour ofxslt java engines changes a lot from version to version and provider to providerand using the wrong xslt jar with a version of our ant scripts will cause it tofail)Micromanaging ant installations on all the build systems would be a nightmare :some are bundled with other products (forte) and anyway there is a very highrisk they would have to be changed on *all* system every time the main buildsystem changed a little or we tried to build an old version.I appreciate this is a difficult problem and I do not expect ant to be fixed atonce but I really think it is worth it (and the number of bugs open on thissubjets kind of suggests it)***has been marked as a duplicate of this bug. ***This now isStefan, your suggested solution "remove the class that loads the external library from the CLASSPATH" is not sufficient. It fails, for example, inthe case of Mappers. The problem is quite the same: The regexp packagesare not found, unless they are in lib. And, of course, I cannot removethe Mapper classes from the classpath.There has to be another solution for the problem. For example, TomCathad quite the same thing with JAR's in WEB-INF/lib. For example, ifI have a WEB-INF/classes/jndi.properties then it wasn't found beforeTomCat 3.2.4. I had to put it into TomCat's system class path. Nowadaysthis works.I have a suggestion, which fixes most of the problems. More precise,it someone depends on the "right" behaviour of the software componentsin use. But the most important software component is, of course, Ant.First, let me explain my view of the problem: Any Java class hasan associated ClassLoader. The ClassLoader has loaded the class,thus it is able to load other classes from the same JAR file (orfiles). If two classes A and B have different ClassLoaders and Btries to load a class from A's JAR file with Class.forName(...),then this doesn't necessarily work.This is exactly the problem with Ant. For example, if I load aclass MyTask with <taskdef name="mytask" ...><classpath>...</classpath></taskdef>then the class MyTask has the right ClassLoader, but the Antclasses don't. So, if for example the MyTask uses a class MyTypeimplementing a nested element of MyTask, then the IntrospectionHelperwill do a Class.forName("MyType"). This fails, because Class.forNameuses the ClassLoader of the IntrospectionHelper.The proposed solution works as follows: - Never use Class.forName(). Replace it with a helper method like public static Class myForName(String name) throws ClassNotFoundException { try { return Class.forName(name); } catch (ClassNotFoundException e) { ClassLoader cl = Thread.currentThread().getContextClassLoader(); if (cl == null) { throw e; } return cl.loadClass(name); } - Make sure, that the threads ContextClassLoader is right. For example, use a helper class ThreadContextClassLoader, which grabs the current ContextClassLoader and an additional ClassLoader and uses Class.forName(), original thread ContextClassLoader and the additional ClassLoader, in that order and enclose the perform() method with Task task; ThreadClassLoader tcl = new ThreadClassLoader(task.getClass().getClassLoader()); ClassLoader cl = Thread.currentThread().getClassLoader(); try { Thread.currentThread.setContextClassLoader(tcl); task.perform; } finally { Thread.currentThread.setContextClassLoader(cl); }I offer to create a patch, if someone (Stefan?) discusses detailswith me and advices me, which version to patch exactly.Regards,JochenOne problem with you approach is, that the context classloader is not availablein JDK 1.1.We already have at least half of the infrastructure for your proposal in place,i.e. AntClassLoader knows how to set and unset itself as the context classloader(via reflection) and the oat.ant.util.LoaderUtil class can retrieve the contextclass loader via reflection.All that we'd need now:(1) set the context class loader in the right places(2) use it consistently(3) ensure we don't break backwards compatibilityI'll be happy to assist here, the context would be no earlier than Ant 1.6 ofcourse.The statement below is not correct.Introspection Helper gets the class of the nested type from the class beingintrospected through reflection. This class will have been loaded by MyTask'sloader when the MyTask class was resolved by the VM. Class.forName is not usedin IntrospectionHelper unless you are trying to contruct a Class argument whichwould be very uncommon (probably never).Just FYI...***has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ***Since my bug report (22170) has been marked as a duplicate of this one, I guessits better that I put my comment in this instead.I think I might know what the problem is. Since reporting my bug I have made myown ClassLoader for my project, and I got exactly the same exception as Antdoes! It complains that it cannot find "Path.class". What I didn't notice atfirst was that the class it complains about not finding is not having anypackage! In my case the Path class does belong to a package. But somewhere in mycode there is a if (this.interfaceClass.getPackage() != null) ...getPackage() did actually return null when this.interfaceClass had been loadedby my class loader causing "Path.class" without a package to be looked for a bitdown using Class.forName(). This ofcourse failed with the ClassNotFoundException.What I did was to add a call to definePackage(...) with the class package beforecalling defineClass() in findClass(). When the class is then defined the package the class file refers to is also defined and the defined class can reference it.After this fix my class loader works perfectly.If I bring up AntClassLoader.java in my editor and do a find on "definePackage"nothing is found! So AntClassLoader never calls definePackage()! That is thesource of my problem!Best Regards, Tommy Svensson***has been marked as a duplicate of this bug. ***I have a patch that makes junit work without any changes to the ant installation.The only requirement is that you fork a new JVM for the junit test.How did I do this? I changed the ant-junit.jar so it does not load/reference anyjunit classes before it forks a new JVM. That way the classloader problemsdisappear since junit classes are not loaded inside ant's JVM.If you want to give it a testspin, get the jar file here:1) Download ant-junit.jar from here2) Drop it in $ANT_HOME/lib3) Run your tests, remember to set fork="yes" in your tests and have thejunit.jar in the classpath of the test. <junit fork="yes" printsummary="yes"> <classpath> include junit.jar here... </classpath>...I have compiled the ant-junit.jar using jdk 1.4.2_08 and against ant 1.6.4.I would like to submit the patch, but I have no experience in submittingpatches. Please tell me how. The patch is quite big, since almost all classes inorg/apache/tools/ant/taskdefs/optional/junit are changed and I have introducedsome new interfaces.to read about how to createpatches ....(In reply to)This could be quite useful. E.g. the NetBeans IDE usesfrom the module bundling junit.jar in order to force it to be added to Ant'sprimary classpath. Even though the build scripts that the IDE generates willhave an explicit path for junit.jar - which is used e.g. when compiling unittests - it cannot currently run <junit> unless this has been done. A patch suchas is described here would make it possible to do omit junit.jar from Ant'sprimary classpath, and would mean that a command-line build incl. unit testingwould succeed so long as the path was set up correctly, without requiring theuser to run Ant with junit.jar in $CLASSPATH (or ant/lib/ or whatever).Why not simply remove the optional ant tasks from the classpath in the default install of ant? This seems pretty reasonable:* For those that don't mind modifying their ant home directory, it's not really any extra work: since you can't use tasks like <junit> without manually adding junit.jar to the ant lib folder anyway, it doesn't seem like much more work to drag in two jars (ant-junit.jar & junit.jar) rather than one (junit.jar) into the lib folder.* For this (like me) who want to create a user-friendly build process that does not require users to modify their ant installs, I can redistribute the relevant jars with my project and load them within my build.xml file.I want my users to just be able to download my package and type 'ant' to have everything work. The only way I see to do this right now is to copy the source for the JUnit optional task into my own package, so that it is not loaded by the core classloader. This is a pretty crummy workaround.***has been marked as a duplicate of this bug. ***(In reply to)install of ant? Thisreally any extra work: since youfolder anyway, it doesn'trather than one (junit.jar) intodoes not require users toproject and load them within myeverything work. Theoptional task into my owncrummy workaround.I think this is the best all-around solution to the problem, with one addition:Put the optional ant tasks into a separate folder (optional/lib, opt/lib or somesuch) and then add a default property called ant.optional.library.dir that issimilar to the ant.library.dir except that it points to this new optionallibrary directory.This way, if you want to use an optional task you can simply include theexternal jars in your distribution and then write a taskdef whose classpathincludes the appropriate optional.jar file and its dependencies.While this does create a little more work for anyone wanting to use the optionaltasks, the volume of help requests would be simplified by adding an sampletaskdef to the doc page of each optional task.As it stands right now, the only workable solution to this problem makes antmuch less useful to people trying to build code rather than people trying towrite ant scripts. I would imagine that usability should focus first on peopletrying to build code before it focuses on people trying to write ant scriptsbecause (typically) the person writing the ant script will be somewhat moretechnical than the other. Besides, it makes the ant script more self documentingwith respect to its dependencies.tasks, i.e. it would break every single build file on the planet that uses any optionaltask.Sam, we cannot do that. we cannot break every build file. We need a bettersolution. For reference, my project's build process is documented here:We have a directory of libraries under SCM, this directory contains the releasesof add on tasks that we require for the build. You need either to declare thiswith a -lib option in ANT_OPTS, or pull the files into ANT_HOME/lib${user.home}/.ant/libLife would be simpler if we could somehow declare in a per-project basis anextra dir of stuff to load. All that stuff would need to be unloaded at the endof the build of course, or you have just contaminated a hosting JVM. I just dontsee an easy way of doing this.I want to stress that there is a _simple_ solution for this problem. No need tobreak any existing build files or rearrange jar files. This is true for thejunit task anyway.How? Simply fork junit tests into a separate JVM and make sure only the new JVMuses junit classes. There is really no need for the JUnitTask.java task toimport junit.framework.Test, it is just sloppy code. I have made this change andit can be done in less than a day! The only drawback is that people that do not fork tests into a separate JVMstill needs the current workaround.@SeeI have filed a separateto cover the specific case of junit.jar in<junit>, since this metabug seems too broad to close directly.I've got two cross references, both patches available.makes AntClassLoader in combination with the <classloader> task much more powerful, by ensuring that the ant core class loader will always be an AntClassLoader, which in turn allows addition of class path elements. The result is that you can simply use <classloader> to extend the core ant classpath at need, and don't have to worry about a complicated classloader hierarchy.takes yet another shot at <junit> and the missing TraXLiaison. This time without forking, but instead through yet another nested <classpath> element.Unfortunately both patches have seen no comments or review yet. Still waiting...	31.0	id=17887	6	False	False	ceki	1
id=5003	REOPENED	None	Ant	Core tasks (	1.4.1	PC All	P3 blocker	Ant Notifications List	2001-11-21 08:22 UTC by	Adam Sotona	2013-07-30 16:03 UTC (	6 users	Exec command stays waiting for output and error streams to be closed even when executed command already finished.This bug prevents Ant from execution of processes, that are not closing out and err stream correctly on Windows.Small example is java class only executing its argument: public static void main (String args[]) throws Exception { Runtime.getRuntime().exec(args[0]); System.out.println("finished"); }and build.xml containing something like this: <exec executable="java" > <arg line=" -cp . test rmid"/> </exec>This task starts rmid using test class, writes "finished" and stays hanged on Windows.The same code on Linux(Solaris) starts rmid, writes "finshed" and realy finishes.Main problem is waiting for error and output stream to be closed in org.apache.tools.ant.taskdefs.PumpStreamHandler method stop() code inputThread.join(); and errorThread.join();Output with Full thread dump of blocked exec task is:Buildfile: build.xmltest: [exec] finishedFull thread dump:"Thread-1" daemon prio=5 tid=0x8b8ad48 nid=0x604 runnable [0x8f2f000..0x8f2fdbc] at java.io.FileInputStream.readBytes(Native Method) at java.io.FileInputStream.read(FileInputStream.java:166) at org.apache.tools.ant.taskdefs.StreamPumper.run(StreamPumper.java:99) at java.lang.Thread.run(Thread.java:484)"Thread-0" daemon prio=5 tid=0x8b3da98 nid=0x57c runnable [0x8eef000..0x8eefdbc] at java.io.FileInputStream.readBytes(Native Method) at java.io.FileInputStream.read(FileInputStream.java:183) at java.io.BufferedInputStream.fill(BufferedInputStream.java:186) at java.io.BufferedInputStream.read1(BufferedInputStream.java:225) at java.io.BufferedInputStream.read(BufferedInputStream.java:280) at java.io.FilterInputStream.read(FilterInputStream.java:93) at org.apache.tools.ant.taskdefs.StreamPumper.run(StreamPumper.java:99) at java.lang.Thread.run(Thread.java:484)"Signal Dispatcher" daemon prio=10 tid=0x960620 nid=0x670 waiting on monitor [0..0]"Finalizer" daemon prio=9 tid=0x95c880 nid=0x4e8 waiting on monitor [0x8daf000..0x8dafdbc] at java.lang.Object.wait(Native Method) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:108) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:123) at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:162)"Reference Handler" daemon prio=10 tid=0x8af0368 nid=0x4fc waiting on monitor [0x8d6f000..0x8d6fdbc] at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:420) at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:110)"main" prio=5 tid=0x284950 nid=0x60c waiting on monitor [0x6f000..0x6fc34] at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Thread.java:930) at java.lang.Thread.join(Thread.java:983) at org.apache.tools.ant.taskdefs.PumpStreamHandler.stop(PumpStreamHandler.java:111) at org.apache.tools.ant.taskdefs.LogStreamHandler.stop(LogStreamHandler.java:85) at org.apache.tools.ant.taskdefs.Execute.execute(Execute.java:397) at org.apache.tools.ant.taskdefs.ExecTask.runExecute(ExecTask.java:250) at org.apache.tools.ant.taskdefs.ExecTask.runExec(ExecTask.java:279) at org.apache.tools.ant.taskdefs.ExecTask.execute(ExecTask.java:177) at org.apache.tools.ant.Task.perform(Task.java:217) at org.apache.tools.ant.Target.execute(Target.java:184) at org.apache.tools.ant.Target.performTasks(Target.java:202) at org.apache.tools.ant.Project.executeTarget(Project.java:601) at org.apache.tools.ant.Project.executeTargets(Project.java:560) at org.apache.tools.ant.Main.runBuild(Main.java:454) at org.apache.tools.ant.Main.start(Main.java:153) at org.apache.tools.ant.Main.main(Main.java:176)"VM Thread" prio=5 tid=0x8b5e1c0 nid=0x3c8 runnable"VM Periodic Task Thread" prio=10 tid=0x95f320 nid=0x558 waiting on monitor"Suspend Checker Thread" prio=10 tid=0x95fc70 nid=0x608 runnable	Createdsuggested fix of org.apache.tools.ant.taskdefs.PumpStreamHandlerthis bug prevents Ant from running on Windows and I found no workaroundThis is an interesting problem, and not one I have seen myself, despite my extensive use of ant on NT. I wonder if it is showing some interesting side effects of the call to exec() inside the sub process.Whatever, your supplied path [NB, please use diff -u in future] would seem to ensure ant continues, and given that the stop() method is called after the process has terminated naturally or been killed by the watchdog should not affect the sub process.However, it runs the risk of leaking threads. This may not seem much on a single ant run, but in an ant-in-gui or automated build system thread leakage can become an issue. Not as much a one as the build blocking, but still an issue.I think therefore that for a patch like this to go into the build, it has to print out big warning messages to the effect that something is wrong with the client app. Also we need to see if anyone else has replicated the problemCreatedanother proposal how to fix this bug by implementing interruptable readCreatedAnother more powerfull patch, because bug still ocures in several casesThis one has been here forever and I'm wondering if it is not related in some way to #10345 and #8510. Adam, out of curiosity do you have a testcase for this ?Oops stupid question. the testcase is here...having a look.The patch (id=996J) did stop problems I had with execute hanging. Unfortunately the patch also causes the output of my cvs log command to be prematurely truncated. Before applying the patch, my code would hang the second time I executed a CVS log command but all of the output made it to my client code's input buffer.I made the additional following change:The patch to PumpStreamHandler makes changes like:while (inputThread.isAlive()) { inputThread.interrupt(); inputThread.join(TIMEOUT);}I changed these to instead be: if (inputThread.isAlive()) { inputThread.join(TIMEOUT); while (inputThread.isAlive()) { inputThread.interrupt(); inputThread.join(TIMEOUT); }}From reading the previous patches this seems to have been the intent of Adam Sotona all along. He started out with something similar to this and then lost the initial wait in the later version.Immediately interupting the thread is more likely to cause premature closing of the thread. Thereby preventing the client code from obtaining all the output of the executed command. (cvs log in my case) At least with my additional change there is a better chance all the output is pushed into the client's input buffer.Am I correct in thinking that a call to Process.waitFor() would work, exceptthat StreamPumper does not know about such things? The complicationis that the command may not finish if you do not read the output streams butI am still wondering whether more fundamental changes might eventually beworthwhile e.g. Ant 2. StreamPumper looks to me like it might be mostly avoided.Still, the latest proposed fix looks like it would work.I no longer think StreamPumper is likely to be avoidedbut it is a shame that it is not dead simple.My own software that encountered the same problem underwindoze relied on Process.waitFor(). The nearest thing I haveto an ant task can now detect and interrupt a thread processing astream that is never closed. Sorry I don't have time rightnow to look into this possibility in the ant case.Assigning back to ant-dev as Stephane is now jetsetting aboutAdam, would you like to try the latest CVS version of ant, where <exec/> seems to be implemented by a different class org.apache.tools.ant.taskdefs.ExecTask if I read properly the defaults.properties file.The code is quite different from the code of the old exec task.It works for me with :java version "1.4.1_01"Java(TM) 2 Runtime Environment, Standard Edition (build 1.4.1_01-b01)Java HotSpot(TM) Client VM (build 1.4.1_01-b01, mixed mode)on Win 2K, Service Pack 2However, I wonder whether what is really happening is not that rmid has been fixed to close its stdout and stderr properly.I tried also with cvs log (which I am doing over ssh), and did not reproduce the problem.So the next question is :how can one create a test class or a shell or Perl script or C program which does not close properly its stdin/stderr streams, so that the problem can be "lab studied" ?Without a possibility to reproduce the problem, this should be closed as WONTFIX or WORKSFORME.Hi, I mark this bug as resolved for ant 1.6 since nobody has voiced remarks.bug is still present in 1.6 aand it cause problems for NetBeans 4.0 execution(NetBeans 4.0 build system is now based on Ant).See bug:simple case I've described before is still reproducible but better try toexecute notepad instead of rmid: public static void main(String args[]) throws Exception { Runtime.getRuntime().exec("notepad"); System.out.println("finished"); }Hi Adam,I do not follow your example with notepad.The Runtime.getRuntime().exec("notepad") will notreturn until the notepad process stops.Hi Peter,I do follow, the exec method starts the process and returns. Did you tried that ?BTW: part of the Runtime.exec javadoc: "Executes the specified string command in a separate process."Notepad is special; it is a GUI app. If you look at how windows execs guis, itdoes *odd* things, things that only make sense from a historical perspective.try on a command line app instead of notepad.Adam, I still do not follow.Ant exec is not the same as just calling process.exec().It's job is to start the process and handle it's inputand output <file descriptors|handles> and wait for it to finish.One can use the "spawn" attribute to spawn off the process andnot care about it.note, the notepad program is not the issue the follows alsoworks in unix:public class Test { public static void main(String[] args) { try { Runtime.getRuntime().exec("emacs"); System.out.println("finished"); Thread.sleep(10 * 1000); } catch (Exception ex) { ex.printStackTrace(); } }}The parent process of the emacs process is the java program, and when itdies, the parent process becomes the init process.so once again:- this bug occures on Windows only- if you just execute emacs from Java on Linux - the Java process finishes - OK- if you'll do it using Ant - it finishes - OK- if you execute notepad (or whatever you want) on Windows - the Java processfinishes - again correct behavior- but if you'll do it through Ant on Windows it will wait till ALL executedprocesses close their streams and that's not correcteverything was already described here and several patches were proposedyou just need to cut the streams pumping when the process dies on Windows aftersome timeout - that's allCreatedbuild file showing the problemJust do ant in the directory with the build fileIt makes a src directory, and populates it with two java filesThe files are compiles, and an exec is run "java -cp classes CallHello"On unix, the build finishes just after the "finished" message from CallHelloOn windows, the build finishes about 19 seconds after the "finished" messageOk, I see what you are saying now.On windows child processes keep the std and std outout file handles ofthe master process (or at least Runtime#exec() is implemented in thisway), on Unix this does not happen.This means that one can start a master process from ant. This masterprocess can create a number of child processes. The master processthen terminates, but the child processes of the master process arestill running. On Unix, the exec task while end at this time, buton windows this will not happen, in fact the exec task will wait untilall the children of the master process have terminated - this is *not* good,especially for something like rmid.I have added an attachment that shows the problem.I am reassigning this bug to the whole ant community, because I do not have anyspecial solution (I think my name was in there since 2003, at a time when theissue was inactive).***has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ***CreatedINTERVIEW EVALUATION FORMComment onINTERVIEW EVALUATION FORMthis has nothing to do with the bug; marking as obsolete.a loooooooooooooooong time, I know.Ant's code has changed a bit, so some extra work has become necessary. Thatother classes are now using StreamPumper as well didn't help either.With the original patch (even if adapted) several unit tests of Ant would hangand never return - I guess this has been true seven years ago as well.One major problem I faced was that available() returns 0 on a closed streamon some VMs (it did on Suns 1.4.2 for Windows, for example) and thus theavailable trick doesn't work unless you are sure you are going to interruptthe thread running StreamPumper eventually.I've also noticed that the approach using available impacts performance considerably, so I've restricted it to the platform (Windows) where it is needed (like the original patch did, but for a different reason).svn***has been marked as a duplicate of this bug. ***I just want to mention my fix which is posted at. (sorry for creating a duplicate)This bug is caused by closing unowned streams: new XyzStream() --> close it getXyzStream() --> don't close it, close/destroy the underlying objectreopened since an unresolved bug is merged into this.if there really is an issue caused by closing the streams thanis no duplicate ofUsing Ant 1.8.2 I still reproduce the error.(In reply to)How?Createdpatch to StreamPumper.run to make it responsive to interruptsI ran into the same issue and was able to get the ant JVM unstuck by modifying the org.apache.tools.ant.taskdefs.StreamPumper.run() to make it more responsive to interrupt conditions. Please see my diff to /ant-trunk/org/apache/tools/ant/taskdefs/StreamPumper.java and let me know if addreses any pending issues.Createdrevised earlier patch slightly ... does the write more efficientlyrevised earlier patchI had something like this in some of my code.I found process.join() was returning, but my joins on the workers reading the input streams were not. you could assume if the process has been dead, or has been dead a certain amount of time, you no longer care about the data on the streams. I ran into something like this when killing an external process. In my case, the choice is easy. I am forcibly terminating it. I don't care about the streams. Basically you get a reference to the streams while the process is still alive. And then at any point after it's dead, (or should be) you close the streams. This worked like a charm for me. I could do this where I join the process as well, instead of where I kill it.So.... OutputStream outputStream = process.getOutputStream(); InputStream inputStream = process.getInputStream(); InputStream errorStream = process.getErrorStream(); process.destroy(); try { outputStream.flush(); } catch(IOException e) { } try { outputStream.close(); } catch(IOException e) { } try { errorStream.close(); } catch(IOException e) { } try { inputStream.close(); } catch(IOException e) { }Ha. Strike that. Sometimes the calls to close() still deadlock in a native method.	39.0	id=28059	4	False	False	yoavs	1
id=11560	REOPENED	None	Ant	Core tasks (	1.5	All All	P3 enhancement	Ant Notifications List	2002-08-08 14:18 UTC by	Lo	2008-02-22 12:18 UTC (	0 users	When using 'reverseLoader="true"' within a 'taskdef', standard ant tasks are not reloaded using the specified classpathExplanation:When Definer creates an AntClassLoader, it specifies "org.apache.tools.ant" as a system package root to uniquely load org.apache.tools.ant.Task and friends (otherwise breakages are to be expected)But not all classes inside org.apache.tools.ant package and furthermore inside sub-packages need to be loaded only once, but instead can benefit from being reloaded using an extended classpath: <taskdef name="javac" classname="org.apache.tools.ant.taskdefs.Javac" reverseLoader="true"> <classpath> <!-- contains tools.jar --> <fileset dir="${system.JAVA_HOME}/lib" includes="*.jar,*.zip" /> <!-- contains ant.jar --> <fileset dir="${build.lib}" includes="*.jar,*.zip" /> </classpath> </taskdef>Workaround:Set CLASSPATH with an external script which requires to re-code some of ant features like fileset and classpath using a non-portable script language(as is done today) or bootstrap ant itself using a second JVM - both are similarly inefficientSolution:Force the currently re-defined class (as Definer is not only used for tasks) to be loaded by the AntClassLoader by addLoaderPackageRoot when reverseLoader is defined---8<----------------------------------------------------------------- Definer.java.old Tue Jul 09 11:28:38 2002+++ Definer.java Thu Aug 08 15:37:24 2002@@ -224,9 +224,15 @@ * create the classloader then hand the definition off to the subclass; * @throws BuildException when the class wont load for any reason */- private void addDefinition(ClassLoader al, String name, String value)+ private void addDefinition(AntClassLoader al, String name, String value) throws BuildException { try {+ // allow reload even for ant classes+ if (reverseLoader) {+ al.addLoaderPackageRoot(+ value.substring(0, value.lastIndexOf('.')));+ }+ Class c = al.loadClass(value); AntClassLoader.initializeClass(c); addDefinition(name, c);---8<--------------------------------------------------------------	This patch does not really resolve all my problems, thus I changed it to INVALID state - I'm still investigation to find a proper solution based on a list of packages/classes for addLoaderPackageRoot()This new patch alloxs to alter the system and loader package root lists at classloader instantiation through new attributes and new elements.---8<----------------------------------------------------------------- Definer.java.orig Tue Jul 09 11:28:38 2002+++ Definer.java.packageroots Fri Aug 09 18:14:17 2002@@ -63,6 +63,8 @@ import java.util.Properties; import java.util.Enumeration;+import java.util.Vector;+import java.util.StringTokenizer; import java.io.File; import java.io.InputStream; import java.io.FileInputStream;@@ -88,7 +90,63 @@ private String classpathId = null; private static final String REUSE_LOADER_REF = "ant.reuse.loader";++ private Vector systemPackageRootList = new Vector();+ private Vector loaderPackageRootList = new Vector();++ public static class PackageRoot {++ public PackageRoot() {+ }++ private String name = null;++ public void setName(String s) {+ name = s;+ }++ public String getName() {+ return name;+ }+ + } + public void setSystemPackageRoots(String s) {+ setPackageRoots(false, s);+ }++ public void setLoaderPackageRoots(String s) {+ setPackageRoots(true, s);+ }++ protected void setPackageRoots(boolean loaderPackageRoot, String s) {+ StringTokenizer t = new StringTokenizer(s,",");+ while (t.hasMoreTokens()) {+ String p = t.nextToken().trim();+ if (p != null && p.length() != 0) {+ createPackageRoot(loaderPackageRoot).setName(p);+ }+ }+ }++ public PackageRoot createSystemPackageRoot() {+ return createPackageRoot(false);+ }++ public PackageRoot createLoaderPackageRoot() {+ return createPackageRoot(true);+ }++ protected PackageRoot createPackageRoot(boolean loaderPackageRoot) {+ PackageRoot pr = new PackageRoot();+ if (loaderPackageRoot) {+ loaderPackageRootList.addElement(pr);+ } else {+ systemPackageRootList.addElement(pr);+ }+ return pr;+ }+ /** * @deprecated stop using this attribute * @ant.attribute ignore="true"@@ -282,6 +340,15 @@ // be wrapped into a TaskAdapter. al.addSystemPackageRoot("org.apache.tools.ant"); + for (int i = 0; i < systemPackageRootList.size(); i++) {+ PackageRoot pr = (PackageRoot)systemPackageRootList.get(i);+ al.addSystemPackageRoot(pr.getName());+ }++ for (int i = 0; i < loaderPackageRootList.size(); i++) {+ PackageRoot pr = (PackageRoot)loaderPackageRootList.get(i);+ al.addLoaderPackageRoot(pr.getName());+ } // If the loader is new, record it for future uses by other // task/typedefs---8<--------------------------------------------------------------It solved my problem and is much more useable than my previous attempt.I can now use: <taskdef name="javac" classname="org.apache.tools.ant.taskdefs.Javac"> <loaderpackageroot name="org.apache.tools.ant.taskdefs" /> <classpath> <!-- contains tools.jar --> <fileset dir="${SYSTEM.JAVA_HOME}/lib" includes="*.jar,*.zip" /> <!-- contains ant.jar --> <fileset dir="${buildlibs}" includes="*.jar,*.zip" /> </classpath> </taskdef> <javac destdir="${destdir}" srcdir="${srcdir}" classpathref="javac-classpath"> <!-- referenciation needed to workaround bug to be corrected in 1.5.1 --> <classpath id="javac-classpath"> <!-- contains source dependencies --> <fileset dir="${dep.java.lib}" includes="*.jar,*.zip" /> </classpath> </javac> After the task redefinition bug is corrected, I won't need any launch script at all :)	2.0	id=5003	39	False	True	adam.sotona	1
id=15031	REOPENED	None	Ant	Core (	1.5	Sun other	P3 normal	Ant Notifications List	2002-12-03 18:39 UTC by	JL	2008-02-22 12:18 UTC (	1 user	BELOW IS AN ANT CFG FILE THAT DESCRIBES THE PROBLEM<!--Ant 1.5 has a bug/limitation that can mask problems during make.The <copy> operation does not spot or report files that are bad symlinksit just ignores them.to set up test case ....copy this file to /tmp and then run ant .../tmp/target ends up with ... goodfile realfileThere is no mention of the fact that there is a bad symlink.Ant does not report it and it does not return any indication there was a problem.If however I use an explicit file name (eg <copy file="/tmp/src_with_some_bad_links/badlink" todir="/tmp/target"/>) then I do get an error (a nasty java error)But this is not what I want to do - nor is it the sort of error message I expect (cos it looks like ant has dumped its pants).--><project name="AntBug" default="default" basedir="."> <target name="default"> <copy todir="/tmp/target"> <fileset dir="/tmp/src_with_some_bad_links"/> </copy> </target></project>	I'm not sure what we can do here. Java does not support symlinks. The methodsjava.io.File.getAbsolutePath() and getCanonicalPath() return the same value forthe broken link - so the trick of detecting links from the difference betweenthese two is not available. Also, according to java.io.File the file does not exist.BTW, for me Ant did not drop its pants on the explicit copy of the broken link.I get/tmp/bad.xml:12: Warning: Could not find file/tmp/src_with_some_bad_links/badlink to copy.Seems reasonable behaviour given the limitations above.I'm going to mark this as WONTFIX meaning CANTFIX. Sorry.BUT YOU CANFIX !You say "Also, according to java.io.File the file does not exist." - thats the key though isn't it!Any file that exists in the directory but for which java.io.File.exists() return false is almost certainly a dead symlink.We don't actually need to know its a symlink all we are trying to trap is the error where ant cannot copy the given file for some reason. I would argue that ant should also report an error for files that DO exist but however do not have read access.The program below traps these cases easily.ls -l /tmp/src_with_some_bad_links/ lrwxrwxrwx 1 badlink -> nonexistantfile lrwxrwxrwx 1 goodlink -> realfile -rw-r--r-- 1 realfile ---------- 1 unreadable... we get ..../tmp/src_with_some_bad_links: is a directory/tmp/src_with_some_bad_links/realfile: exists and is readable/tmp/src_with_some_bad_links/realfile: exists and is readable/tmp/src_with_some_bad_links/badlink:exists in the parent directory, but does not actually exist - this is a dead symlink - ANT SHOULD REPORT THIS ERROR/tmp/src_with_some_bad_links/unreadable: exists but is not readable ANT SHOULD REPORT THIS ERRORGiven this proof I would say it is entirely unreasonable that ant ignore such build errors.==================================================import java.io.File;class test { public static void main(String [] args ) { test("/tmp//src_with_some_bad_links"); } static void test(String name) { test(new java.io.File(name)); } static void test(File f) { try { String filename = f.getCanonicalPath(); try { if (f.isDirectory()) { System.err.println(filename+": is a directory"); File[] files = f.listFiles(); for (int iFile=0; iFile < files.length; iFile++) { test(files[iFile]); } } else { if ( f.exists() ) { if ( f.canRead() ) System.err.println(filename+": exists and is readable"); else System.err.println(filename+": exists but is not readable ANT SHOULD REPORT THIS ERROR"); } else { System.err.println(filename+":exists in the parent directory, but does not actually exist - this is a dead symlink - ANT SHOULD REPORT THIS ERROR"); } } } catch(Exception e) { System.err.println(filename + ":exception " + e.getMessage()); } } catch(Exception e) { System.err.println(e.getMessage()); } }}I think there is some possibility that JL may have a way to fix this, but moreconsideration is needed. How does it react to things like named pipes and otherfile system wierdos? Are these results true on all *nix's? What about acrossdifferent filesystems that may be mounted. And what about Cygwin?JL, you might be interested to readfor which I found a way to inferthe existance of Symlinks for the followsymlinks atribute. You will need toanswer the same types of questions before you can say that your method reallyidentifies broken symlinks only.Even if it doesn't identify them specifically, it is possible that the otherthings identified could be worth reporting with a warning, or even failing thebuild.Lets say the symlink issue is a red herring.The underlying problem is that Ant is not detecting unreadable or nonexistant files consistently and so my builds are apparently suceeding where infact the is a problem in our package and fies are missing - this is bad however you look at it.I have already demonstrated that ant can be tightened up so that it reports an error in the case where the directory says the file exists but io.File reports it as notexistantAnt does correctly detect that it can't copy a regular file that is permissioned as unreadble but it doesn't detect the case I mention above.Lets focus on detection of files that cannot be read (for whatever reason) and not let symlink detection sidetrack us.Anyway looking at CopyDir.java I see that scanDir does this (srcFile.lastModified() > destFile.lastModified())) According to the JDK API docs ... "lastModified() Returns: ... 0L if the file does not exist or if an I/O error occurs". So for my dead link lastModified() is returning 0 (to indicate an error) and the lack of error detection code tricks the method into thinking a copy is not required. To test this I set the overwrite="yes" attribute. Hey presto ant now dies when it encounters the dead sym link. So the code should actually be detecting the error value zero returned from lastModified() and in the case of the source file it should treat this as an error.So right now ant behaves inconsistently. -If you explicitely tell ant to copy a dead link then it detects this and failed.-If you are copying a dir and set overwrite='yes' then it detects dead links within dirs.-If you are copying direct and -don-t set overwrite="yes" then ant fails to detect the unreadable dead link.So perhaps the focus of this discussion should be on making ant behave consistently with respect to unreadable and nonexistant files - ie abort.Thx JohnSurely we don't need to do much more than call File.exists() and if it returns false then abort with an error ?Doing this explicitely is much cleared than trying to infer a problem by looking out for zero returned from lastModified(). I still think the tag needs to error check the return value from lastModified() but I think the code should explicitely check File.exists() and File.readable()Well, the situation you describe does seem to be somewhat inconsistant. Yourcomments suggest that overwrite could be used as a workaround, but that isadmittedly inefficient at best, and in some cases might not be satisfactory. I'msold that there is something to be fixed, but not being a commiter I can'tpromise anything. In your place, I would create a patch and attach it. Then wecan hope Connor or another commiter agrees, and commits your patch.Coming back to a really old report.I completely agree that Ant is inconsistent here, even though it has come to behavebetter in single-file copy here, it will report that the file doesn't exist (whatConor notes below).But this inconsistency goes further than just <copy>, we don't deal with thoseunreadable/non-existant files in other tasks either.As for the copy case, I'm afraid we are getting struck by backwards compatibilityagain. Touching the current beahvior would lead to breaking builds if we madecopy fail on bad symlinks. All we could do was issuing a warning, IMHO.ReCausing builds to break might be acceptable if you consider that those builds may actually be broken anyway, or at least on borrowed time, because of these unreported problems with the build. I think its far better to abort and force the user to fix the problem by way of removing the dead link or by coding round it in build.xml.An earlier posting suggested that ... <copy overwrite="yes" is a workaround (though an obscure and v.poor one) in that forcing the copy causes the task to abort on dead symlinks.However, since the original posting something may have changes in the Java code. This 'workaound' doesn't work - ie no abort happens.So Ant still does not report this dead link problem and now there's no way to cause ant to fallover either.	9.0	id=6606	27	False	False	bodewig	1
id=15949	REOPENED	None	Ant	Documentation (	1.1	Other other	P3 enhancement	Ant Notifications List	2003-01-10 08:22 UTC by	Ralf Hauser	2008-02-22 12:18 UTC (	0 users	Otherwise, I have a hard time recreating your html file.(see also:on missingerror messages from anakia that didn't help me resolve the problem either.)	Is the FAQ no clearer?If so, please close this bug."no" should have been "now", sorry.Hhmm, have your changes already been deployed - I don't really see much changeyet...?Also, the page is broader than my Mozilla1.3/MSIE window and doesn't adapt onre-sizing - is that intentional??Yes, they have been deployed.Two changes:* it now says "docs.xml at the top level of the jakarta-ant CVS module"* and "jakarta-site2 CVS module" instead of just "jakarta-site2 module"hmm, that's kind of minimalisitc ;)Or 1) do you anyway advocate to rather use forrest/cocoon?Also, 2) what do you say about the window size/horizontal scroll-bar/resizing issue?minimalistic? Well, yes, certainly, patches welcome 8-)forrest/cocoon? We (as in "the Ant developers") have not yet decided what we wantto do with our doc building system in the future. We are watching Forrest.IM (as in "Stefan's") HO Forrest is not yet up to the task completely.layout? Well, it's always been that way ;-) See ant.apache.org for an idea ofwhat we may be up to in the future (this looks like Forrest but uses Anakia).If I were able to reproduce the faq.html, I would be happy to provide thepatches you are asking for... (see my comment to)!The new ant.apache.org looks nice! congrats! Would it be possible to produce also w3c validating xhtml11 instead of html4/loose ?The easiest way to reproduce the FAQ is to check out the complete jakarta-ant andthe complete jakarta-site2 CVS module so that the directories are siblings.After that, run Ant on the docs.xml file inside the jakarta-ant directory.The minimal amount of stuff you need from the Ant module is docs.xml, xdocs/faq.xmland the complete xdocs/stylesheets directory. You also need all the jars thatAnakia requires, which means you need the complete lib directory of jakarta-site2.Createdfree docs.xml from jakarta-site2 subtree dependencyIt appears that all the jars needed for docs.xml/anakia to be able to properlycreate faq.html from faq.xml can be found in the jakarta-velocity tree as well.And this one is really needed to perform the translation. site2 appears to besomewhat unneccessary overhead to a location-independent way to build a faq.The last <echo> statement mentioning the velocity.log file addresses the issuesraised inOne remaining issue: While the faq gets translated perfectly, it fails totranslate the following macros: header, footer, source, table.First, in velocity.log, I got the error message:[error] ResourceManager : unable to find resource 'VM_global_library.vm' in anyresource loader.Found the missing file in jakarta-velocity\test\templates and placed in thestylesheets directory.No change in the faq.html, but a new error message in .log:[error] VM #recurse: error : too few arguments to macro. Wanted 1 got 0CreatedThe patch you were asking for.O.k. figured that the VM_global_library.vm was the wrong direction, gettingvelocity.properties into xdocs solved it.Therefore, the second attachment/patch now is what I propose to have the nextperson trying to do what I tried doing it in minutes, not hours (assuming youupdate docs.xml with the first patch).OK, just did a clean ant build, have jakarta-site checked out and did ant -f docs.xmlIt all works, so I don't think we need to change that. ThanksConor,Sure, it works as is. But wouldn't a little bit more user-friendliness be usefulfor two reasons:1) more people using this velocity-based approach to generate html mightincrease the prevalence of velocity in general2) as per my patches - checking out jakarta-site isn't necessary, since all youneed is in velocity/build/lib alreadyAlso, did you have a chance to look at the patches I provided?It would be strange if an apache.org-person asks me for a patch in the firstplace and now my contribution gets dumped apparently even without being lookedat at all... :(r.If you feel the bug has not been addressed, please feel free to reopen.Of course I have looked at the patches. I don't see that replacing a dependencyon jakarta-site2 with a dependency on jakarta-velocity is a great change.< <property name="velocity.dir" location="../jakarta-velocity" />It is not really my mission to promote velocity usage. I like velocity ingeneral but all I want is a system which can build the Ant website. Theunderlying technology is not that relevant.As for the second patch, I didn't see the benefit of putting links into CVS fromthe documentation since it is likely that the user will already have everythingchecked out. What issue is it that you are trying to solve?the issue to solve is: get a generic way to manage a website more effectivelythan just plain html in a cvs.In order to use the great mechanism you have developed for the ant website, Ionly need the velocity engine, but not the entire jakarta-site tree and I am upand running. The benefits are that your approach is a lot simpler than whatforrest/cocoon does and as Stefan wrote above: you are watching forrest/cocoon,but even with your new ant.jakarta.org website you do use their layout ideas,but not their underlying technology.OK, I'm going to reopen as an enhancement - we can revisit.	17.0	id=15031	5	False	False	conor	1
id=46048	REOPENED	None	Fop - Now in Jira	images (	0.95	PC Windows 2000	P3 normal	None	2008-10-21 02:30 UTC by	M.H.	2012-04-07 01:51 UTC (	0 users	The problem: if an (SVG) image with the same file name (but different file path!) is generated more than 1 times, FOP always uses the (wrong) first image.We use relative paths in our XSLs to reference SVG images. This worked good in FOP 0.20.5 but doesn't work in FOP 0.95 anymore (well, it works with FOP 0.95 when called via FOP.bat but it doesn't work when called from within a Java app via the FOP Java API). So I wrote a custom URIResolver to change the file name to the current unique file path. This solves the problem of the (so far) not working relative paths for images.However, if a subsequent document generates the same report with the same file but with different data, FOP doesn't use the newly generated file content but the old image of the first report. I guess the internal image cache doesn't use the resolved image file name but the first generated one.Example:1. XSL content:<svg:image width="170mm" height="120mm" xlink:href="C_PerfRiskCons_M.svg" xmlns:xlink=""/>2. custom URIResolver changes 'file:/D:/Tmp/iComps/amc/reports/C_PerfRiskCons_M.svg' to 'file:///D:\Tmp\iComps\amc\reports\dVwIIqKYfobFQDzUFJDQ5Er60ovA0G7YMpAVypnaMhY=\C_PerfRiskCons_M.svg'.with "dVwIIqKYfobFQDzUFJDQ5Er60ovA0G7YMpAVypnaMhY=" being a unique GUID for each report.=> it seems FOP 0.95 uses 'file:/D:/Tmp/iComps/amc/reports/C_PerfRiskCons_M.svg' for the image cache which would explain the faulty behaviour.I tried to work around this by serializing report generation and clearing the image cache before each report, but there is no org.apache.fop.image.FopImageFactory.resetCache() anymore in FOP 0.95 and I didn't find any other resetCache() method in the API.How can I work around this?	Have you tried the suggested work-around atalready (adding a unique dummy URL parameter)?(In reply to)No, because I can't set a unique URI in the XSL for each report generation. I try to achieve this with my custom URIResolver. But I noticed that for the second call (i.e. the second report) the custom URIResolver's resolve() is not called!!! This is another hint, that the image cache uses the URI before changed by a custom URIResolver! This would explain the the image cache lokks into the cache with the (non-unique) URI and finds it there and doesn't need to call the custom URIResolver.resolve(). If the cache would use the custom URIResolver's resolved URI, this would probably work.Ok. What you describe is all expected and correct behaviour. Obviously, it doesn't cover 100% of all requirements. I must say I'm not ready to believe that it wouldn't be possible to add a unique value for the report. You managed in the URIResolver.One idea could be to add a set of regular expressions to match URIs that should not be cached. We've also talked about an extension attribute on fo:external-graphic to disable the cache for certain images. But that's all not implemented, yet.To put you out of you misery ;-) here's the code to clear the image cache:fopFactory.getImageManager().getCache().clearCache();Doesn't the cache check for the modification date of file: URIs? Seems like anatural thing to do.(In reply to)No. Natural it may be if you only look at file URLs, but not all URLs provide a modification date. And we're actually working with URIs, not URLs, which don't have a modification date. Maybe this can be improved. Experiments welcome.(In reply to)That's why I said /file:/ URIs. Something like:if (uri.getScheme() == "file") { check the modification date of the corresponding file}Doesn't seem complicated, but I'm obviously missing the big picture.As there is no method to clear the image cache, I now have a working workaround:iin my custom "FOP" class I introduced a new constructor to create a new FopFactory to finally get rid of all cached images:----------------------- /** * Constructor. * * Workaround image cache problem: each FopFactory has its own image cache. As a custom URIResolver (to set unique image file names) * is not considered by the imageCache (FOP 0.95), the image cache must be cleared. But as there is not such clear method anymore (FOP 0.95) * we create a complete new FopFactory. * * This constructor should be called with parameter 'true' for serialized FOP calls to avoid image caching problems. * * @param newFactory If true, create a new FopFactory and try to copy config values from the former FopFactory and UserAgent. */ public FOP(final boolean newFactory) throws Exception { super(); if (newFactory) { final FopFactory ff = FopFactory.newInstance(); final FOUserAgent ua = ff.newFOUserAgent(); ua.setBaseURL(fopUserAgent.getBaseURL()); ua.setURIResolver(fopUserAgent.getURIResolver()); ff.setStrictValidation(fopFactory.validateStrictly()); if (fopFactory.getFontBaseURL() != null) { ff.setFontBaseURL(fopFactory.getFontBaseURL()); } ff.setUserConfig(fopFactory.getUserConfig()); fopFactory = ff; } }//FOP()-----------------------------------I guess the problem is the image caching of FOP not taking custom URIResolvers into account.(Why is this bug "resolved worksforme"? This is a clear bug as I described it ...)(In reply to)<snip/>Would you care to look at my reply #3 again? I gave you the code necessary to clean the image cache. Here it is again:fopFactory.getImageManager().getCache().clearCache();(In reply to)Thanks, that did also the trick!Is there hope to fix this issue when using custom URIResolvers?(In reply to)Not the way you thought. A URIResolver returns a JAXP Source object and that can't be cached. It's not even guaranteed that the resulting Source object has a system ID. I wouldn't even know where to start to approach this the way you explained. If there's anything that can be improved then it's either looking at Vincent's proposal about checking the last modified date for file URLs (which would only solve this special case) or bypassing caching for certain URIs as I suggested. The URIResolvers are completely irrelevant for image caching, they just provide access to the actual resource when given a URI.(In reply to)Thanks for this clearing up! Now I can stop playing around with the URIResolver in hope to get it somehow fixed.Then I don't understand your: as I can't write a unique path to the XSL (as the XSL never changes because it is the layout of the report), we use relative paths (as they worked with FOP 0.20.5 Java API flawlessly). These relative paths result in the very same image file name for the same report but other data. The only workaround I see here, is to make an additional XML transformation of the XSL to find such relative paths and replace them with temporary full paths, which is not very elegant.I wonder, how I can get FOP working to process multiple documents in multiple threads. I guess, the only promising approach so far (FOP 0.95) is, to use new FopFactories and UserAgents for each thread and each report generated. But the note in("Apache FOP may currently not be completely thread safe.") is not very encouraging.(In reply to)Hint: XSLT parametersI'd simply determine a unique ID for each report instance (something simple as a counter) and pass that in as an XSLT parameter. The stylesheet can then append that to the URI: file:myimage.png?id=12345, file:myimage.png?id=12346....That's what I was thinking, too. You're not going to be happy when you always write the same file. IMO that's a really bad idea. The trick with the different directories is not a bad idea if you actually have to write the image to a file in the first place.That's just to cover our collective asses. FOP is thread-safe (if no little bug has sneaked in somewhere due to some oversight, multi-threading testing is not part of our normal test suite). But that doesn't mean it's not a good idea to do careful multi-threading testing of your application as a whole.I'm afraid I can't give you the best idea with the information I currently have. I don't know how you create your image. I assume you generate it before you call FOP. Assuming you have some information that lets you identify the data that needs to be turned into an image, consider passing that information [1] into the XSLT stylesheet as a XSLT parameter (similar to what I suggested above) but this time, you use this information to build up a private URI that holds all information to uniquely identify the image belonging to that report. Then, write a URIResolver that can deal with this private URI scheme to generate the image on the fly. That might actually allow you to bypass writing the image to a file, thus making the whole thing faster.[1] I'll try to show this by example: Assuming you can gather all your data from your database (assuming you use one) you could pass in the ID of the main record. my:report?id=873468&color=red could then be your unique URI for the report 873468 and some data shall be highlighted in red (random example feature). Your URIResolver will listen to the "my" scheme and parse it, then return an InputStream (or for example a DOM (DOMSource) in case of SVG) that accesses the finished image.Maybe that helps.Thanks for all these ideas! We first create all data (XSL, XML, SVG) in a temporary directory for each report. Then we call FOP to transform the XSL+XML to PDF. The references to the external SVG files are in the XSLs (as relative paths to these files).With this approach, other developers first design the report "offline" on their workstation with their tools. We can also re-create the PDF anytime later with third party tools like fop.bat instead of our Java application. For debugging, we can look in any xsl, xml and svg as the files are there. Anyway, I wonder how you can generate the SVG on the fly and pass the SVGDOMSource to FOP, as the image cache is ignoring the URIResolver anyway (as I learned now).(In reply to)>This sounds like we might have to rethink how we treat the URI when caching the image. If it's a relative URI, we'd have to prepend the base URI and only use the absolute URI in the image cache. I don't think we do that now. That could actually solve your problem.The thing is: We assume that a resource can be identified uniquely by its URI. After all there's the word "identifier" in "URI". If the same URI comes back, we assume it's the same image. The URIResolver is only used when we have to load the image (which is done once). The image loader framework then puts the loaded image (subclass of org.apache.xmlgraphics.image.loader.Image) into the cache under this URI, further identified by the ImageFlavor (as multiple representations of the same image can be stored in the image cache). So if you re-request the same URI again, the cache returns the image directly. No detour through the URIResolver.So, to pass in an SVG DOM, your URIResolver will create a DOMSource instead of a StreamSource. PreloaderSVG can make use of a DOMSource, so it doesn't have to be serialized to a stream first, in case you build your SVG as a DOM somewhere.(In reply to)Okay, I think I fully understood this and this is basically okay. So, the URIResolver is no suitable way of changing these URIs, alas. So what is a URIResolver good for, if it's just ignored in some cases (here: image cache already has URI and doesn't call URIResolver anymore)? Even the first time my URIResolver is called, the image cache still has the original URI instead of the changed URI from the URIResolver. Or is this behaviour also as intended?(In reply to)Yes, the URIResolver is ignored when the image is in the cache, but it's really only used for loading the actual image. The behaviour is as I intended it to be. I designed the cache so it uses the image's URI (i.e. an identifier) to uniquely identify the resource. Let me explain the design decision with some more information, to show you what kind of cases need to be handled:We have to support different kinds of URIs on fo:external-graphic and for other resources:(this is a URL, and therefore a URI)file:///C:/Images/logo.jpg (this is a URL, and therefore a URI)--> Direct access because they are URLsurn:images:13487973 (this is a URN, and therefore a URI)--> Identifier, the specifier doesn't care where the resource comes fromURI resolution means: Turn URI into a Resource.The resource could be represented by a URL but doesn't have to be, because:Possibilities:- CatalogResolver: Map URIs to URLs. Example: urn:images:13487973 -->(provided by a servlet somewhere on a server) (Resolved system ID (URL) available)- Private URI Resolver: Example: urn:images:13487973 --> URIResolver directly returns a StreamSource with a InputStream for accessing the image (There might or might not be a system ID (URL) in this case)If we implemented what you're wishing for, what would we do if there's no systemID? Can we be sure that the system ID is always more stable/correct than the original URI for use in the image cache? How do we decide what to use? IMO, if you use the same URI for different images, you violate the "identifier" purpose of the URI. The resource is no longer unique. Using the resolved system ID would only be a work-around. Well, this is my view and I can be wrong. But it's also how FOP has done it the last few years even before the image loader framework.Notes on Image Loader Framework implementation:For normal URLs (HTTP, FTP...) a stream is opened and decorated with an ImageIO ImageInputStream which provides random I/O access to the image. For that, the image is mirrored locally, either in memory or temporary file. Special care has to be taken that for the same URL, no two requests have to be initiated (for pre-loading and loading) which would cause additional round-trips.File URLs are handled in a particular way. For those, direct random I/O access can be provided directly.I guess I'll look into prepending the base URI to relative URIs tomorrow to make the original URI "more unique". That is almost certain to fix your problem.(In reply to)Yes, that would probably solve the issue. I also played around with some XSLT code to detect the current base URI (that has the unique subdirectory), but this would requires XSLT 2.0 and some nasty string cutting. With the standard Java Xalan (XSLT 1.0) I found no way to change my relative path to a unqiue full path during runtime.So, I'm looking forward to this change! Thanks for taking time!CreatedProposed patch against FOP Trunk for URI pre-resolutionAs promised I've looked into it (had some precious train time yesterday). I found two possible approaches to "pre-resolve" the URI relative to a base URI, so the image cache gets more absolute URIs. My first attempt was to build that into the image loading framework but that caused a lot of changes (even API changes). The second attempt is less invasive but needs changes in more than one place in FOP (ExternalGraphic and all renderers). To illustrate this I've just patched the PDFRenderer for the moment.The patch uses java.net.URI (since Java 1.4) to do the URI resolution (using URI.resolve(URI), not JAXP-style URIResolver resolution!). That seems to do the job just fine. I'm not 100% sure this is ultimately the right approach which is why I'm just posting a proposed patch here rather than doing the change directly. The change itself should be pretty safe because if there's a problem parsing the URI, the original URI is simply returned. Only relative URIs should be affected.The patch requires the Base URI (FOUserAgent.setBaseURL(String)) to be set for the document. From the command-line this will be done automatically (the source file's directory is used).Feedback and further ideas welcome.BTW, just to explain what happens with this patch:If you have src="chart.svg" on your external-graphic and the base URI is "file:/C:/reports/321cb123db23/", the image loader framework receives as URI: "file:/C:/reports/321cb123db23/chart.svg". Before the patch it would only receive "chart.svg".Maybe it would actually be better to delay the "pre-resolution" as long as possible, i.e. to do it inside the image loader framework (my first approach). But it would still require a change for ExternalGraphic and all renderers because the currently applicable base URI is passed along.If someone wanted to go even further, support for "xml:base" () could be added to FOP to override the base URI for certain elements. Should be too hard to implement.Wow, Jeremias! Thanks for working in this! I guess I have to find out how to get the latest developent version of FOP and how to compile it. I would like to see, if your patch fixed my specific problem ...Just downloaded the current trunk, built it and get a"I/O exception while reading font cache (org.apache.fop.fonts.EmbedFontInfo; local class incompatible: stream classdesc serialVersionUID = -9075848379822693399, local class serialVersionUID = 8755432068669997367). Discarding font cache file."Error. As only fop.jar and xmlgraphics-commons.jar changed and double checked that my classpath contained these, I wonder what else is wrong.(In reply to)You can safely ignore that. It just means that the font cache file is being rebuilt.This message is a warning that FOP failed to read from the Font cache. Which means any Font auto detection or Font directories will be re-scanned. So this failure doesn't break anything. To avoid the warning you can simply delete the old Font Cache file, which according to [1] lives in ${base}\conf\font.cache. Or you can disable Font Caching altogether using the option "use-cache"I think I will create a FAQ for this as it comes up a lot.[1]So, I replaced fop.jar and xmlgraphics-commons.jar with the new trunk version but the problem persists: the image cache retrieves the same (first) SVG. Putting back the FOP.clearImageCache();it works again.(By the way with the new JARs I get lots of new errors and warnings, e.g.:WARNING T16: Ascender and descender together are larger than the em box. This could lead to a wrong baseline placement in Apache FOP.SEVERE T16: Unsupported TrueType font: Unicode cmap table not present. AbortingWARNING T16: Unable to load font file: file:/C:/WINNT/FONTS/mapsym.ttf. Reason: java.io.IOException: TrueType font is not supported: file:/C:/WINNT/FONTS/mapsym.ttf...)(In reply to)I haven't applied the patch, yet. I was waiting for your feedback.That particular one is gone since yesterday in FOP Trunk.Those are normal and expected on a Windows machine. You've got font auto-detection turned on, but FOP doesn't support all fonts it finds. Those error and warnings are there to inform you which fonts could not be made available inside FOP.Oh, I see that there is an attachement. How can I aply this patch? When I look into the text file there are new (+) lines and removed (-) lines. I guess there is some kind of tool to simple run with this file? Or do I have to fiund the places in the code and replace them by hand?(In reply to)The attached patch is a unified diff which is the most popular format for patches. The easiest way to automatically apply it is Team/Apply Patch... inside Eclipse if you use that. Otherwise, TortoiseSVN on Windows will also make it easy. The most universal way is the "patch" utility:)HTHI am experiencing problems with the same image caching issue, and have a few suggestions for an alternate approach to resolving this issue. Unfortunately our base URI does not change so the patch included already for this defect does not help us here.We have a FOP-based webserver, that generates PDF files with embedded images. We also use a URIResolver to intercept requests for images and map them onto different resources. Many of the images are static (logos etc), but some of these images change infrequently (perhaps once a week).I discovered this problem when an image was missing on our webserver, but replacing the image did not fix the PDF (the absence of the image was cached as well). Similarly, deleting or changing an image did not alter the PDF.I am concerned about performance and scalability, so I would rather not create a new FopFactory for each request. The workaround to call fopFactory.getImageManager().getCache().clearCache() is brute-force - this will flush images for all threads and reports (even if another thread is rendering a PDF at the time).I think the ideal behaviour is to allow image caching to be optionally configurable on the session/run rather than globally on the FopFactory.My suggestion is to allow the FOUserAgent to be (optionally) configured with its own ImageManager. The FOUserAgent constructor would default to the ImageManager from the FopFactory (to preserve existing behaviour by default).The externalGraphic and PDFRenderer classes would change from FOUserAgent userAgent = getUserAgent(); ImageManager manager = userAgent.getFactory().getImageManager();to ImageManager manager = userAgent.getImageManager();To use session based image caching would simply require a new method to be invoked on the FOUserAgent to create its own ImageManager eg. FOUserAgent enableSessionCaching() {this.imageManager = new ImageManager(factory); }.An additional benefit is that session based images would be cleaned up much sooner (helping with our monitoring of free memory within the app).Another useful enhancement would allow the ImageCache to be configurable to exclude some URI patterns from its cache (perhaps by extending ImageCacheListener). This could enable a session based cache to cache some images (based on uri) and then fall through to the global cache for other images.(In reply to)Hi Alex,attaching the image cache to the FOUserAgent doesn't make much sense IMO as you cannot profit from cached images over multiple document runs. Usually, the renderer itself already caches the image once per document. I don't think there's a problem with cleanup, as you mention. We're using soft references: if not enough memory is there, the images get automatically discarded. This is well tested. Your suggestion about patterns to exclude certain URIs from caching is an idea that could be investigated but I'm not sure it helps here. I think it's better that we try this pre-resolution approach. I just have to find some time and motivation to finish that.Incidentally, the problem of the missing image that was replaced but then not picked up by FOP has been solved a few days ago by:Please check out XML Graphics Commons Trunk to see if your situation improves.Hi Jeremias,Thanks for the feedback. We currently render most of our PDF's twice - once while discarding output to calculate the page count and then again with the page count embedded within the document - I had expected the image cache to get more of a workout within a single session/run especially if logos are rendered repeatedly on each page.However, I was not aware that the renderers were caching the images themselves - can you point me to where in the code base that is happening?My thinking is that the current image caching strategy works well for a mostly static set of images - but is less flexible when the images are more dynamic in nature. I don't expect FOP to handle all of the various caching optimisations that different people might want, but it might be a very small code change to let people take care of it themselves.At the moment there is no way to alter the image cache behaviour (the objects are private and there are no setters to substitute them) - assuming that I do not want to modify the FOP code base in my system.I agree that only using the ImageManager as a cache for a single run would be far less beneficial for performance, but it would allow people to implement their own caching strategy via the UriResolver hooks.Even a simple code modification to disable (nullify) the cache on the ImageManager, would allow people to implement their own image caching via the UriResolver hooks.Thanks again.btw - would you prefer me to raise this as a separate issue?Hi Alex(In reply to)Just to speed things up, you may want to render to the area tree XML (or to the new intermediate format with FOP Trunk) instead of to PDF if all you need is the page count. That should be faster and will not need to load all images on the first pass.Inin putImage(): PDFXObject xobject = pdfDoc.getXObject(uri); if (xobject != null) { float w = (float) pos.getWidth() / 1000f; float h = (float) pos.getHeight() / 1000f; placeImage((float)pos.getX() / 1000f, (float)pos.getY() / 1000f, w, h, xobject); return; }It's not really "caching", but an image only needs to be embedded once in a PDF and can be reused multiple times. Not all output formats can do that, though.>Ok, I see what you mean. I guess there are various level where this can happen. One would be on the level your suggest. HTTP, for example, allows to check file stamps. That could be used to trigger a reload of an image. But that is going to be difficult to implement with the URIResolver approach. In a high-volume system this might also result in too many round-trips for file stamp checking. How about just doing the same I've done with the expiration for the invalid URIs? We specify an expiration for the cached images. When the lease expires, the image is discarded and reloaded. That has very little management overhead, should address your use case and should be implementable with only a few lines of code now that we've already got the expiration code. Would that work for you?No, I think it's fine to gather all informantion here, although it's maybe not the exact same case.BTW, I'd still be grateful for feedback on the URI "pre-resolution" idea in my patch.Hi Jeremias,Thanks for the pointers to the code base - that is a real help to my understanding.I had considered the expiration idea for all images (rather than just for missing images), but was not sure if it was the ideal solution. This solution would be perfect for me (with my current problem), but it would not have helped M.H. who originally raised this issue. It would depend upon how configurable the expiration was and how expensive it was to re-fetch an image.Can you explain your comments about the UriResolver being expensive in high-volume applications? I didn't quite understand the part about HTTP and timestamps.I know that the built-in (default) UriResolver will create connections to HTTP webservers or local FileSystems (etc) - and this can become expensive without any caching strategies.However, when a developer plugs-in their own UriResolver it can be as smart and efficient as they like (and does not need to create external connections). We have a global ResourceResolver class that implements the UriResolver interface. This implements its own caching strategy (and caches fonts, nested XSLT, imported XML as well as images). Our implementation primarily loads resources from disk (file), but future extensions to our system could allow this to generate XML, Images or even XSLT on the fly.I guess that is why I would prefer a hook that will let me take care of (part of) the caching solutuion.Sorry I cannot help with the patch - we only specify "logical" resources within our XSLT, they are all mapped to real resources via our UriResolver. We do not use the Base parameter. For what it is worth, I think the patch looks OK to me and may help some users - but it does not really address my concerns.Cheers!AlexJust to update my problem: we still serialize FOP processing and clear the image cache after each single PDF generation. (I didn't have time to re-build a new FOP from the CSV tree as my first try with the latest FOP trunk produced lots of errors during building FOP. So I wait for a next official release.)So, FOP 1.0 is out. I would like to test if the new URI handling in FOP 1.0 solves this issue. However, due to the randomness of this bug, I wonder how I can really test this and not just say "hey, it doesn't occur anymore" just to get killed by my boss, when it still happens in production environment with high concurrent FOP processings.The last reply #32 from Alex Whatson "Sorry I cannot help with the patch" is not very comforting. So is there an explicit change in URI handling so that relative paths in the XSL-FO are expanded to the full URL/file name in the cache?move to normal priority, pending further actionAs there is still no new release, I can't go into an evaluation phase. So, I'm waiting for a release to use to get some time to built a reproducable test case. Is there any planned dead line for a new FOP release?(In reply to)can you use a nightly build [1] to test? see also [2][1][2]resetting P2 open bugs to P3 pending further review	38.0	id=15949	5	False	True	conor	1
id=18391	REOPENED	None	Ant	Core tasks (	1.7.0	Other other	P3 enhancement	Ant Notifications List	2003-03-26 21:29 UTC by	Gus Heck	2009-07-30 07:38 UTC (	0 users	I was just fooling around with subant, and became really annoyed that it failedevery time it couldn't find a build file, but I definately wanted it to fail ifany of the subbuilds failed. I decided it was faster and prevented future maintainence to patch subant thanspecifically exclude directories that happened to live at the same level, butdon't yet have build files or don't need them. So the request is for the addition of a ignoreMissingBuildFiles atribute forsubant. It should of course default to false to preserve the existing behavior,and when true cause subant to simply ignore any specified build file that is notfound. (thus ignoring both a missing (default) build.xml, and any explicit buildfile that doesn't exist) so here comes the patch...	CreatedA patch to add ignoreMissingBuildFiles atribute to subantHmmm, I never thought that would be an issue, as I've never setup a buildpath that includes sub-projects without a build file, but I guess it could be useful to some people (obviously ;-). I would definitely keep the current default behavior though, as you indicated you did in your patch. --DDI'd have expected the project to take a fileset, and you'd just sayserver/*/build.xml. But I get the implication that directories without abuild.xml fail? That's cheesy. Given this task is in its infancy, I'm happy with not only the change, but anyunderlying behaviour changes we need (i.e. get the defaults right)Actually I was giving it a DirSet :) and letting it just look for build.xml ineach directory. Seems to work great on my machine with my patch applied.I'm not following Steve... What does cheesy means in this context?<subant> can take a <fileset> or a <dirset> or whatever else a path can take. java.io.Files which are not build files (i.e. !isFile() || isDirectory()) are appended the default build file name ('antfile' attribute, same as <ant> BTW). Of course, like in most places in Ant, explicitly telling a task to do something that missing the essential processing unit, the build file, is an error by default in <subant>.What is wrong with that? I guess I'm not following... --DDThen you could have simply specified a fileset...Assuming:<dirset dir="${dir}"> <include name="${somepattern}"/></dirset>using <fileset dir="${dir}"> <include name="${somepattern}/build.xml"/></dirset>Would only have found the *existing* build files. Not need for a patch in this case, and still as order less as before. --DDI agree with the current default. I can see utility in both directions. In mycase I am willing to commit to being sure that the build file is there, becausethe target I am writing is for develop-time compiling of multiple packages thatrely on a library (which I am going to edit and don't want to break). But whendeploying a product it would be very bad to silently ignore a missing build filebecause the finished product that was deployed might come out broken. I thinkDominique got it right, I just want the ability to loosen things up.hehe mid-aired... hmm the fileset solution does sound like it would work. I suppose that pushes mypatch into the realm of syntax sugar... I kinda like it though because it suitshow I think about the problem...I was thinking about this on the drive to and from work and it seems that subantclass clearly was intended to support specifying a directory that contained abuild.xml as well as specifying the build file explicitly. There is some workdone to append the antfile to non-file path elements. Specifically this: for (int i=0; i<count; ++i) { File file = new File(filenames[i]); if (file.isDirectory()) { file = new File(file, antfile); } execute(file); }Since the task clearly is meant to support the use of directories, I think it isa good idea to give the users some control over how it handles missing buildfiles. Alternately, one might decide that it is clearer if the task only workswhen the build file is specified and directories should always fail. I have abuild working, now and it could be conducted either with dirsets and my patch orusing Dominique's fileset method. It would be nice to know if I need to switchto the fileset method or not.My personal bias tends to be toward giving the user multiple options (so long asit doesn't lead to confusion or really messy practices). If there are multipleways to do the same thing, a wider array of individuals will find that theirfirst instinct on how to acomplish something is correct. To me this is whatuser-friendliness is all about :). I don't think my patch is large enough toraise perfomance issues, so I am still hoping it gets applied.Any more thoughts on this? Dominique, what do you think, is this a reasonablyuseful addition to subant? Any objections to someone commiting it? If not, any1wanna do the commit? :)I'm still not enthused by it, given filesets do the right thing.Maybe selecting by directories isn't what we want then? If one can specify directories, then the build breaks if someone adds adirectory that becomes included without also adding a working build file, oradds an exclude to the build to handle the new directory. This brittleness maybe desireable, but if it is discovered accidentally, right now the only fix isto recode the build file with a fileset that selects specific buildfile names(making the antfile atribute useless). With this patch adding theignoreMissingBuildFiles atribute fixes the problem with no further rethinking ofthe logic.My concern is for people who do just what I did and assume that the way to usesubant is to tell it a antfile name and feed it directories. (why else would theantfile atribute be there?)This logic leads one down the brittle path first. If nothing else, I think thisissue might be good to mention in whatever manual docs or xdocs are provided forsubant in the future.I suppose it could be that I stumbled into this minor hole simply because Ithink in wierd ways...Sorry Gus, I'll have to go with Steve on this one. I'm in favor of the changes for supporting an absolute antfile (not really the impl, more the use case), but your use case is already supported as-is, and I'm not too fond on your proposed changes personnally. Cheers, --DDHmm well I am open to suggestions as to how to improve my patch if there arethings you don't like about it. I'm not quite sure what you mean by "absolutebuildfile".I'm referring to.I was trying to say that I see that other use case as valid, and yours as invalid (but that's just me). --DDAh that bug slipped under my radar it would seem. I am perfectly happy if mybehavior is a useful part of a more interesting extension. I don't much care ifit is my patch or his. I also like his ideas.I have committed the code for the, which I have also closed for now.Gus, if you have any issues, reopen 18715. Cheers.*** This bug has been marked as a duplicate of***I've stubbed my toe on a use case that "just use filesets" does not cover now soI am going to reopen this... I discovered it when I tried to migrate away frommy old patched version of ant (patched with the patch I submitted, and thusallowing the ignoreMissingBuildFile atribute) to a newer version. <subant target="compile" buildpathref="path.dirs.copied.psets" ignoreMissingBuildFile="true"> <property name="compile.failonerror" value="true"/> </subant>Elswhere I define <path id="path.dirs.copied.psets"> <dirset dir="${dir.build.src}"> <include name="*/*/*/psets/*"/> </dirset> </path>I definately want a dirset for this path, when it is used for copying as I wantthe entire directories which include among other things, java code, libraries,images and build files, but I definately don't want to fail if it copies adirectory that doesn't have a build file (there are many). I DO very much wantthe build to fail if one of the sub builds fail. I would like to not need to maintain 2 specifications of this path. (the secondone being a fileset with the pattern "*/*/*/psets/*/build.xml). I am entirely open to an alternate (shorter!) name for the atribute, but don'tseem to be able to think of one I like betterCreatedSince I had to re-add this for my build to work with 1.7alpha, here is the patch recreated vs the latest CVS	18.0	id=20344	11	False	False	kris.bravo	1
id=20344	REOPENED	None	Ant	Core tasks (	1.7.0	All other	P3 enhancement	Ant Notifications List	2003-05-29 23:21 UTC by	Kris Bravo	2009-07-30 08:05 UTC (	0 users	Background:Currently, Ant supports logging as XML by specifying a logger on the command line:ant -logger org.apache.tools.ant.XmlLogger -verbose -logfile build_log.xmlProblem:There are a few limits to this. The log file and listener must be specified on the command line instead of in property fields or a file. Also, the log file is produced at the end of the build, so transforms and other followup operations have to be performed from another build script.Solution:This patch modifies the record task to allow a Recorder to be specified.XmlRecorder, a subclass of XmlLogger, is included in this patch and can be used to produce the XML log output. This may then be transformed from within the build.xml.Documentation:The ant Record core task's documentation is updated to reflect the change.Listener is removed from the wishlist at the bottom of Record's html file. After looking into the code, I found it more appropriate to refer to the attribute as a recorder (sticking to the codebase' naming convention).Example:Here, a java compile task is recorded, and the resulting log.xml file is used to produce html output. ... <record name="log/log.xml" recorder="org.apache.tools.ant.XmlRecorder" action="start"/> <javac ... <record name="log.xml" action="stop"/> <style basedir="log/" includes="log.xml" destdir="doc/" extension=".html"style="log.xsl"/> ...The tar.gz attachment includes the patch, new files. See recorder.make forthe list of changed and new files.Tests: The code has been tested with the XmlRecorder, no recorder, and for regression using the command line method. ToDo: The XmlRecorder will not add entries when the recorder is restarted. I figured it would be best to get this in first and learn the process (my first public patch ever). If restarting is needed I'll improve the code later.	CreatedPatch file and new files in tar.gzCreatedChanged source filesA few people have shown some interest in my change, so I've updated it for theANT_162 tag.Since I appear to have botched the first patch I attached, I've simply attachedthe changes files this time.Note: This change implements the following from the wish list on theCoreTasks/Recorder.html page:listener A classname of a build listener to use from this point oninstead of the default listener.This patch request has now past the three year mark. I'm not sure what theoutcome has been of other patches but to anyone looking for this type offunctionality I'd recommend moving from ant to Maven 2.Also, continuum (same site) handles automated builds well.No more updates on this patch will be done.I'm sorry your experience has been negative. It can be hard to get patchesnoticed if you don't nag the right people and find a committer who is interestedenough to push them through. I am reopening this because I didn't see anyreason other than your disgust that this patch couldn't go in in one form oranother... unless you are actually rescinding the IP grant you made when youclosed the issue report.-Mattespecially with ~400 outstanding enchancement requestsI'm not rescinding the offer, but did want to create a pointer if someone waslooking for this type of behavior from a build tool. I assumed that eventuallyall patches get either merged, rejected or overcome by events. This patch isgetting closer to the latter given the timeframe. The comment on the ticket isto keep folks from hitting a dead end.As for nagging someone to roll it in, I suppose I should have. In Maven's case,I became a committer on the plugins project to avoid this type of situation.	6.0	id=18391	11	False	False	gus.heck	1
id=20867	REOPENED	None	Ant	Core tasks (	unspecified	All All	P3 enhancement	Ant Notifications List	2003-06-18 12:06 UTC by	Tomislaw Kitynski	2008-02-22 12:18 UTC (	0 users	Currently I can load properties via eg. <property/> or <loadproperties/> and this is good. Now, I have several projects that depend on each other in much sophisticated way that could <ant/> task to resolve. I put as many properties to external files as possible and I load them into via appriopriate task into ant scripts where necessary, but still I have to play with some properties in each build script. Let's say I want to include current velocity jar into each project---if there's newer version of it, I have to change every build file to point to correct file. Well, you can say I could have one properties file with all the paths and load it, but since I have different machines, I would like to spare myself the nescessity of setting absolute paths. Instead I rather set $VELOCITY_HOME and change it when new velocity appears, but using env variables is not possible in properties files, and again: loading just the current jar name and joining it within an ant script currently forces me to do it in every build.xml file.So it would be great to have something, that have the possibility of setting props like now, use $ references to names, and that would allow include those properties in running script---this is exactly inversely to what <ant/> task does (in the meaning of passing properties).I've looked at the faq and documentation, but I haven't found anything like that---if this actually IS implemented, then please let me know.TIA	Have a look at how I deal with libraries in the JavaDevWithAnt project: http://www.ehatchersolutions.com/JavaDevWithAnt/ant.html#lib With a bit of indirection you can achieve the separation you are aiming for.Hmm, looks interresting---first of all it turned out, that actually using $ references in properties IS allowed 8) Great! 8) I guess this solves much of much concerns, and with help of your description it seems that I'll be able to do what I intented to---thanks a lot!Just marking as WONTFIX so its not an issue on the to-do list.Uhm, I let myself to reopen this rfi, since after my enthusiasm on your solution I shortly find out, that it still doesn't resolve all my problems.But let's get to the point. It is great thing, that the loaded properties are parsed against ${var} references. It really is very helpfull, but still I can not get rid of redundant code. Similary to what you'd suggested in the link you have provided last time, I've defined lib.properties, prj.properties (common properties for most projects) and commons.xml (common tasks for most projects). All those files are placed in .ant directory available to all build scripts. Besides I have build.xml and build.properties in each project's ant folder. File build.properties holds release, version, revision numbers, compiler options, some per-projects settings, etc.File commons.xml contains (amongs others) three targets: release, version and revision, which are called in init target of the commons.xml, which of course is called from each build.xml/init target. These targets utilize <propertyfile/> task to eventually set new version numbers, depending on build mode passed via -D property. This works fine.The point is, that now I have to 1) put version numbers (or shall I say: version string) into lib.properties and 2) replace it every time I change version number of some of my projects. Besides it's redundant. Well, I could work this around, and place in lib.properties file just the path to the project dir (some libs are "external"---provided by other people/companies, and some are "internal"---provided by me and their sources are somewhere in my fs) and read the build.properties for needed subproject (lib) in each build.xml and set necessary strings, but I have to do it in each build file---the code is exactly the same and from the point of view of a programmer it's terrible solution.What shall be done, then? Well, it would be great to add more processing power to properties files or (that's what I would vote for) have the possibility to include properties (and eventually all refs) to build files. With this I could change lib.properties into lib.xml and set there complete <path/> or <fileset/>---in most cases it's common for most of the projects, I could set there some filterset, properties a.s.o.I have prj.properties skelton like this (just two properties to make it more clear):...prj.tag = ${prj.name}-${release}.${version}.${revision}prj.jar = ${prj.tag}.jar...This file is placed in shared folder (let's call it .ant). Version numbers of each project are placed in build.properties, eg:...release = 1version = 3revision = 43...To set default properties in each build file I have to:<property file="build.properties"/><property file="${.ant}/prj.properties"/>and that's okay. Worse if I need to use subproject and to know this subproject version numbers. I can ofcourse do<property file="${subprj.path}/ant/build.properties" prefix="${subprj.name}"/>but I can not make use of ${subprj.name}.prj.tag and -.jar like I did in the main build file.I think of some kind of task like this:<include prefix="${subprj.name}"> <filelist dir="${subprj.path}/ant" files="build.properties"/> <filelist dir="${.ant}" files="prj.properties"/></include>I think that <include/> element (or whatever would it be called) could make use of inheritAll and -Refs attrs too. By default all inherit attrs should be set to false IMO. Further, we could include not only props and refs, but also targets---in this case the immutability rule should apply too. (It should be discussed if targets are to be prefixed also---maybe we could add a switch prefixTargets="true|false" for that reason). The prefix atribute is optional and the sequence of <filelist/> elements and files named in files attribute must be preserved. Maybe, for shortening, an attribute files (and optionally dir?) could appear in DTD for <include/> element too.The ability to include props, refs and targets into build scripts would greately reduce the amount of code in each script and thus the nescessity of updating of tones of files when something changes.Well, I am not sure if I am completly understandable with my explenation---English is not my native language, besides I am writting it as I am thinking of it, so I do not have precise vision of including build scripts. But I hope in general it's obvious what I think of---thank you for your time.	4.0	id=11560	5	False	True	loic.peron	1
id=22020	REOPENED	None	Ant	Core tasks (	unspecified	All All	P3 enhancement	Ant Notifications List	2003-07-31 14:20 UTC by	jesse farinacci	2009-07-31 03:17 UTC (	5 users	hello and good day--i apologize if the feature i am requesting has already been considered, to my knowledge it is not in existence or on the list of accepted / rejected items for consideration. i would like to see a new optional flag be added to the <target..> task which allows it to be marked as internal use only. this would effectively keep out users from tasks which should not be invoked from the command line. also, this would 'hide' the target, i.e. not be listed as a possible target task when -projecthelp is passed to ant. thanks for such a great tool!! :-)-jlf	I implement something of the sort currently It works like this: <target name="public"> <property name="invoked.public.target" value="true"/> </target> <target name="init"> <fail unless="invoked.public.target"> This target is not available to the general public. For a list of public targets use: ant targets </fail> ... rest of initialization ... </target> <target name="internal1" depends="init"/> <target name="internal2" depends="init"/> <target name="invokeMe" depends="public, init, internal, internal2"/> <target name="targets"> <echo>Public Targets for this ant file:--------------------------------------------------------------------------- invokeMe Does incredibly useful stuff--------------------------------------------------------------------------- </echo> </target>The key is to make every target depend on init and only available targets, themajor drawback is you have to maintain your own target to handle project helptype stuff. I would like to see this feature added as well, but I havn't takenthe time to come up with a patch because I already have the workaround in place :).ack bitten by my own editing errors again. The above should say:The key is to make every target depend on init and only available targets dependon public, the major drawback...Also, the depends for invokeMe should say internal1, internal2, but you probablyguessed that.1. give the target a name beginning with a hyphen '-' and it cannot be called onthe command line2. give public entry points a description attribute and omit it from the others,and when you do -projecthelp, the others dont get listed (except in -verbose mode)When was this targets beginning with - thing introduced? Has it been there allalong? if so where is it documented I know I looked at one point for some formof public/private target distinction and missed it (which isn't a very good testat all, but there you have it) After testing it myself, I found that anything that begins with "-" is parsed asan option, but what happens when things change? for example what if support for"--" to prevent further option recognition is added? Sounds like a perfectlyreasonable feature if you don't know that people are using -internal1 to keeptargets hidden.Also, the public/privateness is hidden in the text of the target name, ratherthan out in an atribute of it's own where it can be easily changed orprogramatically considered (perhaps in resolving imports? can an imported antfile's "private" targets be called? What about overriding them?)Steve's workaround is cool, clever and provides exactly what I want in terms ofhiding targets, but I am reopening because I think it also looks like acoincidental result of option processing quirk and a more robust system thatdoesn't push funtionality into the target names could be subsituted. This isafterall only an enhancement request. (maybe after I move I'll try to writethis, if Verizon doesn't go on strike and deprive me of a phone line in my newappartment) Others are of course welcome to beat me to it :)I also noticed that this issue sort of came up inbut the reporterseems to have ignored the response given. Perhaps 3807 should be marked a dup ofthis (since this one has more info on it already)Gus, you are 100% correct: any -something target is only private because of thecommand line parsing process. It is a dirty trick, but it is also one we knowpeople use :)Does that prevent us from adding a proper private="true/false" attribute to target, which would correctly enforce the target cannot be called using <ant> from the outside (not problem with leading - in the case), or cannot be called from a build file that imports the private target (in fact this private target would never be visible to the importing build file, which could merrily define a similarly named target... exactly like private methods work in Java BTW ;-).Maybe it's time to get away from the leading dash - trick, and do something clean and documented??? --DDno, it doesnt at all...Ok, I coded this up. I tried to write it in a way that access modifiers could beused fairly generally by simply ehancing the isAccessibleFrom method that Iadded to target. I have only implemented the from the modifiers public andprivate, which will mean works on the command line and doesn't work on thecommand line.Those who would like to consider what should be done with import/overide etc cansimply add another access point name to the isAccessibleFrom method and thencheck it in whatever code makes decisions about inclusion/invocation/precedenceof targets to find out what the author intended.My implementation likely needs to have some constants added instead of the magicwords "command line". I wasn't sure where the best place to put such constantswould be, so I just left it. Edit or not as pleases (of course). A patch fordocs will be forthcomming, I want to see if people like my patch before I spendtime documenting. I suspect that there is no good way to write a unit test forthe invocation on the commandline, but if someone can tell me how I am willingto learn :)The following build file seems to work as expected... (which I used for testing)<?xml version="1.0"?><project name="access-test" basedir="." default="pubtar"><target name="pubtar" access="public"> <echo message="default target called"/> <antcall target="privtar"/></target><target name="pubtar2" access="public" depends="privtar"> <echo message="default target called"/> <antcall target="privtar"/></target><target name="privtar" access="private"> <echo message="internal target called"/></target></project>CreatedAdds public/private access modifiers to <target>Any1 have any thoughts on my patch here?***has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ***The comment in the middle of the post linked below also points to a user desirefor access control on targets, or in this case ways to work around needing it...even if it isn't going to make it into the 1.6 can it go into 1.7? That way wecan start finding out why what I wrote is wrong (or not)?Also to keep info available in one place, I would like to note that some otherrelavant discussion of this RFE occured on the dev list here under the headingof one of the duplicates:This seems to keep coming up and, like DD et al, I cannot think of a good reason to keep it out as long as we preserve BC regarding "-*" targets...***has been marked as a duplicate of this bug. ***Well, now that the big complexity in the access modifiers exists -<import>, whatare we going to do with access. Which of these three do we want:1. the access modifiers control access from the command line2. private targets cannot be called from <ant> calls from other build files than(self)3. private targets cannot be called from imported files.(3) scares me. Big time. add that and people will want protected, maybe evenpackage private. And it complicates importing no end. Would it be an error toredefine a target that was private in an import? It is in C++ after all. Wouldit be allowable to depend on a private target you import? I am personally biased towards (1) -we just need to make explicit thatpublic/private only applies to command line access, not to anything else. Nb, assuming we say "-*" defaults to private, what is the semantics of <target name="-secret" access="public"> ?I would agree that (1) is the simplest. For clarification here and now, access defaults to "public". I would treat the -* targets as a different issue; i.e. if a user can find a way to call a public (explicitly defined as such or otherwise) target named -* from the command line, they are welcome to do so.As a long-overdue response to Gus's question(s) about the attached patch... I think an EnumeratedAttribute should be used for the access values... also, I don't know if the whole "access point" thing is warranted, especially if we choose that access is only relevant with respect to the command line.$0.02As an additional note,has historically been associated with this one and might be reexamined in terms of providing a way to implicitly allow -* tasks to be called from the command-line...Well, 1.5 years later, I am again in need of something along these lines, so Ifigured I might as well ping this bug...The current scenario is I am trying to set up a simple text based interactivebuild that uses input to collect various parameters (rather than having 15different -D arguments). This will be distributed as part of a zip file and usedby the customer to create a warfile with the tweaks needed for their environment.Since the customer gets the source code ayway, the quickest way to do this zipup the source and write an small build file that imports the development build,but hides dangerous targets from the user. It would be a disaster if theyaccidentally ran recreate-db against their production DB since the first thingit does is wipe the entire database.This need is slightly different than the intent of my patch, but closelyrelated. I have created duplicate targets in the build file to match the oneimported which helps (forcing the targets to be refered to asbuildname.sometarget instead of just sometarget), but imported targets show upunder project help.Has any further thought been given to controling access to targets? Have Imissed something in the past year? (I'm re-subscribing to the dev list so feelfree to answer me there)	19.0	id=20867	4	False	False	jakarta	1
id=22901	REOPENED	None	Ant	Core (	1.7.0	Other other	P3 enhancement	Ant Notifications List	2003-09-02 22:25 UTC by	Alexey Solofnenko	2009-07-31 03:22 UTC (	0 users	The patch adds '-only' option to be able to execute target(s) without their dependencies.	Createdpatch fileSolution: implement org.apache.tools.ant.Executor: public void executeTargets(Project project, String[] targetNames) throws BuildException { Hashtable targets = project.getTargets(); for (int i = 0; i < targetNames.length; i++) { ((Target)(targets.get(targetNames[i])).performTasks(); } }While trying to port old feature into new ANT I found that one cannot set "only" executor during ANT startup - "only" executor cannot be inherited, because inner <ant>, <antcall>, and Co should work with full dependencies execution. This still requires a special modeof execution. The patch is obsolete, but I can produce a new one.The Ant task and its derivatives use the SingleCheckExecutor for simplicity'ssake because it gives all the behavior they need. Adding the ability to specifya particular Executor would not be that difficult but it doesn't seem like muchmore than clutter, really. In this particular instance, it strikes me as oddthat you want/need to execute a target without its dependencies. If so, why doyou have the dependencies? Ordinarily you declare dependencies and the tasksdecide whether they have any work to do, so that your build incurs as littlework as possible. If you have two tasks and sometimes you want to execute onebased on the other you could impose that structure with helper targets. Itsounds as though restructuring your buildfiles could be immensely helpful.Executors should be inherited from the parent ANT by default. If one wantsto use a parallel executor, subants should use it too. Plus they should usethe same executor instance to have a single thread pool to enforce thread numberlimitation across all subants.Interesting argument. I will have a look at that.A big problem here, again, is the fact the sensible way to process multipletargets specified via nested elements on a single ant(call) invocation is withsingle-check execution; otherwise multiple tasks could be used. Ordinarily,then, you have default target execution for the main Project and single-checkexecution for ant(call)-based Project instances. The parallel case and itsthreadcount limitations is a good one; executor inheritance would definitely beuseful here, but we end up with this conflict. At this point the onlycompromise I see is to inherit the executor, with the rule being that nestedtargets will cause a single-check executor to be used in place of a defaultexecutor; or a new keep-going/single-check executor to be used in place of akeep-going executor; but no other combination of circumstances will result insuch a substitution.I think something likeExecutor getChildExecutor()will solve this problem.Createdproposed patchCreatedOnlyExecutor.javaThis is the patch that uses Executor.getChildExecutor() method.Hmm.. I didn't see where you were going with the childExecutor before, but Ijust looked at this again and I don't hate it. After 1.6.3 comes out I don'twant to alter the Executor interface unless I have to so I will be thinkingabout this.Maybe when (if) we start doing Ant libraries there could be an executor library;otherwise I don't necessarily want to get into adding a million of them to core. But would OnlyExecutor not want to return itself as a child? If I add thechild I will probably do an AbstractExecutor with a default getChildExecutor(){return this;} and make all existing Executors inherit from that. Then I'dprobably add an executor attribute to the Ant task as well.I would expect inner <ant>s to be executed fully, only the first level targets should be executed without dependencies.	13.0	id=22020	16	False	False	stevel	1
id=24711	REOPENED	None	Ant	Core tasks (	1.6.0	All other	P3 enhancement	Ant Notifications List	2003-11-14 15:55 UTC by	Matt Benson	2009-07-31 03:51 UTC (	0 users	The macro creator should be able to supply a description in the <element> element informing the user what nested contents are expected.	Added a description attribute to the element and to the attributeelements.Thanks, Peter. However, I originally envisioned the descriptions as showing up in error output. Attachments forthcoming...Createddiffs to MacroInstance.javaCreateddiffs to the test case scriptCreateddiffs to the test classCreatednew diffs to MacroInstance.javaCreatednew diffs to the MacroDef test caseCreatednew diffs to the macrodef test case Ant filepatches 4, 5, and 6 affect attributes and elements and contain the nested text changes.Createdhandle text descriptionCreatedtest case; handle text descriptionCreatedtest script; handle text descriptionOkay... patches 7, 8, and 9 (!) add error message enhancements and related test cases for nested text as well... these diffs are against current CVS; i.e. they are not cumulative within this issue or anything.	13.0	id=22901	11	False	False	mbenson	1
id=24694	REOPENED	None	Ant	Wrapper scripts (	1.5.4	Macintosh All	P3 enhancement	Ant Notifications List	2003-11-13 23:35 UTC by	Levi Brown	2008-02-22 12:18 UTC (	0 users	The new ant wrapper shell script automatically determines what the JAVA_HOME variable should be set to, but it should also export the variable so the entire ant build process can have access to it. For instance, I use ant to build my java files, create my native JNI headers using javah, and then to compile the native code by exec-ing a shell script which I populate from the ant buildl.xml. Within this script I need to know where JAVA_HOME is so I can include the jni.h file in my C compilation path. Previous to your new wrapper script, I had my own, which exported JAVA_HOME and all worked wonderfully.I have made the following one line change to your script and would suggest something similar:case "`uname`" in CYGWIN*) cygwin=true ;; Darwin*) darwin=true if [ -z "$JAVA_HOME" ] ; then JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Home export JAVA_HOME fi ;;esacThanks for listening, and keep up the excellent work! Ant is great!Cheers,Levi	*** This bug has been marked as a duplicate of***This bug was closed because "This bug has been marked as a duplicate of 24553" which is incorrect.is dealing with ANT_HOME, while this bug is related to JAVA_HOME which are completely different.Oops, yes, sorry.	3.0	id=22661	13	False	False	antoine	1
id=23629	REOPENED	None	Ant	Core tasks (	1.8.2	All other	P3 enhancement	Ant Notifications List	2003-10-06 17:41 UTC by	Ben Litchfield	2011-02-14 09:09 UTC (	0 users	Let's say you have two targets; ejbdoclet and compile. I would like to always run ejbdoclet if called directly. I would also like to run ejbdoclet when compile is called if and only if it has not been run before(or after a clean).What I wanted to do was this<target name="ejbdoclet">...</target><target name="compile" depends="init"> <available property="ejb-jar.xml.available" file="build\META-INF\ejb-jar.xml" /> <antcall target="ejbdoclet" unless="ejb-jar.xml.available" />...</target>This way if ejbdoclet is called directly it will always get run and if compile is called ejbdoclet will only get called if ejb-jar.xml is not available.I know there are workarounds to this but this enhancement is a lot cleaner.Ben	Why not just doing:<target name="compile" depends="init"> <if> <available property="ejb-jar.xml.available" file="build\META-INF\ejb-jar.xml" /> <then> <antcall target="ejbdoclet" unless="ejb-jar.xml.available" /> </then> </if>...</target>This to me is 10 times most understandable.Ups sorry, replace <else> for <then> in the above comment.Where is if/then/else construct documented?It is part of antcontrib. ONce I started using this task my buildfiles became10 times more readable. And much fewer temporary properties required to control the build.Hope it is helpful.Seems to me that adding if/unless attributes to antcall would still be useful.AFAICT, the only work-round when using standard Ant (1.8.2) is to add the if/unless to the antcall target,	5.0	id=24694	5	False	False	bodewig	1
id=25998	REOPENED	None	Ant	Core (	1.6.0	PC Windows XP	P3 normal	Ant Notifications List	2004-01-08 20:14 UTC by	Sean Timm	2008-02-22 12:18 UTC (	0 users	I'm deriving a class from PropertyHelper to use via the ant.PropertyHelper reference. I'm doing a quick code check, and while the static getPropertyHelper method of PropertyHelper gets called to return the reference, the setProject method is never called on the reference if it is found, so I'm not sure if I even have access to the project...Note that setProject *does* get called if a reference isn't found and a new PropertyHelper instance is created.	The setProject() is called as you say, when the PropertyHelper is created.Ok, looking at the code, it is a bit confusing - espfor property helpers in child projects.	2.0	id=23629	5	False	False	mbenson	1
id=22661	REOPENED	None	Ant	Core tasks (	1.7.0	PC All	P3 normal	Ant Notifications List	2003-08-22 15:45 UTC by	Antoine Levy-Lambert	2009-07-31 07:58 UTC (	2 users	The replace task does not preserve line endings properly.In.46, I did a change to avoid \r\n to be changed into \r\r\n on Windows (problem reported via email by Morten Mortensen).Now I have the problem that \n gets replaced into the system's line separator, in both source and replacement tokens.This showed with the test9 of ReplaceTest failing on Windows when the files of the test cases were downloaded with the cygwin cvs client (so having \n instead or \r\n)I am not sure what is the best way to tackle this issue.Maybe not doing any replacements of line endings in (value, token) pairs ?Not sure.	did not work on this bug, so I am putting it back to the community ...***has been marked as a duplicate of this bug. ****** This bug has been marked as a duplicate of***This must have been made duplicate by mistake?Yes, sorry, must have clicked on the wrong link.	5.0	id=24711	8	False	True	peter.reilly	1
id=26286	REOPENED	None	Ant	Optional Tasks (	1.6.1	All All	P3 normal	Ant Notifications List	2004-01-20 16:55 UTC by	Jayson Raymond	2014-06-10 06:55 UTC (	1 user	Problem #1: There is no way to set a property to empty using XmlProperty. Example: Ant (1.5.4) believes 'a' has not been set, but in fact 'a' has been set to an empty value. test.xml: <?xml version="1.0" encoding="UTF-8"?> <a></a>build.xml: <?xml version="1.0"?> <project default="default"> <target name="-setup"> <xmlproperty file="test.xml"/> </target> <target name="-a-set" if="a" depends="-setup"> <echo message="a is set to:'${a}'"/> </target> <target name="-a-not-set" unless="a" depends="-setup"> <echo message="a is not set"/> </target> <target name="default" depends="-a-set,-a-not-set"/> </project>Results: Buildfile: build.xml -setup: -a-set: -a-not-set: [echo] a is not set default:Problem #2: A tag that has been set to empty, is not recognized within Ant. One implication resulting from this is that composite objects can not be properly modelled within the XmlProperty file. Example: In this example, 'a' is an element composed of a 'b' element. While 'a' is clearly declared by the user, Ant says it has not been set. test.xml: <?xml version="1.0" encoding="UTF-8"?> <a><b attr="attr"/></a>build.xml: (same as above)result: (same as above)	Confirmed problem still exists in 1.6.1beta1#1 is fixed in svn and will be in ant 1.7.Thanks for the report.I am not too sure about #2.For example:<a> <b> <c>value</c> <d/> </b></a>a.b.c is "value"a.b.d is ""What is the value of a.b? and a ?I will leave this bug open at the moment.Setting WS branch nodes to properties willcause too much properties to be set.for example:<project> <message>value</message></project>would cause project to be set as wellas project.messageMarking as fixed since the main UseCase of empty leafelements has been fixed.Tested on Apache Ant(TM) version 1.8.2 compiled on December 20 2010Detected Java version: 1.7 in: /usr/java/jdk1.7.0_02-b08/jreDetected OS: LinuxSeems not resolved.Reproduction:test: [echo] par1a=1a [echo] par1b= [echo] par2a=1a [echo] par2b=${par2b}Expectation:test: [echo] par1a=1a [echo] par1b= [echo] par2a=1a [echo] par2b= <<<<==========Filestest.xml:<?xml version="1.0" encoding="iso-8859-1"?><root><par1a>1a</par1a><par1b></par1b><par2a>${par1a}</par2a><par2b>${par1b}</par2b></root>build.xml:<?xml version="1.0" encoding="iso-8859-1"?><project name="common" ><target name="test"> <xmlproperty file="test.xml" keepRoot="false" semanticAttributes="true"/> <echo>par1a=${par1a}par1b=${par1b}par2a=${par2a}par2b=${par2b} </echo></target></project>	4.0	id=43925	13	False	True	markt	1
id=29997	REOPENED	None	Ant	Core tasks (	1.7.0	Other other	P3 enhancement	Ant Notifications List	2004-07-09 10:27 UTC by	Jose Alberto Fernandez	2008-02-22 12:18 UTC (	1 user	The documentation for the <record/> task states:There is some functionality that I would like to be able to add in the future. They include things like the following:Attribute: Description (Required)listener: A classname of a build listener to use from this point on instead of the default listener. (no)includetarget: A comma-separated list of targets to automatically record. If this value is "all", then all targets are recorded. ([Default = all] no) excludetarget: (no) includetask: A comma-separated list of task to automatically record or not. This could be difficult as it could conflict with the includetarget/excludetarget. (e.g.: includetarget="compile" exlcudetask="javac", what should happen?) (no) excludetask: (no) action: add greater flexibility to the action attribute. Things like close to close the print stream. (no) I just want to make sure we get to do this things sometime soon.	*** This bug has been marked as a duplicate of***I'll switch the duplicates around since this one is more general.***has been marked as a duplicate of this bug. ***	3.0	id=25998	4	False	False	peter.reilly	1
id=34633	REOPENED	None	Ant	Optional Tasks (	1.7.0	All All	P3 normal	Ant Notifications List	2005-04-26 20:20 UTC by	None	2013-02-14 16:16 UTC (	2 users	Recently I encountered an specific laptop on which the replaceregexp task intermittently fails with errors like: - Couldn't rename temporary file C:\<builddir>\rep1253479585.tmp - Couldn't rename temporary file C:\<builddir>\replace690408058.txtI see that this error message is generated by line 431 of the ReplaceRegExp task. The task creates a temp file, manipulates it, then overwrites the original file with the temp file. The "Couldn't rename temp file" error signifies a failure to mv the tempfile to the original. The user is running as root, so it's not a permissions issue. And replaceregexp works most of the time, but certain calls always seem to crash with this error. So a single target might call replaceregexp successfully four or five times, then error out. Googling the error, I found one other person who has encountered it and was also stumped:%40ant.apache.org/msg01075.html+Ant+%22Couldn%27t+rename+temporary+file%22&hl=enSearching through ASF Bugzilla, I could not find any bug that appears to be related.	I have the same problem with Apache Ant version 1.6.1 compiled on February 12 2004.Ant is executed via EclipseIm Running Windows XP with service pack 2.I also encountered this effect when running ant 1.6.5 from console.Forgot to mention the java version, using j2sdk1.4.2_04i have updated to j2sdk1.4.2_08 and disabled indexing for fast search, maybe update to 1.4.2_08 is enough, all run since then proceeded without errors.(In reply to)i was too fast, its still there occassionally.It seems that if this behaviour occured once it occures on every run thereafter.It seems to help if you delete all temporary files.There's a possibility that this is a file naming bug related to long vs. shortfile names in Windows. The error I got was: An error occurred processing file:'C:\dev\LongFilenameExample\build\text\Example.java.html':C:\dev\LongFilenameExample\build.xml:138: Couldn't rename temporary fileC:\DOCUME~1\mcrocker\LOCALS~1\Temp\replace1599386392.txtNotice that the source file and build file names use the full file names, butthe temporary file uses the tilde vFAT naming style.This error occurs on Windows XP Pro 2002 SP2, with ANT 1.6.5 under Java 1.4.2_04 and 1.5.0_02 from the command line or inside of Eclipse 3.1.I'm also experiencing this problem with Ant 1.7.0 and Java 1.5.0_11 underWindows XP SP2Marking as windows xp, but it applies to all windows versions.I think I have encountered it under Linux. I am using Fedora 7, ant (1.6.5) isrunning under the hood while using maven (2.0.7) with appfuse project. My stacktrace is:[INFO] ------------------------------------------------------------------------[ERROR] FATAL ERROR[INFO] ------------------------------------------------------------------------[INFO] Couldn't rename temporary file /tmp/replace688779408.txt[INFO] ------------------------------------------------------------------------[INFO] TraceCouldn't rename temporary file /tmp/replace688779408.txt atorg.apache.tools.ant.taskdefs.optional.ReplaceRegExp.doReplace(ReplaceRegExp.java:431) atorg.apache.tools.ant.taskdefs.optional.ReplaceRegExp.execute(ReplaceRegExp.java:491) atorg.appfuse.mojo.installer.InstallSourceMojo.removeWarpathPlugin(InstallSourceMojo.java:609) atorg.appfuse.mojo.installer.InstallSourceMojo.execute(InstallSourceMojo.java:207) atorg.apache.maven.plugin.DefaultPluginManager.executeMojo(DefaultPluginManager.java:443) atorg.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(DefaultLifecycleExecutor.java:539) atorg.apache.maven.lifecycle.DefaultLifecycleExecutor.executeStandaloneGoal(DefaultLifecycleExecutor.java:493) atorg.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(DefaultLifecycleExecutor.java:463) atorg.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHandleFailures(DefaultLifecycleExecutor.java:311) atorg.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegments(DefaultLifecycleExecutor.java:278) atorg.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLifecycleExecutor.java:143) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:334) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:125) at org.apache.maven.cli.MavenCli.main(MavenCli.java:280)It seems I am not alone with this issue:I can now confirm that this a serious Java bug. I discovered this by doing the following:I downloaded the ant source and I discovered that the original file is copied toa temp file and then that temp file is renamed to the original file. So inshort, the original file is copied, edited and then overriden. So I decided todo some experimenting. I moved my source code from 'My Documents' directorywhich is located under the Document and Settings directory and moved it intoc:\Project. Now the replaceregexp task works.Apparently Java doesnt know to handle files like this:C:\Documents and Settings\User\Mydocuments\Projects\MyApp\projects\dao\src\conf\hibernate.propertiesI tried the following:if (!new File("C:\\Documents and Settings\\User\\Mydocuments\\Projects\\MyApp\\projects\\dao\\src\\conf\\hibernate.properties").delete()) System.out.println( "Could not delete" );else System.out.println( "Could delete" );As output I get the message 'Could delete' and the file is deleted. This makesme beleive that there is something totally messed up in Java. Deleteing a filein one instance doesnt work but manually creating a File object pointing to thatfile and then deleting it will work.This isnt a Ant issue but a Java issue.I would suggest that it may be a windows issue.Things like this can happen with virus checkers. If a viruschecker is configured to check a file when a file is created/read,and the file is short lived - for example a temp file createdwith there replaceregexp, there exists a race condition where thefile cannot be deleted (while the virus checker is checking the file).[This is a windows "feature" apparently added with WindowsNT - previouslyone could delete files while other "processes" were reading them - corruptingthe file system]Sometimes a virus checker is configured to only do "heavy" checking oncertain directories - (My documents for example) so you may be seeing thisfor some directories and not others.To test this, I disabled my virus scanner and retested it. Still the same error.So it isnt a virus scanner that is causing the problem. It was a good suggestionthough.(In reply to)This sounds like a problem I was experiencing with files in a differentsituation. Virus scanning software was also the expected culprit, but wasn't. Instead, I tracked it down to the TSVNCache.exe process of TortoiseSVN. I wasable to work around the problem by going to the Icon Overlays page of theTortoiseSVN settings and under "Status Cache" choosing "Shell" instead of "Default".(In reply to)This is not an issue with Windows long vs. short names. I ran into the same error you did in the Documents and Settings Temp area with Ant 1.7.0, Windows Server 2003, and Java 1.4.2_13 (don't ask). So, I switched the user's environment TEMP to point to C:\temp and got:BUILD FAILEDK:\O_build_stable\RSA_Build\Build\build.xml:268: The following error occurred while executing this line: K:\O_build_stable\RSA_Build\Build\subprojects\buildsummary.xml:350: Couldn't rename temporary file C:\Temp\replace8787748.txtCreatedImprove Error Message for ReplaceRegExp Rename FailureThe problem (at least the one that I encountered) is definitely not a Java bug or even a 'real' bug in Ant. Instead it is one of misdirection due to a bad error message. It turns out that everyone seems to be focused on the temporary file (e.g., C:\<builddir>\replace690408058.txt) when the problem is with the original file due to the way the Ant FileUtils rename works in order to work across file systems: 1 - delete original file if there 2 - copy updated file (temp file) to original file name and location 3 - delete updated file (temp file)The diff patch that I have attached to org/apache/tools/ant/taskdefs/optional/ReplaceRegExp.java will help sort out the bad error message confusion. In one of my tests the original output was: BUILD FAILED C:\AntTestDir\build.xml:32: Couldn't rename temporary file C:\DOCUME~1\eolsen\LOCALS~1\Temp\replace2143556994.txtThe new (correct) output is: BUILD FAILED C:\Documents and Settings\eolsen\My Documents\AntTestDir\build.xml:32: Failed to delete W:\test2.txt while trying to rename C:\DOCUME~1\eolsen\LOCALS~1\Temp\replace1613181649.txtThe real issue as you can now see from the improved error message was a failure to delete the original file (which was on a filesystem to which I only had read permissions).To misquote Matt Inger: "Read code, not error messages. Error messages can lie."Updating versions and systems since the patch applies to all platforms.One more note -- the real problem was that ReplaceRegExp.java gobbled up the good error message from org.apache.tools.ant.util.FileUtils.rename() instead of passing it on up the chain to the build failure where we could see it.the fix formay have improved the situation. For other cases (like when Ant simply cannot write to the file), svnwill now include the original error information (as suggested by Eric).I am still seeing this issue in Ant 1.8.1. Would there be a way to possibly handle the exception with a retry of the failed command (in this case looks like the original file delete). This seems to happen often enough in my windows build execution since I started using the ReplaceRegExp task. Once every 50 or so uses, which based on my CI build, fails every fourth or fifth build execution.See ant -v full stact trace below for the underlying cause in the ant FileUtilsBUILD FAILEDc:\hudson\workspace\main_windows\build.xml:298: The following error occurred while executing this line:c:\hudson\workspace\main_windows\build.xml:140: The following error occurred while executing this line:c:\hudson\workspace\main_windows\modules\build-module-common.xml:399: The following error occurred while executing this line:c:\hudson\workspace\main_windows\modules\build-module-common.xml:532: Couldn't rename temporary file C:\WINDOWS\TEMP\replace7543532359339722099.txt at org.apache.tools.ant.taskdefs.optional.ReplaceRegExp.doReplace(ReplaceRegExp.java:480) at org.apache.tools.ant.taskdefs.optional.ReplaceRegExp.execute(ReplaceRegExp.java:536) at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291) at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) at org.apache.tools.ant.Task.perform(Task.java:348) at org.apache.tools.ant.Target.execute(Target.java:390) at org.apache.tools.ant.Target.performTasks(Target.java:411)... (Extra stack trace clipped) ...Caused by: java.io.IOException: Failed to delete c:\hudson\workspace\main_windows\modules\dfm-cli\target\reports\javac\javac_stdout.txt while trying to rename C:\WINDOWS\Temp\replace7543532359339722099.txt at org.apache.tools.ant.util.FileUtils.rename(FileUtils.java:1239) at org.apache.tools.ant.taskdefs.optional.ReplaceRegExp.doReplace(ReplaceRegExp.java:474) ... 72 moreI know this is an old thread but I have it today. It is also not a Windows specific problem as I am running Mandriva 2010.2, ant 1.7.1 and I don't know what version of replaceregexp I have. This error seems to be like a rash on the internet with no one coming up with a solution or even a workaround. Some have said it goes way on its own but mine hasn't yet and I cannot find any way to get rid of it even temporarily. My project is at a dead standstill at this point. Will rebooting help any?(In reply to)Since replaceregexp is a built-in task it should be the same version as Ant itself.Before tyring to dig any deeper, you should give a more recent version of Ant a try (1.8.2 is current with 1.8.3 being close). Even ifsays it is about Windows, it also applies to other platforms in that a failed delete operation is retried once (on Windows there is an additional GC between the attempts to delete the file).1.8.2 is not yet a stable release for Mandriva. I am reluctant to try the cooker release if there is a workaround. I can survive for a while if there is some way to manually clear this problem even if temporarily.As for the version, I had lots of trouble getting replaceregexp since it kept telling me it was an ant-contrib optional function. I had to install it separately which is why I thought it might be a different version. Whatever I did, when I installed the Mandriva ant rpm, replaceregex was not included.(In reply to)Well, we don't have any influence on the RPMs (or any other distribution you don't download from one of the ASF's mirrors), I don't have any idea whether the Mandriva distribution has made any modifications.In Ant 1.7.x the replaceregexp task has been in a jar called ant-nodeps.jar, with Ant 1.8.x we've merged those into ant.jar.You should be able to download a binary release from Ant's download page, untar the tarball anywhere, set ANT_HOME to point to that location and start using it without messing with your OS installation.I'm not aware of any workaround if the problem is that Java is holding on to a lock to the file it has just written even after it has been closed.I took your advice and installed 1.8. No joy, same error. However, I wonder if I needed to do some kind of clean up.At least 1.8.x should tell you which file it failed to delete.I've always had that, from replaceregexp:Couldn't rename temporary file /tmp/replace5358820582224705130.txtHowever, I've just discovered what I think is an important piece of information and, as an aside, may be related to what I thought might be a different bug. I have been running this build via a perl script. What I have found is everything works as expected when the script is run from the command line. When it fails with this error, it is running in background which means no console (tty).The possible related bug is that I use the echo property in a couple of places. When I try to capture the ant output, the echos do not show up when run in background. They do from command line. I was able to work around that problem by using -logfile and reading that into my script but I shouldn't have had to do that.The bottom line is that ant behaves differently when run in background and/or with no console. It does not seem to matter if I use 'nohup'. Unless it is trying to write directly to a tty rather than STDOUT and STDERR, this behavior should not happen. I suppose it is possible that there is some environment variable missing that it needs but there is no indication in the error message if or what that might be.I'm also seeing this on a CentOS installation.I can check on sunday what version exactly the ant is and I think I can update it without a problem.This is a real problem for me.Any workarounds? (BTW, the file isn't held by java in my case)HiI'm too facing the same issue, I tried with ANT version ant_1.6.5 on RHEL 5.4.Any quick workaround can help me lot.ThanksRaghavI just faced the same problem, but got it fixed very easily.Turns out that apparently, it is related to the path/filename of the file to which the regexp replacement is being done.So, instead of trying to do the regexp directly to thefile (located in a path like C:\build\nightly\files\product_20121210_1600\file.txt), I just tried the following:1)copy the file to C:\Temp2)attempt the regexp to that file (C:\Temp\file.txt)3)copy the file from C:\Temp to C:\build\nightly\files\product_20121210_1600\file.txtand that's it. Worked like a charm.I was using ANT 1.7.1, then moved to 1.8.4 to see if it solved the problem (which didn't). However, I'm assuming it should work with 1.7.1 as well.In fact, by double checking, my previous comment is wrong. When I tried to copy back the file to the original path, I got an access denied error. And after verifying, indeed the original file was always having read-only permissions. That was my problem (not the file path/ file name).Check your file permissions, you might be facing the same thing I was (rookie's problem!)Another workaround is redefining temporary dir location. Сan't find some rule, but overriding the temporary dir to the drive where you run ant should help.ant -Djava.io.tmpdir=D:\temp ...	34.0	id=34633	16	False	False	heinz.van.pee	1
id=36653	REOPENED	None	Ant	Core tasks (	1.6.5	All All	P3 enhancement	Ant Notifications List	2005-09-14 12:10 UTC by	Jens Elkner	2012-04-18 07:00 UTC (	6 users	Actually, many people wanna use XIncludes in their projects (e.g. for docbookrelated stuff), and this requires at least for xalan2, that the System propertykey "org.apache.xerces.xni.parser.XMLParserConfiguration" is set to"org.apache.xerces.parsers.XIncludeParserConfiguration". Since the xslt task does not allow to set system properties temporarily, anotheroption is need - the invocation of ant with -D... is a bad workaround, sincethan all task would see/use this value. So one option would be, to allow a parser.config attribute, where one could setappropriate value.Even easier, if the processor is xalan2 aka trax, allow an attribute"xinclude={on|off}" and if on, just set the system property as described aboveand reset it to its original value, when the task is finished ...	***has been marked as a duplicate of this bug. ******has been marked as a duplicate of this bug. ***svnallows setting of arbitrary system properties during processing.<xslt ...> <sysproperty key="org.apache.xerces.xni.parser.XMLParserConfiguration" value="org.apache.xerces.parsers.XIncludeParserConfiguration" /><xslt>should have the desired effect.More details on the Saxon side of this can be found here:together withwould correspond to<xslt ...> <factory> <param name="" value="true"/> </factory></xslt>but the system property should also work.I don't think the task is lacking any features that would prevent you from usingXInclude with either Saxon or Xalan.replace param with attribute inI'm trying out the current Ant 1.8.0 nightly build with Saxon-B 9.1.0.3, but I still can't Xinclude to work. I've tried both the <factory> and the system property techniques, but it's simply ignoring my <xi:include> directives. Any suggestions?On second thought the system property approach may be failing because saxon is started in the same VM that runs Ant and ends up using the same Xerces which could be ignoring the system property because it had a different value when Ant started (it might evaluate the property only once).Depending on what Saxon does when the attribute is set, it may fail to enable XInclude for the same reason (Xerces started without XInclude and isn't willing to change).I'm not sure what we can do short of adding a fork mode to <xslt> which doesn't look like a trivial task at first glance.Actually, I think the problem is more serious than that. Xinclude still doesn't work even if you work around the issue inby setting the system property ahead of time.Using the current Ant trunk, I started out by setting ANT_OPTS="-Dorg.apache.xerces.xni.parser.XMLParserConfiguration=org.apache.xerces.parsers.XIncludeParserConfiguration". This has the desired effect of changing Xerces to use XIncludeParserConfiguration instead of the default XIncludeAwareParserConfiguration. I've confirmed this through debugging. XIncludeParserConfiguration is definitely being instantiated with this setting.Next, since I'm using Saxon 9.1, I added this nested element to my <xslt> task: <factory name="net.sf.saxon.TransformerFactoryImpl"> <attribute name="" value="true"/> </factory>This is where problems start. Adding the above factory attribute causes org.apache.xerces.xni.parser.XMLConfigurationException.I've done some debugging to find out why this is being thrown, and I can see that Saxon is definitely trying to configure the parser for Xinclude. The sendSAXSource method in its Sender.java is making this call: parser.setFeature("", true);But when this call propagates to Xerces, its ParserConfiguration.java looks in its list of recognized feature strings, doesn't find "", and throws XMLConfigurationException.Actually, this is to be expected because the current Ant trunk is bundled with the Xerces 2.9.0 parser, and that version doesn't know about "". It only knows about "". (Not sure why.)Saxon attempts to handle this gracefully, as shown in this snippet from its Sender.java: boolean tryAgain = false; try { // This feature name is supported in the version of Xerces bundled with JDK 1.5 parser.setFeature("", true); } catch (SAXNotRecognizedException err) { tryAgain = true; } catch (SAXNotSupportedException err) { tryAgain = true; } if (tryAgain) { try { // This feature name is supported in Xerces 2.9.0 parser.setFeature("", true); } catch (SAXNotRecognizedException err) { ... } catch (SAXNotSupportedException err) { ... } } Unfortunately, this failover technique does not work, apparently due to a Xerces problem. When Xerces encounters the unrecognized feature string in its ParserConfigurationSettings.checkFeature method, it throws its own proprietary org.apache.xerces.xni.parser.XMLConfigurationException. I don't know why it doesn't use the standard org.xml.sax.SAXNotRecognizedException or org.xml.sax.SAXNotSupportedException. It is either a bug in Saxon (because it catches the wrong exception) or a bug in Xerces (because it throws the wrong exception).In this case, though, it doesn't actually matter because even if Saxon specified "xinclude" instead of "xinclude-aware", it still wouldn't work. That's because the feature list variable in Xerces doesn't contain either one! When ParserConfigurationSettings.checkFeature is called, I see that "" and "" are there, but "" is not.So the root of this problem is that Xerces is not adding "" to its feature list. I even tried to add it explicitly in Ant's TraXLiaison.getSource methods: spFactory.setFeature("", true);or: spFactory.setXIncludeAware(true);But they simply cause javax.xml.parsers.ParserConfigurationException to be thrown in org.apache.xerces.jaxp.SAXParserFactoryImpl.newSAXParser.So now I'm basically stuck, but I hope that the analysis above might help someone.CreatedResolve xi:include using xsltSince the XInclude standard is pretty simple, you can also just implement your own xi:include resolver using XSLT and XPath (see attachment).This is a very basic implementation: it only supports the xpointer attribute to resolve an element based on a match with the xml:id attribute.It is probably not as perfect as using the XInclude resolving methods of the xslt parsers (although i have found that those implementation also differ in regard to the xpointer, so personally I like to use the snippet for it gives me more flexibility in my projects).(In reply to)I agree that if it doesn't work that way then we are out of luck on the Ant side (but we should make it possible to get as close as we can). This means we should implement a fork-option in <xslt>, IMHO.It would be nice if you took your findings here and reported bugs against Xerces-J and Saxon respectively.	11.0	id=26286	8	False	False	jraymond	1
id=39108	REOPENED	None	Ant	Optional Tasks (	1.6.5	Other other	P3 normal	Ant Notifications List	2006-03-26 20:56 UTC by	henrik	2008-02-22 12:18 UTC (	1 user	The FTP.java class has two local methods used to check is a file entry is reallya file or a directory. The problem with the current implementation is it is nottaking into account that the entries can be unknown, and the current logicfurther assumes that isFile==!isDirectory.The two methods areisFunctioningAsFile andisFunctioningAsDirectoryI have tested the following changes to the functions: private boolean isFunctioningAsDirectory(FTPClient ftp, String dir, FTPFile file) { boolean result = false; String currentWorkingDir = null; if (file.isDirectory()) { return true; } else if (file.isFile()) { return false; } <b> else if (file.isUnknown()) { return false; }</b> ...} private boolean isFunctioningAsFile(FTPClient ftp, String dir, FTPFile file) { if (file.isDirectory()) { return false; } else if (file.isFile()) { return true; }<b> else if (file.isUnknown()) { return false; }</b> return !isFunctioningAsDirectory(ftp, dir, file); }Henrik	I think the original idea was that everything not functioning as a directory isassumed to be functioning as a file, as to be able to compare / download as muchas possible.Also, something which is of type unknown may still be functioning as file ordirectory, so to return false right away doesn't seem to be the right decisionfor those checks.For example fifo's are neither files nor directories, yet they will function asfile when read from / written to.Maybe unknown is also used for FTP servers that do not propagate any obviousdifference between file entries and directory entries.Therefore, it is not clear to me what would be the benefit of always havingisFunctioningAsDirectory and isFunctioningAsFile returning false if the type isunknown.but if that is the case what is the point of setting the type to unknown ?The type is probably set to unknown unless the commons FTP library is able todetermine the type by parsing the listing, which may not be the case (orimplemented) for all listings.My question still remain. And further, would you blindly trust a rmdir, let alone read or write on an filetype unknown ?Once you have determined if you can read or write to the file, why not just setthe type accordingly ?Frankly I do not follow you line of argument here.Sorry, but I do not understand the issue, what would be the benefit of nottrying to determine the behaviour of an unknown entry?Ant does not set the type to unknown.Commons does set the type to unknown when it can not -for whatever reason-determine what an entry is by inspecting the listing. Commons will not try todetermine if something unknown proves to be a file or a directory. Ant does try to determine whether it acts as a directory or a file. Ant does not try to determine this by inspecting the entry of a listing.For unix ftp listings commons will treat block and character devices as file,but will treat fifo's as unknown. Fifo's for instance will act like files.Ant does not change the entry it has received from the commons FTP library, I amnot familiar enough with the ftp library of commons to see if it could do anyharm to change this, but again, what would be the benefit?So please indicate what is wrong on a functional level.Hello Henrik,I did not look at your bug report before.Are you closing it as WONTFIX because you believe that there is no one in theAnt community who wants to process this bug report ?I think that your bug report makes a valid point. Actually, I am the one whowrote these methods isFunctioningAsFile and isFunctioningAsDirectory. Yes, I didnot have in mind directory entries which are neither files nor directories. Yesit might be a good idea to avoid doing anything with these directory entries FTPwise.I am reopening the report.Antoine(In reply to)yes, that is a pretty accurate description. But I understand you have a lot to do, and my solution works for me.yes.I had to patch Ant locally to avoid the problem when thecommons.net.MVSFTPEntryParser returns filetype unknown.thanks.Basically, if filetype is unknown both methods isFile and isDir should return false.	7.0	id=39108	4	False	False	henrik.sorensen	1
id=18813	REOPENED	None	Slide	WebDAV client (	Nightly	All All	P3 minor	Slide Developer List	2003-04-08 13:31 UTC by	Max Kellermann	2004-11-16 19:05 UTC (	1 user	org/apache/webdav/lib/methods/XMLResponseMethodBase.java: when a SAXExceptionoccurs in parseXMLResponse, it is re-thrown as IOException. IOException isignored by parseResponse (empty catch block). This results in aNullPointerException, because responseDocument is null.A trace of both exceptions which happened at my workstation:org.xml.sax.SAXParseException: Premature end of file. at org.apache.xerces.parsers.DOMParser.parse(Unknown Source) at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source) atorg.apache.webdav.lib.methods.XMLResponseMethodBase.parseXMLResponse(XMLResponseMethodBase.java:315) atorg.apache.webdav.lib.methods.XMLResponseMethodBase.parseResponse(XMLResponseMethodBase.java:288) atorg.apache.webdav.lib.methods.XMLResponseMethodBase.readResponseBody(XMLResponseMethodBase.java:211) atorg.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1717) atorg.apache.commons.httpclient.HttpMethodBase.processRequest(HttpMethodBase.java:2313) atorg.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:957) atorg.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:564) atorg.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:476) atorg.apache.webdav.lib.WebdavResource.propfindMethod(WebdavResource.java:3015) atorg.apache.webdav.lib.WebdavResource.propfindMethod(WebdavResource.java:2985) atorg.apache.webdav.lib.WebdavResource.setNamedProp(WebdavResource.java:869) atorg.apache.webdav.lib.WebdavResource.setBasicProperties(WebdavResource.java:818) atorg.apache.webdav.lib.WebdavResource.setProperties(WebdavResource.java:1691) at org.apache.webdav.lib.WebdavResource.setHttpURL(WebdavResource.java:1131)[trace from my own code removed]java.lang.NullPointerException atorg.apache.webdav.lib.methods.XMLResponseMethodBase.initResponseHashtable(XMLResponseMethodBase.java:355) atorg.apache.webdav.lib.methods.XMLResponseMethodBase.getResponseHashtable(XMLResponseMethodBase.java:340) atorg.apache.webdav.lib.methods.XMLResponseMethodBase.getResponses(XMLResponseMethodBase.java:172) atorg.apache.webdav.lib.WebdavResource.propfindMethod(WebdavResource.java:3029) atorg.apache.webdav.lib.WebdavResource.propfindMethod(WebdavResource.java:2985) atorg.apache.webdav.lib.WebdavResource.setNamedProp(WebdavResource.java:869) atorg.apache.webdav.lib.WebdavResource.setBasicProperties(WebdavResource.java:818) atorg.apache.webdav.lib.WebdavResource.setProperties(WebdavResource.java:1691) at org.apache.webdav.lib.WebdavResource.setHttpURL(WebdavResource.java:1131)[trace from my own code removed]	***has been marked as a duplicate of this bug. ****** This bug has been marked as a duplicate of***closed by mistake, sorry	3.0	id=29997	10	False	False	mbenson	1
id=27952	REOPENED	None	Slide	WebDAV client (	2.0	Other other	P3 enhancement	Slide Developer List	2004-03-25 17:45 UTC by	Robert Flaherty	2004-11-16 19:05 UTC (	0 users		This is no bug, right?I listed it as a bug because of this snippet from the top of the client docs:Slide includes a full featured command-line based WebDAV client, which allows full access to the Slide server using the WebDAV protocol. Although the client is part of the Slide project, it is not tied to the Slide server, and was proven to be interoperable with a wide variety of WebDAV servers from other vendors. The reason why it is part of Slide instead of being fully independent and developed separately is the need to manitain aFor example, if ACL support is added to the server, we will want to also have ACL support in the client to be able to manipulate the ACLs.Should address this in 2.1I just checked both the client and command line and both don't have support yet, but this is marked closed. Was closing this premature or was a fix not applied?The client library seems to have bind support, but the command line client doesnot reflect thatWebdavResource lacks any bindMethod(), unbindMethod() and rebindMethod() methods. Also, BindMethod, UnbindMethod and RebindMethod have no code in them for params etc..., just the method name override.So, you are right...The latest code from CVS Head, does have the correct Bind/Unbind/Rebind methods, as well as support for them in WebdavResource (though not in commandline).For Ref: There was a 'cvs commit' mail from Ingo Brunberg dated 07/02/04.Robert>> Are you referring to a different branch?I am assuming that release 2.1 will be built from the main branch.I'm not using any code in cvs, just the posted milestone build.	9.0	id=36653	14	False	False	robert.flaherty	1
id=31908	REOPENED	None	Slide	Security (	2.1	Other other	P3 normal	Slide Developer List	2004-10-27 00:56 UTC by	Waz	2004-11-16 19:05 UTC (	0 users	As noted in defect 31907 the getUri(String) calls is inappropriate for places where the returned Uri object will be used to make store calls because the token in the uri is null and doesn't contain an indicator as to whether the store calls should be enlisted for the current transaction. There are some places in hasPermission() in the SecurityImpl class where getUri(String) is used to make store calls. hasPermission() makes calls to the store using three different Uris it constructs using just getUri(String). So for those using the access control and a jdbc store these calls may result in a deadlock just as happened for defect 31907. hasPermission() needs to be revised to determine whether the slide token for the current transaction needs to be passed to the getUri() calls so that a valid Uri is returned that can make store calls within another transaction.	Patch appliedSorry, misunderstood that and thought it was fixed with the same patch as 31907.Reopening it...	2.0	id=27952	9	False	True	ozeigermann	1
id=3277	REOPENED	None	Xerces-J	Schema-Structures (	1.4.3	All All	P3 normal	Xerces-J Developers Mailing List	2001-08-27 04:22 UTC by	None	2004-11-16 19:05 UTC (	5 users	I tried to validate a small xml-file (931 bytes) against a set of xml schema files.The schema file, includes other schemas.The file size of all schemas (sum) is 324.330 bytes.If necessary, I can send them for reproduction of this error.1.4.3 Xerces-JRunning the sax.SAX2Count (from xercesSamples.jar) WinNT4.0, 256 MB RAMi got a StackOverflowError at org.apache.xerces.validators.common.Grammar.buildSyntaxTree(Grammar.java:856).The java option -Xss didn't help.I also reproduced this error on a Sun Ultra Sparc 400/Solaris 8, and PC/Win2k.On Sun/Solaris:java -Xss8192Kb -cp xercesSamples.jar:xerces.jar sax.SAX2Count -v xxx.xmlresulted in a java.lang.OutOfMemoryError.pete	Hi Pete. Could you check whether there is a large "maxOccurs" value in your schema file(s)? It's a known limitation that Xerces1 schema can't handle large occurrence values. But in most cases, you can use maxOccurs="unbounded" instead. Thx.The greatest maxOccurs value is 9999, the next biggest one is 999.I replaced both with maxOccurs="unbounded" as you suggested and itworks.Thx. for your help. Pete***has been marked as a duplicate of this bug. ***This is a known limitation of Xerces1. We are trying to introduce some mechanism in Xerces2 to handle large maxOccurs values.In the meanwhile, specifying maxOccurs="unbounded" would meet most users' need.***has been marked as a duplicate of this bug. ***Re-opened to ensure that someone will eventually look at it - in Xerces-1 or Xerces-2.***has been marked as a duplicate of this bug. ***	7.0	id=31705	4	False	False	luetzkendorf	1
id=3537	REOPENED	None	Xerces-J	Other (	1.4.3	PC All	P3 critical	Xerces-J Developers Mailing List	2001-09-10 14:12 UTC by	None	2005-03-20 17:06 UTC (	0 users	This happens since version 1.4.1 up to 1.4.31.4.0 works fine.Basically, the upper boundary of 4 is not being checked. I found this using sun's xsdlib, which uses your regex libraries, and works fine with whatever version it comes with, but Xerces-J (aforementioned versions) regex, has the problem. Below is the output of a test program the xsdlib comes with, note that {1,4} becomes {1,} in the error message:D:\xsdlib-20010424\src>java -classpath .;d:\xerces-1_4_3\xerces.jar;..\xsdlib.jar com.sun.tranquilo.datatype.CommandLineTesterXML Schema Part 2 command line tool-->base string-->add pattern \d{1,4}\.\d{2}-->test 12345.99valid value-->test .00invalid: the value does not match the regular expression pattern"[0-9?-??-??-??-??-??-??-??-??-??-??-??-??-??-?]{1,}\.[0-9?-??-??-??-??-??-??-??-??-??-??-??-??-??-?]{2}".-->	Mistake made by closing bugIt seems this bug was introduced by.5 of RegexParser.java.-- TAMURA KentCreatedThis patch fixes the bug.	3.0	id=18813	7	False	False	ib	1
id=40026	REOPENED	None	Apache httpd-2	Core (	2.2.9	All All	P4 normal	Apache HTTPD Bugs Mailing List	2006-07-12 07:43 UTC by	Sebastian Nohn	2016-09-11 17:21 UTC (	7 users	These work:Header always add X-Test-Header: SuccessHeader always unset Content-LengthThis does not work:Header always unset Server	This currently works as designed. In the proxy case we have the following situation:If no Server header is set (either because the backend does not set one orbecause you have unset it the Server header is set with the default value).In the non proxy case the Server header is always set to the predefined value.It cannot be changed.So I mark this as invalid. Feel free to reopen if you think that this is eithera documentation bug or an enhancement.BTW: Unsetting the Content-Length header is not really a smart idea as it breaksHTTP/1.1 connections.SIn this case, the design is broken. "Server" is not required by RFC 2616.The Content-Length header was unset for testing purposes (if unsetting headersdoes work).It has long been the policy of the httpd developers that Server cannot beomitted nor lied about using configuration. I don't think that is unreasonable. There are very very few good reasons to omit the Server header, and if youreally need to, you have the source code. The fact that Server is not requiredby the spec certainly doesn't mean that it is required to make it optional. Theheader is configurable via ServerTokens, but it can't be omitted.If you disagree with this policy, you should take it up on the dev@httpd mailinglist. This is not a bug or a design flaw.CreatedDisable Server HeaderThis disables it at all. Very crude but may help people searching for thisthrough the bugs database.CreatedMake sending the Server header configurable via httpd.confCreatedMake sending the Server header configurable via httpd.conf (Documentation)RFC 2616 says "Server implementors are encouraged to make this field aconfigurable option".Find the patch for making this configurable attached.Out of context quote. Try the whole paragraph:" Note: Revealing the specific software version of the server might allow the server machine to become more vulnerable to attacks against software that is known to contain security holes. Server implementors are encouraged to make this field a configurable option."The display of specific version information is already configurable.I didn't notice you reopened this. Thanks for the patches, which may indeed beuseful for people. But as I said, there is no bug here. If you wish to discussthe policy decision, dev@httpd is the correct place.(I personally think it is stupid to remove this field, but I wouldn't stronglyobject to your patch simply because there are so many silly people who requestit that it continually wastes developer time.)I don't see where my quote is of out context. Anyway, most people like to*change* the Server header. This is indeed silly. Removing it starts to makesense when you have to pay exorbitant high amounts of money for your traffic -for what reason - and have a high traffic site. In this case saving these 17Bytes can save you several hundred € a month.this is an enhancement request, not a bug report.CreatedPatch against trunk (438824), respects and contains patch proposed in <>CreatedPatch against trunk (438824), respects, completes and contains patch proposed in <>Apache documentation (v2.0-v.2.2) states that the "header unset Server" directive should work: "The header is modified just after the content handler and output filters are run, allowing outgoing headers to be modified."This is in contrast to what the documentation *used* to say (v.1.3): "The Header directives are processed just before the response is sent by its handler. These means that some headers that are added just before the response is sent cannot be unset or overridden. This includes headers such as 'Date' and 'Server'."This change in the documentation implies that someone intended the "header unset Server" directive to work. Either the directive should be made to work (preferably) OR the documentation should be changed. Otherwise, this is a bug, not a feature or "enhancement request".I personally consider this important since, according to the HTTP specification, the "Server" field is unnecessary and, given I'm planning to publish a large number of pages less than 2 KB (compressed), it could represent more than 1% of my outgoing traffic."Except in early mode, the Header directives are processed just before the response is sent to the network. These means that it is possible to set and/or override most headers, except for those headers added by the header filter."The document says not "all headers" but "most headers".I'm not sure what "the header filter" exactly means.I feel the document is somehow unkind."somehow" -> "a little"The project decided not to do this twice -- see "vote on concept of ServerTokens Off"Out of principal, server administrators should have 100% control over what comes out of their server as long as it conforms to the spec. What if "App: Notepad" or "App: Nano" was prepended to every text document touched by these applications? It would be absurd. Please reconsider this "feature".Please do not reopen the bug. WONTFIX is the proper status until the development community decides otherwise on the dev@httpd mailing list. Feel free to provide your input there. (Personally, I've lost count of the different ways I have patched different versions of httpd to remove the server header. :( )Ok so the "opinions" of not fixing this were from 2006 - surely 10 years later dev's have realised that it makes no sense for this limitation? Or are we still going to work against the few people who are still using this software?(In reply to Gunter Grodotzki from)Seeif you're motivated to see a change here. Ignoring it and posting more patronizing comments here is unlikely to help.	21.0	id=3537	4	False	False	luis.ochoa	1
id=31705	REOPENED	None	Slide	WebDAV client (	2.0	Other other	P3 normal	Slide Developer List	2004-10-13 14:59 UTC by	Bryan Carpenter	2005-02-23 05:35 UTC (	0 users	The lexer for the command line client rejects strings that containthe underscore character. This means we can't enter URLs that include"_", which I believe are legal according to RFC 1738.	inadvertently closed	1.0	id=31908	4	False	False	ozeigermann	1
id=48465	REOPENED	None	Apache httpd-2	mod_fcgid (	2.2.14	All All	P4 enhancement	Apache HTTPD Bugs Mailing List	2009-12-30 06:23 UTC by	Olaf van der Spek	2011-03-04 13:33 UTC (	0 users	Could you add support for TCP to connect to backends?	Doesn't mod_proxy_fcgi satisfy this feature request?Committed	2.0	id=5586	4	False	False	elena	1
id=5586	REOPENED	None	Xerces-J	Core (	1.4.4	All All	P3 normal	Xerces-J Developers Mailing List	2001-12-24 12:48 UTC by	Genady	2004-11-16 19:05 UTC (	0 users	The parsing of simple xml files is much slower relatively to older xml4jversion from ibm (e.g. 2.0.15). Even after turning off all features it is still almost twice slower thanxml4j 2.0.15.I checked only parsing (very) large files, not parsing many small files.	You don't give any specific details on what parser are you using: do you use DOM or SAX? Is it a validating parser? Do you use DTDs or XML Schemas?Since XML4J 2.0.15 we've added several enhancements to the parser, like W3C DOM L2 implementation, W3C XML Schema implementation.Thus, it is acceptable that the parser became slower.We are shifting our development efforts towards Xerces2, and we've stopped working on Xerces (1.4.4 is probably the last release).If you provide more additional information and patches to the code, we will gladly accept those.Thank you!Ok, few details -I'm using sax parser using the sax 2.0 framework,although i don't use any features specific to 2.0.I don't use validation.I have a dtd embedded into the file.I see the same performance both in 1.4.4 and in 2.0.0 beta3.Also any tips on making the parsing faster will be welcomed!(I already used those on the web).GenadyI'll also try to benchmark the parser and send you the results.GenadyGenady, given your requirements you should use Xerces2. In Xerces2 there are different parser configurations that include different components in the pipeline. By default, Xerces2b4 parsers are created with xerces.parsers.StandardParserCofiguration which includes: Scanner, DTDValidator, DTDScanner, NamespaceBinder. Validating parser must read DTD if it is present, even if you don't need validation. If you don't want external DTD to be read set-dtd to false [the internal subset will be always read]. If you have more about performance email to the xerces-j-dev list.	4.0	id=3277	9	False	False	sandygao	1
id=45494	REOPENED	None	APR	APR test (	HEAD	PC Solaris	P3 normal	Apache Portable Runtime bugs mailinglist	2008-07-28 14:22 UTC by	Mikhail T.	2012-03-26 21:25 UTC (	1 user	I built apr-1.3.2 fresh on Solaris-10. Using the following configure:env CC=cc CFLAGS="-xarch=v9b -fast" CXXFLAGS="-xarch=v9b -fast" ./configure --with-sendfile --with-devrandom --prefix=/apps/fooThe build completed cleanly, but two of the tests failed:[...]testsleep : SUCCESStestshm : SUCCESStestsock : FAILED 1 of 8testsockets : FAILED 1 of 7testsockopt : SUCCESSteststr : SUCCESSteststrnatcmp : SUCCESStesttable : SUCCESStesttemp : SUCCESStestthread : SUCCESStesttime : SUCCESStestud : SUCCESStestuser : SUCCESStestvsn : SUCCESSFailed Tests Total Fail Failed %===================================================testsock 8 1 12.50%testsockets 7 1 14.29%*** Error code 1Please, advise. Thanks!	I'm not a Solaris guy, but I kinda remember problems with these tests when no IPv6 interfaces were configured on the machine. Do you have IPv6 configured on this system?No, there are no IPv6 interfaces. Here is the output of "ifconfig -a":lo0: flags=2001000849<UP,LOOPBACK,RUNNING,MULTICAST,IPv4,VIRTUAL> mtu 8232 index 1 inet 127.0.0.1 netmask ff000000 ce0: flags=1000843<UP,BROADCAST,RUNNING,MULTICAST,IPv4> mtu 1500 index 2 inet 10.77.11.220 netmask ffffff00 broadcast 10.77.11.255Thats likely the reason why it fails. Create an ipv6 loopback interface and things should work:ifconfig lo0 inet6 plumbifconfig lo0 inet6 ::1 upIf you run "testall -v" in the test sub directory, you will get additional information concerning the failure reason.The failure of testsock should be fixed by now, seeHere the test case was not really valid.The failure of testsockets was reproducible here and fixed by Rüdiger's recommendation on how to configure an IPv6 interface with address ::1.Here are the two tests' verbose output:testsock : |Line 92: Problem getting http service (2): No such file or directory|Line 290: Cannot test if connect completes synchronouslyFAILED 1 of 8testsockets : /Line 131: Could not bind socket (126): Cannot assign requested address-Line 185: Condition is false, but expected trueFAILED 1 of 7Closing. The problem with http service was fixed and the failure related to IPv6 can be avoided by configuring an IPv6 interface on the system.With all due respect, having IPv6 configured should not be a requirement for the test's passing. In fact, having /any/ networking configured should not be a requirement.If IPv6 (or v4) is not set up, the test should detect that and skip quietly -- without a false negative.If the tests are known to give false alarms, a real alarm will be mistaken for a false one -- arp is the foundation of many software packages, it better be /squeaky/ clean :-)All right, fair enough. I changed the name of the bug to reflect what's still broken.Sorry, but re-closing.IF IPV6 networking is not installed, the tests /are/ clean.If IPv6 interfaces are not configured for an IPv6 machine, the testsrightfully fail. This is true because such a misconfigured system wouldnot perform these unit tests, and therefore (under your suggestion) mayprovide a false positive for success where the code may be faulty.Absence of IPv6 is NOT a misconfiguration... Neither is the lack of IPv4.The test's status in this case should be "SKIPPED". "FAILURE" is worse, as it raises a false alarm. A "SUCCESS" would be misleading (false positive), but not as bad.If adding the third state ("SKIPPED") requires changes to the entire test-harness -- please, rename/reclassify this bug.But the simple rule should be: "run `make check' after building". If you want people to do this, the tests should not raise false alarms, even if their coverage may sometimes be incomplete -- is it ever, really?Nobody will blame you for NOT testing IPv6 on a machine, that does not have it, but FAILING a test of a piece, that's not enabled on a system, is ridiculous.You are missing the point, you installed IPv6 on this machine. You did notconfigure an interface.Either; 1) install IPv4 layer only, and you have nothing more to do, or...2) install an IPv6 loopback device, to complete the installation of IPv6.If you build for IPv6, the ./testall cannot report success without validatingcertain exception conditions which vary from platform to platform, and as-yetundiscovered flaws on future platform/releases.If you want to have an apr which doesn't test or validate ipv6, you can do so; ./configure --disable-ipv6If you are concerned that IPv6 networking components are installed by Solaris 10without configuring a loopback endpoint, contact Sun.If you are concerned that we should pass these tests for IPv6 in spite of thismisconfiguration, it would stand to reason that we would have to give a pass toany misconfigured IPv4-only machine as well. The purpose of the test suite isto validate all enabled apr interfaces, so the first half of this bug reportwas 'FIXED', the second half (and current title) is INVALID.= you installed IPv6 on this machine. You did not configure an interface.I did not install anything on this machine -- it came from Sun preinstalled with the standard Solaris 10. Having a configured IPv6, when there is no use for it is a waste, so I agree with Sun, that it is useless to enabled it.The test should not have reported a failure. It should've SKIPPED.= ./configure --disable-ipv6Yeah, that's a work-around. Except I may wish to -- some day -- be able to use it with IPv6 too. Telling me, that a test was SKIPPED because IPv6 is not currently in use on the testing machine, is Ok.Telling me, the test FAILED is bogus...This is all patently obvious, I might add... Even IF the machine is misconfigured, the test should not "FAIL", because there is nothing wrong with the build.Still a problem... Solaris 10, building apr-1.4.6:testatomic : SUCCESStestdir : SUCCESStestdso : SUCCESStestdup : SUCCESStestenv : SUCCESStestfile : SUCCESStestfilecopy : SUCCESStestfileinfo : SUCCESStestflock : SUCCESStestfmt : SUCCESStestfnmatch : SUCCESStestargs : SUCCESStesthash : SUCCESStestipsub : SUCCESStestlock : SUCCESStestcond : SUCCESStestlfs : SUCCESStestmmap : SUCCESStestnames : SUCCESStestoc : SUCCESStestpath : SUCCESStestpipe : SUCCESStestpoll : SUCCESStestpools : SUCCESStestproc : SUCCESStestprocmutex : SUCCESStestrand : SUCCESStestsleep : SUCCESStestshm : SUCCESStestsock : SUCCESStestsockets : FAILED 1 of 7testsockopt : SUCCESSteststr : SUCCESSteststrnatcmp : SUCCESStesttable : SUCCESStesttemp : SUCCESStestthread : SUCCESStesttime : SUCCESStestud : SUCCESStestuser : SUCCESStestvsn : SUCCESSFailed Tests Total Fail Failed %===================================================testsockets 7 1 14.29%Rehashing this 4-year old argument, that the build-machine does not have an IPv6 interface should not cause the test to fail. The test's verdict may be "skipped", but it should not be a "fail".Using the ``--disable-ipv6'' switch wish configure may be a work-around for some, but not for all -- the machine(s), where the compiled binaries will be used may well have/need IPv6 interfaces.	13.0	id=40026	40	True	True	rpluem	1
id=49746	REOPENED	None	Apache httpd-2	mod_rewrite (	2.5-HEAD	All All	P4 enhancement	Apache HTTPD Bugs Mailing List	2010-08-12 15:07 UTC by	jhmartin@toger.us	2016-01-06 23:48 UTC (	0 users	I suggest adding a flag to mod_rewrite to optionally url-encode captured values. Something like:RewriteRule /(.*) /some/other/url=&target=[UE]where UE causes the (.*) value to be urlencoded before being placed into $1.This handles cases where you want to rewrite the request url into an argument of another url, and the requested url might have a multivalue query string in it.Example:becomes	This feature is already available.Look for the internal map for "escape".This does not appear to work for escaping query-string arguments, as the & sign is not being escaped:(5) map lookup OK: map=escape key=foo=bar&zed=zee&fib/fib -> val=foo=bar&zed=zee&fib/fibThis is withApache/2.2.15 (Unix) mod_ssl/2.2.15 OpenSSL/1.0.0 I'm usingRewriteMap escape int:escapeand${escape:%{QUERY_STRING}} in the rewrite rule.wget -O- ""Perhaps there is use in adding another map function with a more comprehensive translation list?Since it looks like the escape map really just calls ap_escape_uri, which is a macro for ap_escape_os_path; how about a htmlescape map that calls ap_escape_html?That's already done by the B flag.Ah didn't see that (was looking at 2.0). Thank you.I'm sorry, have to reopen again. Neither int:escape or [B] handle the case where the original request had a query string, and I now want to package up the entire original request and pass it in the query string.Example:/foo/bar?zed=zee&ivy=true Should be come /some/other/url?target=/foo/bar%3Fzed=zee%26ivy=true, or something along those lines.With the current mechanisms, the & in the query string is not escaped by either int:escape or [B] (as there is no way to backreference query_string), meaning the servlet thinks everything after the first name/value pair is a separate argument and not part of target.RewriteMap escape int:escapeRewriteRule /(.*) /some/url?resource=fibble&target=}} [B,PT,L](2) init rewrite engine with requested uri /foo/bar(3) applying pattern '/(.*)' to uri '/foo/bar'(5) escaping backreference 'foo/bar' to 'foo%2fbar'(5) map lookup OK: map=escape key=zed=zee&ivy=true -> val=zed=zee&ivy=true(2) rewrite '/foo/bar' -> '/some/url?resource=fibble&target='(3) split uri=/some/url?resource=fibble&target=-> uri=/some/url, args=resource=fibble&target=(2) forcing '/some/url' to get passed through to next API URI-to-filename handlerCreatedAdd urlencode function.Patch against 2.2.31 that adds urlencode function from apr. Sample usage:RewriteMap urlencode int:urlencodeRewriteRule /(.*) /some/url?resource=fibble&target=}} [B,PT,L]	7.0	id=48465	5	True	False	wrowe	1
id=56758	REOPENED	None	APR	APR (	HEAD	Other Linux	P5 normal	Apache Portable Runtime bugs mailinglist	2014-07-22 12:45 UTC by	Ritesh Prajapati	2014-07-23 12:37 UTC (	0 users	I have one Custom Linux board on which I want to run apache web server (httpd) to test HTML and other web based pages.I have configured, compiled and installed httpd 2.4.4 as well as 2.4.9 on my linux custom board. When I checked httpd.pid file in logs folder of apache directory, it contains invalid PID which shows 19 Digit long PID (Ex:- 2983363998394615403).I have also tried to change httpd.pid file path into some another location but still causes same issue.Please find following output of ps and httpd.pid file for more reference. 1643 root 10936 S /usr/local/apache2/bin/httpd -k start 1644 daemon 16812 S /usr/local/apache2/bin/httpd -k start 1645 daemon 16812 S /usr/local/apache2/bin/httpd -k start 1646 daemon 16812 S /usr/local/apache2/bin/httpd -k start 1730 daemon 16820 S /usr/local/apache2/bin/httpd -k start$ cat httpd.pid 2983363998394615403Please let me know if any one has idea about this issue.	I have also started httpd process using strace utility and found that process itself writing wrong PID into httpd.pid file.Please find following logs for more information../strace -o /usr/local/strace.out -f ./apache2/bin/apachectl start3033 stat64("/usr/local/apache2/logs/httpd.pid", 0x7ba1e9b8) = -1 ENOENT (No such file or directory)3033 open("/usr/local/apache2/logs/httpd.pid", O_WRONLY|O_CREAT|O_TRUNC|O_CLOEXEC, 0644) = 7 3033 write(7, "2983363998394616793\n", 20) = 20 ------> This shows grabage PID written in httpd.pid file 3033 close(7) = 0So, Above 19 digit ("2983363998394616793") wrong PID is written into httpd.pid file.Please let me know if any one has idea about this issue.I have debugged code of APR and found that size of pid_t becomes 8 bit instead of 4 bit from configuration of APR.I looked into configuration file of APR and found that ac_cv_sizeof_pid_t becomes 8 bit if you are cross compiling APR.if test "$cross_compiling" = yes; then : ac_cv_sizeof_pid_t=8so, that size of PID becomes 64 and store Process ID of httpd as garbage in to httpd.pid file.I have set this option as 4 (32 Bit as per my board configuration) like ac_cv_sizeof_pid_t=4 and configured, compiled and installed that APR Package with httpd process.Checked httpd.pid file created valid and contains proper PID of httpd root process after starting httpd process.So, Problem is solved from above configuration.I'm moving this to APR as a doc issue. We need to have a place to collect hints for those doing cross compilation.	3.0	id=58052	10	True	True	vvo	1
id=56727	REOPENED	None	APR	APR (	HEAD	Other Linux	P5 critical	Apache Portable Runtime bugs mailinglist	2014-07-16 11:00 UTC by	Ritesh Prajapati	2014-07-23 11:57 UTC (	1 user	I have one Custom Linux board on which I want to run apache web server (httpd) to test HTML and other web based pages.I have configured, cross compiled and installed httpd (2.2.24, 2.4.1, 2.4.4 and 2.4.9 packages) on my Linux PC (Ubuntu 12.04 LTS) as well ason my own custom Linux board. Then I have added support of SSL Module (mod_ssl) to test HTTP as well as HTTPS request.Both HTTP and HTTPS request works fine without any issue on my Linux PC (Ubuntu 12.04 LTS). But when I tried to execute same HTTP requeston my Linux Board using httpd (2.4.4 and 2.4.9 with SSL Module Enabled) at that time browser page goes into loading state and can not be came out from that situation.Also I have seen that HTTPS request works fine at that time.I have also did some debugging task through wire-shark tool and found that connection is established successfully after sending request through HTTPbut can not get response of that request. I have also found that response of that HTTP request received on wire-shark after closing that HTTPrequested page from browser.Also, I can run HTTP and HTTPS requests successfully using httpd (2.2.24 and 2.2.27 with SSL Module enabled) on my Linux Board as well but failed to execute same requestusing httpd (2.4.X with SSL Module enabled) package.I have also changed some configurations by creating different virtual host for HTTP (Port 80) and HTTPS (Port 443) but still failed toexecute that HTTP request.I have also tried to listen on different ports like (Listen 80 and Listen 8000) without SSL module (using httpd 2.4.4. and 2.4.9 ) at thattime HTTP request goes into loading state.Does anyone has idea about this issue or help me to solve this type of issue?	I have also added following flags in configuration script of httpd to solve that issue but still failed to execute HTTP request.ac_cv_file__dev_zero=yes ac_cv_func_setpgrp_void=yesapr_cv_process_shared_works=yes apr_cv_mutex_robust_shared=noapr_cv_tcp_nodelay_with_cork=yes apr_cv_mutex_recursive=yesap_cv_void_ptr_lt_long=no ac_cv_sizeof_struct_iovec=8 ac_cv_struct_rlimit=yes \ac_cv_o_nonblock_inherited=noI have also debug httpd process using starce and found that read system call goe into blocking state which never returns.$ ./strace -p 1811Process 1811 attachedrestart_syscall(<... resuming interrupted call ...>) = 0poll([{fd=6, events=POLLIN}, {fd=4, events=POLLIN}], 2, 10000) = 0 (Timeout)poll([{fd=6, events=POLLIN}, {fd=4, events=POLLIN}], 2, 10000) = 0 (Timeout)poll([{fd=6, events=POLLIN}, {fd=4, events=POLLIN}], 2, 10000) = 1 ([{fd=4, revents=POLLIN}])accept(4, {sa_family=AF_INET6, sin6_port=htons(40827), inet_pton(AF_INET6, "::ffff:192.168.0.45", &sin6_addr), sin6_flowinfo=0, sin6_scope_id=0}, [28]) = 11fcntl64(11, F_GETFD) = 0fcntl64(11, F_SETFD, FD_CLOEXEC) = 0semop(950296, {{0, 1, SEM_UNDO}}, 1) = 0gettimeofday({1405747420, 372062}, NULL) = 0getsockname(11, {sa_family=AF_INET6, sin6_port=htons(80), inet_pton(AF_INET6, "::ffff:192.168.0.183", &sin6_addr), sin6_flowinfo=0, sin6_scope_id=0}, [28]) = 0gettimeofday({1405747420, 374162}, NULL) = 0gettimeofday({1405747420, 374976}, NULL) = 0read(11, "GET / HTTP/1.1\r\nHost: 192.168.0."..., 8000) = 289gettimeofday({1405747420, 377850}, NULL) = 0gettimeofday({1405747420, 380080}, NULL) = 0gettimeofday({1405747420, 382529}, NULL) = 0gettimeofday({1405747420, 384638}, NULL) = 0gettimeofday({1405747420, 386798}, NULL) = 0gettimeofday({1405747420, 388903}, NULL) = 0gettimeofday({1405747420, 390971}, NULL) = 0gettimeofday({1405747420, 393006}, NULL) = 0gettimeofday({1405747420, 395390}, NULL) = 0gettimeofday({1405747420, 397474}, NULL) = 0stat64("/usr/local/apache2/htdocs/", {st_mode=S_IFDIR|0777, st_size=4096, ...}) = 0stat64("/usr/local/apache2/htdocs/index.html", {st_mode=S_IFREG|0777, st_size=45, ...}) = 0open("/usr/local/apache2/htdocs/index.html", O_RDONLY|O_CLOEXEC) = 12open("/etc/localtime", O_RDONLY) = 13fstat64(13, {st_mode=S_IFREG|0644, st_size=127, ...}) = 0fstat64(13, {st_mode=S_IFREG|0644, st_size=127, ...}) = 0old_mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x2a01d000read(13, "TZif2\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\1\0\0\0\1\0\0\0\0"..., 4096) = 127_llseek(13, -11, [116], SEEK_CUR) = 0read(13, "\n<GMT-8>-8\n", 4096) = 11close(13) = 0munmap(0x2a01d000, 4096) = 0gettimeofday({1405747420, 441452}, NULL) = 0read(11, Please let me know if anyone has idea about this issue or any work arounds for that.This bug appears to be related to.I have already tried with all possible compilation flags as well as some other configurations in httpd compilation process to send HTTP request on multiple ports listen directive but still failed to send HTTP request.Then I have started debugging task from httpd source code and found that request is stuck in check_pipeline() function of http_request.c file.There is one function call "ap_get_brigade(c->input_filters, bb, AP_MODE_SPECULATIVE,APR_NONBLOCK_READ, 1)" in check_pipeline() function which never returns after sending HTTP request with multiple ports Listen directive.ap_get_brigade() function defined in server/util_filter.c fileAP_DECLARE(apr_status_t) ap_get_brigade(ap_filter_t *next, apr_bucket_brigade *bb, ap_input_mode_t mode, apr_read_type_e block, apr_off_t readbytes){ if (next) { return next->frec->filter_func.in_func(next, bb, mode, block, readbytes); } return AP_NOBODY_READ;}So, This above function call never returns from check_pipeline() function call after sending HTTP request on multiple port Listen.Please let me know if any one has idea about this problem.I have debugged ap_get_brigade() function in details from httpd source code and found that it executes ap_core_input_filter() function from core_filters.c file.Then, I have looked into ap_core_input_filter() function definition and found that HTTP request execution with multiple listen ports stuck after calling apr_input_read() function which is called from ap_core_input_filter() API.After that, I have looked for apr_read_bucket() function definition which is defined as read() system call in apr_buckets.h file of Apr-Util Package.#define apr_bucket_read(e,str,len,block) (e)->type->read(e, str, len, block)I have also downgraded and tested Apr and Apr-Util packages from 1.5.X to 1.4.X but still failed to execute HTTP request on multiple listen ports.Please let me know if any one knows about this issue.I have also found while debugging apr_bucket_read() function that read() system call executed from apr_socket_recv() function (network_io/unix/sendrecv.c file of Apr Package) blocks HTTP request after listening on multiple ports. apr_status_t apr_socket_recv(apr_socket_t *sock, char *buf, apr_size_t *len){ apr_ssize_t rv; apr_status_t arv; if (sock->options & APR_INCOMPLETE_READ) { sock->options &= ~APR_INCOMPLETE_READ; goto do_select; } do { rv = read(sock->socketdes, buf, (*len)); ---> Never return from Here } while (rv == -1 && errno == EINTR);That apr_socket_recv() function is called from socket_bucket_read() function of Apr-Util source package (buckets/apr_buckets_socket.c file).And that socket_bucket_read() function is called from ap_get_brigade() API (util_filters.c file) of http source package to read data from bucket.So, this issue is caused while reading data from bucket from httpd->Apr-Util->Apr when sending HTTP request on multiple Listen Ports.We have also checked NON_Block Mode using apr_is_option_set() function and it gives that APR mode is set as NON_Block Mode but still HTTP request never returns from that read() system call of apr_socket_recv() function.Please let me know if any one has idea about this.We have debugged code of APR and found that there is one macro defined #define APR_O_NONBLOCK_INHERITED 1which creates problem to create inherited socket as non block from listener socket while requesting HTTP page in multiple Listen Ports.So, We have set ac_cv_o_nonblock_inherited=no in configuration options of APR Package and tested HTTP request on multiple Listen ports as well as enabling SSL Modules which works fine without any issue.APR should have some hints for cross-compiling. Let's leave this open for now and assign to APR.	7.0	id=44181	6	False	False	slive	1
id=58052	REOPENED	None	Tomcat 8	Catalina (	8.0.21	PC All	P4 enhancement	Tomcat Developers Mailing List	2015-06-19 06:07 UTC by	Volker Voßkämper	2015-11-17 06:11 UTC (	1 user	Using rewite valve for example to rewite only some URL's to https does not work:RewriteCond %{REQUEST_URI} !^/some/exception/.*$RewriteCond %{REQUEST_URI} ^/.*$RewriteCond %{HTTPS} offRewriteRule ^/(.*)$}becauseis rewritten tohttps%3A//localhost%3A8443/The colon ":" is encoded to "%3A"	Using Java 1.8.0_31-b13Tomcat is configured with http and https connector (org.apache.coyote.http11.Http11Nio2Protocol)Unless you're using a redirect, this won't work. The mapper will only accept URLs relative to the server root, so fixing this does not make sense since this is not usable.Please use the user list for user questions instead.Rules like this are working with Apache httpd mod_rewrite and are subject to many howtos regarding https redirection.For exampleFollowing this description"The rewrite.config file contains a list of directives which closely resemble the directives used by mod_rewrite, in particular the central RewriteRule and RewriteCond directives."So I would expect this to work.The behavior corresponds to what is documented in the Tomcat documentation. However, it seems reading the mod_rewrite documentation that additional capabilities were added or officially documented to the RewriteRule directive.Tomcat's RewriteValve only supports the "-" and URL-path options for the substitution, while now mod_rewrite has external rewrite auto detect (useful, but all you need to do is manually add the "R" flag, so it's a very minor enhancement) and a file serving feature (that could be questionable for Servlet security). So this becomes a low priority enhancement that will either be implemented or the documentation be further clarified that unlike mod_rewrite it doesn't support file serving and/or auto external redirect.From the current mod_rewrite documentation:The Substitution of a rewrite rule is the string that replaces the original URL-path that was matched by Pattern. The Substitution may be a:file-system path Designates the location on the file-system of the resource to be delivered to the client. Substitutions are only treated as a file-system path when the rule is configured in server (virtualhost) context and the first component of the path in the substitution exists in the file-systemURL-path A DocumentRoot-relative path to the resource to be served. Note that mod_rewrite tries to guess whether you have specified a file-system path or a URL-path by checking to see if the first segment of the path exists at the root of the file-system. For example, if you specify a Substitution string of /www/file.html, then this will be treated as a URL-path unless a directory named www exists at the root or your file-system (or, in the case of using rewrites in a .htaccess file, relative to your document root), in which case it will be treated as a file-system path. If you wish other URL-mapping directives (such as Alias) to be applied to the resulting URL-path, use the [PT] flag as described below.Absolute URL If an absolute URL is specified, mod_rewrite checks to see whether the hostname matches the current host. If it does, the scheme and hostname are stripped out and the resulting path is treated as a URL-path. Otherwise, an external redirect is performed for the given URL. To force an external redirect back to the current host, see the [R] flag below.- (dash) A dash indicates that no substitution should be performed (the existing path is passed through untouched). This is used when a flag (see below) needs to be applied without changing the path.I have documented explicitly the difference with the current mod_rewrite in that area.	5.0	id=56727	14	True	True	ritesh.prajapati	1
id=18482	REOPENED	None	BCEL - Now in Jira	Main (	unspecified	All All	P5 enhancement	issues@commons.apache.org	2003-03-28 21:47 UTC by	Mark Crocker	2006-03-24 19:37 UTC (	1 user	While using the JustIce Verifier that is built into BCEL to investigate someissues with the Purifier project (a pure Java preverifier. See), I noticed that theinternal state of JustIce occasionally disagreed with the StackMaps produced bySun's preverifier.The problem seems to be that JustIce considers Exception handlers to be possiblesuccessors for EVERY instruction in a try block that result in merge changeswhen the outgoing frame is merged with the incoming frame of the firstinstruction in the Exception handler.I believe that this is an overly broad interpretation of the specification. Only instructions that can actually throw the type (or subtype) of Exceptionthat a handler is designed to catch should be considered to have possible asuccessor of the handler.An excruciatingly detailed report with source code, bytecode and Data FlowAnalysis can be found at:The solution would be to check if an instruction can throw the type of Exceptionthat the handler can catch BEFORE checking to see if a merge causes a change. This would probably be a fairly involved task.	Yes, that might be a tough task that is prone to errors, and will probablynot really speed up verification. However, if you'd do the coding and submita patch, I'll read through it and would be very happy to add it.	1.0	id=56758	11	True	True	ritesh.prajapati	1
id=52477	REOPENED	None	Fop - Now in Jira	pdf (	all	PC All	P5 enhancement	None	2012-01-17 16:03 UTC by	quamis	2012-01-18 12:11 UTC (	0 users	After having some problems with ghostscript while trying to concatenate PDF files generated by FOP, we went to the conclusion that FOP generates the embedded fonts prefix by always using the same sequence.@seefor the initial bug reportAccording to Ken Sharp from ghostscript, the embedded font should have an unique name, non-repeatable across multiple generations. I couldn't find this in the PDF specs, but i kinda lost myself trying to find anything in there, so this is not really relevant.Basically, it seems that FOP always generates the embedded font prefix by using EAAAAA, EAAAAB, EAAAAC etc sequentially when it should generate unique prefixes.Because it generates the same prefix, gs(and the PDF viewer) cannot display the required fonts. I cannot contribute a patch as i have 0 knowledge of java, but i think that the prefix should be based on the current timestamp+the current index(easiest), or be based on the currently embedded font glyphs, this should be more accurate, but any method will do for now. It should be able to disable this through the command line to allow automatic unit-tests that tests binary files to not fail because of always having something different in otherwise identical files.I have font-embeding enabled, according to, and it only embeds used glyphs. Same thing happens though if i embed the whole font (using encoding-mode).I have located the culprit in java\org\apache\fop\pdf\PDFFactory.java in function createSubsetFontPrefix(), but as mentioned i'm unable to provide a patch.I have found this as a related issue.	Hi,This isn't a bug, the PDF specification doesn't mandate that the font prefixes are unique outside scope of the document. The only mandate is:"The tag consists of exactly six uppercase letters; the choice of letters is arbitrary, but different subsets in the same PDF file must have different tags."From Section 5.5.3 PDF v1.4 Reference.As such this isn't a bug. Sorry to be dismissive, but as you said in your post on the ghostscript bug report, making these "unique" doesn't solve the issue since there could likely be clashes since the prefix is only 6 chars.In my opinion, Ken Sharp is mistaken when he says "If the font has the same name and prefix then it is the same font", that isn't what the PDF specification says (though understandably that's how it could be interpreted). The spec only says that each subset has to be unique within the scope of a document, which is what FOP already does.Mehdi(In reply to)Yes, he might not be exactly wrong, but this doesn't mean that FOP shouldn't try to be as arbitrary as possible. The algorithm used in the code is completely predictable.(In reply to)</snip>There are 2 points to address there:1) We can't arbitrarily make changes to FOP in order for it to "better" (not even fully!!) support the client systems, in this case ghostscript. The bug is in ghostscript, it should know if it is using a new PDF and change any prefixes accordingly.2) We use the deterministic trait of the prefixes in our testing framework. The value of having a comprehensive test suite is far greater than making the code change for this scenario.I understand that none of the above particularly helps you, but we can't very well go changing FOP to accommodate nuanced bugs in ghostscript.Mehdi(In reply to)That why i was saying that a command-line switch to disable the "randomized" behavior should exist. The change seemed trivial enough.I understand that, but generating the same sequence over and over just seems to be a compromise for easier automated testing, not for an actual working&tested product.For now we'll go on by using pdftk, which seems to handle multiple fonts-same-name case correctly, but its too bad one would have to use 3 different applications all with their own quirks and bugs and usage patterns simply because the standard isn't very clear for a specific issue, and that issue could easily be fixed by any of the 2 applications involved in this chain...I agree that making the prefix unique will make it easier for applications that process the PDF to extract or de-duplicate font resources when merging multiple PDF Files. However, the suggestion in this bug report to make the prefix random based on time introduces problems for regression testing PDFs generated by FOP, not just for the FOP project itself, but users of FOP who wish to regression test their documents.We could change FOP to make the prefix dependent on the glyphs in the subset, but that would be a lot of work.An alternative approach that will also make it easier for applications to extract or de-duplicate font resources when merging multiple PDFs is to allow FOP to fully embed the font resources in the PDF, rather than creating a subset. I believe this is possible today for a limited use-case, by specifying encoding-mode="single-byte" on the font element within the fop.xconf file. I say "limited" because that only works if no characters outside the ASCII range are required.Luis Bernardo, a new contributor to the FOP project is working on a new feature embedding-mode="full" which will fully embed the font and this will work for character ranges outside ASCII. If the font is fully embedded it will allow applications to more readily de-duplicate font resources when merging FOP generated PDF.I've re-opend this bugzilla, but as an enhancement request rather than a bug. As Mehdi stated, this isn't a bug, but rather a convenience feature.(In reply to)</snip>That wouldn't necessarily fix the issue here. Fully embedding a font means that the pseudo-unique prefix isn't used, however this isn't necessarily a good thing. A parser like ghostscript, could and apparently does assume that if 2 fonts have the same name (prefix or not) that they are the same font. This is an assumption that I've made previously and has proved manifestly naive. Also, any implementation CANNOT clash within the same document. Using a glyph subset idea, there could be a scenario in which the 2 fonts with the same glyph subsets produce the same prefix.We have to be careful what we're supporting here. There is no standardised method to identify a font, since anyone can call any font by any name. I don't agree that making the prefix "more unique" (not sure there is a scale by which something can be measured unique, it's binary, it is or it isn't), would help here, because given time, inevitably you'll get a clash. Then what?The prefixes are 6 chars long, the guys at Adobe made no indication that they wanted it to be unique in a global sense, only within a document.(In reply to)But if 2 fonts have the same glyph subsets used within a document, then it wouldn't be necessary to include them twice, so no clashing would occur. I think that glyph subsets are a good idea, but i do realize that it would be more complex to implement.Because the prefix is 6 chars long, its inevitably that one would eventually get a clash, if he uses enough millions of different fonts within the same file. But this is an acceptable limitation.Yes, the Adobe guys probably meant that the prefix should be unique within the same file and it would be the pdf reader/writer's job to handle duplicate fonts coming from different fonts. It makes sense. This is why i think both fop and gs handle this particular case wrong, as they both assumed things about that prefix, and it seems that this assumptions are now proven wrong. gs in particular should warn about merging files with embedded fonts, either when merging, or at least in the manual, or a "known-issues" page.Since font files are versionned, how this will be handled when 2 subsets use the same glyphes of the same font, but in different version?subset reduction should take care of that.	8.0	id=52477	6	True	True	med1985	1
id=24334	REOPENED	None	BCEL - Now in Jira	Main (	unspecified	Other other	P5 enhancement	issues@commons.apache.org	2003-11-02 19:46 UTC by	Enver Haase	2006-03-24 19:37 UTC (	0 users	"The native verifier possibly allows the method tobe declared in some superclass or implemented interface, which the Java VirtualMachine Specification, Second Edition does not"..is complete nonsense since Gilad Bracha told me vmspec2 was meant to statesuperclasses ARE searched. They will maybe re-phrase it.Implementing the search could be an involved task, I'd like to leave thisone to somebody else.Sorry, I'm just too busy with other things.	 	0.0	id=59943	10	True	False	p.mouawad	1
id=44181	REOPENED	None	Apache httpd-2	mod_alias (	2.5-HEAD	Other other	P5 enhancement	Apache HTTPD Bugs Mailing List	2008-01-07 16:33 UTC by	Dan Jacobson	2016-02-24 18:00 UTC (	1 user	Just as bash has an unalias command, so should Apache!	Use case? The analogy with bash is not at all helpful, since the type ofprocessing is completely different. In most cases where you want to "undo" anAlias, you can simply add another Alias the remaps to the old location.#Counteract alias.conf interference:Alias /icons/ "/home/nordsburg/mailman/icons/"#Wish could just do#Unalias /icons/#but alas, I must hardwire the paths here as there is no Unalias.#Furthermore, alias.conf could have been nice and made a softer alias,#which would then tried my icons dir. if the file weren't found in#its, but maybe there is no such softer alias, for better or worse.Anyway, there should be a way to say "Unpollute my environment forthis item. Just make it behave like it would originally, whatever thatwas." Or maybe you have already thrown away that information. (Or maybeallowing Unalias in .htaccess would allow security problems.)Anyway, imagine in bash if one had to doalias rm=rm oralias rm=\\rm oralias rm=/bin/rm orwhatever. Yuck. And one day it is moved to /usr/bin and kaboom.Anyway, you should show how there already is a simpler way to achieveUnalias /norfelfeller/ orUnalias <*> [some way to say Unalias Everything].P.S., Unalias probably better than UnAlias.Also noteApache trunk (2.3-head) installations (nolonger?) pollutes your namespace with aliases. Thus a request for /icons/myicon.gif will search in <docroot>/icons/myicon.gif rather than <apacheroot>/icons/myicon.gifThis should take care of your problem (as specified in the site you link to)OK good. Also did hurtUpdating the bug as it is no longer an issue.This bug (namespace polluting) still exists in 2.3.6 (out-of-the-box configuration) when using httpd-autoindex.conf, even when disabling autoindex for a particular site:[Tue Aug 10 04:36:10.406428 2010] [notice] [pid 15584:tid 16384] SIGHUP received. Attempting to restartDigest: cleaning up shared memory[Tue Aug 10 04:36:11.244445 2010] [notice] [pid 15584:tid 16384] Digest: generating secret for digest authentication ...[Tue Aug 10 04:36:11.244642 2010] [notice] [pid 15584:tid 16384] Digest: done[Tue Aug 10 04:36:12.095625 2010] [notice] [pid 15584:tid 16384] Apache/2.3.6 (Unix) DAV/2 PHP/5.3.0 configured -- resuming normal operations[Tue Aug 10 04:36:12.095956 2010] [notice] [pid 15584:tid 16384] Command line: '/usr/bin/httpd'[Tue Aug 10 04:36:16.946550 2010] [error] [pid 27512:tid 32771] [client 192.168.1.66:1146] File does not exist: /usr/apache2/icons/pdf_s.png, referer:The file exists in <docroot>/icons/pdf_s.png but Apache tries to access it in <wwwroot>. If autoindex is disabled is commented out the icon in question is accessed without problems. It is also accessed if the Alias-line in httpd-autoindex.conf is commented out, but this yields broken icons for the FancyIndexing option.So, I'll still vote for an Unalias-command because atleast I want autoindex / FancyIndexing available on one site and disabled on another.I am dealing with one situation where an unAlias directive would be very helpful.I have a couple thousand auto-generated virtual hosts. They each include some combination of a few common config files. These files include Aliases.In a few cases, I wish to include everything in the config files, but I wish to unAlias the aliases so some particular virtual hosts serve everything from the docroot.An explicit realias ("Alias /foo /virthost/doc/root/foo") doesn't work (should it?), nor is it ideal. An unalias is better, because the virtual hosts on which I want to unalias paths are all including another common config file. I want to put "unAlias /foo" in that common file.CreatedPatch to allow "Alias /whatever/ !" to stop alias processing.This hasn't seen any activity in a few years, but it's actually very easy to do in 2.4 with a small patch (attached).The ProxyPass directive works very similar to Alias, except it maps a proxy rather than another directory. With that directive, syntax like "ProxyPass /mirror/foo/i !" can be used to stop proxy processing for a given path. That same syntax can be applied to alias processing. I.e. "Alias /whatever/ !" can be made to stop alias processing for a given path.For example, if you have 1000 vhosts, 999 of which need an alias, then you can create a config like this:Alias /my_alias/ /my/real/path/... 900 VirtualHost entries that need that alias ...<VirtualHost> ... Alias /my_alias/ !</VirtualHost>... 99 more VirtualHost entries that need that alias ...There are two spots in mod_alias that have to be touched for this to work, both in try_alias_list: one for regular aliases and one for regular expressions. In both cases, if an alias matches and it maps to "!" you just stop there and return that there was no match.	9.0	id=24334	7	True	True	ehaase	1
id=59943	REOPENED	None	JMeter	HTTP (	2.4	All All	P5 normal	JMeter issues mailing list	2016-08-04 12:52 UTC by	David Hubbard	2016-08-19 07:46 UTC (	1 user	CreatedSample HTML response fileHiI'm writing a PostProcessor in Java for JMeter - using HtmlParsingUtils.getDOM() to access the HTML response data.In this I'm trying to pick out the Form fields from a response so that - when JMeter submits the form back - I can process the manage setting some of the posted param data.My problem is the site I am trying to test returns some slightly wacky HTML, whereby several (showing one here) input field is included in a <table> element<input name="1"><input name="2" /> <table><tr><td><input name="3"></td><input name="4"></tr> </table>When fields get posted back by the browser they send them (form params) in-order (1+2+3+4) as expected.However when using HtmlParsingUtils.getDOM() the node tree for the <table> seems to get included only when the </table> is parsed.So the tree returned has 3 and 4 swapped(input 1}(input 2}(input 4}(table (tr (td (input 3} ) ))Is this a bug? I confess I'm not sure - it depends if the start tag <table> or end tag </table> are the trigger for adding, however, from my reading "input 4 is within the table node.In this I can only assume that "input" fields are handled specially in the parser (since in html they don't need to include a closing tag or "/>" ? What it means for me is that I order I post params from JMeter is affected and the app dosesn't like this. You might think that the app should handle params based on name even in wrong order, but the reality is that the input field names are not unique on the page - the app is using a framework with we can't change, I have attached an example html file which show this.	Hello,Thanks for report.You're using here internal APIs of JMeter. Besides, this part of the API uses an old library jtidy (which makes cleanup of html) which may explain your issue.I suggest you rather rely on jsoup or jodd-lagarto which are embedded in JMeter , so you can use them in your custom code.Regarding your report, it appears it could affect one Element in JMeter "HTML Link Parser" which I would advise not to use either because of limitations in terms of performance and distributed testing not working for it.If you solve your issue by following the above comments, it would be nice to ping us for feedback.Regards(In reply to Philippe Mouawad from)Phillepe hi Thanks for the feedback - I have successfully switched to jsoup, which handles my scenario. RegardsDavid(In reply to David Hubbard from)Thanks for your feedback.I will reopen the bug, as it affects a component of JMeter(In reply to Philippe Mouawad from)OkA couple of observations in passing: 1) as you are bundling jsoup would you also consider adding xsoup (for xpath support on top of jsoup) <> ?2) should HtmlParsingUtils be marked @Deprecated in code <>?ThanksDavid(In reply to David Hubbard from)Interesting, XPath performances in JMeter are not that great.Would you mind raising the subject on dev mailing list ?Same answer.	5.0	id=49746	9	True	False	poirier	1
id=46592%0D%0A	REOPENED	None	Log4j - Now in Jira	Appender (	1.2	PC Windows 2000	P5 trivial	log4j-dev	2009-01-23 05:28 UTC by	Peter Jelitsch	2010-05-07 07:17 UTC (	0 users	I just spent some time finding out, why i could not tell log4j to write log files in UTF-8. I got the error "Error initializing output writer." and "Unsupported encoding?" When debugging into it, i discovered, that it was a UTS fault, as the line in the property file said "log4j.appender.ESP.Encoding=UTF-8 " (note the space at the end). Therefore I'd like to suggest to introduce some string trimming in the method WriterAppend.createWriter(OutputStream). String enc=getEncoding().trim(); or so.	CreatedPatch adds trim() to encoding setterThis patch adds trimming of the encoding string to the setter method.I have added trimming to the setter of the encoding property.The trimming should already be done. Please see PropertySetter class:- snip Object convertArg(String val, Class type) { if(val == null) return null; String v = val.trim(); if (String.class.isAssignableFrom(type)) { return val;- snipWhy does this not work for you?I've just encountered this bug with log4j 1.2.15.	4.0	id=18482	11	True	True	ehaase	1
id=47541	REOPENED	None	Fop - Now in Jira	fo tree (	all	All All	P3 normal	None	2009-07-16 06:07 UTC by	Vincent Hennebert	2012-04-07 01:52 UTC (	0 users	CreatedSample FO file showing the problemSee attached file: block b4 inherits the space-before property from the parent block and overrides the .conditionality component to "retain". This results into a space before the first block, at the top of the first page. Normally block 4 should start the second page with a space before it. The bug doesn't occur if all the components of the space are specified on the block instead of being inherited.	CreatedThe true sample file that shows the problemNot a 100% sure, but I think this behavior has been discussed (way back when), and this is actually not a bug.By specifying the .conditionality component, you implicitly set the other property components to their initial value. Either all the components are inherited, or none.To achieve the desired result, it is wrong to rely on implicit inheritance. Instead, specify a value of 'inherit' for the .length component to avoid using the initial value (0pt).Just some additional notes explaining the behavior, that even though it seems unexpected, is actually OK.For compound properties, specifying only one of the components is equivalent to setting all the other components to their initial value. They are not initialized with the inherited value, since the components are NOT(!) separate properties. Implicit inheritance is always defined on the level of the complete property. If one of the components appears as a specified value, the entire property is no longer inherited. If we want to inherit some of the components and override others, we need to use explicit values of 'inherit'. (Seems to be precisely one of the prime use-cases for that keyword)IOW:<block space-before="2em"> <block space-before.conditionality="retain">is semantically equivalent to<block space-before.optimum="2em" space-before.conditionality="discard" space-before.precedence="0"> <block space-before.optimum="0pt" space-before.conditionality="retain" space-before.precedence="0">To achieve the expected result, the correct spec would be:<block space-before="2em"> <block space-before.optimum="inherit" space-before.conditionality="retain">Technically, in FOP (see also the Wiki about property resolution) specified values are always processed first and added to the PropertyList. Later, when binding the PropertyList to the FObj, if there was no specified value for a given applicable property, inheritance will always be tried before reverting to the initial value.When processing the specified properties, and encountering a specified component, we first check if the compound property already exists on the PropertyList. In this context, we don't try inheritance, as we're actually looking for a base property that possibly resulted from other components that were processed before (= appeared earlier in the Attributes). If it does not exist, then we generate that base property, also bypassing inheritance, since we're creating an instance for a specified value.I think this is completely in line with the Recommendation (5.1.4), which hints at the inherited value actually being a fallback for an absent specified value (not a replacement for the initial value; note the order of the resolution rules in 5.1.1).That doesn't justify the space before the first block on the first page. It clearly looks like some object is directly passed to the child FO, instead of a copy of it. Setting the conditionality to "retain" on block 4 seems to have set it also to the surrounding block.Also:(In reply to)This is not what the following sentence from section 5.11 says: "Compound values of properties are inherited as a unit and not as individual components. After inheritance any complete form specification for a component is used to set its value." So non-specified components take the inherited value.Anyway, that doesn't apply to space-before since it's not an inheritable property.Which is what is done in the sample FO file. According to the following sentence in section 5.11: "Short forms may be used together with complete forms; the complete forms have precedence over the expansion of a short form." the result for block b4 should be the following one: space-before.minimum=10pt space-before.optimum=12pt space-before.maximum=50pt space-before.precedence=0 space-before.conditionality=retain<snip/>Like above, since space-before is not an inheritable property, section 5.1.4 doesn't apply here.Vincent(In reply to)Sorry, mixing things up with the inherited indent properties. Should have looked at the sample, of course...OK, now I see it. The SpaceProperty instance is re-used/shared between the blocks, due to the value of inherit, but when the additional component is specified on the fourth block, it also sets the component for all the other blocks. Specifying a value other than inherit, or specifying it for the individual components, generates a separate instance, so there are no side-effects. SpaceProperty is one type that is not cached yet, so even identical property specs will yield separate instances (unless inheritance of the base property is used, obviously).Not sure how easy it would be to fix this behavior, yet. Could be as simple as modifying the behavior in PropertyList.findBaseProperty() to conditionally duplicate the instance...resetting P2 open bugs to P3 pending further review	6.0	id=46592%0D%0A	4	False	False	sdiedrichsen	1
